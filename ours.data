I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.7.5 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:1e.0
Total memory: 11.17GiB
Free memory: 11.11GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0)
Created training directory cv
reading train
reading valid
reading test

actual longest token length is: 21
size of word vocabulary: 10000
size of char vocabulary: 51
number of tokens in train: 929589
number of tokens in valid: 73760
number of tokens in test: 82430
initialized all dataset readers
Created and initialized fresh model. Size: 14019865
     5: 0 [    5/ 1327], train_loss/perplexity = 7.95668507/2854.5944824 secs/batch = 0.2645s, grad.norm=10.12496281
    10: 0 [   10/ 1327], train_loss/perplexity = 15.96720123/8599385.0000000 secs/batch = 0.2648s, grad.norm=281.79187012
    15: 0 [   15/ 1327], train_loss/perplexity = 7.73785973/2293.5583496 secs/batch = 0.2664s, grad.norm=11.79969597
    20: 0 [   20/ 1327], train_loss/perplexity = 7.59385586/1985.9562988 secs/batch = 0.2647s, grad.norm=9.85819912
    25: 0 [   25/ 1327], train_loss/perplexity = 7.45497894/1728.4476318 secs/batch = 0.2655s, grad.norm=8.28738689
    30: 0 [   30/ 1327], train_loss/perplexity = 7.14322472/1265.5026855 secs/batch = 0.2649s, grad.norm=15.61296654
    35: 0 [   35/ 1327], train_loss/perplexity = 7.38891268/1617.9459229 secs/batch = 0.2651s, grad.norm=13.46753216
    40: 0 [   40/ 1327], train_loss/perplexity = 7.12273788/1239.8403320 secs/batch = 0.2659s, grad.norm=7.63151836
    45: 0 [   45/ 1327], train_loss/perplexity = 6.89349556/985.8414917 secs/batch = 0.2657s, grad.norm=6.61496592
    50: 0 [   50/ 1327], train_loss/perplexity = 7.03089952/1131.0476074 secs/batch = 0.2656s, grad.norm=8.62851143
    55: 0 [   55/ 1327], train_loss/perplexity = 7.23561144/1387.9892578 secs/batch = 0.2644s, grad.norm=7.66588068
    60: 0 [   60/ 1327], train_loss/perplexity = 7.06224966/1167.0677490 secs/batch = 0.2639s, grad.norm=6.07050657
    65: 0 [   65/ 1327], train_loss/perplexity = 6.96372032/1057.5606689 secs/batch = 0.2659s, grad.norm=9.68289757
    70: 0 [   70/ 1327], train_loss/perplexity = 7.02463484/1123.9840088 secs/batch = 0.2654s, grad.norm=23.57067680
    75: 0 [   75/ 1327], train_loss/perplexity = 6.82285690/918.6056519 secs/batch = 0.2654s, grad.norm=9.20359421
    80: 0 [   80/ 1327], train_loss/perplexity = 6.95333004/1046.6292725 secs/batch = 0.2661s, grad.norm=16.67199516
    85: 0 [   85/ 1327], train_loss/perplexity = 7.52412128/1852.1849365 secs/batch = 0.2660s, grad.norm=35.45649338
    90: 0 [   90/ 1327], train_loss/perplexity = 6.83877516/933.3452148 secs/batch = 0.2648s, grad.norm=4.53734112
    95: 0 [   95/ 1327], train_loss/perplexity = 6.86173534/955.0229492 secs/batch = 0.2638s, grad.norm=4.19577980
   100: 0 [  100/ 1327], train_loss/perplexity = 6.96671152/1060.7288818 secs/batch = 0.2650s, grad.norm=5.23812723
   105: 0 [  105/ 1327], train_loss/perplexity = 7.03761673/1138.6706543 secs/batch = 0.2660s, grad.norm=5.00012445
   110: 0 [  110/ 1327], train_loss/perplexity = 6.71138811/821.7104492 secs/batch = 0.2660s, grad.norm=5.71051884
   115: 0 [  115/ 1327], train_loss/perplexity = 6.71024752/820.7737427 secs/batch = 0.2657s, grad.norm=9.62715149
   120: 0 [  120/ 1327], train_loss/perplexity = 6.90264416/994.9019165 secs/batch = 0.2672s, grad.norm=4.71218681
   125: 0 [  125/ 1327], train_loss/perplexity = 6.89749050/989.7877197 secs/batch = 0.2661s, grad.norm=4.14779186
   130: 0 [  130/ 1327], train_loss/perplexity = 6.95316887/1046.4605713 secs/batch = 0.2652s, grad.norm=5.52457476
   135: 0 [  135/ 1327], train_loss/perplexity = 6.73902798/844.7392578 secs/batch = 0.2669s, grad.norm=5.98559141
   140: 0 [  140/ 1327], train_loss/perplexity = 7.05164051/1154.7515869 secs/batch = 0.2662s, grad.norm=4.45540714
   145: 0 [  145/ 1327], train_loss/perplexity = 6.95454979/1047.9066162 secs/batch = 0.2661s, grad.norm=6.60202026
   150: 0 [  150/ 1327], train_loss/perplexity = 6.97134447/1065.6545410 secs/batch = 0.2639s, grad.norm=12.79884720
   155: 0 [  155/ 1327], train_loss/perplexity = 6.95300770/1046.2918701 secs/batch = 0.2675s, grad.norm=5.06891060
   160: 0 [  160/ 1327], train_loss/perplexity = 6.50923300/671.3113403 secs/batch = 0.2651s, grad.norm=4.46150303
   165: 0 [  165/ 1327], train_loss/perplexity = 6.71616793/825.6475220 secs/batch = 0.2667s, grad.norm=4.23602676
   170: 0 [  170/ 1327], train_loss/perplexity = 6.86359406/956.7996826 secs/batch = 0.2663s, grad.norm=4.07730436
   175: 0 [  175/ 1327], train_loss/perplexity = 6.95164919/1044.8714600 secs/batch = 0.2664s, grad.norm=4.26910639
   180: 0 [  180/ 1327], train_loss/perplexity = 7.05257702/1155.8334961 secs/batch = 0.2663s, grad.norm=12.88602829
   185: 0 [  185/ 1327], train_loss/perplexity = 6.97462749/1069.1588135 secs/batch = 0.2659s, grad.norm=3.41737866
   190: 0 [  190/ 1327], train_loss/perplexity = 6.76545620/867.3618164 secs/batch = 0.2666s, grad.norm=3.72832918
   195: 0 [  195/ 1327], train_loss/perplexity = 6.59599304/732.1555786 secs/batch = 0.2663s, grad.norm=3.83905363
   200: 0 [  200/ 1327], train_loss/perplexity = 6.89779282/990.0869751 secs/batch = 0.2660s, grad.norm=4.33838224
   205: 0 [  205/ 1327], train_loss/perplexity = 6.76427031/866.3338013 secs/batch = 0.2685s, grad.norm=4.05423260
   210: 0 [  210/ 1327], train_loss/perplexity = 6.78332520/883.0000000 secs/batch = 0.2664s, grad.norm=4.09350777
   215: 0 [  215/ 1327], train_loss/perplexity = 6.82665730/922.1033325 secs/batch = 0.2655s, grad.norm=4.24511909
   220: 0 [  220/ 1327], train_loss/perplexity = 6.74739742/851.8389282 secs/batch = 0.2700s, grad.norm=4.64717007
   225: 0 [  225/ 1327], train_loss/perplexity = 6.90935516/1001.6011353 secs/batch = 0.2657s, grad.norm=5.23747826
   230: 0 [  230/ 1327], train_loss/perplexity = 6.67874479/795.3201904 secs/batch = 0.2650s, grad.norm=3.10086250
   235: 0 [  235/ 1327], train_loss/perplexity = 6.68498898/800.3018799 secs/batch = 0.2661s, grad.norm=4.95620155
   240: 0 [  240/ 1327], train_loss/perplexity = 6.55493975/702.7067871 secs/batch = 0.2659s, grad.norm=4.57545090
   245: 0 [  245/ 1327], train_loss/perplexity = 6.71525574/824.8947144 secs/batch = 0.2658s, grad.norm=3.79732037
   250: 0 [  250/ 1327], train_loss/perplexity = 6.57442331/716.5322876 secs/batch = 0.2663s, grad.norm=3.66517162
   255: 0 [  255/ 1327], train_loss/perplexity = 6.80564785/902.9325562 secs/batch = 0.2662s, grad.norm=5.38682318
   260: 0 [  260/ 1327], train_loss/perplexity = 6.80783129/904.9061890 secs/batch = 0.2674s, grad.norm=4.30857992
   265: 0 [  265/ 1327], train_loss/perplexity = 6.69229507/806.1703491 secs/batch = 0.2676s, grad.norm=5.90623331
   270: 0 [  270/ 1327], train_loss/perplexity = 6.81862783/914.7290039 secs/batch = 0.2672s, grad.norm=7.55633926
   275: 0 [  275/ 1327], train_loss/perplexity = 6.99251413/1088.4545898 secs/batch = 0.2669s, grad.norm=8.43284416
   280: 0 [  280/ 1327], train_loss/perplexity = 6.57156754/714.4889526 secs/batch = 0.2710s, grad.norm=4.57016182
   285: 0 [  285/ 1327], train_loss/perplexity = 6.77647352/876.9706421 secs/batch = 0.2586s, grad.norm=5.56642866
   290: 0 [  290/ 1327], train_loss/perplexity = 6.66745043/786.3881226 secs/batch = 0.2642s, grad.norm=6.87605476
   295: 0 [  295/ 1327], train_loss/perplexity = 6.56499100/709.8054810 secs/batch = 0.2626s, grad.norm=4.80234098
   300: 0 [  300/ 1327], train_loss/perplexity = 6.45369720/635.0458374 secs/batch = 0.2667s, grad.norm=5.71213627
   305: 0 [  305/ 1327], train_loss/perplexity = 6.47408295/648.1245728 secs/batch = 0.2662s, grad.norm=4.22218466
   310: 0 [  310/ 1327], train_loss/perplexity = 6.55673027/703.9661255 secs/batch = 0.2669s, grad.norm=4.62465096
   315: 0 [  315/ 1327], train_loss/perplexity = 6.47700405/650.0205688 secs/batch = 0.2657s, grad.norm=8.06803226
   320: 0 [  320/ 1327], train_loss/perplexity = 6.51836872/677.4723511 secs/batch = 0.2659s, grad.norm=4.92723799
   325: 0 [  325/ 1327], train_loss/perplexity = 6.34675074/570.6355591 secs/batch = 0.2658s, grad.norm=6.44723606
   330: 0 [  330/ 1327], train_loss/perplexity = 6.77551222/876.1279907 secs/batch = 0.2652s, grad.norm=10.57308865
   335: 0 [  335/ 1327], train_loss/perplexity = 6.02661228/414.3090820 secs/batch = 0.2668s, grad.norm=8.95961761
   340: 0 [  340/ 1327], train_loss/perplexity = 6.59690857/732.8262329 secs/batch = 0.2652s, grad.norm=5.24548149
   345: 0 [  345/ 1327], train_loss/perplexity = 6.47667980/649.8098755 secs/batch = 0.2658s, grad.norm=4.64040756
   350: 0 [  350/ 1327], train_loss/perplexity = 6.55728626/704.3576660 secs/batch = 0.2637s, grad.norm=5.17050362
   355: 0 [  355/ 1327], train_loss/perplexity = 6.65990496/780.4767456 secs/batch = 0.2656s, grad.norm=6.54817486
   360: 0 [  360/ 1327], train_loss/perplexity = 6.77752972/877.8973999 secs/batch = 0.2653s, grad.norm=4.95181608
   365: 0 [  365/ 1327], train_loss/perplexity = 6.56449604/709.4542847 secs/batch = 0.2589s, grad.norm=6.32360649
   370: 0 [  370/ 1327], train_loss/perplexity = 6.56437111/709.3656616 secs/batch = 0.2670s, grad.norm=4.19356585
   375: 0 [  375/ 1327], train_loss/perplexity = 6.18519258/485.5064697 secs/batch = 0.2663s, grad.norm=4.96182489
   380: 0 [  380/ 1327], train_loss/perplexity = 6.45134640/633.5547485 secs/batch = 0.2651s, grad.norm=4.39767694
   385: 0 [  385/ 1327], train_loss/perplexity = 6.48145580/652.9207764 secs/batch = 0.2655s, grad.norm=8.98641205
   390: 0 [  390/ 1327], train_loss/perplexity = 6.41399670/610.3281250 secs/batch = 0.2665s, grad.norm=5.10059834
   395: 0 [  395/ 1327], train_loss/perplexity = 6.70740700/818.4456787 secs/batch = 0.2673s, grad.norm=6.33160734
   400: 0 [  400/ 1327], train_loss/perplexity = 6.34572077/570.0480957 secs/batch = 0.2663s, grad.norm=9.16351128
   405: 0 [  405/ 1327], train_loss/perplexity = 6.60764122/740.7337036 secs/batch = 0.2660s, grad.norm=5.64779854
   410: 0 [  410/ 1327], train_loss/perplexity = 6.40147686/602.7345581 secs/batch = 0.2669s, grad.norm=7.22006655
   415: 0 [  415/ 1327], train_loss/perplexity = 6.35056877/572.8184204 secs/batch = 0.2652s, grad.norm=5.68829727
   420: 0 [  420/ 1327], train_loss/perplexity = 6.35542774/575.6085205 secs/batch = 0.2649s, grad.norm=8.53994179
   425: 0 [  425/ 1327], train_loss/perplexity = 6.61442280/745.7741699 secs/batch = 0.2640s, grad.norm=11.19024086
   430: 0 [  430/ 1327], train_loss/perplexity = 6.44724655/630.9625854 secs/batch = 0.2663s, grad.norm=5.67931557
   435: 0 [  435/ 1327], train_loss/perplexity = 6.45388889/635.1676025 secs/batch = 0.2659s, grad.norm=4.31013727
   440: 0 [  440/ 1327], train_loss/perplexity = 6.38542509/593.1368408 secs/batch = 0.2650s, grad.norm=10.90846634
   445: 0 [  445/ 1327], train_loss/perplexity = 6.45064020/633.1074829 secs/batch = 0.2652s, grad.norm=11.68566418
   450: 0 [  450/ 1327], train_loss/perplexity = 6.30642700/548.0831299 secs/batch = 0.2655s, grad.norm=7.90834475
   455: 0 [  455/ 1327], train_loss/perplexity = 5.97339201/392.8359070 secs/batch = 0.2711s, grad.norm=4.95281124
   460: 0 [  460/ 1327], train_loss/perplexity = 6.28351831/535.6699829 secs/batch = 0.2667s, grad.norm=6.43716669
   465: 0 [  465/ 1327], train_loss/perplexity = 6.18056297/483.2639465 secs/batch = 0.2648s, grad.norm=6.03066683
   470: 0 [  470/ 1327], train_loss/perplexity = 6.51956987/678.2865601 secs/batch = 0.2651s, grad.norm=6.60822630
   475: 0 [  475/ 1327], train_loss/perplexity = 6.38040018/590.1638184 secs/batch = 0.2645s, grad.norm=4.72507811
   480: 0 [  480/ 1327], train_loss/perplexity = 6.33744669/565.3509521 secs/batch = 0.2658s, grad.norm=6.05191898
   485: 0 [  485/ 1327], train_loss/perplexity = 6.14876270/468.1378174 secs/batch = 0.2653s, grad.norm=5.51087379
   490: 0 [  490/ 1327], train_loss/perplexity = 6.21490908/500.1505127 secs/batch = 0.2657s, grad.norm=6.61020994
   495: 0 [  495/ 1327], train_loss/perplexity = 6.00207901/404.2684021 secs/batch = 0.2663s, grad.norm=10.07597923
   500: 0 [  500/ 1327], train_loss/perplexity = 6.33775139/565.5232544 secs/batch = 0.2647s, grad.norm=6.85408258
   505: 0 [  505/ 1327], train_loss/perplexity = 6.18181753/483.8706055 secs/batch = 0.2634s, grad.norm=7.46185398
   510: 0 [  510/ 1327], train_loss/perplexity = 6.40675783/605.9259644 secs/batch = 0.2629s, grad.norm=5.48475695
   515: 0 [  515/ 1327], train_loss/perplexity = 6.08013058/437.0862732 secs/batch = 0.2658s, grad.norm=6.29328489
   520: 0 [  520/ 1327], train_loss/perplexity = 6.39271879/597.4788208 secs/batch = 0.2663s, grad.norm=7.56659937
   525: 0 [  525/ 1327], train_loss/perplexity = 6.15607882/471.5753174 secs/batch = 0.2641s, grad.norm=6.19068003
   530: 0 [  530/ 1327], train_loss/perplexity = 6.09238672/442.4762268 secs/batch = 0.2660s, grad.norm=8.41653919
   535: 0 [  535/ 1327], train_loss/perplexity = 6.19025326/487.9696655 secs/batch = 0.2610s, grad.norm=6.70361900
   540: 0 [  540/ 1327], train_loss/perplexity = 6.11650658/453.2784424 secs/batch = 0.2659s, grad.norm=5.80654621
   545: 0 [  545/ 1327], train_loss/perplexity = 6.27326727/530.2069092 secs/batch = 0.2660s, grad.norm=7.77473497
   550: 0 [  550/ 1327], train_loss/perplexity = 6.17574883/480.9430237 secs/batch = 0.2661s, grad.norm=6.22532749
   555: 0 [  555/ 1327], train_loss/perplexity = 6.06736660/431.5427551 secs/batch = 0.2665s, grad.norm=7.31457329
   560: 0 [  560/ 1327], train_loss/perplexity = 6.19487524/490.2302856 secs/batch = 0.2651s, grad.norm=7.40800142
   565: 0 [  565/ 1327], train_loss/perplexity = 6.20088959/493.1875916 secs/batch = 0.2624s, grad.norm=8.55685234
   570: 0 [  570/ 1327], train_loss/perplexity = 6.07179356/433.4574280 secs/batch = 0.2670s, grad.norm=8.60875511
   575: 0 [  575/ 1327], train_loss/perplexity = 6.07999182/437.0256042 secs/batch = 0.2668s, grad.norm=8.76821232
   580: 0 [  580/ 1327], train_loss/perplexity = 6.10894680/449.8646545 secs/batch = 0.2649s, grad.norm=7.09532833
   585: 0 [  585/ 1327], train_loss/perplexity = 5.84667158/346.0805664 secs/batch = 0.2650s, grad.norm=7.66728401
   590: 0 [  590/ 1327], train_loss/perplexity = 6.20783997/496.6273499 secs/batch = 0.2657s, grad.norm=6.78631210
   595: 0 [  595/ 1327], train_loss/perplexity = 6.03470802/417.6768494 secs/batch = 0.2653s, grad.norm=6.72895145
   600: 0 [  600/ 1327], train_loss/perplexity = 6.34129238/567.5292969 secs/batch = 0.2663s, grad.norm=6.94856977
   605: 0 [  605/ 1327], train_loss/perplexity = 6.32181454/556.5820312 secs/batch = 0.2676s, grad.norm=7.55104542
   610: 0 [  610/ 1327], train_loss/perplexity = 6.26802015/527.4321289 secs/batch = 0.2619s, grad.norm=6.99650383
   615: 0 [  615/ 1327], train_loss/perplexity = 5.73937988/310.8715820 secs/batch = 0.2662s, grad.norm=7.99523544
   620: 0 [  620/ 1327], train_loss/perplexity = 5.96290684/388.7384949 secs/batch = 0.2669s, grad.norm=7.40924406
   625: 0 [  625/ 1327], train_loss/perplexity = 6.15961361/473.2451782 secs/batch = 0.2668s, grad.norm=6.89891243
   630: 0 [  630/ 1327], train_loss/perplexity = 6.08227921/438.0263977 secs/batch = 0.2663s, grad.norm=6.47306156
   635: 0 [  635/ 1327], train_loss/perplexity = 6.02798080/414.8764648 secs/batch = 0.2660s, grad.norm=9.27704906
   640: 0 [  640/ 1327], train_loss/perplexity = 6.05768871/427.3864746 secs/batch = 0.2666s, grad.norm=7.88444376
   645: 0 [  645/ 1327], train_loss/perplexity = 6.11137962/450.9604492 secs/batch = 0.2654s, grad.norm=7.02886105
   650: 0 [  650/ 1327], train_loss/perplexity = 5.94484329/381.7795410 secs/batch = 0.2661s, grad.norm=9.13415337
   655: 0 [  655/ 1327], train_loss/perplexity = 5.90470028/366.7572937 secs/batch = 0.2653s, grad.norm=7.73466682
   660: 0 [  660/ 1327], train_loss/perplexity = 5.87264347/355.1866760 secs/batch = 0.2660s, grad.norm=7.53360462
   665: 0 [  665/ 1327], train_loss/perplexity = 6.01202297/408.3084717 secs/batch = 0.2674s, grad.norm=7.11068201
   670: 0 [  670/ 1327], train_loss/perplexity = 5.91923046/372.1252441 secs/batch = 0.2651s, grad.norm=8.60141850
   675: 0 [  675/ 1327], train_loss/perplexity = 5.64743376/283.5628357 secs/batch = 0.2663s, grad.norm=8.46173382
   680: 0 [  680/ 1327], train_loss/perplexity = 6.01452732/409.3323059 secs/batch = 0.2664s, grad.norm=7.27525806
   685: 0 [  685/ 1327], train_loss/perplexity = 6.01220226/408.3816833 secs/batch = 0.2662s, grad.norm=8.09001160
   690: 0 [  690/ 1327], train_loss/perplexity = 6.02678871/414.3822021 secs/batch = 0.2645s, grad.norm=7.70673132
   695: 0 [  695/ 1327], train_loss/perplexity = 5.82323790/338.0649109 secs/batch = 0.2670s, grad.norm=8.04117012
   700: 0 [  700/ 1327], train_loss/perplexity = 6.01179743/408.2164001 secs/batch = 0.2669s, grad.norm=6.68993378
   705: 0 [  705/ 1327], train_loss/perplexity = 5.75246525/314.9661865 secs/batch = 0.2662s, grad.norm=8.05846977
   710: 0 [  710/ 1327], train_loss/perplexity = 5.88767862/360.5672913 secs/batch = 0.2658s, grad.norm=7.30508089
   715: 0 [  715/ 1327], train_loss/perplexity = 5.79039097/327.1408997 secs/batch = 0.2590s, grad.norm=8.03615952
   720: 0 [  720/ 1327], train_loss/perplexity = 5.89232445/362.2463379 secs/batch = 0.2667s, grad.norm=8.38280392
   725: 0 [  725/ 1327], train_loss/perplexity = 5.60547209/271.9102478 secs/batch = 0.2623s, grad.norm=9.09674263
   730: 0 [  730/ 1327], train_loss/perplexity = 5.76785183/319.8499146 secs/batch = 0.2664s, grad.norm=7.94768000
   735: 0 [  735/ 1327], train_loss/perplexity = 5.87846279/357.2596436 secs/batch = 0.2660s, grad.norm=7.55371857
   740: 0 [  740/ 1327], train_loss/perplexity = 5.38708258/218.5648041 secs/batch = 0.2657s, grad.norm=9.12865543
   745: 0 [  745/ 1327], train_loss/perplexity = 5.81117725/334.0121155 secs/batch = 0.2653s, grad.norm=7.63535929
   750: 0 [  750/ 1327], train_loss/perplexity = 5.73982859/311.0111084 secs/batch = 0.2663s, grad.norm=8.36484337
   755: 0 [  755/ 1327], train_loss/perplexity = 5.70112038/299.2024231 secs/batch = 0.2636s, grad.norm=7.90938139
   760: 0 [  760/ 1327], train_loss/perplexity = 5.68626022/294.7891235 secs/batch = 0.2660s, grad.norm=7.75522232
   765: 0 [  765/ 1327], train_loss/perplexity = 5.67089367/290.2938538 secs/batch = 0.2642s, grad.norm=8.69783115
   770: 0 [  770/ 1327], train_loss/perplexity = 5.71523046/303.4541321 secs/batch = 0.2667s, grad.norm=7.93233776
   775: 0 [  775/ 1327], train_loss/perplexity = 5.77873898/323.3511658 secs/batch = 0.2666s, grad.norm=8.14698315
   780: 0 [  780/ 1327], train_loss/perplexity = 5.94614601/382.2771912 secs/batch = 0.2669s, grad.norm=8.17685604
   785: 0 [  785/ 1327], train_loss/perplexity = 5.79987431/330.2580566 secs/batch = 0.2670s, grad.norm=8.59173584
   790: 0 [  790/ 1327], train_loss/perplexity = 5.60128355/270.7737427 secs/batch = 0.2658s, grad.norm=9.87604809
   795: 0 [  795/ 1327], train_loss/perplexity = 5.88978910/361.3290710 secs/batch = 0.2659s, grad.norm=8.65132904
   800: 0 [  800/ 1327], train_loss/perplexity = 5.85919476/350.4418335 secs/batch = 0.2671s, grad.norm=8.63923264
   805: 0 [  805/ 1327], train_loss/perplexity = 6.10399628/447.6430969 secs/batch = 0.2660s, grad.norm=8.29356956
   810: 0 [  810/ 1327], train_loss/perplexity = 5.90795612/367.9533386 secs/batch = 0.2665s, grad.norm=8.47482300
   815: 0 [  815/ 1327], train_loss/perplexity = 5.74740982/313.3778992 secs/batch = 0.2657s, grad.norm=8.23472023
   820: 0 [  820/ 1327], train_loss/perplexity = 5.26984787/194.3863831 secs/batch = 0.2676s, grad.norm=7.69099903
   825: 0 [  825/ 1327], train_loss/perplexity = 5.48276711/240.5113068 secs/batch = 0.2658s, grad.norm=7.99557495
   830: 0 [  830/ 1327], train_loss/perplexity = 5.37686253/216.3424377 secs/batch = 0.2670s, grad.norm=9.29268169
   835: 0 [  835/ 1327], train_loss/perplexity = 5.66660070/289.0502930 secs/batch = 0.2647s, grad.norm=8.00043678
   840: 0 [  840/ 1327], train_loss/perplexity = 5.77854013/323.2868958 secs/batch = 0.2655s, grad.norm=8.01249790
   845: 0 [  845/ 1327], train_loss/perplexity = 5.64664602/283.3395691 secs/batch = 0.2657s, grad.norm=8.12101173
   850: 0 [  850/ 1327], train_loss/perplexity = 5.71360588/302.9615479 secs/batch = 0.2660s, grad.norm=8.74111557
   855: 0 [  855/ 1327], train_loss/perplexity = 5.66490602/288.5608521 secs/batch = 0.2652s, grad.norm=9.06899357
   860: 0 [  860/ 1327], train_loss/perplexity = 5.40624428/222.7932587 secs/batch = 0.2580s, grad.norm=8.52028751
   865: 0 [  865/ 1327], train_loss/perplexity = 5.78601122/325.7112427 secs/batch = 0.2655s, grad.norm=8.43284607
   870: 0 [  870/ 1327], train_loss/perplexity = 5.91335392/369.9448547 secs/batch = 0.2669s, grad.norm=8.57680130
   875: 0 [  875/ 1327], train_loss/perplexity = 5.45184469/233.1879272 secs/batch = 0.2660s, grad.norm=8.48919201
   880: 0 [  880/ 1327], train_loss/perplexity = 5.54497004/255.9469147 secs/batch = 0.2618s, grad.norm=8.71077633
   885: 0 [  885/ 1327], train_loss/perplexity = 5.56841469/262.0184021 secs/batch = 0.2661s, grad.norm=9.37678242
   890: 0 [  890/ 1327], train_loss/perplexity = 5.79261827/327.8703613 secs/batch = 0.2656s, grad.norm=8.89798927
   895: 0 [  895/ 1327], train_loss/perplexity = 5.77873135/323.3487244 secs/batch = 0.2661s, grad.norm=8.51414013
   900: 0 [  900/ 1327], train_loss/perplexity = 5.70454216/300.2279968 secs/batch = 0.2654s, grad.norm=8.12825489
   905: 0 [  905/ 1327], train_loss/perplexity = 5.57547998/263.8761902 secs/batch = 0.2681s, grad.norm=8.43322563
   910: 0 [  910/ 1327], train_loss/perplexity = 5.64514160/282.9136047 secs/batch = 0.2656s, grad.norm=8.89542294
   915: 0 [  915/ 1327], train_loss/perplexity = 5.89373350/362.7571106 secs/batch = 0.2660s, grad.norm=7.73330021
   920: 0 [  920/ 1327], train_loss/perplexity = 5.93792057/379.1456909 secs/batch = 0.2657s, grad.norm=8.34614182
   925: 0 [  925/ 1327], train_loss/perplexity = 5.65209770/284.8884583 secs/batch = 0.2656s, grad.norm=8.12485886
   930: 0 [  930/ 1327], train_loss/perplexity = 5.61223602/273.7556763 secs/batch = 0.2661s, grad.norm=9.92171478
   935: 0 [  935/ 1327], train_loss/perplexity = 5.69632435/297.7708740 secs/batch = 0.2642s, grad.norm=8.62986469
   940: 0 [  940/ 1327], train_loss/perplexity = 5.62228727/276.5211487 secs/batch = 0.2661s, grad.norm=9.49903870
   945: 0 [  945/ 1327], train_loss/perplexity = 5.87147665/354.7724609 secs/batch = 0.2653s, grad.norm=8.78386116
   950: 0 [  950/ 1327], train_loss/perplexity = 5.59489536/269.0494995 secs/batch = 0.2654s, grad.norm=9.10114098
   955: 0 [  955/ 1327], train_loss/perplexity = 5.78361797/324.9326782 secs/batch = 0.2659s, grad.norm=8.46805573
   960: 0 [  960/ 1327], train_loss/perplexity = 5.89844370/364.4697876 secs/batch = 0.2675s, grad.norm=8.62410164
   965: 0 [  965/ 1327], train_loss/perplexity = 5.64176369/281.9595642 secs/batch = 0.2658s, grad.norm=8.44267082
   970: 0 [  970/ 1327], train_loss/perplexity = 5.83740664/342.8889465 secs/batch = 0.2672s, grad.norm=7.68995142
   975: 0 [  975/ 1327], train_loss/perplexity = 5.64120436/281.8019104 secs/batch = 0.2651s, grad.norm=9.10099697
   980: 0 [  980/ 1327], train_loss/perplexity = 5.46407986/236.0585480 secs/batch = 0.2614s, grad.norm=8.85753250
   985: 0 [  985/ 1327], train_loss/perplexity = 5.69656754/297.8433228 secs/batch = 0.2668s, grad.norm=9.88212395
   990: 0 [  990/ 1327], train_loss/perplexity = 5.78320599/324.7988281 secs/batch = 0.2640s, grad.norm=8.89000416
   995: 0 [  995/ 1327], train_loss/perplexity = 5.75284576/315.0860596 secs/batch = 0.2663s, grad.norm=8.80476475
  1000: 0 [ 1000/ 1327], train_loss/perplexity = 5.27924347/196.2213745 secs/batch = 0.2627s, grad.norm=9.46123314
  1005: 0 [ 1005/ 1327], train_loss/perplexity = 5.79456949/328.5107422 secs/batch = 0.2665s, grad.norm=10.23833370
  1010: 0 [ 1010/ 1327], train_loss/perplexity = 5.28938675/198.2218323 secs/batch = 0.2663s, grad.norm=8.97469139
  1015: 0 [ 1015/ 1327], train_loss/perplexity = 5.69136047/296.2964478 secs/batch = 0.2664s, grad.norm=8.54271030
  1020: 0 [ 1020/ 1327], train_loss/perplexity = 5.93391132/377.6286621 secs/batch = 0.2659s, grad.norm=8.98595619
  1025: 0 [ 1025/ 1327], train_loss/perplexity = 5.70275068/299.6906128 secs/batch = 0.2653s, grad.norm=8.51839924
  1030: 0 [ 1030/ 1327], train_loss/perplexity = 5.60709429/272.3517151 secs/batch = 0.2626s, grad.norm=8.71033192
  1035: 0 [ 1035/ 1327], train_loss/perplexity = 5.43789673/229.9580078 secs/batch = 0.2650s, grad.norm=8.91330242
  1040: 0 [ 1040/ 1327], train_loss/perplexity = 5.67476940/291.4211426 secs/batch = 0.2645s, grad.norm=8.05124283
  1045: 0 [ 1045/ 1327], train_loss/perplexity = 5.37710667/216.3952637 secs/batch = 0.2658s, grad.norm=8.63044167
  1050: 0 [ 1050/ 1327], train_loss/perplexity = 5.42724085/227.5206146 secs/batch = 0.2665s, grad.norm=10.22798920
  1055: 0 [ 1055/ 1327], train_loss/perplexity = 5.64996719/284.2821350 secs/batch = 0.2671s, grad.norm=8.97128296
  1060: 0 [ 1060/ 1327], train_loss/perplexity = 5.32360268/205.1215363 secs/batch = 0.2657s, grad.norm=10.38222408
  1065: 0 [ 1065/ 1327], train_loss/perplexity = 5.34190083/208.9094391 secs/batch = 0.2644s, grad.norm=9.10559273
  1070: 0 [ 1070/ 1327], train_loss/perplexity = 5.71152210/302.3309021 secs/batch = 0.2669s, grad.norm=9.54112053
  1075: 0 [ 1075/ 1327], train_loss/perplexity = 5.52115202/249.9227905 secs/batch = 0.2663s, grad.norm=10.07237148
  1080: 0 [ 1080/ 1327], train_loss/perplexity = 5.31529188/203.4238739 secs/batch = 0.2652s, grad.norm=9.17440414
  1085: 0 [ 1085/ 1327], train_loss/perplexity = 5.23134899/187.0449524 secs/batch = 0.2650s, grad.norm=9.51892757
  1090: 0 [ 1090/ 1327], train_loss/perplexity = 5.48173046/240.2621155 secs/batch = 0.2656s, grad.norm=9.36274624
  1095: 0 [ 1095/ 1327], train_loss/perplexity = 5.57308817/263.2457886 secs/batch = 0.2635s, grad.norm=9.56102467
  1100: 0 [ 1100/ 1327], train_loss/perplexity = 5.60286570/271.2024841 secs/batch = 0.2662s, grad.norm=9.63959217
  1105: 0 [ 1105/ 1327], train_loss/perplexity = 5.38760900/218.6799011 secs/batch = 0.2666s, grad.norm=10.25664139
  1110: 0 [ 1110/ 1327], train_loss/perplexity = 5.97354794/392.8971863 secs/batch = 0.2621s, grad.norm=8.85298347
  1115: 0 [ 1115/ 1327], train_loss/perplexity = 5.40122557/221.6779327 secs/batch = 0.2664s, grad.norm=9.15005016
  1120: 0 [ 1120/ 1327], train_loss/perplexity = 5.57185936/262.9225159 secs/batch = 0.2661s, grad.norm=9.31205273
  1125: 0 [ 1125/ 1327], train_loss/perplexity = 5.80414677/331.6720886 secs/batch = 0.2683s, grad.norm=9.53590393
  1130: 0 [ 1130/ 1327], train_loss/perplexity = 5.44714928/232.0955811 secs/batch = 0.2652s, grad.norm=8.93316746
  1135: 0 [ 1135/ 1327], train_loss/perplexity = 5.47229815/238.0065308 secs/batch = 0.2657s, grad.norm=9.87699127
  1140: 0 [ 1140/ 1327], train_loss/perplexity = 5.72688293/307.0108032 secs/batch = 0.2664s, grad.norm=9.28130627
  1145: 0 [ 1145/ 1327], train_loss/perplexity = 5.45214558/233.2581024 secs/batch = 0.2667s, grad.norm=9.71424580
  1150: 0 [ 1150/ 1327], train_loss/perplexity = 5.43780708/229.9373932 secs/batch = 0.2657s, grad.norm=9.56755543
  1155: 0 [ 1155/ 1327], train_loss/perplexity = 5.52225590/250.1988220 secs/batch = 0.2665s, grad.norm=9.24845886
  1160: 0 [ 1160/ 1327], train_loss/perplexity = 5.57572031/263.9396057 secs/batch = 0.2658s, grad.norm=9.64544201
  1165: 0 [ 1165/ 1327], train_loss/perplexity = 5.65037823/284.3990173 secs/batch = 0.2666s, grad.norm=9.55580330
  1170: 0 [ 1170/ 1327], train_loss/perplexity = 5.49366140/243.1458282 secs/batch = 0.2664s, grad.norm=9.69649696
  1175: 0 [ 1175/ 1327], train_loss/perplexity = 5.27556610/195.5011139 secs/batch = 0.2664s, grad.norm=10.36452103
  1180: 0 [ 1180/ 1327], train_loss/perplexity = 5.18161488/177.9699860 secs/batch = 0.2645s, grad.norm=9.79113483
  1185: 0 [ 1185/ 1327], train_loss/perplexity = 5.44789743/232.2692871 secs/batch = 0.2662s, grad.norm=9.36381817
  1190: 0 [ 1190/ 1327], train_loss/perplexity = 5.43427753/229.1272583 secs/batch = 0.2663s, grad.norm=9.46322346
  1195: 0 [ 1195/ 1327], train_loss/perplexity = 5.30358601/201.0565033 secs/batch = 0.2660s, grad.norm=9.98151970
  1200: 0 [ 1200/ 1327], train_loss/perplexity = 5.16816807/175.5928650 secs/batch = 0.2673s, grad.norm=9.60498905
  1205: 0 [ 1205/ 1327], train_loss/perplexity = 5.40918970/223.4504547 secs/batch = 0.2657s, grad.norm=10.10879993
  1210: 0 [ 1210/ 1327], train_loss/perplexity = 5.07881784/160.5841064 secs/batch = 0.2635s, grad.norm=10.07667160
  1215: 0 [ 1215/ 1327], train_loss/perplexity = 5.08325243/161.2978210 secs/batch = 0.2650s, grad.norm=9.91509533
  1220: 0 [ 1220/ 1327], train_loss/perplexity = 5.29607534/199.5520935 secs/batch = 0.2666s, grad.norm=9.64724541
  1225: 0 [ 1225/ 1327], train_loss/perplexity = 5.25559807/191.6360626 secs/batch = 0.2662s, grad.norm=10.30930233
  1230: 0 [ 1230/ 1327], train_loss/perplexity = 5.37759399/216.5007477 secs/batch = 0.2659s, grad.norm=10.43375206
  1235: 0 [ 1235/ 1327], train_loss/perplexity = 5.42119455/226.1491089 secs/batch = 0.2669s, grad.norm=10.57232571
  1240: 0 [ 1240/ 1327], train_loss/perplexity = 5.45676184/234.3373718 secs/batch = 0.2647s, grad.norm=9.68846035
  1245: 0 [ 1245/ 1327], train_loss/perplexity = 5.29552603/199.4425049 secs/batch = 0.2658s, grad.norm=10.25024128
  1250: 0 [ 1250/ 1327], train_loss/perplexity = 5.49717999/244.0028687 secs/batch = 0.2665s, grad.norm=9.86212158
  1255: 0 [ 1255/ 1327], train_loss/perplexity = 5.40913200/223.4375610 secs/batch = 0.2664s, grad.norm=9.95665741
  1260: 0 [ 1260/ 1327], train_loss/perplexity = 5.41896677/225.6458588 secs/batch = 0.2667s, grad.norm=10.66263771
  1265: 0 [ 1265/ 1327], train_loss/perplexity = 5.49011564/242.2852173 secs/batch = 0.2621s, grad.norm=9.43185234
  1270: 0 [ 1270/ 1327], train_loss/perplexity = 5.29341078/199.0210876 secs/batch = 0.2659s, grad.norm=9.74218273
  1275: 0 [ 1275/ 1327], train_loss/perplexity = 5.61642075/274.9036560 secs/batch = 0.2650s, grad.norm=10.22088623
  1280: 0 [ 1280/ 1327], train_loss/perplexity = 5.28248882/196.8592072 secs/batch = 0.2682s, grad.norm=10.16417694
  1285: 0 [ 1285/ 1327], train_loss/perplexity = 5.34674597/209.9240875 secs/batch = 0.2670s, grad.norm=9.33940411
  1290: 0 [ 1290/ 1327], train_loss/perplexity = 5.48443413/240.9125824 secs/batch = 0.2668s, grad.norm=10.38341236
  1295: 0 [ 1295/ 1327], train_loss/perplexity = 5.52432537/250.7171326 secs/batch = 0.2639s, grad.norm=10.27593136
  1300: 0 [ 1300/ 1327], train_loss/perplexity = 5.52325296/250.4484100 secs/batch = 0.2667s, grad.norm=9.43642616
  1305: 0 [ 1305/ 1327], train_loss/perplexity = 5.71867466/304.5010986 secs/batch = 0.2667s, grad.norm=10.43511486
  1310: 0 [ 1310/ 1327], train_loss/perplexity = 5.88220024/358.5973816 secs/batch = 0.2658s, grad.norm=8.88082600
  1315: 0 [ 1315/ 1327], train_loss/perplexity = 5.71865940/304.4964294 secs/batch = 0.2657s, grad.norm=9.81961536
  1320: 0 [ 1320/ 1327], train_loss/perplexity = 5.67114782/290.3676453 secs/batch = 0.2654s, grad.norm=9.09342384
  1325: 0 [ 1325/ 1327], train_loss/perplexity = 5.56285906/260.5667419 secs/batch = 0.2658s, grad.norm=9.73783970
Epoch training time: 353.65773725509644
	> validation loss = 5.63710451, perplexity = 280.64892578
	> validation loss = 5.53993654, perplexity = 254.66183472
	> validation loss = 5.36063337, perplexity = 212.85972595
	> validation loss = 5.53410435, perplexity = 253.18092346
	> validation loss = 5.75427389, perplexity = 315.53634644
	> validation loss = 5.48857450, perplexity = 241.91210938
	> validation loss = 5.45569706, perplexity = 234.08798218
	> validation loss = 5.43395901, perplexity = 229.05427551
	> validation loss = 5.66635799, perplexity = 288.98013306
	> validation loss = 5.46305752, perplexity = 235.81733704
	> validation loss = 5.49026251, perplexity = 242.32081604
	> validation loss = 5.57155371, perplexity = 262.84216309
	> validation loss = 5.44254398, perplexity = 231.02917480
	> validation loss = 5.39627647, perplexity = 220.58354187
	> validation loss = 5.15741205, perplexity = 173.71430969
	> validation loss = 5.22969532, perplexity = 186.73590088
	> validation loss = 5.50636482, perplexity = 246.25431824
	> validation loss = 5.19267130, perplexity = 179.94860840
	> validation loss = 5.59936523, perplexity = 270.25479126
	> validation loss = 5.56115723, perplexity = 260.12368774
	> validation loss = 5.39908791, perplexity = 221.20455933
at the end of epoch: 0
train loss = 5.56150952, perplexity = 260.21534118
validation loss = 5.44807634, perplexity = 232.31084954
Saved model cv/epoch000_5.4481.model
  1332: 1 [    5/ 1327], train_loss/perplexity = 5.76499367/318.9370117 secs/batch = 0.2680s, grad.norm=9.00009251
  1337: 1 [   10/ 1327], train_loss/perplexity = 5.39422131/220.1306610 secs/batch = 0.2676s, grad.norm=12.60361862
  1342: 1 [   15/ 1327], train_loss/perplexity = 5.34351635/209.2472076 secs/batch = 0.2641s, grad.norm=9.13226128
  1347: 1 [   20/ 1327], train_loss/perplexity = 5.85559511/349.1826477 secs/batch = 0.2662s, grad.norm=10.23139381
  1352: 1 [   25/ 1327], train_loss/perplexity = 5.59225416/268.3398132 secs/batch = 0.2658s, grad.norm=9.90456867
  1357: 1 [   30/ 1327], train_loss/perplexity = 5.42036772/225.9622040 secs/batch = 0.2669s, grad.norm=9.87462997
  1362: 1 [   35/ 1327], train_loss/perplexity = 5.26972103/194.3617401 secs/batch = 0.2639s, grad.norm=9.53552628
  1367: 1 [   40/ 1327], train_loss/perplexity = 5.42399883/226.7841797 secs/batch = 0.2668s, grad.norm=9.65455437
  1372: 1 [   45/ 1327], train_loss/perplexity = 5.11809349/167.0166473 secs/batch = 0.2671s, grad.norm=9.81029415
  1377: 1 [   50/ 1327], train_loss/perplexity = 5.59137487/268.1039734 secs/batch = 0.2657s, grad.norm=9.79237461
  1382: 1 [   55/ 1327], train_loss/perplexity = 5.43358803/228.9693146 secs/batch = 0.2666s, grad.norm=9.31594849
  1387: 1 [   60/ 1327], train_loss/perplexity = 5.59419632/268.8614807 secs/batch = 0.2659s, grad.norm=10.04573345
  1392: 1 [   65/ 1327], train_loss/perplexity = 5.16536999/175.1022339 secs/batch = 0.2663s, grad.norm=9.59730339
  1397: 1 [   70/ 1327], train_loss/perplexity = 5.05223799/156.3720245 secs/batch = 0.2646s, grad.norm=10.63513851
  1402: 1 [   75/ 1327], train_loss/perplexity = 5.01365948/150.4543152 secs/batch = 0.2644s, grad.norm=10.28587914
  1407: 1 [   80/ 1327], train_loss/perplexity = 5.43076706/228.3243103 secs/batch = 0.2663s, grad.norm=10.95214653
  1412: 1 [   85/ 1327], train_loss/perplexity = 5.46428585/236.1071777 secs/batch = 0.2666s, grad.norm=10.09483528
  1417: 1 [   90/ 1327], train_loss/perplexity = 5.43268490/228.7626190 secs/batch = 0.2690s, grad.norm=10.61470413
  1422: 1 [   95/ 1327], train_loss/perplexity = 5.18786812/179.0863495 secs/batch = 0.2658s, grad.norm=9.84652519
  1427: 1 [  100/ 1327], train_loss/perplexity = 5.50719547/246.4589539 secs/batch = 0.2681s, grad.norm=9.89451694
  1432: 1 [  105/ 1327], train_loss/perplexity = 5.57572317/263.9403687 secs/batch = 0.2685s, grad.norm=10.88797569
  1437: 1 [  110/ 1327], train_loss/perplexity = 5.34379625/209.3057861 secs/batch = 0.2632s, grad.norm=9.33427048
  1442: 1 [  115/ 1327], train_loss/perplexity = 5.15388012/173.1018372 secs/batch = 0.2658s, grad.norm=10.91758442
  1447: 1 [  120/ 1327], train_loss/perplexity = 5.37832355/216.6587524 secs/batch = 0.2666s, grad.norm=10.85497761
  1452: 1 [  125/ 1327], train_loss/perplexity = 5.46769667/236.9138794 secs/batch = 0.2658s, grad.norm=10.43453312
  1457: 1 [  130/ 1327], train_loss/perplexity = 5.34526873/209.6142120 secs/batch = 0.2686s, grad.norm=10.51393127
  1462: 1 [  135/ 1327], train_loss/perplexity = 5.33298635/207.0553894 secs/batch = 0.2664s, grad.norm=9.81655884
  1467: 1 [  140/ 1327], train_loss/perplexity = 5.61335373/274.0618286 secs/batch = 0.2665s, grad.norm=9.79272747
  1472: 1 [  145/ 1327], train_loss/perplexity = 5.60630512/272.1368713 secs/batch = 0.2641s, grad.norm=10.78618526
  1477: 1 [  150/ 1327], train_loss/perplexity = 5.45320892/233.5062714 secs/batch = 0.2659s, grad.norm=9.97200775
  1482: 1 [  155/ 1327], train_loss/perplexity = 5.71079779/302.1119995 secs/batch = 0.2660s, grad.norm=9.84644032
  1487: 1 [  160/ 1327], train_loss/perplexity = 5.37238073/215.3750153 secs/batch = 0.2604s, grad.norm=10.04297256
  1492: 1 [  165/ 1327], train_loss/perplexity = 5.57314920/263.2618713 secs/batch = 0.2644s, grad.norm=10.03376484
  1497: 1 [  170/ 1327], train_loss/perplexity = 5.41760778/225.3394165 secs/batch = 0.2666s, grad.norm=10.38036251
  1502: 1 [  175/ 1327], train_loss/perplexity = 5.56777763/261.8515320 secs/batch = 0.2664s, grad.norm=9.28777027
  1507: 1 [  180/ 1327], train_loss/perplexity = 5.46689844/236.7248383 secs/batch = 0.2656s, grad.norm=9.97741890
  1512: 1 [  185/ 1327], train_loss/perplexity = 5.72890902/307.6334534 secs/batch = 0.2644s, grad.norm=10.01640224
  1517: 1 [  190/ 1327], train_loss/perplexity = 5.27507019/195.4041901 secs/batch = 0.2659s, grad.norm=10.24120426
  1522: 1 [  195/ 1327], train_loss/perplexity = 5.41206169/224.0931244 secs/batch = 0.2649s, grad.norm=9.24657631
  1527: 1 [  200/ 1327], train_loss/perplexity = 5.42773628/227.6333618 secs/batch = 0.2669s, grad.norm=10.33339691
  1532: 1 [  205/ 1327], train_loss/perplexity = 5.47576571/238.8332672 secs/batch = 0.2654s, grad.norm=9.69111347
  1537: 1 [  210/ 1327], train_loss/perplexity = 5.37897158/216.7991943 secs/batch = 0.2651s, grad.norm=9.47402287
  1542: 1 [  215/ 1327], train_loss/perplexity = 5.46914005/237.2560730 secs/batch = 0.2651s, grad.norm=9.48253155
  1547: 1 [  220/ 1327], train_loss/perplexity = 5.56223488/260.4041443 secs/batch = 0.2661s, grad.norm=9.82654667
  1552: 1 [  225/ 1327], train_loss/perplexity = 5.68749285/295.1527100 secs/batch = 0.2651s, grad.norm=9.39717484
  1557: 1 [  230/ 1327], train_loss/perplexity = 5.49302197/242.9904022 secs/batch = 0.2650s, grad.norm=9.63909721
  1562: 1 [  235/ 1327], train_loss/perplexity = 5.39398527/220.0787201 secs/batch = 0.2651s, grad.norm=9.40949249
  1567: 1 [  240/ 1327], train_loss/perplexity = 5.19125462/179.6938629 secs/batch = 0.2664s, grad.norm=10.71708965
  1572: 1 [  245/ 1327], train_loss/perplexity = 5.48175621/240.2682953 secs/batch = 0.2645s, grad.norm=9.57468700
  1577: 1 [  250/ 1327], train_loss/perplexity = 5.16704607/175.3959656 secs/batch = 0.2654s, grad.norm=9.31773853
  1582: 1 [  255/ 1327], train_loss/perplexity = 5.26527929/193.5003510 secs/batch = 0.2634s, grad.norm=9.73950768
  1587: 1 [  260/ 1327], train_loss/perplexity = 5.63349104/279.6366272 secs/batch = 0.2649s, grad.norm=10.37925911
  1592: 1 [  265/ 1327], train_loss/perplexity = 5.62354755/276.8698425 secs/batch = 0.2656s, grad.norm=9.41729450
  1597: 1 [  270/ 1327], train_loss/perplexity = 5.57288170/263.1914368 secs/batch = 0.2661s, grad.norm=10.13088799
  1602: 1 [  275/ 1327], train_loss/perplexity = 5.77490664/322.1143494 secs/batch = 0.2653s, grad.norm=10.05619240
  1607: 1 [  280/ 1327], train_loss/perplexity = 5.45077991/232.9397736 secs/batch = 0.2656s, grad.norm=9.50670910
  1612: 1 [  285/ 1327], train_loss/perplexity = 5.63219309/279.2739258 secs/batch = 0.2654s, grad.norm=9.96039391
  1617: 1 [  290/ 1327], train_loss/perplexity = 5.54542542/256.0634766 secs/batch = 0.2652s, grad.norm=10.15643597
  1622: 1 [  295/ 1327], train_loss/perplexity = 5.24438190/189.4986572 secs/batch = 0.2668s, grad.norm=9.67664051
  1627: 1 [  300/ 1327], train_loss/perplexity = 4.83817005/126.2381287 secs/batch = 0.2654s, grad.norm=10.36771202
  1632: 1 [  305/ 1327], train_loss/perplexity = 5.30309868/200.9585571 secs/batch = 0.2595s, grad.norm=10.02588367
  1637: 1 [  310/ 1327], train_loss/perplexity = 5.37191296/215.2742767 secs/batch = 0.2651s, grad.norm=9.74953651
  1642: 1 [  315/ 1327], train_loss/perplexity = 5.07765150/160.3969269 secs/batch = 0.2576s, grad.norm=9.86774063
  1647: 1 [  320/ 1327], train_loss/perplexity = 5.28656864/197.6640015 secs/batch = 0.2651s, grad.norm=12.65372181
  1652: 1 [  325/ 1327], train_loss/perplexity = 4.95076084/141.2824097 secs/batch = 0.2623s, grad.norm=10.37796402
  1657: 1 [  330/ 1327], train_loss/perplexity = 5.41992855/225.8629913 secs/batch = 0.2657s, grad.norm=10.98724842
  1662: 1 [  335/ 1327], train_loss/perplexity = 4.73519468/113.8856277 secs/batch = 0.2652s, grad.norm=10.32882118
  1667: 1 [  340/ 1327], train_loss/perplexity = 5.49104261/242.5099182 secs/batch = 0.2628s, grad.norm=10.12665272
  1672: 1 [  345/ 1327], train_loss/perplexity = 5.34583187/209.7322845 secs/batch = 0.2651s, grad.norm=9.95707226
  1677: 1 [  350/ 1327], train_loss/perplexity = 5.47799873/239.3671875 secs/batch = 0.2593s, grad.norm=10.16069317
  1682: 1 [  355/ 1327], train_loss/perplexity = 5.61383915/274.1948853 secs/batch = 0.2652s, grad.norm=10.20042515
  1687: 1 [  360/ 1327], train_loss/perplexity = 5.65825081/286.6467896 secs/batch = 0.2657s, grad.norm=9.75716114
  1692: 1 [  365/ 1327], train_loss/perplexity = 5.52631807/251.2172394 secs/batch = 0.2649s, grad.norm=9.26258850
  1697: 1 [  370/ 1327], train_loss/perplexity = 5.52716923/251.4311523 secs/batch = 0.2651s, grad.norm=10.71504974
  1702: 1 [  375/ 1327], train_loss/perplexity = 4.83766270/126.1741028 secs/batch = 0.2648s, grad.norm=10.62188435
  1707: 1 [  380/ 1327], train_loss/perplexity = 5.17738867/177.2194214 secs/batch = 0.2656s, grad.norm=10.79795837
  1712: 1 [  385/ 1327], train_loss/perplexity = 5.28286171/196.9326324 secs/batch = 0.2657s, grad.norm=10.37320232
  1717: 1 [  390/ 1327], train_loss/perplexity = 5.33875322/208.2528992 secs/batch = 0.2664s, grad.norm=9.90880203
  1722: 1 [  395/ 1327], train_loss/perplexity = 5.57476950/263.6887817 secs/batch = 0.2608s, grad.norm=9.85379219
  1727: 1 [  400/ 1327], train_loss/perplexity = 5.30173731/200.6851654 secs/batch = 0.2658s, grad.norm=10.52976799
  1732: 1 [  405/ 1327], train_loss/perplexity = 5.63128471/279.0203552 secs/batch = 0.2664s, grad.norm=10.77982426
  1737: 1 [  410/ 1327], train_loss/perplexity = 5.34369564/209.2847290 secs/batch = 0.2659s, grad.norm=9.93983269
  1742: 1 [  415/ 1327], train_loss/perplexity = 5.14596653/171.7373962 secs/batch = 0.2658s, grad.norm=10.89161587
  1747: 1 [  420/ 1327], train_loss/perplexity = 5.08567381/161.6888428 secs/batch = 0.2656s, grad.norm=11.48155022
  1752: 1 [  425/ 1327], train_loss/perplexity = 5.32996511/206.4307709 secs/batch = 0.2656s, grad.norm=10.72327423
  1757: 1 [  430/ 1327], train_loss/perplexity = 5.49057150/242.3956909 secs/batch = 0.2645s, grad.norm=10.24167538
  1762: 1 [  435/ 1327], train_loss/perplexity = 5.51692438/248.8684387 secs/batch = 0.2664s, grad.norm=9.99961948
  1767: 1 [  440/ 1327], train_loss/perplexity = 5.29122353/198.5862579 secs/batch = 0.2651s, grad.norm=11.16999817
  1772: 1 [  445/ 1327], train_loss/perplexity = 5.36290836/213.3445282 secs/batch = 0.2662s, grad.norm=11.74486542
  1777: 1 [  450/ 1327], train_loss/perplexity = 5.25378704/191.2893219 secs/batch = 0.2654s, grad.norm=10.38106155
  1782: 1 [  455/ 1327], train_loss/perplexity = 5.04757547/155.6446381 secs/batch = 0.2657s, grad.norm=10.54202461
  1787: 1 [  460/ 1327], train_loss/perplexity = 5.34829235/210.2489624 secs/batch = 0.2656s, grad.norm=11.35229874
  1792: 1 [  465/ 1327], train_loss/perplexity = 5.20494318/182.1705170 secs/batch = 0.2639s, grad.norm=11.49259853
  1797: 1 [  470/ 1327], train_loss/perplexity = 5.51467323/248.3088226 secs/batch = 0.2655s, grad.norm=10.19056034
  1802: 1 [  475/ 1327], train_loss/perplexity = 5.18433905/178.4554596 secs/batch = 0.2652s, grad.norm=10.69638920
  1807: 1 [  480/ 1327], train_loss/perplexity = 5.35583973/211.8417969 secs/batch = 0.2657s, grad.norm=10.39552021
  1812: 1 [  485/ 1327], train_loss/perplexity = 5.20525742/182.2277679 secs/batch = 0.2665s, grad.norm=10.37872696
  1817: 1 [  490/ 1327], train_loss/perplexity = 5.13541412/169.9346771 secs/batch = 0.2622s, grad.norm=11.20529747
  1822: 1 [  495/ 1327], train_loss/perplexity = 5.09561920/163.3049316 secs/batch = 0.2662s, grad.norm=11.28995419
  1827: 1 [  500/ 1327], train_loss/perplexity = 5.50194025/245.1671600 secs/batch = 0.2653s, grad.norm=10.23849487
  1832: 1 [  505/ 1327], train_loss/perplexity = 5.37522078/215.9875488 secs/batch = 0.2640s, grad.norm=10.51827049
  1837: 1 [  510/ 1327], train_loss/perplexity = 5.71153736/302.3355103 secs/batch = 0.2671s, grad.norm=9.55203533
  1842: 1 [  515/ 1327], train_loss/perplexity = 5.37636900/216.2357025 secs/batch = 0.2655s, grad.norm=9.75652885
  1847: 1 [  520/ 1327], train_loss/perplexity = 5.58420992/266.1898804 secs/batch = 0.2651s, grad.norm=10.26663780
  1852: 1 [  525/ 1327], train_loss/perplexity = 5.10569191/164.9581604 secs/batch = 0.2655s, grad.norm=10.27154350
  1857: 1 [  530/ 1327], train_loss/perplexity = 5.22162294/185.2345581 secs/batch = 0.2663s, grad.norm=10.76543522
  1862: 1 [  535/ 1327], train_loss/perplexity = 5.26842403/194.1098175 secs/batch = 0.2660s, grad.norm=10.17393208
  1867: 1 [  540/ 1327], train_loss/perplexity = 5.36999130/214.8609924 secs/batch = 0.2653s, grad.norm=9.71817207
  1872: 1 [  545/ 1327], train_loss/perplexity = 5.50194168/245.1675110 secs/batch = 0.2655s, grad.norm=10.58043575
  1877: 1 [  550/ 1327], train_loss/perplexity = 5.38459349/218.0214539 secs/batch = 0.2635s, grad.norm=10.62359333
  1882: 1 [  555/ 1327], train_loss/perplexity = 5.21770334/184.5099335 secs/batch = 0.2659s, grad.norm=10.52840042
  1887: 1 [  560/ 1327], train_loss/perplexity = 5.30649757/201.6427460 secs/batch = 0.2629s, grad.norm=11.67278767
  1892: 1 [  565/ 1327], train_loss/perplexity = 5.31504107/203.3728638 secs/batch = 0.2656s, grad.norm=11.34161663
  1897: 1 [  570/ 1327], train_loss/perplexity = 5.18161964/177.9708252 secs/batch = 0.2658s, grad.norm=11.60409451
  1902: 1 [  575/ 1327], train_loss/perplexity = 5.14700365/171.9156036 secs/batch = 0.2658s, grad.norm=11.05326271
  1907: 1 [  580/ 1327], train_loss/perplexity = 5.46576309/236.4562225 secs/batch = 0.2663s, grad.norm=11.12582684
  1912: 1 [  585/ 1327], train_loss/perplexity = 5.06971407/159.1288147 secs/batch = 0.2658s, grad.norm=10.88944054
  1917: 1 [  590/ 1327], train_loss/perplexity = 5.31311893/202.9823303 secs/batch = 0.2644s, grad.norm=9.94842625
  1922: 1 [  595/ 1327], train_loss/perplexity = 5.25134754/190.8232422 secs/batch = 0.2666s, grad.norm=10.93667030
  1927: 1 [  600/ 1327], train_loss/perplexity = 5.52188635/250.1063843 secs/batch = 0.2657s, grad.norm=10.37290764
  1932: 1 [  605/ 1327], train_loss/perplexity = 5.47453308/238.5390625 secs/batch = 0.2662s, grad.norm=10.89261055
  1937: 1 [  610/ 1327], train_loss/perplexity = 5.55133963/257.5823975 secs/batch = 0.2657s, grad.norm=10.75749683
  1942: 1 [  615/ 1327], train_loss/perplexity = 5.00285053/148.8368225 secs/batch = 0.2642s, grad.norm=10.32157421
  1947: 1 [  620/ 1327], train_loss/perplexity = 5.38987160/219.1752472 secs/batch = 0.2664s, grad.norm=11.10342979
  1952: 1 [  625/ 1327], train_loss/perplexity = 5.46857357/237.1217194 secs/batch = 0.2641s, grad.norm=9.81645584
  1957: 1 [  630/ 1327], train_loss/perplexity = 5.52292347/250.3659058 secs/batch = 0.2656s, grad.norm=10.07128429
  1962: 1 [  635/ 1327], train_loss/perplexity = 5.24543667/189.6986389 secs/batch = 0.2658s, grad.norm=10.49367237
  1967: 1 [  640/ 1327], train_loss/perplexity = 5.34685659/209.9473114 secs/batch = 0.2655s, grad.norm=10.91926670
  1972: 1 [  645/ 1327], train_loss/perplexity = 5.49018431/242.3018646 secs/batch = 0.2655s, grad.norm=11.00015450
  1977: 1 [  650/ 1327], train_loss/perplexity = 5.14943361/172.3338623 secs/batch = 0.2644s, grad.norm=11.51488495
  1982: 1 [  655/ 1327], train_loss/perplexity = 5.23198509/187.1639709 secs/batch = 0.2653s, grad.norm=10.68376255
  1987: 1 [  660/ 1327], train_loss/perplexity = 5.08189678/161.0792999 secs/batch = 0.2661s, grad.norm=10.43962860
  1992: 1 [  665/ 1327], train_loss/perplexity = 5.39786386/220.9339600 secs/batch = 0.2650s, grad.norm=10.75809765
  1997: 1 [  670/ 1327], train_loss/perplexity = 5.17948055/177.5905457 secs/batch = 0.2645s, grad.norm=10.63352299
  2002: 1 [  675/ 1327], train_loss/perplexity = 4.95249796/141.5280609 secs/batch = 0.2642s, grad.norm=11.45364189
  2007: 1 [  680/ 1327], train_loss/perplexity = 5.32988596/206.4144287 secs/batch = 0.2652s, grad.norm=11.11182785
  2012: 1 [  685/ 1327], train_loss/perplexity = 5.25474930/191.4734802 secs/batch = 0.2650s, grad.norm=11.19139957
  2017: 1 [  690/ 1327], train_loss/perplexity = 5.42728090/227.5297241 secs/batch = 0.2640s, grad.norm=10.49323082
  2022: 1 [  695/ 1327], train_loss/perplexity = 5.18349361/178.3046570 secs/batch = 0.2647s, grad.norm=10.60234547
  2027: 1 [  700/ 1327], train_loss/perplexity = 5.43280983/228.7912140 secs/batch = 0.2659s, grad.norm=10.13201427
  2032: 1 [  705/ 1327], train_loss/perplexity = 5.10991669/165.6565552 secs/batch = 0.2651s, grad.norm=10.65120506
  2037: 1 [  710/ 1327], train_loss/perplexity = 5.22927380/186.6571960 secs/batch = 0.2617s, grad.norm=10.81058407
  2042: 1 [  715/ 1327], train_loss/perplexity = 5.12928915/168.8970184 secs/batch = 0.2655s, grad.norm=10.43672371
  2047: 1 [  720/ 1327], train_loss/perplexity = 5.21954107/184.8493347 secs/batch = 0.2663s, grad.norm=10.63414574
  2052: 1 [  725/ 1327], train_loss/perplexity = 5.00302982/148.8635101 secs/batch = 0.2653s, grad.norm=11.19528294
  2057: 1 [  730/ 1327], train_loss/perplexity = 5.20403671/182.0054626 secs/batch = 0.2654s, grad.norm=10.82129002
  2062: 1 [  735/ 1327], train_loss/perplexity = 5.37839031/216.6732178 secs/batch = 0.2641s, grad.norm=10.97729492
  2067: 1 [  740/ 1327], train_loss/perplexity = 4.68599987/108.4186249 secs/batch = 0.2662s, grad.norm=11.27873516
  2072: 1 [  745/ 1327], train_loss/perplexity = 5.22535658/185.9274597 secs/batch = 0.2656s, grad.norm=10.67199802
  2077: 1 [  750/ 1327], train_loss/perplexity = 5.08186722/161.0745392 secs/batch = 0.2649s, grad.norm=11.38195229
  2082: 1 [  755/ 1327], train_loss/perplexity = 5.08103275/160.9401855 secs/batch = 0.2650s, grad.norm=10.64955044
  2087: 1 [  760/ 1327], train_loss/perplexity = 5.00206089/148.7193451 secs/batch = 0.2657s, grad.norm=11.33226204
  2092: 1 [  765/ 1327], train_loss/perplexity = 5.10384989/164.6545868 secs/batch = 0.2661s, grad.norm=10.99221992
  2097: 1 [  770/ 1327], train_loss/perplexity = 5.00206900/148.7205505 secs/batch = 0.2648s, grad.norm=10.79884148
  2102: 1 [  775/ 1327], train_loss/perplexity = 5.11710930/166.8523560 secs/batch = 0.2650s, grad.norm=10.94874382
  2107: 1 [  780/ 1327], train_loss/perplexity = 5.44785070/232.2584381 secs/batch = 0.2663s, grad.norm=10.54474831
  2112: 1 [  785/ 1327], train_loss/perplexity = 5.18009520/177.6997223 secs/batch = 0.2650s, grad.norm=11.21814823
  2117: 1 [  790/ 1327], train_loss/perplexity = 5.04331779/154.9833679 secs/batch = 0.2649s, grad.norm=11.86480904
  2122: 1 [  795/ 1327], train_loss/perplexity = 5.36075115/212.8847961 secs/batch = 0.2658s, grad.norm=10.94432926
  2127: 1 [  800/ 1327], train_loss/perplexity = 5.28551483/197.4558105 secs/batch = 0.2665s, grad.norm=11.66262341
  2132: 1 [  805/ 1327], train_loss/perplexity = 5.58731508/267.0177307 secs/batch = 0.2654s, grad.norm=10.93569851
  2137: 1 [  810/ 1327], train_loss/perplexity = 5.23836231/188.3613739 secs/batch = 0.2635s, grad.norm=11.12160110
  2142: 1 [  815/ 1327], train_loss/perplexity = 5.12183809/167.6432343 secs/batch = 0.2656s, grad.norm=10.48274994
  2147: 1 [  820/ 1327], train_loss/perplexity = 4.82434511/124.5049057 secs/batch = 0.2647s, grad.norm=10.83959007
  2152: 1 [  825/ 1327], train_loss/perplexity = 5.04290152/154.9188690 secs/batch = 0.2640s, grad.norm=10.56323338
  2157: 1 [  830/ 1327], train_loss/perplexity = 4.84707594/127.3674164 secs/batch = 0.2595s, grad.norm=10.81108665
  2162: 1 [  835/ 1327], train_loss/perplexity = 5.15617037/173.4987488 secs/batch = 0.2654s, grad.norm=10.97672749
  2167: 1 [  840/ 1327], train_loss/perplexity = 5.26684856/193.8042450 secs/batch = 0.2653s, grad.norm=10.80753517
  2172: 1 [  845/ 1327], train_loss/perplexity = 5.09808445/163.7080231 secs/batch = 0.2644s, grad.norm=10.57914925
  2177: 1 [  850/ 1327], train_loss/perplexity = 5.16420794/174.8988800 secs/batch = 0.2593s, grad.norm=11.33219910
  2182: 1 [  855/ 1327], train_loss/perplexity = 5.15239811/172.8454895 secs/batch = 0.2654s, grad.norm=10.98273563
  2187: 1 [  860/ 1327], train_loss/perplexity = 4.82051182/124.0285568 secs/batch = 0.2648s, grad.norm=11.02688980
  2192: 1 [  865/ 1327], train_loss/perplexity = 5.30090809/200.5188141 secs/batch = 0.2631s, grad.norm=11.04157162
  2197: 1 [  870/ 1327], train_loss/perplexity = 5.32175732/204.7433624 secs/batch = 0.2645s, grad.norm=11.04531288
  2202: 1 [  875/ 1327], train_loss/perplexity = 4.84540129/127.1542969 secs/batch = 0.2643s, grad.norm=11.27755070
  2207: 1 [  880/ 1327], train_loss/perplexity = 4.99552631/147.7506866 secs/batch = 0.2653s, grad.norm=10.60226154
  2212: 1 [  885/ 1327], train_loss/perplexity = 5.09337378/162.9386597 secs/batch = 0.2652s, grad.norm=11.04604530
  2217: 1 [  890/ 1327], train_loss/perplexity = 5.27719307/195.8194580 secs/batch = 0.2650s, grad.norm=10.55020809
  2222: 1 [  895/ 1327], train_loss/perplexity = 5.36303186/213.3708801 secs/batch = 0.2648s, grad.norm=10.69471741
  2227: 1 [  900/ 1327], train_loss/perplexity = 5.18516588/178.6030731 secs/batch = 0.2650s, grad.norm=10.66926575
  2232: 1 [  905/ 1327], train_loss/perplexity = 5.04715681/155.5794983 secs/batch = 0.2656s, grad.norm=11.58606815
  2237: 1 [  910/ 1327], train_loss/perplexity = 5.14910173/172.2766724 secs/batch = 0.2656s, grad.norm=10.48183823
  2242: 1 [  915/ 1327], train_loss/perplexity = 5.35763407/212.2222443 secs/batch = 0.2647s, grad.norm=10.61477566
  2247: 1 [  920/ 1327], train_loss/perplexity = 5.44304180/231.1442108 secs/batch = 0.2663s, grad.norm=10.87455463
  2252: 1 [  925/ 1327], train_loss/perplexity = 5.22431326/185.7335815 secs/batch = 0.2663s, grad.norm=10.92037773
  2257: 1 [  930/ 1327], train_loss/perplexity = 5.13726759/170.2499390 secs/batch = 0.2660s, grad.norm=11.01693916
  2262: 1 [  935/ 1327], train_loss/perplexity = 5.19200611/179.8289490 secs/batch = 0.2627s, grad.norm=10.58742142
  2267: 1 [  940/ 1327], train_loss/perplexity = 5.20502567/182.1855469 secs/batch = 0.2649s, grad.norm=11.53782082
  2272: 1 [  945/ 1327], train_loss/perplexity = 5.42481327/226.9689636 secs/batch = 0.2640s, grad.norm=11.17702103
  2277: 1 [  950/ 1327], train_loss/perplexity = 5.14109135/170.9021759 secs/batch = 0.2648s, grad.norm=10.89954472
  2282: 1 [  955/ 1327], train_loss/perplexity = 5.24787378/190.1615143 secs/batch = 0.2656s, grad.norm=10.55480766
  2287: 1 [  960/ 1327], train_loss/perplexity = 5.50411749/245.7015228 secs/batch = 0.2654s, grad.norm=10.75687218
  2292: 1 [  965/ 1327], train_loss/perplexity = 5.24266434/189.1734467 secs/batch = 0.2642s, grad.norm=10.88875771
  2297: 1 [  970/ 1327], train_loss/perplexity = 5.42274618/226.5002747 secs/batch = 0.2632s, grad.norm=10.12512684
  2302: 1 [  975/ 1327], train_loss/perplexity = 5.18908358/179.3041534 secs/batch = 0.2647s, grad.norm=12.03663826
  2307: 1 [  980/ 1327], train_loss/perplexity = 5.02034473/151.4635162 secs/batch = 0.2640s, grad.norm=10.91083622
  2312: 1 [  985/ 1327], train_loss/perplexity = 5.21156883/183.3815308 secs/batch = 0.2657s, grad.norm=11.58372402
  2317: 1 [  990/ 1327], train_loss/perplexity = 5.28655529/197.6613617 secs/batch = 0.2632s, grad.norm=10.79225731
  2322: 1 [  995/ 1327], train_loss/perplexity = 5.36292934/213.3489990 secs/batch = 0.2655s, grad.norm=10.38457680
  2327: 1 [ 1000/ 1327], train_loss/perplexity = 4.75544596/116.2154694 secs/batch = 0.2654s, grad.norm=10.53103161
  2332: 1 [ 1005/ 1327], train_loss/perplexity = 5.32545519/205.5018768 secs/batch = 0.2634s, grad.norm=11.65800190
  2337: 1 [ 1010/ 1327], train_loss/perplexity = 4.81562901/123.4244232 secs/batch = 0.2655s, grad.norm=10.57318687
  2342: 1 [ 1015/ 1327], train_loss/perplexity = 5.33029461/206.4987946 secs/batch = 0.2636s, grad.norm=10.94389153
  2347: 1 [ 1020/ 1327], train_loss/perplexity = 5.55482817/258.4825439 secs/batch = 0.2657s, grad.norm=10.55198193
  2352: 1 [ 1025/ 1327], train_loss/perplexity = 5.24205685/189.0585632 secs/batch = 0.2667s, grad.norm=10.16958904
  2357: 1 [ 1030/ 1327], train_loss/perplexity = 5.16062641/174.2735901 secs/batch = 0.2659s, grad.norm=10.98256302
  2362: 1 [ 1035/ 1327], train_loss/perplexity = 4.98273802/145.8732452 secs/batch = 0.2653s, grad.norm=10.51203156
  2367: 1 [ 1040/ 1327], train_loss/perplexity = 5.32425690/205.2557831 secs/batch = 0.2647s, grad.norm=10.39424419
  2372: 1 [ 1045/ 1327], train_loss/perplexity = 4.95462847/141.8298950 secs/batch = 0.2655s, grad.norm=10.60200310
  2377: 1 [ 1050/ 1327], train_loss/perplexity = 4.93395758/138.9282379 secs/batch = 0.2628s, grad.norm=10.81022644
  2382: 1 [ 1055/ 1327], train_loss/perplexity = 5.15204430/172.7843475 secs/batch = 0.2650s, grad.norm=11.62238026
  2387: 1 [ 1060/ 1327], train_loss/perplexity = 4.82745647/124.8928909 secs/batch = 0.2654s, grad.norm=12.35866737
  2392: 1 [ 1065/ 1327], train_loss/perplexity = 4.87197542/130.5786133 secs/batch = 0.2656s, grad.norm=10.83211422
  2397: 1 [ 1070/ 1327], train_loss/perplexity = 5.29106855/198.5554810 secs/batch = 0.2660s, grad.norm=11.45810032
  2402: 1 [ 1075/ 1327], train_loss/perplexity = 5.03963947/154.4143372 secs/batch = 0.2659s, grad.norm=11.78627491
  2407: 1 [ 1080/ 1327], train_loss/perplexity = 4.89273214/133.3173218 secs/batch = 0.2675s, grad.norm=11.33319283
  2412: 1 [ 1085/ 1327], train_loss/perplexity = 4.77621174/118.6540070 secs/batch = 0.2658s, grad.norm=11.12178230
  2417: 1 [ 1090/ 1327], train_loss/perplexity = 5.06586123/158.5169067 secs/batch = 0.2647s, grad.norm=11.55400467
  2422: 1 [ 1095/ 1327], train_loss/perplexity = 5.11823750/167.0406952 secs/batch = 0.2658s, grad.norm=11.45714092
  2427: 1 [ 1100/ 1327], train_loss/perplexity = 5.08815670/162.0908051 secs/batch = 0.2654s, grad.norm=12.18242931
  2432: 1 [ 1105/ 1327], train_loss/perplexity = 4.87961006/131.5793457 secs/batch = 0.2645s, grad.norm=11.05379200
  2437: 1 [ 1110/ 1327], train_loss/perplexity = 5.49472189/243.4038239 secs/batch = 0.2652s, grad.norm=11.43546486
  2442: 1 [ 1115/ 1327], train_loss/perplexity = 4.91898823/136.8640747 secs/batch = 0.2661s, grad.norm=11.41235924
  2447: 1 [ 1120/ 1327], train_loss/perplexity = 5.14316797/171.2574463 secs/batch = 0.2644s, grad.norm=11.08089161
  2452: 1 [ 1125/ 1327], train_loss/perplexity = 5.34974718/210.5550537 secs/batch = 0.2651s, grad.norm=11.19609451
  2457: 1 [ 1130/ 1327], train_loss/perplexity = 5.01627636/150.8485565 secs/batch = 0.2658s, grad.norm=11.06453323
  2462: 1 [ 1135/ 1327], train_loss/perplexity = 5.04922628/155.9017944 secs/batch = 0.2657s, grad.norm=11.00182629
  2467: 1 [ 1140/ 1327], train_loss/perplexity = 5.33371067/207.2054138 secs/batch = 0.2660s, grad.norm=11.10863781
  2472: 1 [ 1145/ 1327], train_loss/perplexity = 5.09014225/162.4129639 secs/batch = 0.2625s, grad.norm=11.40527916
  2477: 1 [ 1150/ 1327], train_loss/perplexity = 5.05417490/156.6752014 secs/batch = 0.2671s, grad.norm=11.42390060
  2482: 1 [ 1155/ 1327], train_loss/perplexity = 5.14632511/171.7989807 secs/batch = 0.2659s, grad.norm=11.05291939
  2487: 1 [ 1160/ 1327], train_loss/perplexity = 5.16461658/174.9703522 secs/batch = 0.2658s, grad.norm=10.94703579
  2492: 1 [ 1165/ 1327], train_loss/perplexity = 5.23926163/188.5308380 secs/batch = 0.2662s, grad.norm=11.20834255
  2497: 1 [ 1170/ 1327], train_loss/perplexity = 5.14864922/172.1987305 secs/batch = 0.2647s, grad.norm=11.31337738
  2502: 1 [ 1175/ 1327], train_loss/perplexity = 4.80567408/122.2018356 secs/batch = 0.2644s, grad.norm=11.68624592
  2507: 1 [ 1180/ 1327], train_loss/perplexity = 4.83178139/125.4342117 secs/batch = 0.2639s, grad.norm=11.86044979
  2512: 1 [ 1185/ 1327], train_loss/perplexity = 5.05394745/156.6395721 secs/batch = 0.2661s, grad.norm=11.51930237
  2517: 1 [ 1190/ 1327], train_loss/perplexity = 5.05979252/157.5578156 secs/batch = 0.2672s, grad.norm=11.17641068
  2522: 1 [ 1195/ 1327], train_loss/perplexity = 4.95054674/141.2521667 secs/batch = 0.2669s, grad.norm=10.95127296
  2527: 1 [ 1200/ 1327], train_loss/perplexity = 4.85611963/128.5245056 secs/batch = 0.2649s, grad.norm=11.01164532
  2532: 1 [ 1205/ 1327], train_loss/perplexity = 4.97776890/145.1501770 secs/batch = 0.2652s, grad.norm=11.38679886
  2537: 1 [ 1210/ 1327], train_loss/perplexity = 4.62442493/101.9441299 secs/batch = 0.2616s, grad.norm=11.43201065
  2542: 1 [ 1215/ 1327], train_loss/perplexity = 4.74047709/114.4888077 secs/batch = 0.2645s, grad.norm=10.78130722
  2547: 1 [ 1220/ 1327], train_loss/perplexity = 4.93965912/139.7226105 secs/batch = 0.2632s, grad.norm=11.43942165
  2552: 1 [ 1225/ 1327], train_loss/perplexity = 4.76128912/116.8965225 secs/batch = 0.2624s, grad.norm=12.43491459
  2557: 1 [ 1230/ 1327], train_loss/perplexity = 5.00365162/148.9561005 secs/batch = 0.2658s, grad.norm=11.75243092
  2562: 1 [ 1235/ 1327], train_loss/perplexity = 4.99444485/147.5909882 secs/batch = 0.2664s, grad.norm=11.60724449
  2567: 1 [ 1240/ 1327], train_loss/perplexity = 5.05020905/156.0550842 secs/batch = 0.2647s, grad.norm=10.99087811
  2572: 1 [ 1245/ 1327], train_loss/perplexity = 4.93808365/139.5026550 secs/batch = 0.2656s, grad.norm=10.96351910
  2577: 1 [ 1250/ 1327], train_loss/perplexity = 5.11533833/166.5571289 secs/batch = 0.2648s, grad.norm=10.89792538
  2582: 1 [ 1255/ 1327], train_loss/perplexity = 5.08976173/162.3511810 secs/batch = 0.2615s, grad.norm=10.95644855
  2587: 1 [ 1260/ 1327], train_loss/perplexity = 5.01425505/150.5439453 secs/batch = 0.2665s, grad.norm=11.93374062
  2592: 1 [ 1265/ 1327], train_loss/perplexity = 5.10569239/164.9582520 secs/batch = 0.2652s, grad.norm=10.97536373
  2597: 1 [ 1270/ 1327], train_loss/perplexity = 4.90785265/135.3484650 secs/batch = 0.2652s, grad.norm=11.51666641
  2602: 1 [ 1275/ 1327], train_loss/perplexity = 5.14781904/172.0558319 secs/batch = 0.2656s, grad.norm=11.55450535
  2607: 1 [ 1280/ 1327], train_loss/perplexity = 4.88779831/132.6611786 secs/batch = 0.2662s, grad.norm=11.63718510
  2612: 1 [ 1285/ 1327], train_loss/perplexity = 4.92266893/137.3687592 secs/batch = 0.2659s, grad.norm=11.46123123
  2617: 1 [ 1290/ 1327], train_loss/perplexity = 5.07954597/160.7010803 secs/batch = 0.2663s, grad.norm=11.12903214
  2622: 1 [ 1295/ 1327], train_loss/perplexity = 5.09303856/162.8840485 secs/batch = 0.2661s, grad.norm=11.48313618
  2627: 1 [ 1300/ 1327], train_loss/perplexity = 5.18136692/177.9258575 secs/batch = 0.2654s, grad.norm=10.87112904
  2632: 1 [ 1305/ 1327], train_loss/perplexity = 5.39699125/220.7412567 secs/batch = 0.2654s, grad.norm=11.25730515
  2637: 1 [ 1310/ 1327], train_loss/perplexity = 5.56582546/261.3408508 secs/batch = 0.2653s, grad.norm=11.09735298
  2642: 1 [ 1315/ 1327], train_loss/perplexity = 5.37630939/216.2228088 secs/batch = 0.2654s, grad.norm=11.00229454
  2647: 1 [ 1320/ 1327], train_loss/perplexity = 5.34396791/209.3417206 secs/batch = 0.2645s, grad.norm=10.84715462
  2652: 1 [ 1325/ 1327], train_loss/perplexity = 5.19930840/181.1469116 secs/batch = 0.2651s, grad.norm=10.98949146
Epoch training time: 352.0240800380707
	> validation loss = 5.27838802, perplexity = 196.05358887
	> validation loss = 5.14842892, perplexity = 172.16079712
	> validation loss = 5.04377460, perplexity = 155.05418396
	> validation loss = 5.11603260, perplexity = 166.67280579
	> validation loss = 5.39880037, perplexity = 221.14097595
	> validation loss = 5.15120125, perplexity = 172.63874817
	> validation loss = 5.10636520, perplexity = 165.06927490
	> validation loss = 5.04344416, perplexity = 155.00294495
	> validation loss = 5.20288181, perplexity = 181.79539490
	> validation loss = 5.04908466, perplexity = 155.87971497
	> validation loss = 5.12868595, perplexity = 168.79516602
	> validation loss = 5.13530207, perplexity = 169.91563416
	> validation loss = 5.03156328, perplexity = 153.17227173
	> validation loss = 5.00809860, perplexity = 149.61997986
	> validation loss = 4.75311756, perplexity = 115.94519043
	> validation loss = 4.79599476, perplexity = 121.02471161
	> validation loss = 5.19596291, perplexity = 180.54190063
	> validation loss = 4.82320166, perplexity = 124.36261749
	> validation loss = 5.18189335, perplexity = 178.01954651
	> validation loss = 5.14413118, perplexity = 171.42248535
	> validation loss = 4.95182657, perplexity = 141.43305969
at the end of epoch: 1
train loss = 5.20450403, perplexity = 182.09053886
validation loss = 5.06740506, perplexity = 158.76181503
Saved model cv/epoch001_5.0674.model
  2659: 2 [    5/ 1327], train_loss/perplexity = 5.38129711/217.3039551 secs/batch = 0.2654s, grad.norm=10.59868813
  2664: 2 [   10/ 1327], train_loss/perplexity = 5.00038767/148.4707031 secs/batch = 0.2643s, grad.norm=12.53804684
  2669: 2 [   15/ 1327], train_loss/perplexity = 5.07974529/160.7331085 secs/batch = 0.2635s, grad.norm=10.66180325
  2674: 2 [   20/ 1327], train_loss/perplexity = 5.31687832/203.7468567 secs/batch = 0.2649s, grad.norm=11.98856258
  2679: 2 [   25/ 1327], train_loss/perplexity = 5.24614620/189.8332825 secs/batch = 0.2653s, grad.norm=11.20705700
  2684: 2 [   30/ 1327], train_loss/perplexity = 5.16924238/175.7816162 secs/batch = 0.2656s, grad.norm=11.40979004
  2689: 2 [   35/ 1327], train_loss/perplexity = 4.96249199/142.9495850 secs/batch = 0.2658s, grad.norm=10.92348671
  2694: 2 [   40/ 1327], train_loss/perplexity = 5.08538628/161.6423645 secs/batch = 0.2663s, grad.norm=11.50329494
  2699: 2 [   45/ 1327], train_loss/perplexity = 4.80097485/121.6289291 secs/batch = 0.2651s, grad.norm=11.20344448
  2704: 2 [   50/ 1327], train_loss/perplexity = 5.05753708/157.2028656 secs/batch = 0.2656s, grad.norm=11.13069534
  2709: 2 [   55/ 1327], train_loss/perplexity = 5.05537701/156.8636627 secs/batch = 0.2631s, grad.norm=11.62826633
  2714: 2 [   60/ 1327], train_loss/perplexity = 5.28154707/196.6739044 secs/batch = 0.2648s, grad.norm=11.64456177
  2719: 2 [   65/ 1327], train_loss/perplexity = 4.83275461/125.5563431 secs/batch = 0.2660s, grad.norm=11.29327297
  2724: 2 [   70/ 1327], train_loss/perplexity = 4.73147583/113.4628906 secs/batch = 0.2652s, grad.norm=11.77032185
  2729: 2 [   75/ 1327], train_loss/perplexity = 4.60932016/100.4158630 secs/batch = 0.2659s, grad.norm=11.82130814
  2734: 2 [   80/ 1327], train_loss/perplexity = 5.07062149/159.2732849 secs/batch = 0.2654s, grad.norm=12.54651165
  2739: 2 [   85/ 1327], train_loss/perplexity = 5.03628159/153.8966980 secs/batch = 0.2653s, grad.norm=11.77394199
  2744: 2 [   90/ 1327], train_loss/perplexity = 5.03463602/153.6436615 secs/batch = 0.2655s, grad.norm=11.76477718
  2749: 2 [   95/ 1327], train_loss/perplexity = 4.84994221/127.7330093 secs/batch = 0.2658s, grad.norm=11.40520000
  2754: 2 [  100/ 1327], train_loss/perplexity = 5.20968533/183.0364532 secs/batch = 0.2657s, grad.norm=11.16407776
  2759: 2 [  105/ 1327], train_loss/perplexity = 5.25230074/191.0052185 secs/batch = 0.2624s, grad.norm=11.84668732
  2764: 2 [  110/ 1327], train_loss/perplexity = 4.99203587/147.2358704 secs/batch = 0.2651s, grad.norm=10.77825642
  2769: 2 [  115/ 1327], train_loss/perplexity = 4.89669561/133.8467712 secs/batch = 0.2639s, grad.norm=12.10727215
  2774: 2 [  120/ 1327], train_loss/perplexity = 5.01622248/150.8404236 secs/batch = 0.2669s, grad.norm=11.59421062
  2779: 2 [  125/ 1327], train_loss/perplexity = 5.04737043/155.6127319 secs/batch = 0.2622s, grad.norm=11.69306469
  2784: 2 [  130/ 1327], train_loss/perplexity = 4.94842672/140.9530334 secs/batch = 0.2655s, grad.norm=11.85654545
  2789: 2 [  135/ 1327], train_loss/perplexity = 4.99523067/147.7070160 secs/batch = 0.2648s, grad.norm=10.98632050
  2794: 2 [  140/ 1327], train_loss/perplexity = 5.27017784/194.4505463 secs/batch = 0.2649s, grad.norm=11.17904854
  2799: 2 [  145/ 1327], train_loss/perplexity = 5.26193094/192.8535156 secs/batch = 0.2666s, grad.norm=12.03150463
  2804: 2 [  150/ 1327], train_loss/perplexity = 5.11812162/167.0213470 secs/batch = 0.2606s, grad.norm=11.14822292
  2809: 2 [  155/ 1327], train_loss/perplexity = 5.36473417/213.7344055 secs/batch = 0.2661s, grad.norm=11.10767078
  2814: 2 [  160/ 1327], train_loss/perplexity = 5.05779123/157.2428131 secs/batch = 0.2661s, grad.norm=11.00187588
  2819: 2 [  165/ 1327], train_loss/perplexity = 5.28853464/198.0529938 secs/batch = 0.2654s, grad.norm=11.00829315
  2824: 2 [  170/ 1327], train_loss/perplexity = 5.02744198/152.5423126 secs/batch = 0.2646s, grad.norm=10.86717129
  2829: 2 [  175/ 1327], train_loss/perplexity = 5.30695677/201.7353668 secs/batch = 0.2654s, grad.norm=11.46243858
  2834: 2 [  180/ 1327], train_loss/perplexity = 5.16183567/174.4844513 secs/batch = 0.2652s, grad.norm=11.34654236
  2839: 2 [  185/ 1327], train_loss/perplexity = 5.44821024/232.3419495 secs/batch = 0.2657s, grad.norm=11.57062721
  2844: 2 [  190/ 1327], train_loss/perplexity = 4.90491247/134.9510956 secs/batch = 0.2661s, grad.norm=11.40994644
  2849: 2 [  195/ 1327], train_loss/perplexity = 5.15965509/174.1044006 secs/batch = 0.2631s, grad.norm=10.61797047
  2854: 2 [  200/ 1327], train_loss/perplexity = 5.15864563/173.9287262 secs/batch = 0.2658s, grad.norm=11.15660381
  2859: 2 [  205/ 1327], train_loss/perplexity = 5.17053080/176.0082397 secs/batch = 0.2652s, grad.norm=11.14106750
  2864: 2 [  210/ 1327], train_loss/perplexity = 5.11850071/167.0846710 secs/batch = 0.2586s, grad.norm=10.61992836
  2869: 2 [  215/ 1327], train_loss/perplexity = 5.21648884/184.2859955 secs/batch = 0.2653s, grad.norm=10.86345005
  2874: 2 [  220/ 1327], train_loss/perplexity = 5.24939537/190.4510803 secs/batch = 0.2655s, grad.norm=11.20904160
  2879: 2 [  225/ 1327], train_loss/perplexity = 5.41315174/224.3375244 secs/batch = 0.2654s, grad.norm=10.65559673
  2884: 2 [  230/ 1327], train_loss/perplexity = 5.20579481/182.3257294 secs/batch = 0.2652s, grad.norm=11.34757519
  2889: 2 [  235/ 1327], train_loss/perplexity = 5.08021927/160.8093109 secs/batch = 0.2602s, grad.norm=11.33046627
  2894: 2 [  240/ 1327], train_loss/perplexity = 4.92621136/137.8562317 secs/batch = 0.2652s, grad.norm=12.37617493
  2899: 2 [  245/ 1327], train_loss/perplexity = 5.20119905/181.4897308 secs/batch = 0.2645s, grad.norm=10.99398518
  2904: 2 [  250/ 1327], train_loss/perplexity = 4.93019390/138.4063416 secs/batch = 0.2659s, grad.norm=10.49284267
  2909: 2 [  255/ 1327], train_loss/perplexity = 4.96357536/143.1045380 secs/batch = 0.2650s, grad.norm=11.12204552
  2914: 2 [  260/ 1327], train_loss/perplexity = 5.35522175/211.7109222 secs/batch = 0.2645s, grad.norm=12.16742897
  2919: 2 [  265/ 1327], train_loss/perplexity = 5.29063988/198.4703827 secs/batch = 0.2662s, grad.norm=10.39900780
  2924: 2 [  270/ 1327], train_loss/perplexity = 5.38202953/217.4631805 secs/batch = 0.2669s, grad.norm=10.90364742
  2929: 2 [  275/ 1327], train_loss/perplexity = 5.49124289/242.5584869 secs/batch = 0.2658s, grad.norm=11.49107456
  2934: 2 [  280/ 1327], train_loss/perplexity = 5.16397858/174.8587646 secs/batch = 0.2654s, grad.norm=11.35000610
  2939: 2 [  285/ 1327], train_loss/perplexity = 5.38297844/217.6696320 secs/batch = 0.2667s, grad.norm=11.28647995
  2944: 2 [  290/ 1327], train_loss/perplexity = 5.26191044/192.8495636 secs/batch = 0.2657s, grad.norm=11.45295906
  2949: 2 [  295/ 1327], train_loss/perplexity = 5.02797651/152.6238708 secs/batch = 0.2663s, grad.norm=10.99079609
  2954: 2 [  300/ 1327], train_loss/perplexity = 4.59786844/99.2724838 secs/batch = 0.2652s, grad.norm=11.40101242
  2959: 2 [  305/ 1327], train_loss/perplexity = 5.06601763/158.5417023 secs/batch = 0.2665s, grad.norm=11.04824066
  2964: 2 [  310/ 1327], train_loss/perplexity = 5.04406452/155.0991364 secs/batch = 0.2660s, grad.norm=11.39048672
  2969: 2 [  315/ 1327], train_loss/perplexity = 4.75579166/116.2556534 secs/batch = 0.2657s, grad.norm=11.88909245
  2974: 2 [  320/ 1327], train_loss/perplexity = 4.72624254/112.8706589 secs/batch = 0.2660s, grad.norm=13.80066490
  2979: 2 [  325/ 1327], train_loss/perplexity = 4.57679176/97.2020493 secs/batch = 0.2653s, grad.norm=11.12935734
  2984: 2 [  330/ 1327], train_loss/perplexity = 5.09896278/163.8518677 secs/batch = 0.2671s, grad.norm=12.28187943
  2989: 2 [  335/ 1327], train_loss/perplexity = 4.48276615/88.4790802 secs/batch = 0.2657s, grad.norm=11.34043503
  2994: 2 [  340/ 1327], train_loss/perplexity = 5.19903040/181.0965729 secs/batch = 0.2671s, grad.norm=10.70300102
  2999: 2 [  345/ 1327], train_loss/perplexity = 5.12020206/167.3691864 secs/batch = 0.2604s, grad.norm=10.90209675
  3004: 2 [  350/ 1327], train_loss/perplexity = 5.16799974/175.5633087 secs/batch = 0.2664s, grad.norm=11.81832886
  3009: 2 [  355/ 1327], train_loss/perplexity = 5.24647045/189.8948364 secs/batch = 0.2672s, grad.norm=11.71707249
  3014: 2 [  360/ 1327], train_loss/perplexity = 5.34370136/209.2859192 secs/batch = 0.2661s, grad.norm=10.92780209
  3019: 2 [  365/ 1327], train_loss/perplexity = 5.23110962/187.0001831 secs/batch = 0.2664s, grad.norm=11.10475731
  3024: 2 [  370/ 1327], train_loss/perplexity = 5.23940277/188.5574493 secs/batch = 0.2639s, grad.norm=11.98696995
  3029: 2 [  375/ 1327], train_loss/perplexity = 4.55521679/95.1273727 secs/batch = 0.2664s, grad.norm=11.77050686
  3034: 2 [  380/ 1327], train_loss/perplexity = 4.82345486/124.3941116 secs/batch = 0.2684s, grad.norm=12.00106525
  3039: 2 [  385/ 1327], train_loss/perplexity = 5.02528572/152.2137451 secs/batch = 0.2608s, grad.norm=11.62841129
  3044: 2 [  390/ 1327], train_loss/perplexity = 5.05571365/156.9164734 secs/batch = 0.2656s, grad.norm=11.40604019
  3049: 2 [  395/ 1327], train_loss/perplexity = 5.23917150/188.5138550 secs/batch = 0.2645s, grad.norm=11.80484104
  3054: 2 [  400/ 1327], train_loss/perplexity = 5.04854536/155.7956696 secs/batch = 0.2665s, grad.norm=11.41665173
  3059: 2 [  405/ 1327], train_loss/perplexity = 5.42227077/226.3926239 secs/batch = 0.2659s, grad.norm=11.37347507
  3064: 2 [  410/ 1327], train_loss/perplexity = 5.00558376/149.2441864 secs/batch = 0.2661s, grad.norm=11.46809387
  3069: 2 [  415/ 1327], train_loss/perplexity = 4.92020559/137.0307770 secs/batch = 0.2653s, grad.norm=11.41341972
  3074: 2 [  420/ 1327], train_loss/perplexity = 4.70520163/110.5205688 secs/batch = 0.2651s, grad.norm=12.18851852
  3079: 2 [  425/ 1327], train_loss/perplexity = 4.95479918/141.8541107 secs/batch = 0.2653s, grad.norm=12.09428883
  3084: 2 [  430/ 1327], train_loss/perplexity = 5.16713953/175.4123535 secs/batch = 0.2612s, grad.norm=11.88921547
  3089: 2 [  435/ 1327], train_loss/perplexity = 5.20169544/181.5798340 secs/batch = 0.2654s, grad.norm=11.67484093
  3094: 2 [  440/ 1327], train_loss/perplexity = 4.94378614/140.3004456 secs/batch = 0.2649s, grad.norm=13.24310493
  3099: 2 [  445/ 1327], train_loss/perplexity = 5.10293102/164.5033569 secs/batch = 0.2659s, grad.norm=12.59447384
  3104: 2 [  450/ 1327], train_loss/perplexity = 5.02921200/152.8125458 secs/batch = 0.2650s, grad.norm=11.86254978
  3109: 2 [  455/ 1327], train_loss/perplexity = 4.82377148/124.4335022 secs/batch = 0.2662s, grad.norm=11.11309910
  3114: 2 [  460/ 1327], train_loss/perplexity = 5.07335806/159.7097473 secs/batch = 0.2667s, grad.norm=12.34712029
  3119: 2 [  465/ 1327], train_loss/perplexity = 4.87101603/130.4533997 secs/batch = 0.2669s, grad.norm=13.12881947
  3124: 2 [  470/ 1327], train_loss/perplexity = 5.29256773/198.8533783 secs/batch = 0.2665s, grad.norm=11.08773804
  3129: 2 [  475/ 1327], train_loss/perplexity = 4.83771038/126.1801147 secs/batch = 0.2658s, grad.norm=11.56565475
  3134: 2 [  480/ 1327], train_loss/perplexity = 5.05494165/156.7953796 secs/batch = 0.2630s, grad.norm=11.67115307
  3139: 2 [  485/ 1327], train_loss/perplexity = 4.93051291/138.4505005 secs/batch = 0.2629s, grad.norm=11.84693050
  3144: 2 [  490/ 1327], train_loss/perplexity = 4.89093161/133.0774994 secs/batch = 0.2663s, grad.norm=13.04038620
  3149: 2 [  495/ 1327], train_loss/perplexity = 4.84904385/127.6183090 secs/batch = 0.2590s, grad.norm=11.66625881
  3154: 2 [  500/ 1327], train_loss/perplexity = 5.20217657/181.6672211 secs/batch = 0.2662s, grad.norm=11.72856998
  3159: 2 [  505/ 1327], train_loss/perplexity = 5.12912750/168.8697205 secs/batch = 0.2659s, grad.norm=10.96738338
  3164: 2 [  510/ 1327], train_loss/perplexity = 5.49533033/243.5519714 secs/batch = 0.2651s, grad.norm=10.25291920
  3169: 2 [  515/ 1327], train_loss/perplexity = 5.13190556/169.3394928 secs/batch = 0.2651s, grad.norm=10.66428185
  3174: 2 [  520/ 1327], train_loss/perplexity = 5.30418587/201.1771545 secs/batch = 0.2654s, grad.norm=11.52100182
  3179: 2 [  525/ 1327], train_loss/perplexity = 4.85775614/128.7350159 secs/batch = 0.2651s, grad.norm=11.81207371
  3184: 2 [  530/ 1327], train_loss/perplexity = 4.98050404/145.5477295 secs/batch = 0.2661s, grad.norm=12.06668663
  3189: 2 [  535/ 1327], train_loss/perplexity = 5.02659035/152.4124451 secs/batch = 0.2663s, grad.norm=11.02425003
  3194: 2 [  540/ 1327], train_loss/perplexity = 5.10040903/164.0890045 secs/batch = 0.2662s, grad.norm=10.49726582
  3199: 2 [  545/ 1327], train_loss/perplexity = 5.25358295/191.2502899 secs/batch = 0.2674s, grad.norm=11.37414455
  3204: 2 [  550/ 1327], train_loss/perplexity = 5.16577005/175.1723022 secs/batch = 0.2660s, grad.norm=11.26375294
  3209: 2 [  555/ 1327], train_loss/perplexity = 5.04899740/155.8661194 secs/batch = 0.2659s, grad.norm=11.76256657
  3214: 2 [  560/ 1327], train_loss/perplexity = 5.02459621/152.1088257 secs/batch = 0.2606s, grad.norm=13.16959572
  3219: 2 [  565/ 1327], train_loss/perplexity = 5.01747990/151.0302124 secs/batch = 0.2653s, grad.norm=12.80502605
  3224: 2 [  570/ 1327], train_loss/perplexity = 4.92537975/137.7416382 secs/batch = 0.2650s, grad.norm=12.92008209
  3229: 2 [  575/ 1327], train_loss/perplexity = 4.84298372/126.8472672 secs/batch = 0.2651s, grad.norm=12.60054684
  3234: 2 [  580/ 1327], train_loss/perplexity = 5.18777990/179.0705566 secs/batch = 0.2654s, grad.norm=11.48748112
  3239: 2 [  585/ 1327], train_loss/perplexity = 4.70356083/110.3393707 secs/batch = 0.2672s, grad.norm=12.20602036
  3244: 2 [  590/ 1327], train_loss/perplexity = 5.05944872/157.5036621 secs/batch = 0.2666s, grad.norm=11.09615707
  3249: 2 [  595/ 1327], train_loss/perplexity = 4.92597055/137.8230438 secs/batch = 0.2636s, grad.norm=11.94655704
  3254: 2 [  600/ 1327], train_loss/perplexity = 5.25391912/191.3145905 secs/batch = 0.2674s, grad.norm=11.08291245
  3259: 2 [  605/ 1327], train_loss/perplexity = 5.18201733/178.0416107 secs/batch = 0.2663s, grad.norm=11.70300579
  3264: 2 [  610/ 1327], train_loss/perplexity = 5.36205626/213.1628113 secs/batch = 0.2623s, grad.norm=12.50572205
  3269: 2 [  615/ 1327], train_loss/perplexity = 4.78648138/119.8788147 secs/batch = 0.2648s, grad.norm=11.77953148
  3274: 2 [  620/ 1327], train_loss/perplexity = 5.14789391/172.0687103 secs/batch = 0.2655s, grad.norm=11.41837788
  3279: 2 [  625/ 1327], train_loss/perplexity = 5.24032021/188.7305298 secs/batch = 0.2645s, grad.norm=11.16971207
  3284: 2 [  630/ 1327], train_loss/perplexity = 5.27602005/195.5898895 secs/batch = 0.2652s, grad.norm=11.46900558
  3289: 2 [  635/ 1327], train_loss/perplexity = 4.96727371/143.6347656 secs/batch = 0.2637s, grad.norm=11.45652962
  3294: 2 [  640/ 1327], train_loss/perplexity = 5.02339411/151.9260864 secs/batch = 0.2657s, grad.norm=11.49254704
  3299: 2 [  645/ 1327], train_loss/perplexity = 5.26365852/193.1869812 secs/batch = 0.2646s, grad.norm=11.75742245
  3304: 2 [  650/ 1327], train_loss/perplexity = 4.88921070/132.8486786 secs/batch = 0.2643s, grad.norm=12.10538006
  3309: 2 [  655/ 1327], train_loss/perplexity = 5.03920650/154.3474884 secs/batch = 0.2667s, grad.norm=11.67135811
  3314: 2 [  660/ 1327], train_loss/perplexity = 4.83937597/126.3904572 secs/batch = 0.2659s, grad.norm=11.62190056
  3319: 2 [  665/ 1327], train_loss/perplexity = 5.11889076/167.1498566 secs/batch = 0.2648s, grad.norm=11.38085079
  3324: 2 [  670/ 1327], train_loss/perplexity = 5.02873611/152.7398376 secs/batch = 0.2660s, grad.norm=11.81642342
  3329: 2 [  675/ 1327], train_loss/perplexity = 4.76744270/117.6180725 secs/batch = 0.2661s, grad.norm=12.50325394
  3334: 2 [  680/ 1327], train_loss/perplexity = 5.05226135/156.3756866 secs/batch = 0.2634s, grad.norm=11.88949585
  3339: 2 [  685/ 1327], train_loss/perplexity = 4.99539709/147.7315979 secs/batch = 0.2670s, grad.norm=11.91912842
  3344: 2 [  690/ 1327], train_loss/perplexity = 5.24036694/188.7393494 secs/batch = 0.2652s, grad.norm=11.17564678
  3349: 2 [  695/ 1327], train_loss/perplexity = 4.98114586/145.6411743 secs/batch = 0.2656s, grad.norm=12.02139664
  3354: 2 [  700/ 1327], train_loss/perplexity = 5.21472359/183.9609680 secs/batch = 0.2666s, grad.norm=11.26940918
  3359: 2 [  705/ 1327], train_loss/perplexity = 4.92218828/137.3027344 secs/batch = 0.2658s, grad.norm=11.08537388
  3364: 2 [  710/ 1327], train_loss/perplexity = 4.93829870/139.5326538 secs/batch = 0.2660s, grad.norm=11.78377247
  3369: 2 [  715/ 1327], train_loss/perplexity = 4.91369247/136.1411896 secs/batch = 0.2645s, grad.norm=11.96273899
  3374: 2 [  720/ 1327], train_loss/perplexity = 4.99582005/147.7940979 secs/batch = 0.2634s, grad.norm=11.62646008
  3379: 2 [  725/ 1327], train_loss/perplexity = 4.75797796/116.5101013 secs/batch = 0.2651s, grad.norm=11.99980545
  3384: 2 [  730/ 1327], train_loss/perplexity = 5.03341770/153.4565887 secs/batch = 0.2670s, grad.norm=11.29714680
  3389: 2 [  735/ 1327], train_loss/perplexity = 5.09451294/163.1243744 secs/batch = 0.2681s, grad.norm=11.52650261
  3394: 2 [  740/ 1327], train_loss/perplexity = 4.44348478/85.0708771 secs/batch = 0.2663s, grad.norm=11.46150494
  3399: 2 [  745/ 1327], train_loss/perplexity = 5.01471043/150.6125183 secs/batch = 0.2655s, grad.norm=11.41551113
  3404: 2 [  750/ 1327], train_loss/perplexity = 4.85076141/127.8376923 secs/batch = 0.2660s, grad.norm=11.89678383
  3409: 2 [  755/ 1327], train_loss/perplexity = 4.82033205/124.0062637 secs/batch = 0.2660s, grad.norm=12.18362427
  3414: 2 [  760/ 1327], train_loss/perplexity = 4.73508978/113.8736801 secs/batch = 0.2661s, grad.norm=12.50369263
  3419: 2 [  765/ 1327], train_loss/perplexity = 4.80057812/121.5806885 secs/batch = 0.2624s, grad.norm=12.32832527
  3424: 2 [  770/ 1327], train_loss/perplexity = 4.71520329/111.6315002 secs/batch = 0.2664s, grad.norm=12.15444183
  3429: 2 [  775/ 1327], train_loss/perplexity = 4.90292120/134.6826477 secs/batch = 0.2656s, grad.norm=11.68795395
  3434: 2 [  780/ 1327], train_loss/perplexity = 5.18128729/177.9116821 secs/batch = 0.2652s, grad.norm=11.59164047
  3439: 2 [  785/ 1327], train_loss/perplexity = 4.98631811/146.3964081 secs/batch = 0.2677s, grad.norm=12.22687054
  3444: 2 [  790/ 1327], train_loss/perplexity = 4.80474377/122.0882034 secs/batch = 0.2670s, grad.norm=12.28773117
  3449: 2 [  795/ 1327], train_loss/perplexity = 5.16297150/174.6827545 secs/batch = 0.2656s, grad.norm=11.95143318
  3454: 2 [  800/ 1327], train_loss/perplexity = 5.05137873/156.2377319 secs/batch = 0.2633s, grad.norm=12.06941032
  3459: 2 [  805/ 1327], train_loss/perplexity = 5.37777948/216.5409088 secs/batch = 0.2659s, grad.norm=11.74794865
  3464: 2 [  810/ 1327], train_loss/perplexity = 4.98172808/145.7259827 secs/batch = 0.2666s, grad.norm=11.48775482
  3469: 2 [  815/ 1327], train_loss/perplexity = 4.95901728/142.4537354 secs/batch = 0.2647s, grad.norm=11.44650555
  3474: 2 [  820/ 1327], train_loss/perplexity = 4.55828190/95.4193954 secs/batch = 0.2658s, grad.norm=11.14338303
  3479: 2 [  825/ 1327], train_loss/perplexity = 4.81179714/122.9523849 secs/batch = 0.2666s, grad.norm=12.00864124
  3484: 2 [  830/ 1327], train_loss/perplexity = 4.61562157/101.0506210 secs/batch = 0.2648s, grad.norm=11.91793251
  3489: 2 [  835/ 1327], train_loss/perplexity = 4.92498970/137.6879272 secs/batch = 0.2666s, grad.norm=12.06773376
  3494: 2 [  840/ 1327], train_loss/perplexity = 5.03034544/152.9858551 secs/batch = 0.2663s, grad.norm=12.00533581
  3499: 2 [  845/ 1327], train_loss/perplexity = 4.83800316/126.2170639 secs/batch = 0.2656s, grad.norm=11.48891449
  3504: 2 [  850/ 1327], train_loss/perplexity = 4.94318104/140.2155762 secs/batch = 0.2661s, grad.norm=11.49018669
  3509: 2 [  855/ 1327], train_loss/perplexity = 4.90533686/135.0083771 secs/batch = 0.2655s, grad.norm=12.19312286
  3514: 2 [  860/ 1327], train_loss/perplexity = 4.60218048/99.7014771 secs/batch = 0.2660s, grad.norm=11.90089989
  3519: 2 [  865/ 1327], train_loss/perplexity = 5.11881733/167.1375885 secs/batch = 0.2648s, grad.norm=12.07265663
  3524: 2 [  870/ 1327], train_loss/perplexity = 5.11471176/166.4527893 secs/batch = 0.2657s, grad.norm=12.47459698
  3529: 2 [  875/ 1327], train_loss/perplexity = 4.59159613/98.6517639 secs/batch = 0.2665s, grad.norm=11.73485565
  3534: 2 [  880/ 1327], train_loss/perplexity = 4.81554461/123.4140091 secs/batch = 0.2671s, grad.norm=11.23099136
  3539: 2 [  885/ 1327], train_loss/perplexity = 4.89896679/134.1511078 secs/batch = 0.2658s, grad.norm=11.70869064
  3544: 2 [  890/ 1327], train_loss/perplexity = 5.10642385/165.0789490 secs/batch = 0.2656s, grad.norm=11.66236305
  3549: 2 [  895/ 1327], train_loss/perplexity = 5.17133379/176.1496277 secs/batch = 0.2665s, grad.norm=12.03741264
  3554: 2 [  900/ 1327], train_loss/perplexity = 4.99143362/147.1472321 secs/batch = 0.2665s, grad.norm=11.72917366
  3559: 2 [  905/ 1327], train_loss/perplexity = 4.81660843/123.5453644 secs/batch = 0.2656s, grad.norm=12.10026360
  3564: 2 [  910/ 1327], train_loss/perplexity = 4.88368607/132.1167603 secs/batch = 0.2665s, grad.norm=11.81385708
  3569: 2 [  915/ 1327], train_loss/perplexity = 5.12384605/167.9801941 secs/batch = 0.2658s, grad.norm=12.40864277
  3574: 2 [  920/ 1327], train_loss/perplexity = 5.24712849/190.0198364 secs/batch = 0.2654s, grad.norm=11.56635571
  3579: 2 [  925/ 1327], train_loss/perplexity = 5.04108286/154.6373749 secs/batch = 0.2646s, grad.norm=11.36557484
  3584: 2 [  930/ 1327], train_loss/perplexity = 4.96946287/143.9495544 secs/batch = 0.2668s, grad.norm=11.70243549
  3589: 2 [  935/ 1327], train_loss/perplexity = 5.04865646/155.8129883 secs/batch = 0.2649s, grad.norm=11.17867851
  3594: 2 [  940/ 1327], train_loss/perplexity = 5.00932407/149.8034515 secs/batch = 0.2638s, grad.norm=11.84814739
  3599: 2 [  945/ 1327], train_loss/perplexity = 5.21512556/184.0349274 secs/batch = 0.2660s, grad.norm=11.75947666
  3604: 2 [  950/ 1327], train_loss/perplexity = 5.00773716/149.5659027 secs/batch = 0.2660s, grad.norm=11.86226082
  3609: 2 [  955/ 1327], train_loss/perplexity = 5.02200937/151.7158508 secs/batch = 0.2654s, grad.norm=11.48817062
  3614: 2 [  960/ 1327], train_loss/perplexity = 5.25596142/191.7057037 secs/batch = 0.2665s, grad.norm=11.26802444
  3619: 2 [  965/ 1327], train_loss/perplexity = 5.08937407/162.2882538 secs/batch = 0.2642s, grad.norm=11.62308121
  3624: 2 [  970/ 1327], train_loss/perplexity = 5.24163342/188.9785309 secs/batch = 0.2665s, grad.norm=11.40222836
  3629: 2 [  975/ 1327], train_loss/perplexity = 5.05553484/156.8884125 secs/batch = 0.2645s, grad.norm=12.52655792
  3634: 2 [  980/ 1327], train_loss/perplexity = 4.79382086/120.7619019 secs/batch = 0.2664s, grad.norm=11.41761303
  3639: 2 [  985/ 1327], train_loss/perplexity = 4.94131851/139.9546661 secs/batch = 0.2661s, grad.norm=12.76824760
  3644: 2 [  990/ 1327], train_loss/perplexity = 5.10362530/164.6176147 secs/batch = 0.2676s, grad.norm=11.71684265
  3649: 2 [  995/ 1327], train_loss/perplexity = 5.12553120/168.2635040 secs/batch = 0.2664s, grad.norm=10.96815491
  3654: 2 [ 1000/ 1327], train_loss/perplexity = 4.59163952/98.6560440 secs/batch = 0.2627s, grad.norm=11.38164330
  3659: 2 [ 1005/ 1327], train_loss/perplexity = 5.10905933/165.5145874 secs/batch = 0.2657s, grad.norm=11.98488140
  3664: 2 [ 1010/ 1327], train_loss/perplexity = 4.65599632/105.2139969 secs/batch = 0.2674s, grad.norm=11.07422447
  3669: 2 [ 1015/ 1327], train_loss/perplexity = 5.17491198/176.7810516 secs/batch = 0.2666s, grad.norm=11.29521179
  3674: 2 [ 1020/ 1327], train_loss/perplexity = 5.31885386/204.1497650 secs/batch = 0.2673s, grad.norm=10.93547726
  3679: 2 [ 1025/ 1327], train_loss/perplexity = 5.12996149/169.0106049 secs/batch = 0.2660s, grad.norm=10.97789288
  3684: 2 [ 1030/ 1327], train_loss/perplexity = 4.96170521/142.8371582 secs/batch = 0.2675s, grad.norm=11.02265167
  3689: 2 [ 1035/ 1327], train_loss/perplexity = 4.81066418/122.8131638 secs/batch = 0.2658s, grad.norm=10.91974354
  3694: 2 [ 1040/ 1327], train_loss/perplexity = 5.12788343/168.6597595 secs/batch = 0.2666s, grad.norm=11.17304039
  3699: 2 [ 1045/ 1327], train_loss/perplexity = 4.66663551/106.3393631 secs/batch = 0.2661s, grad.norm=11.39389229
  3704: 2 [ 1050/ 1327], train_loss/perplexity = 4.72147512/112.3338394 secs/batch = 0.2657s, grad.norm=11.97476006
  3709: 2 [ 1055/ 1327], train_loss/perplexity = 4.96877432/143.8504639 secs/batch = 0.2660s, grad.norm=12.12716293
  3714: 2 [ 1060/ 1327], train_loss/perplexity = 4.62425709/101.9270248 secs/batch = 0.2669s, grad.norm=12.34026337
  3719: 2 [ 1065/ 1327], train_loss/perplexity = 4.62801790/102.3110733 secs/batch = 0.2662s, grad.norm=11.51046085
  3724: 2 [ 1070/ 1327], train_loss/perplexity = 5.09713840/163.5532074 secs/batch = 0.2656s, grad.norm=12.18908310
  3729: 2 [ 1075/ 1327], train_loss/perplexity = 4.78973341/120.2693024 secs/batch = 0.2673s, grad.norm=12.33353615
  3734: 2 [ 1080/ 1327], train_loss/perplexity = 4.66140795/105.7849197 secs/batch = 0.2665s, grad.norm=11.83361912
  3739: 2 [ 1085/ 1327], train_loss/perplexity = 4.59109306/98.6021500 secs/batch = 0.2660s, grad.norm=12.33861637
  3744: 2 [ 1090/ 1327], train_loss/perplexity = 4.77749729/118.8066406 secs/batch = 0.2661s, grad.norm=11.99373245
  3749: 2 [ 1095/ 1327], train_loss/perplexity = 4.88883066/132.7982025 secs/batch = 0.2638s, grad.norm=12.01832008
  3754: 2 [ 1100/ 1327], train_loss/perplexity = 4.84170771/126.6855087 secs/batch = 0.2656s, grad.norm=13.83519268
  3759: 2 [ 1105/ 1327], train_loss/perplexity = 4.67098761/106.8031693 secs/batch = 0.2621s, grad.norm=12.08718300
  3764: 2 [ 1110/ 1327], train_loss/perplexity = 5.19345617/180.0899048 secs/batch = 0.2658s, grad.norm=12.88297462
  3769: 2 [ 1115/ 1327], train_loss/perplexity = 4.66651535/106.3265839 secs/batch = 0.2672s, grad.norm=11.88576031
  3774: 2 [ 1120/ 1327], train_loss/perplexity = 4.92307663/137.4247742 secs/batch = 0.2671s, grad.norm=11.66735554
  3779: 2 [ 1125/ 1327], train_loss/perplexity = 5.17106485/176.1022644 secs/batch = 0.2666s, grad.norm=12.03004742
  3784: 2 [ 1130/ 1327], train_loss/perplexity = 4.81019592/122.7556686 secs/batch = 0.2675s, grad.norm=12.36001682
  3789: 2 [ 1135/ 1327], train_loss/perplexity = 4.86343145/129.4676971 secs/batch = 0.2668s, grad.norm=11.66087437
  3794: 2 [ 1140/ 1327], train_loss/perplexity = 5.13567972/169.9798126 secs/batch = 0.2682s, grad.norm=12.36842442
  3799: 2 [ 1145/ 1327], train_loss/perplexity = 4.92616940/137.8504486 secs/batch = 0.2635s, grad.norm=12.27893353
  3804: 2 [ 1150/ 1327], train_loss/perplexity = 4.82018232/123.9876938 secs/batch = 0.2661s, grad.norm=11.82060432
  3809: 2 [ 1155/ 1327], train_loss/perplexity = 4.97682953/145.0138855 secs/batch = 0.2667s, grad.norm=11.95811939
  3814: 2 [ 1160/ 1327], train_loss/perplexity = 4.94509792/140.4846039 secs/batch = 0.2653s, grad.norm=11.74078751
  3819: 2 [ 1165/ 1327], train_loss/perplexity = 4.97578764/144.8628845 secs/batch = 0.2602s, grad.norm=11.98316002
  3824: 2 [ 1170/ 1327], train_loss/perplexity = 4.93236542/138.7072296 secs/batch = 0.2665s, grad.norm=12.18425369
  3829: 2 [ 1175/ 1327], train_loss/perplexity = 4.55748844/95.3437195 secs/batch = 0.2661s, grad.norm=12.18039513
  3834: 2 [ 1180/ 1327], train_loss/perplexity = 4.56625509/96.1832352 secs/batch = 0.2670s, grad.norm=12.51302147
  3839: 2 [ 1185/ 1327], train_loss/perplexity = 4.80697250/122.3606110 secs/batch = 0.2667s, grad.norm=12.19161606
  3844: 2 [ 1190/ 1327], train_loss/perplexity = 4.89421558/133.5152283 secs/batch = 0.2663s, grad.norm=11.96937180
  3849: 2 [ 1195/ 1327], train_loss/perplexity = 4.75507641/116.1725311 secs/batch = 0.2581s, grad.norm=12.09032631
  3854: 2 [ 1200/ 1327], train_loss/perplexity = 4.60713530/100.1967010 secs/batch = 0.2664s, grad.norm=11.79052544
  3859: 2 [ 1205/ 1327], train_loss/perplexity = 4.66213989/105.8623734 secs/batch = 0.2673s, grad.norm=11.76264572
  3864: 2 [ 1210/ 1327], train_loss/perplexity = 4.41396999/82.5967255 secs/batch = 0.2667s, grad.norm=12.73669815
  3869: 2 [ 1215/ 1327], train_loss/perplexity = 4.53444147/93.1714630 secs/batch = 0.2652s, grad.norm=11.63697720
  3874: 2 [ 1220/ 1327], train_loss/perplexity = 4.76216221/116.9986267 secs/batch = 0.2676s, grad.norm=12.82189369
  3879: 2 [ 1225/ 1327], train_loss/perplexity = 4.54960918/94.5954285 secs/batch = 0.2674s, grad.norm=13.21389389
  3884: 2 [ 1230/ 1327], train_loss/perplexity = 4.72737980/112.9990921 secs/batch = 0.2684s, grad.norm=12.50289059
  3889: 2 [ 1235/ 1327], train_loss/perplexity = 4.85899639/128.8947754 secs/batch = 0.2679s, grad.norm=12.32670593
  3894: 2 [ 1240/ 1327], train_loss/perplexity = 4.84251595/126.7879410 secs/batch = 0.2658s, grad.norm=11.90191174
  3899: 2 [ 1245/ 1327], train_loss/perplexity = 4.82107306/124.0981827 secs/batch = 0.2665s, grad.norm=11.63337898
  3904: 2 [ 1250/ 1327], train_loss/perplexity = 4.89191961/133.2090454 secs/batch = 0.2661s, grad.norm=11.47660255
  3909: 2 [ 1255/ 1327], train_loss/perplexity = 4.89358282/133.4307709 secs/batch = 0.2667s, grad.norm=11.47675323
  3914: 2 [ 1260/ 1327], train_loss/perplexity = 4.79865742/121.3473892 secs/batch = 0.2656s, grad.norm=12.68500137
  3919: 2 [ 1265/ 1327], train_loss/perplexity = 4.97783184/145.1593170 secs/batch = 0.2663s, grad.norm=12.18649864
  3924: 2 [ 1270/ 1327], train_loss/perplexity = 4.71283913/111.3678970 secs/batch = 0.2665s, grad.norm=12.43134975
  3929: 2 [ 1275/ 1327], train_loss/perplexity = 4.98298407/145.9091339 secs/batch = 0.2666s, grad.norm=12.72298527
  3934: 2 [ 1280/ 1327], train_loss/perplexity = 4.75623655/116.3073883 secs/batch = 0.2668s, grad.norm=12.08048820
  3939: 2 [ 1285/ 1327], train_loss/perplexity = 4.70263100/110.2368240 secs/batch = 0.2660s, grad.norm=12.19997597
  3944: 2 [ 1290/ 1327], train_loss/perplexity = 4.84314632/126.8678894 secs/batch = 0.2659s, grad.norm=11.83752346
  3949: 2 [ 1295/ 1327], train_loss/perplexity = 4.92771530/138.0637207 secs/batch = 0.2667s, grad.norm=12.29357719
  3954: 2 [ 1300/ 1327], train_loss/perplexity = 5.03319693/153.4227142 secs/batch = 0.2663s, grad.norm=11.93541050
  3959: 2 [ 1305/ 1327], train_loss/perplexity = 5.15880919/173.9571838 secs/batch = 0.2688s, grad.norm=12.32348728
  3964: 2 [ 1310/ 1327], train_loss/perplexity = 5.36355019/213.4815063 secs/batch = 0.2655s, grad.norm=11.39364433
  3969: 2 [ 1315/ 1327], train_loss/perplexity = 5.19569731/180.4939575 secs/batch = 0.2659s, grad.norm=11.81806755
  3974: 2 [ 1320/ 1327], train_loss/perplexity = 5.20585299/182.3363342 secs/batch = 0.2664s, grad.norm=11.74210548
  3979: 2 [ 1325/ 1327], train_loss/perplexity = 5.05010271/156.0384979 secs/batch = 0.2691s, grad.norm=11.91523457
Epoch training time: 352.4989149570465
	> validation loss = 5.12787437, perplexity = 168.65823364
	> validation loss = 4.99185467, perplexity = 147.20919800
	> validation loss = 4.94895935, perplexity = 141.02812195
	> validation loss = 4.93359756, perplexity = 138.87823486
	> validation loss = 5.21378374, perplexity = 183.78814697
	> validation loss = 5.03158474, perplexity = 153.17556763
	> validation loss = 4.96710396, perplexity = 143.61038208
	> validation loss = 4.89411592, perplexity = 133.50192261
	> validation loss = 4.89205551, perplexity = 133.22714233
	> validation loss = 4.84825802, perplexity = 127.51805878
	> validation loss = 4.89231777, perplexity = 133.26208496
	> validation loss = 4.98989487, perplexity = 146.92097473
	> validation loss = 4.90709162, perplexity = 135.24549866
	> validation loss = 4.78755808, perplexity = 120.00795746
	> validation loss = 4.63545895, perplexity = 103.07521057
	> validation loss = 4.62061787, perplexity = 101.55676270
	> validation loss = 5.07168770, perplexity = 159.44319153
	> validation loss = 4.69433594, perplexity = 109.32618713
	> validation loss = 5.07063293, perplexity = 159.27510071
	> validation loss = 4.95263243, perplexity = 141.54708862
	> validation loss = 4.80940819, perplexity = 122.65900421
at the end of epoch: 2
train loss = 5.00786859, perplexity = 149.58556780
validation loss = 4.90966361, perplexity = 135.59379422
Saved model cv/epoch002_4.9097.model
  3986: 3 [    5/ 1327], train_loss/perplexity = 5.20924902/182.9566040 secs/batch = 0.2652s, grad.norm=11.71105957
  3991: 3 [   10/ 1327], train_loss/perplexity = 4.73498631/113.8619003 secs/batch = 0.2664s, grad.norm=12.74005222
  3996: 3 [   15/ 1327], train_loss/perplexity = 4.92437363/137.6031189 secs/batch = 0.2645s, grad.norm=10.77036762
  4001: 3 [   20/ 1327], train_loss/perplexity = 5.07957935/160.7064362 secs/batch = 0.2660s, grad.norm=12.75888634
  4006: 3 [   25/ 1327], train_loss/perplexity = 5.04148817/154.7000580 secs/batch = 0.2660s, grad.norm=11.80168533
  4011: 3 [   30/ 1327], train_loss/perplexity = 4.96965790/143.9776306 secs/batch = 0.2660s, grad.norm=11.85036278
  4016: 3 [   35/ 1327], train_loss/perplexity = 4.82412910/124.4780121 secs/batch = 0.2660s, grad.norm=11.41618347
  4021: 3 [   40/ 1327], train_loss/perplexity = 4.88391972/132.1476288 secs/batch = 0.2658s, grad.norm=12.16388702
  4026: 3 [   45/ 1327], train_loss/perplexity = 4.67211580/106.9237366 secs/batch = 0.2662s, grad.norm=12.81368732
  4031: 3 [   50/ 1327], train_loss/perplexity = 4.90392351/134.8177032 secs/batch = 0.2668s, grad.norm=14.16690159
  4036: 3 [   55/ 1327], train_loss/perplexity = 4.83133698/125.3784790 secs/batch = 0.2640s, grad.norm=12.40269947
  4041: 3 [   60/ 1327], train_loss/perplexity = 5.12324381/167.8790588 secs/batch = 0.2664s, grad.norm=12.54143238
  4046: 3 [   65/ 1327], train_loss/perplexity = 4.63095140/102.6116409 secs/batch = 0.2667s, grad.norm=11.61032391
  4051: 3 [   70/ 1327], train_loss/perplexity = 4.57353497/96.8859940 secs/batch = 0.2688s, grad.norm=12.33594322
  4056: 3 [   75/ 1327], train_loss/perplexity = 4.43435907/84.2980804 secs/batch = 0.2657s, grad.norm=12.66451931
  4061: 3 [   80/ 1327], train_loss/perplexity = 4.87850142/131.4335480 secs/batch = 0.2659s, grad.norm=12.85173512
  4066: 3 [   85/ 1327], train_loss/perplexity = 4.82176161/124.1836624 secs/batch = 0.2656s, grad.norm=11.91517067
  4071: 3 [   90/ 1327], train_loss/perplexity = 4.88757515/132.6315765 secs/batch = 0.2660s, grad.norm=12.54744816
  4076: 3 [   95/ 1327], train_loss/perplexity = 4.70034361/109.9849548 secs/batch = 0.2661s, grad.norm=12.07917690
  4081: 3 [  100/ 1327], train_loss/perplexity = 5.02361441/151.9595490 secs/batch = 0.2640s, grad.norm=11.96292305
  4086: 3 [  105/ 1327], train_loss/perplexity = 5.01287746/150.3367004 secs/batch = 0.2664s, grad.norm=13.17656517
  4091: 3 [  110/ 1327], train_loss/perplexity = 4.76155043/116.9270706 secs/batch = 0.2667s, grad.norm=11.46126080
  4096: 3 [  115/ 1327], train_loss/perplexity = 4.68535185/108.3483887 secs/batch = 0.2656s, grad.norm=13.31188774
  4101: 3 [  120/ 1327], train_loss/perplexity = 4.80125618/121.6631546 secs/batch = 0.2662s, grad.norm=12.60704517
  4106: 3 [  125/ 1327], train_loss/perplexity = 4.85818911/128.7907715 secs/batch = 0.2637s, grad.norm=12.75514412
  4111: 3 [  130/ 1327], train_loss/perplexity = 4.70545101/110.5481339 secs/batch = 0.2672s, grad.norm=12.66578674
  4116: 3 [  135/ 1327], train_loss/perplexity = 4.78169727/119.3066711 secs/batch = 0.2683s, grad.norm=11.89433289
  4121: 3 [  140/ 1327], train_loss/perplexity = 5.10547018/164.9216003 secs/batch = 0.2639s, grad.norm=11.73688698
  4126: 3 [  145/ 1327], train_loss/perplexity = 5.04625416/155.4391174 secs/batch = 0.2636s, grad.norm=12.94058514
  4131: 3 [  150/ 1327], train_loss/perplexity = 4.87816095/131.3888092 secs/batch = 0.2652s, grad.norm=12.09618473
  4136: 3 [  155/ 1327], train_loss/perplexity = 5.19253349/179.9238129 secs/batch = 0.2661s, grad.norm=11.43628693
  4141: 3 [  160/ 1327], train_loss/perplexity = 4.89514828/133.6398163 secs/batch = 0.2640s, grad.norm=11.83336639
  4146: 3 [  165/ 1327], train_loss/perplexity = 5.06956816/159.1056061 secs/batch = 0.2660s, grad.norm=11.51839542
  4151: 3 [  170/ 1327], train_loss/perplexity = 4.86923265/130.2209473 secs/batch = 0.2650s, grad.norm=11.77849102
  4156: 3 [  175/ 1327], train_loss/perplexity = 5.08843231/162.1354828 secs/batch = 0.2646s, grad.norm=11.68681049
  4161: 3 [  180/ 1327], train_loss/perplexity = 4.99465513/147.6220245 secs/batch = 0.2674s, grad.norm=12.16211700
  4166: 3 [  185/ 1327], train_loss/perplexity = 5.22410345/185.6946106 secs/batch = 0.2683s, grad.norm=11.77042007
  4171: 3 [  190/ 1327], train_loss/perplexity = 4.78218555/119.3649445 secs/batch = 0.2657s, grad.norm=11.61854935
  4176: 3 [  195/ 1327], train_loss/perplexity = 5.02953625/152.8621063 secs/batch = 0.2645s, grad.norm=11.15960026
  4181: 3 [  200/ 1327], train_loss/perplexity = 5.01153088/150.1343994 secs/batch = 0.2668s, grad.norm=12.22358990
  4186: 3 [  205/ 1327], train_loss/perplexity = 5.07393885/159.8025208 secs/batch = 0.2662s, grad.norm=11.83891869
  4191: 3 [  210/ 1327], train_loss/perplexity = 4.98860598/146.7317352 secs/batch = 0.2671s, grad.norm=11.19356918
  4196: 3 [  215/ 1327], train_loss/perplexity = 5.11701965/166.8373871 secs/batch = 0.2644s, grad.norm=11.79802132
  4201: 3 [  220/ 1327], train_loss/perplexity = 5.09153271/162.6389465 secs/batch = 0.2654s, grad.norm=11.55289173
  4206: 3 [  225/ 1327], train_loss/perplexity = 5.23276186/187.3094177 secs/batch = 0.2661s, grad.norm=12.02875042
  4211: 3 [  230/ 1327], train_loss/perplexity = 5.05908298/157.4460754 secs/batch = 0.2659s, grad.norm=12.84161758
  4216: 3 [  235/ 1327], train_loss/perplexity = 4.86054897/129.0950470 secs/batch = 0.2666s, grad.norm=11.42767429
  4221: 3 [  240/ 1327], train_loss/perplexity = 4.73414278/113.7658920 secs/batch = 0.2649s, grad.norm=12.54440403
  4226: 3 [  245/ 1327], train_loss/perplexity = 5.07180691/159.4622040 secs/batch = 0.2659s, grad.norm=11.45315075
  4231: 3 [  250/ 1327], train_loss/perplexity = 4.74028254/114.4665375 secs/batch = 0.2645s, grad.norm=11.60628891
  4236: 3 [  255/ 1327], train_loss/perplexity = 4.80998135/122.7293243 secs/batch = 0.2664s, grad.norm=11.72003937
  4241: 3 [  260/ 1327], train_loss/perplexity = 5.12868166/168.7944489 secs/batch = 0.2667s, grad.norm=12.47155190
  4246: 3 [  265/ 1327], train_loss/perplexity = 5.17683220/177.1208344 secs/batch = 0.2662s, grad.norm=11.34476376
  4251: 3 [  270/ 1327], train_loss/perplexity = 5.17993021/177.6704102 secs/batch = 0.2656s, grad.norm=11.97405529
  4256: 3 [  275/ 1327], train_loss/perplexity = 5.32780361/205.9850464 secs/batch = 0.2640s, grad.norm=11.56196117
  4261: 3 [  280/ 1327], train_loss/perplexity = 4.95695400/142.1601105 secs/batch = 0.2657s, grad.norm=11.30700493
  4266: 3 [  285/ 1327], train_loss/perplexity = 5.26349449/193.1552887 secs/batch = 0.2639s, grad.norm=11.99953270
  4271: 3 [  290/ 1327], train_loss/perplexity = 5.09740257/163.5964203 secs/batch = 0.2643s, grad.norm=12.28791142
  4276: 3 [  295/ 1327], train_loss/perplexity = 4.82369280/124.4237137 secs/batch = 0.2598s, grad.norm=11.51130962
  4281: 3 [  300/ 1327], train_loss/perplexity = 4.44446659/85.1544418 secs/batch = 0.2679s, grad.norm=12.17655182
  4286: 3 [  305/ 1327], train_loss/perplexity = 4.89157295/133.1628723 secs/batch = 0.2633s, grad.norm=12.30418873
  4291: 3 [  310/ 1327], train_loss/perplexity = 4.88069010/131.7215271 secs/batch = 0.2667s, grad.norm=11.77283096
  4296: 3 [  315/ 1327], train_loss/perplexity = 4.60159111/99.6427307 secs/batch = 0.2668s, grad.norm=13.32911110
  4301: 3 [  320/ 1327], train_loss/perplexity = 4.46544790/86.9599686 secs/batch = 0.2648s, grad.norm=13.51422310
  4306: 3 [  325/ 1327], train_loss/perplexity = 4.45591164/86.1346359 secs/batch = 0.2653s, grad.norm=11.67893124
  4311: 3 [  330/ 1327], train_loss/perplexity = 4.94914341/141.0540924 secs/batch = 0.2656s, grad.norm=12.76383400
  4316: 3 [  335/ 1327], train_loss/perplexity = 4.35492468/77.8609619 secs/batch = 0.2662s, grad.norm=12.33274174
  4321: 3 [  340/ 1327], train_loss/perplexity = 5.14676571/171.8746948 secs/batch = 0.2657s, grad.norm=11.52862930
  4326: 3 [  345/ 1327], train_loss/perplexity = 4.94018030/139.7954559 secs/batch = 0.2661s, grad.norm=11.45449352
  4331: 3 [  350/ 1327], train_loss/perplexity = 4.99794769/148.1088867 secs/batch = 0.2651s, grad.norm=12.07624912
  4336: 3 [  355/ 1327], train_loss/perplexity = 5.03636265/153.9091797 secs/batch = 0.2662s, grad.norm=12.01795959
  4341: 3 [  360/ 1327], train_loss/perplexity = 5.23799229/188.2916870 secs/batch = 0.2658s, grad.norm=11.94592190
  4346: 3 [  365/ 1327], train_loss/perplexity = 5.04545403/155.3148041 secs/batch = 0.2639s, grad.norm=11.82992363
  4351: 3 [  370/ 1327], train_loss/perplexity = 5.08691216/161.8892059 secs/batch = 0.2660s, grad.norm=11.73272324
  4356: 3 [  375/ 1327], train_loss/perplexity = 4.46214914/86.6735840 secs/batch = 0.2658s, grad.norm=12.25250816
  4361: 3 [  380/ 1327], train_loss/perplexity = 4.66240358/105.8902893 secs/batch = 0.2634s, grad.norm=12.69740200
  4366: 3 [  385/ 1327], train_loss/perplexity = 4.84961081/127.6906815 secs/batch = 0.2658s, grad.norm=12.36100674
  4371: 3 [  390/ 1327], train_loss/perplexity = 4.95200300/141.4580231 secs/batch = 0.2650s, grad.norm=11.88151169
  4376: 3 [  395/ 1327], train_loss/perplexity = 5.06838608/158.9176331 secs/batch = 0.2658s, grad.norm=12.17139244
  4381: 3 [  400/ 1327], train_loss/perplexity = 4.87539387/131.0257568 secs/batch = 0.2663s, grad.norm=11.84038639
  4386: 3 [  405/ 1327], train_loss/perplexity = 5.20527935/182.2317657 secs/batch = 0.2665s, grad.norm=12.18274307
  4391: 3 [  410/ 1327], train_loss/perplexity = 4.88988066/132.9377136 secs/batch = 0.2658s, grad.norm=11.38701630
  4396: 3 [  415/ 1327], train_loss/perplexity = 4.76589489/117.4361649 secs/batch = 0.2662s, grad.norm=12.37340260
  4401: 3 [  420/ 1327], train_loss/perplexity = 4.47078753/87.4255447 secs/batch = 0.2659s, grad.norm=12.70853615
  4406: 3 [  425/ 1327], train_loss/perplexity = 4.79019785/120.3251724 secs/batch = 0.2657s, grad.norm=13.58663845
  4411: 3 [  430/ 1327], train_loss/perplexity = 5.01830149/151.1543427 secs/batch = 0.2662s, grad.norm=12.55263710
  4416: 3 [  435/ 1327], train_loss/perplexity = 5.02483177/152.1446533 secs/batch = 0.2655s, grad.norm=12.57442284
  4421: 3 [  440/ 1327], train_loss/perplexity = 4.74931002/115.5045624 secs/batch = 0.2662s, grad.norm=13.51246452
  4426: 3 [  445/ 1327], train_loss/perplexity = 4.86901140/130.1921387 secs/batch = 0.2640s, grad.norm=12.54360008
  4431: 3 [  450/ 1327], train_loss/perplexity = 4.81426668/123.2563934 secs/batch = 0.2665s, grad.norm=12.41015530
  4436: 3 [  455/ 1327], train_loss/perplexity = 4.66551399/106.2201691 secs/batch = 0.2662s, grad.norm=11.76758671
  4441: 3 [  460/ 1327], train_loss/perplexity = 4.85113478/127.8854294 secs/batch = 0.2661s, grad.norm=12.85037613
  4446: 3 [  465/ 1327], train_loss/perplexity = 4.65171099/104.7640839 secs/batch = 0.2664s, grad.norm=13.69823837
  4451: 3 [  470/ 1327], train_loss/perplexity = 5.14477158/171.5323029 secs/batch = 0.2670s, grad.norm=11.34261608
  4456: 3 [  475/ 1327], train_loss/perplexity = 4.69100189/108.9622955 secs/batch = 0.2649s, grad.norm=12.37030506
  4461: 3 [  480/ 1327], train_loss/perplexity = 4.83830738/126.2554703 secs/batch = 0.2664s, grad.norm=12.44168949
  4466: 3 [  485/ 1327], train_loss/perplexity = 4.72976446/113.2688828 secs/batch = 0.2656s, grad.norm=12.32661152
  4471: 3 [  490/ 1327], train_loss/perplexity = 4.73175859/113.4949799 secs/batch = 0.2682s, grad.norm=13.22889137
  4476: 3 [  495/ 1327], train_loss/perplexity = 4.68598270/108.4167633 secs/batch = 0.2658s, grad.norm=12.34049511
  4481: 3 [  500/ 1327], train_loss/perplexity = 4.98712444/146.5145111 secs/batch = 0.2655s, grad.norm=11.94298077
  4486: 3 [  505/ 1327], train_loss/perplexity = 4.93668985/139.3083496 secs/batch = 0.2662s, grad.norm=11.23938179
  4491: 3 [  510/ 1327], train_loss/perplexity = 5.37679052/216.3268585 secs/batch = 0.2665s, grad.norm=10.99140453
  4496: 3 [  515/ 1327], train_loss/perplexity = 4.95638514/142.0792694 secs/batch = 0.2653s, grad.norm=11.12162304
  4501: 3 [  520/ 1327], train_loss/perplexity = 5.18566465/178.6921844 secs/batch = 0.2638s, grad.norm=12.24010372
  4506: 3 [  525/ 1327], train_loss/perplexity = 4.74566793/115.0846481 secs/batch = 0.2654s, grad.norm=12.30683327
  4511: 3 [  530/ 1327], train_loss/perplexity = 4.74960089/115.5381622 secs/batch = 0.2655s, grad.norm=12.80494976
  4516: 3 [  535/ 1327], train_loss/perplexity = 4.91233730/135.9568176 secs/batch = 0.2677s, grad.norm=12.32269478
  4521: 3 [  540/ 1327], train_loss/perplexity = 4.97228765/144.3567505 secs/batch = 0.2664s, grad.norm=11.84904003
  4526: 3 [  545/ 1327], train_loss/perplexity = 5.09268093/162.8258057 secs/batch = 0.2663s, grad.norm=12.15203571
  4531: 3 [  550/ 1327], train_loss/perplexity = 5.00198936/148.7086945 secs/batch = 0.2653s, grad.norm=12.20441914
  4536: 3 [  555/ 1327], train_loss/perplexity = 4.79301882/120.6650848 secs/batch = 0.2643s, grad.norm=12.69088364
  4541: 3 [  560/ 1327], train_loss/perplexity = 4.92299747/137.4138947 secs/batch = 0.2665s, grad.norm=13.99517441
  4546: 3 [  565/ 1327], train_loss/perplexity = 4.81665325/123.5509033 secs/batch = 0.2661s, grad.norm=13.08068848
  4551: 3 [  570/ 1327], train_loss/perplexity = 4.80507946/122.1291962 secs/batch = 0.2615s, grad.norm=13.42089462
  4556: 3 [  575/ 1327], train_loss/perplexity = 4.66249132/105.8995819 secs/batch = 0.2653s, grad.norm=12.90682030
  4561: 3 [  580/ 1327], train_loss/perplexity = 4.99453688/147.6045685 secs/batch = 0.2654s, grad.norm=11.86095333
  4566: 3 [  585/ 1327], train_loss/perplexity = 4.50831413/90.7686691 secs/batch = 0.2613s, grad.norm=12.24258900
  4571: 3 [  590/ 1327], train_loss/perplexity = 4.94360638/140.2752228 secs/batch = 0.2660s, grad.norm=12.45646763
  4576: 3 [  595/ 1327], train_loss/perplexity = 4.81059361/122.8044968 secs/batch = 0.2667s, grad.norm=12.72017956
  4581: 3 [  600/ 1327], train_loss/perplexity = 5.21727562/184.4310455 secs/batch = 0.2673s, grad.norm=12.65236092
  4586: 3 [  605/ 1327], train_loss/perplexity = 4.97982979/145.4496155 secs/batch = 0.2680s, grad.norm=12.15047264
  4591: 3 [  610/ 1327], train_loss/perplexity = 5.14167118/171.0012970 secs/batch = 0.2675s, grad.norm=12.53093529
  4596: 3 [  615/ 1327], train_loss/perplexity = 4.63209105/102.7286530 secs/batch = 0.2659s, grad.norm=11.93453217
  4601: 3 [  620/ 1327], train_loss/perplexity = 5.00549650/149.2311554 secs/batch = 0.2657s, grad.norm=12.46171951
  4606: 3 [  625/ 1327], train_loss/perplexity = 5.09415817/163.0665131 secs/batch = 0.2665s, grad.norm=12.02909374
  4611: 3 [  630/ 1327], train_loss/perplexity = 5.15172243/172.7287445 secs/batch = 0.2646s, grad.norm=12.23931599
  4616: 3 [  635/ 1327], train_loss/perplexity = 4.85793257/128.7577362 secs/batch = 0.2654s, grad.norm=12.21123409
  4621: 3 [  640/ 1327], train_loss/perplexity = 4.84647512/127.2909164 secs/batch = 0.2657s, grad.norm=12.23392677
  4626: 3 [  645/ 1327], train_loss/perplexity = 5.12834740/168.7380371 secs/batch = 0.2658s, grad.norm=12.89902878
  4631: 3 [  650/ 1327], train_loss/perplexity = 4.72225809/112.4218216 secs/batch = 0.2661s, grad.norm=12.85662651
  4636: 3 [  655/ 1327], train_loss/perplexity = 4.85563946/128.4628143 secs/batch = 0.2661s, grad.norm=13.03378868
  4641: 3 [  660/ 1327], train_loss/perplexity = 4.74220991/114.6873703 secs/batch = 0.2657s, grad.norm=12.29763412
  4646: 3 [  665/ 1327], train_loss/perplexity = 4.93136597/138.5686646 secs/batch = 0.2640s, grad.norm=12.45654011
  4651: 3 [  670/ 1327], train_loss/perplexity = 4.84236240/126.7684784 secs/batch = 0.2653s, grad.norm=12.76245785
  4656: 3 [  675/ 1327], train_loss/perplexity = 4.57765102/97.2856064 secs/batch = 0.2651s, grad.norm=12.97405910
  4661: 3 [  680/ 1327], train_loss/perplexity = 4.91094303/135.7673798 secs/batch = 0.2658s, grad.norm=12.99393177
  4666: 3 [  685/ 1327], train_loss/perplexity = 4.78049469/119.1632843 secs/batch = 0.2652s, grad.norm=12.60681725
  4671: 3 [  690/ 1327], train_loss/perplexity = 5.05680132/157.0872345 secs/batch = 0.2670s, grad.norm=11.84958172
  4676: 3 [  695/ 1327], train_loss/perplexity = 4.86878872/130.1631622 secs/batch = 0.2616s, grad.norm=12.07013512
  4681: 3 [  700/ 1327], train_loss/perplexity = 5.02687645/152.4560699 secs/batch = 0.2671s, grad.norm=11.90826702
  4686: 3 [  705/ 1327], train_loss/perplexity = 4.79207230/120.5509262 secs/batch = 0.2660s, grad.norm=11.80543137
  4691: 3 [  710/ 1327], train_loss/perplexity = 4.78689766/119.9287338 secs/batch = 0.2648s, grad.norm=12.62310886
  4696: 3 [  715/ 1327], train_loss/perplexity = 4.74855471/115.4173508 secs/batch = 0.2642s, grad.norm=12.55138588
  4701: 3 [  720/ 1327], train_loss/perplexity = 4.85340309/128.1758423 secs/batch = 0.2657s, grad.norm=12.79321289
  4706: 3 [  725/ 1327], train_loss/perplexity = 4.73280430/113.6137238 secs/batch = 0.2667s, grad.norm=12.54115868
  4711: 3 [  730/ 1327], train_loss/perplexity = 4.85284710/128.1045990 secs/batch = 0.2660s, grad.norm=12.22800827
  4716: 3 [  735/ 1327], train_loss/perplexity = 4.95768118/142.2635345 secs/batch = 0.2642s, grad.norm=12.79002380
  4721: 3 [  740/ 1327], train_loss/perplexity = 4.35467434/77.8414688 secs/batch = 0.2663s, grad.norm=12.15895939
  4726: 3 [  745/ 1327], train_loss/perplexity = 4.83991575/126.4586945 secs/batch = 0.2660s, grad.norm=12.34103966
  4731: 3 [  750/ 1327], train_loss/perplexity = 4.75344896/115.9836197 secs/batch = 0.2636s, grad.norm=12.68654060
  4736: 3 [  755/ 1327], train_loss/perplexity = 4.62206602/101.7039413 secs/batch = 0.2636s, grad.norm=12.53655243
  4741: 3 [  760/ 1327], train_loss/perplexity = 4.52462387/92.2612152 secs/batch = 0.2659s, grad.norm=12.78225899
  4746: 3 [  765/ 1327], train_loss/perplexity = 4.61232138/100.7176819 secs/batch = 0.2651s, grad.norm=12.64431477
  4751: 3 [  770/ 1327], train_loss/perplexity = 4.52860212/92.6289902 secs/batch = 0.2678s, grad.norm=12.18107700
  4756: 3 [  775/ 1327], train_loss/perplexity = 4.76400328/117.2142258 secs/batch = 0.2666s, grad.norm=12.97867298
  4761: 3 [  780/ 1327], train_loss/perplexity = 5.08128977/160.9815521 secs/batch = 0.2671s, grad.norm=11.73605633
  4766: 3 [  785/ 1327], train_loss/perplexity = 4.84394455/126.9692001 secs/batch = 0.2666s, grad.norm=13.02870369
  4771: 3 [  790/ 1327], train_loss/perplexity = 4.61070824/100.5553436 secs/batch = 0.2658s, grad.norm=12.36122227
  4776: 3 [  795/ 1327], train_loss/perplexity = 5.07354784/159.7400513 secs/batch = 0.2667s, grad.norm=12.66107178
  4781: 3 [  800/ 1327], train_loss/perplexity = 4.92357302/137.4929962 secs/batch = 0.2662s, grad.norm=13.12575436
  4786: 3 [  805/ 1327], train_loss/perplexity = 5.26410770/193.2737732 secs/batch = 0.2655s, grad.norm=12.30326939
  4791: 3 [  810/ 1327], train_loss/perplexity = 4.90351534/134.7626801 secs/batch = 0.2656s, grad.norm=12.14328003
  4796: 3 [  815/ 1327], train_loss/perplexity = 4.78150558/119.2838058 secs/batch = 0.2612s, grad.norm=11.61394310
  4801: 3 [  820/ 1327], train_loss/perplexity = 4.43157339/84.0635757 secs/batch = 0.2652s, grad.norm=11.58764362
  4806: 3 [  825/ 1327], train_loss/perplexity = 4.75812531/116.5272675 secs/batch = 0.2655s, grad.norm=12.12310982
  4811: 3 [  830/ 1327], train_loss/perplexity = 4.59305811/98.7960968 secs/batch = 0.2655s, grad.norm=12.76052761
  4816: 3 [  835/ 1327], train_loss/perplexity = 4.84945440/127.6707153 secs/batch = 0.2642s, grad.norm=12.73176861
  4821: 3 [  840/ 1327], train_loss/perplexity = 4.90147686/134.4882507 secs/batch = 0.2661s, grad.norm=12.61852169
  4826: 3 [  845/ 1327], train_loss/perplexity = 4.68900299/108.7447052 secs/batch = 0.2662s, grad.norm=12.40628719
  4831: 3 [  850/ 1327], train_loss/perplexity = 4.83240032/125.5118713 secs/batch = 0.2666s, grad.norm=11.93036366
  4836: 3 [  855/ 1327], train_loss/perplexity = 4.71043015/111.0999374 secs/batch = 0.2667s, grad.norm=12.55082989
  4841: 3 [  860/ 1327], train_loss/perplexity = 4.50325012/90.3101730 secs/batch = 0.2660s, grad.norm=12.29028511
  4846: 3 [  865/ 1327], train_loss/perplexity = 4.94027710/139.8089905 secs/batch = 0.2610s, grad.norm=12.29329967
  4851: 3 [  870/ 1327], train_loss/perplexity = 4.89441252/133.5415344 secs/batch = 0.2659s, grad.norm=12.48079777
  4856: 3 [  875/ 1327], train_loss/perplexity = 4.42583036/83.5821838 secs/batch = 0.2641s, grad.norm=12.19515324
  4861: 3 [  880/ 1327], train_loss/perplexity = 4.56662703/96.2190170 secs/batch = 0.2659s, grad.norm=11.71801281
  4866: 3 [  885/ 1327], train_loss/perplexity = 4.81433439/123.2647400 secs/batch = 0.2660s, grad.norm=12.33615303
  4871: 3 [  890/ 1327], train_loss/perplexity = 4.99380922/147.4972076 secs/batch = 0.2659s, grad.norm=12.34833145
  4876: 3 [  895/ 1327], train_loss/perplexity = 5.04920816/155.8989716 secs/batch = 0.2652s, grad.norm=11.90474892
  4881: 3 [  900/ 1327], train_loss/perplexity = 4.73800564/114.2062073 secs/batch = 0.2656s, grad.norm=12.51698494
  4886: 3 [  905/ 1327], train_loss/perplexity = 4.66397858/106.0571976 secs/batch = 0.2656s, grad.norm=13.02719593
  4891: 3 [  910/ 1327], train_loss/perplexity = 4.76671505/117.5325165 secs/batch = 0.2660s, grad.norm=12.57823753
  4896: 3 [  915/ 1327], train_loss/perplexity = 4.92941713/138.2988739 secs/batch = 0.2667s, grad.norm=12.01118279
  4901: 3 [  920/ 1327], train_loss/perplexity = 5.13127232/169.2322998 secs/batch = 0.2660s, grad.norm=12.59622478
  4906: 3 [  925/ 1327], train_loss/perplexity = 4.93283606/138.7725220 secs/batch = 0.2652s, grad.norm=12.41432476
  4911: 3 [  930/ 1327], train_loss/perplexity = 4.81013584/122.7482910 secs/batch = 0.2657s, grad.norm=11.82481194
  4916: 3 [  935/ 1327], train_loss/perplexity = 4.94367838/140.2853241 secs/batch = 0.2657s, grad.norm=12.22803879
  4921: 3 [  940/ 1327], train_loss/perplexity = 4.94137621/139.9627380 secs/batch = 0.2667s, grad.norm=12.70471382
  4926: 3 [  945/ 1327], train_loss/perplexity = 5.06722879/158.7338257 secs/batch = 0.2666s, grad.norm=12.07841015
  4931: 3 [  950/ 1327], train_loss/perplexity = 4.81554556/123.4141235 secs/batch = 0.2666s, grad.norm=12.79970932
  4936: 3 [  955/ 1327], train_loss/perplexity = 4.90677547/135.2027435 secs/batch = 0.2654s, grad.norm=12.54054737
  4941: 3 [  960/ 1327], train_loss/perplexity = 5.14922523/172.2979431 secs/batch = 0.2668s, grad.norm=12.12076855
  4946: 3 [  965/ 1327], train_loss/perplexity = 4.91043329/135.6981964 secs/batch = 0.2586s, grad.norm=12.16042233
  4951: 3 [  970/ 1327], train_loss/perplexity = 5.08639383/161.8053131 secs/batch = 0.2664s, grad.norm=11.84174156
  4956: 3 [  975/ 1327], train_loss/perplexity = 4.89006853/132.9626923 secs/batch = 0.2621s, grad.norm=12.78184319
  4961: 3 [  980/ 1327], train_loss/perplexity = 4.66092253/105.7335815 secs/batch = 0.2663s, grad.norm=12.54977131
  4966: 3 [  985/ 1327], train_loss/perplexity = 4.83136797/125.3823624 secs/batch = 0.2626s, grad.norm=12.59496593
  4971: 3 [  990/ 1327], train_loss/perplexity = 4.96590328/143.4380493 secs/batch = 0.2663s, grad.norm=12.36325073
  4976: 3 [  995/ 1327], train_loss/perplexity = 4.99206066/147.2395172 secs/batch = 0.2666s, grad.norm=11.56263065
  4981: 3 [ 1000/ 1327], train_loss/perplexity = 4.44578934/85.2671585 secs/batch = 0.2674s, grad.norm=11.85083485
  4986: 3 [ 1005/ 1327], train_loss/perplexity = 4.96962070/143.9722748 secs/batch = 0.2652s, grad.norm=12.43413067
  4991: 3 [ 1010/ 1327], train_loss/perplexity = 4.53553534/93.2734375 secs/batch = 0.2662s, grad.norm=11.70551777
  4996: 3 [ 1015/ 1327], train_loss/perplexity = 5.06313086/158.0846863 secs/batch = 0.2662s, grad.norm=12.50193977
  5001: 3 [ 1020/ 1327], train_loss/perplexity = 5.25080156/190.7190857 secs/batch = 0.2662s, grad.norm=11.96886063
  5006: 3 [ 1025/ 1327], train_loss/perplexity = 5.00325203/148.8965912 secs/batch = 0.2656s, grad.norm=11.73368931
  5011: 3 [ 1030/ 1327], train_loss/perplexity = 4.81751585/123.6575241 secs/batch = 0.2655s, grad.norm=11.59390259
  5016: 3 [ 1035/ 1327], train_loss/perplexity = 4.72944260/113.2324295 secs/batch = 0.2666s, grad.norm=11.51512718
  5021: 3 [ 1040/ 1327], train_loss/perplexity = 5.00433874/149.0584869 secs/batch = 0.2648s, grad.norm=11.69888973
  5026: 3 [ 1045/ 1327], train_loss/perplexity = 4.50441122/90.4150925 secs/batch = 0.2663s, grad.norm=12.13019657
  5031: 3 [ 1050/ 1327], train_loss/perplexity = 4.64591360/104.1584854 secs/batch = 0.2658s, grad.norm=12.70515633
  5036: 3 [ 1055/ 1327], train_loss/perplexity = 4.79284239/120.6437988 secs/batch = 0.2654s, grad.norm=13.27437973
  5041: 3 [ 1060/ 1327], train_loss/perplexity = 4.49991989/90.0099182 secs/batch = 0.2653s, grad.norm=13.91668987
  5046: 3 [ 1065/ 1327], train_loss/perplexity = 4.58375311/97.8810654 secs/batch = 0.2664s, grad.norm=12.58538055
  5051: 3 [ 1070/ 1327], train_loss/perplexity = 4.89071512/133.0486908 secs/batch = 0.2660s, grad.norm=12.54429150
  5056: 3 [ 1075/ 1327], train_loss/perplexity = 4.63448668/102.9750443 secs/batch = 0.2662s, grad.norm=12.42718506
  5061: 3 [ 1080/ 1327], train_loss/perplexity = 4.58794451/98.2921829 secs/batch = 0.2657s, grad.norm=12.86921692
  5066: 3 [ 1085/ 1327], train_loss/perplexity = 4.44562435/85.2530899 secs/batch = 0.2651s, grad.norm=12.51813412
  5071: 3 [ 1090/ 1327], train_loss/perplexity = 4.61515808/101.0037918 secs/batch = 0.2640s, grad.norm=12.62973022
  5076: 3 [ 1095/ 1327], train_loss/perplexity = 4.77507544/118.5192566 secs/batch = 0.2662s, grad.norm=13.23540306
  5081: 3 [ 1100/ 1327], train_loss/perplexity = 4.55830050/95.4211731 secs/batch = 0.2662s, grad.norm=14.40851688
  5086: 3 [ 1105/ 1327], train_loss/perplexity = 4.48136330/88.3550415 secs/batch = 0.2660s, grad.norm=12.79071331
  5091: 3 [ 1110/ 1327], train_loss/perplexity = 4.92636538/137.8774719 secs/batch = 0.2638s, grad.norm=14.15426445
  5096: 3 [ 1115/ 1327], train_loss/perplexity = 4.60699558/100.1827087 secs/batch = 0.2668s, grad.norm=12.12317848
  5101: 3 [ 1120/ 1327], train_loss/perplexity = 4.85826111/128.8000336 secs/batch = 0.2663s, grad.norm=12.74588394
  5106: 3 [ 1125/ 1327], train_loss/perplexity = 5.06266880/158.0116577 secs/batch = 0.2655s, grad.norm=13.23808765
  5111: 3 [ 1130/ 1327], train_loss/perplexity = 4.71921587/112.0803299 secs/batch = 0.2664s, grad.norm=12.56613445
  5116: 3 [ 1135/ 1327], train_loss/perplexity = 4.72254276/112.4538345 secs/batch = 0.2662s, grad.norm=12.50073051
  5121: 3 [ 1140/ 1327], train_loss/perplexity = 5.02186680/151.6942291 secs/batch = 0.2656s, grad.norm=13.05100155
  5126: 3 [ 1145/ 1327], train_loss/perplexity = 4.75966835/116.7072144 secs/batch = 0.2655s, grad.norm=12.62207222
  5131: 3 [ 1150/ 1327], train_loss/perplexity = 4.75319910/115.9546432 secs/batch = 0.2659s, grad.norm=12.58326626
  5136: 3 [ 1155/ 1327], train_loss/perplexity = 4.84466457/127.0606537 secs/batch = 0.2639s, grad.norm=12.31342793
  5141: 3 [ 1160/ 1327], train_loss/perplexity = 4.82976484/125.1815186 secs/batch = 0.2645s, grad.norm=12.37705517
  5146: 3 [ 1165/ 1327], train_loss/perplexity = 4.79020023/120.3254623 secs/batch = 0.2618s, grad.norm=12.32694817
  5151: 3 [ 1170/ 1327], train_loss/perplexity = 4.72356653/112.5690155 secs/batch = 0.2665s, grad.norm=12.94978714
  5156: 3 [ 1175/ 1327], train_loss/perplexity = 4.45230103/85.8242035 secs/batch = 0.2657s, grad.norm=12.84055901
  5161: 3 [ 1180/ 1327], train_loss/perplexity = 4.49136114/89.2428360 secs/batch = 0.2659s, grad.norm=12.66485500
  5166: 3 [ 1185/ 1327], train_loss/perplexity = 4.72784042/113.0511551 secs/batch = 0.2657s, grad.norm=12.41007233
  5171: 3 [ 1190/ 1327], train_loss/perplexity = 4.75833988/116.5522766 secs/batch = 0.2639s, grad.norm=12.53853798
  5176: 3 [ 1195/ 1327], train_loss/perplexity = 4.64261246/103.8152084 secs/batch = 0.2652s, grad.norm=13.01466370
  5181: 3 [ 1200/ 1327], train_loss/perplexity = 4.50863075/90.7974091 secs/batch = 0.2660s, grad.norm=11.89557648
  5186: 3 [ 1205/ 1327], train_loss/perplexity = 4.55302000/94.9186325 secs/batch = 0.2653s, grad.norm=13.00269794
  5191: 3 [ 1210/ 1327], train_loss/perplexity = 4.25658321/70.5684509 secs/batch = 0.2669s, grad.norm=13.25752831
  5196: 3 [ 1215/ 1327], train_loss/perplexity = 4.44830179/85.4816513 secs/batch = 0.2665s, grad.norm=12.23893929
  5201: 3 [ 1220/ 1327], train_loss/perplexity = 4.56007910/95.5910416 secs/batch = 0.2666s, grad.norm=12.81330013
  5206: 3 [ 1225/ 1327], train_loss/perplexity = 4.34980679/77.4634933 secs/batch = 0.2660s, grad.norm=13.67659187
  5211: 3 [ 1230/ 1327], train_loss/perplexity = 4.57523966/97.0512924 secs/batch = 0.2669s, grad.norm=12.24824047
  5216: 3 [ 1235/ 1327], train_loss/perplexity = 4.66569138/106.2390137 secs/batch = 0.2659s, grad.norm=12.60861301
  5221: 3 [ 1240/ 1327], train_loss/perplexity = 4.75391340/116.0374985 secs/batch = 0.2656s, grad.norm=12.79503632
  5226: 3 [ 1245/ 1327], train_loss/perplexity = 4.66225004/105.8740387 secs/batch = 0.2666s, grad.norm=11.78029823
  5231: 3 [ 1250/ 1327], train_loss/perplexity = 4.82709551/124.8478165 secs/batch = 0.2667s, grad.norm=11.99468994
  5236: 3 [ 1255/ 1327], train_loss/perplexity = 4.83205700/125.4687881 secs/batch = 0.2659s, grad.norm=11.82735634
  5241: 3 [ 1260/ 1327], train_loss/perplexity = 4.62664413/102.1706161 secs/batch = 0.2666s, grad.norm=13.19105148
  5246: 3 [ 1265/ 1327], train_loss/perplexity = 4.87260818/130.6612549 secs/batch = 0.2654s, grad.norm=12.30310345
  5251: 3 [ 1270/ 1327], train_loss/perplexity = 4.55443239/95.0527878 secs/batch = 0.2666s, grad.norm=12.60033131
  5256: 3 [ 1275/ 1327], train_loss/perplexity = 4.83456516/125.7838745 secs/batch = 0.2660s, grad.norm=12.89400101
  5261: 3 [ 1280/ 1327], train_loss/perplexity = 4.57667303/97.1905060 secs/batch = 0.2667s, grad.norm=12.59632301
  5266: 3 [ 1285/ 1327], train_loss/perplexity = 4.53886127/93.5841751 secs/batch = 0.2662s, grad.norm=12.25560284
  5271: 3 [ 1290/ 1327], train_loss/perplexity = 4.76486874/117.3157196 secs/batch = 0.2669s, grad.norm=12.19141102
  5276: 3 [ 1295/ 1327], train_loss/perplexity = 4.73375082/113.7213135 secs/batch = 0.2659s, grad.norm=12.82640362
  5281: 3 [ 1300/ 1327], train_loss/perplexity = 4.88380003/132.1318207 secs/batch = 0.2658s, grad.norm=12.00570011
  5286: 3 [ 1305/ 1327], train_loss/perplexity = 5.09179497/162.6816101 secs/batch = 0.2657s, grad.norm=13.41590214
  5291: 3 [ 1310/ 1327], train_loss/perplexity = 5.27051020/194.5151825 secs/batch = 0.2592s, grad.norm=12.33288002
  5296: 3 [ 1315/ 1327], train_loss/perplexity = 5.07502556/159.9762878 secs/batch = 0.2653s, grad.norm=12.11436749
  5301: 3 [ 1320/ 1327], train_loss/perplexity = 5.01415205/150.5284424 secs/batch = 0.2592s, grad.norm=12.30043125
  5306: 3 [ 1325/ 1327], train_loss/perplexity = 4.95554876/141.9604950 secs/batch = 0.2661s, grad.norm=12.57484531
Epoch training time: 352.59437370300293
	> validation loss = 4.99953413, perplexity = 148.34403992
	> validation loss = 4.88757133, perplexity = 132.63107300
	> validation loss = 4.82052517, perplexity = 124.03021240
	> validation loss = 4.84029865, perplexity = 126.50712585
	> validation loss = 5.09430218, perplexity = 163.08999634
	> validation loss = 4.92384768, perplexity = 137.53077698
	> validation loss = 4.88708782, perplexity = 132.56695557
	> validation loss = 4.79560089, perplexity = 120.97705078
	> validation loss = 4.69594097, perplexity = 109.50180054
	> validation loss = 4.71281385, perplexity = 111.36508179
	> validation loss = 4.75185490, perplexity = 115.79888153
	> validation loss = 4.87889004, perplexity = 131.48463440
	> validation loss = 4.78277302, perplexity = 119.43508911
	> validation loss = 4.68163061, perplexity = 107.94594574
	> validation loss = 4.53243065, perplexity = 92.98429871
	> validation loss = 4.50293684, perplexity = 90.28188324
	> validation loss = 5.00472450, perplexity = 149.11599731
	> validation loss = 4.58601189, perplexity = 98.10240173
	> validation loss = 4.95044851, perplexity = 141.23829651
	> validation loss = 4.84068537, perplexity = 126.55606079
	> validation loss = 4.68443203, perplexity = 108.24877167
at the end of epoch: 3
train loss = 4.88843837, perplexity = 132.74611125
validation loss = 4.79792982, perplexity = 121.25912962
Saved model cv/epoch003_4.7979.model
  5313: 4 [    5/ 1327], train_loss/perplexity = 5.02602863/152.3268585 secs/batch = 0.2604s, grad.norm=12.34472561
  5318: 4 [   10/ 1327], train_loss/perplexity = 4.56920624/96.4675064 secs/batch = 0.2665s, grad.norm=12.85400295
  5323: 4 [   15/ 1327], train_loss/perplexity = 4.82243824/124.2677155 secs/batch = 0.2649s, grad.norm=11.30233002
  5328: 4 [   20/ 1327], train_loss/perplexity = 5.03302050/153.3956451 secs/batch = 0.2663s, grad.norm=12.72875214
  5333: 4 [   25/ 1327], train_loss/perplexity = 4.97211170/144.3313446 secs/batch = 0.2661s, grad.norm=12.90602875
  5338: 4 [   30/ 1327], train_loss/perplexity = 4.91536951/136.3696899 secs/batch = 0.2664s, grad.norm=12.38929653
  5343: 4 [   35/ 1327], train_loss/perplexity = 4.69089890/108.9510727 secs/batch = 0.2660s, grad.norm=11.35922146
  5348: 4 [   40/ 1327], train_loss/perplexity = 4.71219254/111.2959137 secs/batch = 0.2642s, grad.norm=12.73498631
  5353: 4 [   45/ 1327], train_loss/perplexity = 4.51722765/91.5813522 secs/batch = 0.2659s, grad.norm=12.14126205
  5358: 4 [   50/ 1327], train_loss/perplexity = 4.75847006/116.5674515 secs/batch = 0.2599s, grad.norm=12.76468086
  5363: 4 [   55/ 1327], train_loss/perplexity = 4.67045879/106.7467041 secs/batch = 0.2641s, grad.norm=12.41936970
  5368: 4 [   60/ 1327], train_loss/perplexity = 5.00533485/149.2070312 secs/batch = 0.2656s, grad.norm=12.27707672
  5373: 4 [   65/ 1327], train_loss/perplexity = 4.58357716/97.8638458 secs/batch = 0.2659s, grad.norm=12.35370159
  5378: 4 [   70/ 1327], train_loss/perplexity = 4.41435003/82.6281204 secs/batch = 0.2616s, grad.norm=12.58065510
  5383: 4 [   75/ 1327], train_loss/perplexity = 4.23721075/69.2145233 secs/batch = 0.2662s, grad.norm=12.48973846
  5388: 4 [   80/ 1327], train_loss/perplexity = 4.67770290/107.5227966 secs/batch = 0.2655s, grad.norm=12.69433308
  5393: 4 [   85/ 1327], train_loss/perplexity = 4.71219206/111.2958603 secs/batch = 0.2660s, grad.norm=12.53314972
  5398: 4 [   90/ 1327], train_loss/perplexity = 4.69414377/109.3051758 secs/batch = 0.2662s, grad.norm=12.60103607
  5403: 4 [   95/ 1327], train_loss/perplexity = 4.61880112/101.3724289 secs/batch = 0.2604s, grad.norm=12.63402843
  5408: 4 [  100/ 1327], train_loss/perplexity = 4.89475536/133.5873260 secs/batch = 0.2671s, grad.norm=12.51887703
  5413: 4 [  105/ 1327], train_loss/perplexity = 4.79907656/121.3982620 secs/batch = 0.2657s, grad.norm=13.22312355
  5418: 4 [  110/ 1327], train_loss/perplexity = 4.63689423/103.2232590 secs/batch = 0.2661s, grad.norm=12.35067844
  5423: 4 [  115/ 1327], train_loss/perplexity = 4.54115152/93.7987518 secs/batch = 0.2664s, grad.norm=13.64967251
  5428: 4 [  120/ 1327], train_loss/perplexity = 4.67308569/107.0274887 secs/batch = 0.2661s, grad.norm=13.15998554
  5433: 4 [  125/ 1327], train_loss/perplexity = 4.79606581/121.0333099 secs/batch = 0.2667s, grad.norm=12.95733452
  5438: 4 [  130/ 1327], train_loss/perplexity = 4.68265533/108.0566177 secs/batch = 0.2654s, grad.norm=13.25625134
  5443: 4 [  135/ 1327], train_loss/perplexity = 4.70143032/110.1045456 secs/batch = 0.2665s, grad.norm=12.42848396
  5448: 4 [  140/ 1327], train_loss/perplexity = 4.97026014/144.0643616 secs/batch = 0.2662s, grad.norm=12.41721344
  5453: 4 [  145/ 1327], train_loss/perplexity = 4.94061327/139.8559875 secs/batch = 0.2662s, grad.norm=13.45179558
  5458: 4 [  150/ 1327], train_loss/perplexity = 4.82621193/124.7375488 secs/batch = 0.2659s, grad.norm=12.63451099
  5463: 4 [  155/ 1327], train_loss/perplexity = 5.11719608/166.8668365 secs/batch = 0.2659s, grad.norm=12.22201538
  5468: 4 [  160/ 1327], train_loss/perplexity = 4.76119804/116.8858795 secs/batch = 0.2662s, grad.norm=11.88534355
  5473: 4 [  165/ 1327], train_loss/perplexity = 4.95978022/142.5624542 secs/batch = 0.2673s, grad.norm=11.92936420
  5478: 4 [  170/ 1327], train_loss/perplexity = 4.72837877/113.1120300 secs/batch = 0.2665s, grad.norm=11.96092510
  5483: 4 [  175/ 1327], train_loss/perplexity = 5.03203678/153.2448273 secs/batch = 0.2659s, grad.norm=12.09860611
  5488: 4 [  180/ 1327], train_loss/perplexity = 4.85965109/128.9791870 secs/batch = 0.2664s, grad.norm=12.77828693
  5493: 4 [  185/ 1327], train_loss/perplexity = 5.13901615/170.5478973 secs/batch = 0.2671s, grad.norm=12.57081699
  5498: 4 [  190/ 1327], train_loss/perplexity = 4.67284155/107.0013580 secs/batch = 0.2664s, grad.norm=12.02523518
  5503: 4 [  195/ 1327], train_loss/perplexity = 4.93088007/138.5013428 secs/batch = 0.2661s, grad.norm=11.93727398
  5508: 4 [  200/ 1327], train_loss/perplexity = 4.87740803/131.2899170 secs/batch = 0.2659s, grad.norm=12.38630772
  5513: 4 [  205/ 1327], train_loss/perplexity = 4.94776821/140.8602448 secs/batch = 0.2661s, grad.norm=12.04542065
  5518: 4 [  210/ 1327], train_loss/perplexity = 4.84013128/126.4859543 secs/batch = 0.2652s, grad.norm=11.66267681
  5523: 4 [  215/ 1327], train_loss/perplexity = 5.04746294/155.6271210 secs/batch = 0.2655s, grad.norm=12.53815937
  5528: 4 [  220/ 1327], train_loss/perplexity = 4.96033669/142.6418152 secs/batch = 0.2650s, grad.norm=11.67343521
  5533: 4 [  225/ 1327], train_loss/perplexity = 5.10490751/164.8288269 secs/batch = 0.2620s, grad.norm=11.86731339
  5538: 4 [  230/ 1327], train_loss/perplexity = 4.91806507/136.7377777 secs/batch = 0.2659s, grad.norm=12.33804893
  5543: 4 [  235/ 1327], train_loss/perplexity = 4.76321173/117.1214828 secs/batch = 0.2648s, grad.norm=12.26171207
  5548: 4 [  240/ 1327], train_loss/perplexity = 4.64747334/104.3210678 secs/batch = 0.2659s, grad.norm=12.80460548
  5553: 4 [  245/ 1327], train_loss/perplexity = 4.89731121/133.9291840 secs/batch = 0.2671s, grad.norm=12.03890610
  5558: 4 [  250/ 1327], train_loss/perplexity = 4.62505817/102.0087051 secs/batch = 0.2672s, grad.norm=11.51228428
  5563: 4 [  255/ 1327], train_loss/perplexity = 4.70961905/111.0098648 secs/batch = 0.2664s, grad.norm=12.36778831
  5568: 4 [  260/ 1327], train_loss/perplexity = 5.02161598/151.6561737 secs/batch = 0.2658s, grad.norm=13.12280846
  5573: 4 [  265/ 1327], train_loss/perplexity = 5.10472584/164.7988892 secs/batch = 0.2666s, grad.norm=11.50572300
  5578: 4 [  270/ 1327], train_loss/perplexity = 5.12195396/167.6626587 secs/batch = 0.2655s, grad.norm=11.89244938
  5583: 4 [  275/ 1327], train_loss/perplexity = 5.17803478/177.3339691 secs/batch = 0.2670s, grad.norm=12.22212219
  5588: 4 [  280/ 1327], train_loss/perplexity = 4.86144066/129.2102203 secs/batch = 0.2637s, grad.norm=12.33045769
  5593: 4 [  285/ 1327], train_loss/perplexity = 5.12404728/168.0139923 secs/batch = 0.2660s, grad.norm=12.12945461
  5598: 4 [  290/ 1327], train_loss/perplexity = 4.93438196/138.9872131 secs/batch = 0.2636s, grad.norm=12.34474659
  5603: 4 [  295/ 1327], train_loss/perplexity = 4.68288279/108.0811996 secs/batch = 0.2671s, grad.norm=12.21424770
  5608: 4 [  300/ 1327], train_loss/perplexity = 4.26221085/70.9667053 secs/batch = 0.2656s, grad.norm=11.92629814
  5613: 4 [  305/ 1327], train_loss/perplexity = 4.77973652/119.0729752 secs/batch = 0.2657s, grad.norm=12.15447712
  5618: 4 [  310/ 1327], train_loss/perplexity = 4.74528980/115.0411377 secs/batch = 0.2613s, grad.norm=12.23731422
  5623: 4 [  315/ 1327], train_loss/perplexity = 4.39660931/81.1751633 secs/batch = 0.2665s, grad.norm=12.93332863
  5628: 4 [  320/ 1327], train_loss/perplexity = 4.44256878/84.9929886 secs/batch = 0.2663s, grad.norm=15.00099373
  5633: 4 [  325/ 1327], train_loss/perplexity = 4.35173512/77.6130142 secs/batch = 0.2658s, grad.norm=12.40859795
  5638: 4 [  330/ 1327], train_loss/perplexity = 4.80896950/122.6052094 secs/batch = 0.2660s, grad.norm=12.77503586
  5643: 4 [  335/ 1327], train_loss/perplexity = 4.22071552/68.0821838 secs/batch = 0.2680s, grad.norm=12.81202793
  5648: 4 [  340/ 1327], train_loss/perplexity = 4.99773645/148.0776062 secs/batch = 0.2659s, grad.norm=11.98248005
  5653: 4 [  345/ 1327], train_loss/perplexity = 4.80665064/122.3212357 secs/batch = 0.2607s, grad.norm=11.78199100
  5658: 4 [  350/ 1327], train_loss/perplexity = 4.86474323/129.6376495 secs/batch = 0.2665s, grad.norm=12.80380249
  5663: 4 [  355/ 1327], train_loss/perplexity = 4.86024523/129.0558472 secs/batch = 0.2671s, grad.norm=12.21572781
  5668: 4 [  360/ 1327], train_loss/perplexity = 5.07586098/160.1099854 secs/batch = 0.2660s, grad.norm=12.60616875
  5673: 4 [  365/ 1327], train_loss/perplexity = 4.97558165/144.8330383 secs/batch = 0.2661s, grad.norm=12.38836861
  5678: 4 [  370/ 1327], train_loss/perplexity = 4.96301222/143.0239716 secs/batch = 0.2652s, grad.norm=12.65499878
  5683: 4 [  375/ 1327], train_loss/perplexity = 4.36221981/78.4310455 secs/batch = 0.2662s, grad.norm=12.82937527
  5688: 4 [  380/ 1327], train_loss/perplexity = 4.46723795/87.1157684 secs/batch = 0.2674s, grad.norm=12.71530437
  5693: 4 [  385/ 1327], train_loss/perplexity = 4.69506931/109.4063950 secs/batch = 0.2652s, grad.norm=12.72382832
  5698: 4 [  390/ 1327], train_loss/perplexity = 4.82236481/124.2585907 secs/batch = 0.2651s, grad.norm=12.89423943
  5703: 4 [  395/ 1327], train_loss/perplexity = 4.95086288/141.2968292 secs/batch = 0.2657s, grad.norm=12.87154961
  5708: 4 [  400/ 1327], train_loss/perplexity = 4.77701235/118.7490387 secs/batch = 0.2665s, grad.norm=12.44224548
  5713: 4 [  405/ 1327], train_loss/perplexity = 5.11136198/165.8961487 secs/batch = 0.2658s, grad.norm=12.81994915
  5718: 4 [  410/ 1327], train_loss/perplexity = 4.75725746/116.4261856 secs/batch = 0.2660s, grad.norm=12.18221951
  5723: 4 [  415/ 1327], train_loss/perplexity = 4.63015938/102.5304031 secs/batch = 0.2646s, grad.norm=12.29169655
  5728: 4 [  420/ 1327], train_loss/perplexity = 4.42688084/83.6700287 secs/batch = 0.2649s, grad.norm=13.43844414
  5733: 4 [  425/ 1327], train_loss/perplexity = 4.67873764/107.6341171 secs/batch = 0.2663s, grad.norm=14.18633175
  5738: 4 [  430/ 1327], train_loss/perplexity = 4.92547798/137.7551727 secs/batch = 0.2658s, grad.norm=13.27172947
  5743: 4 [  435/ 1327], train_loss/perplexity = 4.95419884/141.7689819 secs/batch = 0.2652s, grad.norm=13.27176476
  5748: 4 [  440/ 1327], train_loss/perplexity = 4.56196833/95.7718048 secs/batch = 0.2659s, grad.norm=13.08253193
  5753: 4 [  445/ 1327], train_loss/perplexity = 4.87449741/130.9083405 secs/batch = 0.2657s, grad.norm=13.03114128
  5758: 4 [  450/ 1327], train_loss/perplexity = 4.74622726/115.1490402 secs/batch = 0.2655s, grad.norm=12.85219097
  5763: 4 [  455/ 1327], train_loss/perplexity = 4.59583807/99.0711288 secs/batch = 0.2662s, grad.norm=12.45231533
  5768: 4 [  460/ 1327], train_loss/perplexity = 4.71937037/112.0976486 secs/batch = 0.2661s, grad.norm=13.50276566
  5773: 4 [  465/ 1327], train_loss/perplexity = 4.51072264/90.9875488 secs/batch = 0.2654s, grad.norm=14.60370445
  5778: 4 [  470/ 1327], train_loss/perplexity = 5.05368328/156.5982056 secs/batch = 0.2661s, grad.norm=12.05278206
  5783: 4 [  475/ 1327], train_loss/perplexity = 4.55842543/95.4330978 secs/batch = 0.2673s, grad.norm=12.96868324
  5788: 4 [  480/ 1327], train_loss/perplexity = 5.74371052/312.2207642 secs/batch = 0.2658s, grad.norm=52.57079315
  5793: 4 [  485/ 1327], train_loss/perplexity = 4.70991039/111.0422058 secs/batch = 0.2652s, grad.norm=12.87826061
  5798: 4 [  490/ 1327], train_loss/perplexity = 4.64311743/103.8676453 secs/batch = 0.2665s, grad.norm=14.08627415
  5803: 4 [  495/ 1327], train_loss/perplexity = 4.56875277/96.4237747 secs/batch = 0.2662s, grad.norm=12.26805592
  5808: 4 [  500/ 1327], train_loss/perplexity = 4.89997530/134.2864685 secs/batch = 0.2673s, grad.norm=13.21076584
  5813: 4 [  505/ 1327], train_loss/perplexity = 4.89714909/133.9074707 secs/batch = 0.2660s, grad.norm=12.01319790
  5818: 4 [  510/ 1327], train_loss/perplexity = 5.32650423/205.7175751 secs/batch = 0.2663s, grad.norm=11.35265827
  5823: 4 [  515/ 1327], train_loss/perplexity = 4.87015724/130.3414154 secs/batch = 0.2663s, grad.norm=11.96115208
  5828: 4 [  520/ 1327], train_loss/perplexity = 5.10269308/164.4642334 secs/batch = 0.2651s, grad.norm=12.42734814
  5833: 4 [  525/ 1327], train_loss/perplexity = 4.63381386/102.9057846 secs/batch = 0.2652s, grad.norm=12.72886562
  5838: 4 [  530/ 1327], train_loss/perplexity = 4.65741110/105.3629532 secs/batch = 0.2677s, grad.norm=13.22514057
  5843: 4 [  535/ 1327], train_loss/perplexity = 4.79979086/121.4850082 secs/batch = 0.2655s, grad.norm=12.24402237
  5848: 4 [  540/ 1327], train_loss/perplexity = 4.86593103/129.7917175 secs/batch = 0.2658s, grad.norm=12.21635628
  5853: 4 [  545/ 1327], train_loss/perplexity = 4.99406528/147.5349731 secs/batch = 0.2671s, grad.norm=12.96530819
  5858: 4 [  550/ 1327], train_loss/perplexity = 4.83599186/125.9634628 secs/batch = 0.2661s, grad.norm=12.57533550
  5863: 4 [  555/ 1327], train_loss/perplexity = 4.69335556/109.2190552 secs/batch = 0.2665s, grad.norm=12.41958809
  5868: 4 [  560/ 1327], train_loss/perplexity = 4.77439642/118.4388046 secs/batch = 0.2658s, grad.norm=13.97041512
  5873: 4 [  565/ 1327], train_loss/perplexity = 4.73967314/114.3968048 secs/batch = 0.2662s, grad.norm=13.53752708
  5878: 4 [  570/ 1327], train_loss/perplexity = 4.70682764/110.7004242 secs/batch = 0.2655s, grad.norm=13.99524689
  5883: 4 [  575/ 1327], train_loss/perplexity = 4.44660187/85.3364639 secs/batch = 0.2648s, grad.norm=13.21731091
  5888: 4 [  580/ 1327], train_loss/perplexity = 4.88686895/132.5379486 secs/batch = 0.2656s, grad.norm=13.22551823
  5893: 4 [  585/ 1327], train_loss/perplexity = 4.39629984/81.1500473 secs/batch = 0.2660s, grad.norm=12.86097813
  5898: 4 [  590/ 1327], train_loss/perplexity = 4.80385447/121.9796753 secs/batch = 0.2664s, grad.norm=12.81809616
  5903: 4 [  595/ 1327], train_loss/perplexity = 4.75144100/115.7509613 secs/batch = 0.2677s, grad.norm=12.92692947
  5908: 4 [  600/ 1327], train_loss/perplexity = 4.94929266/141.0751343 secs/batch = 0.2666s, grad.norm=12.27005291
  5913: 4 [  605/ 1327], train_loss/perplexity = 4.86589575/129.7871399 secs/batch = 0.2663s, grad.norm=12.68301010
  5918: 4 [  610/ 1327], train_loss/perplexity = 5.02349567/151.9415131 secs/batch = 0.2653s, grad.norm=12.90860081
  5923: 4 [  615/ 1327], train_loss/perplexity = 4.55177307/94.8003464 secs/batch = 0.2672s, grad.norm=11.83987236
  5928: 4 [  620/ 1327], train_loss/perplexity = 4.91238356/135.9631042 secs/batch = 0.2657s, grad.norm=12.53773499
  5933: 4 [  625/ 1327], train_loss/perplexity = 4.96576071/143.4176025 secs/batch = 0.2653s, grad.norm=12.22235298
  5938: 4 [  630/ 1327], train_loss/perplexity = 5.04907799/155.8786774 secs/batch = 0.2657s, grad.norm=12.24288368
  5943: 4 [  635/ 1327], train_loss/perplexity = 4.73039341/113.3401413 secs/batch = 0.2656s, grad.norm=13.10179138
  5948: 4 [  640/ 1327], train_loss/perplexity = 4.74347210/114.8322220 secs/batch = 0.2655s, grad.norm=12.93955326
  5953: 4 [  645/ 1327], train_loss/perplexity = 5.01008034/149.9167786 secs/batch = 0.2654s, grad.norm=13.23791122
  5958: 4 [  650/ 1327], train_loss/perplexity = 4.60709429/100.1925964 secs/batch = 0.2663s, grad.norm=12.80308247
  5963: 4 [  655/ 1327], train_loss/perplexity = 4.68824768/108.6625977 secs/batch = 0.2663s, grad.norm=13.22590733
  5968: 4 [  660/ 1327], train_loss/perplexity = 4.63218689/102.7384949 secs/batch = 0.2666s, grad.norm=12.74142361
  5973: 4 [  665/ 1327], train_loss/perplexity = 4.81934452/123.8838577 secs/batch = 0.2654s, grad.norm=12.39287663
  5978: 4 [  670/ 1327], train_loss/perplexity = 4.77147388/118.0931702 secs/batch = 0.2656s, grad.norm=12.84562683
  5983: 4 [  675/ 1327], train_loss/perplexity = 4.50227690/90.2223282 secs/batch = 0.2667s, grad.norm=12.95928478
  5988: 4 [  680/ 1327], train_loss/perplexity = 4.76330471/117.1323776 secs/batch = 0.2662s, grad.norm=13.16325474
  5993: 4 [  685/ 1327], train_loss/perplexity = 4.59261465/98.7522964 secs/batch = 0.2657s, grad.norm=13.04490662
  5998: 4 [  690/ 1327], train_loss/perplexity = 4.97477913/144.7168579 secs/batch = 0.2646s, grad.norm=12.09324360
  6003: 4 [  695/ 1327], train_loss/perplexity = 4.75492620/116.1550827 secs/batch = 0.2656s, grad.norm=12.39681911
  6008: 4 [  700/ 1327], train_loss/perplexity = 4.99404716/147.5323029 secs/batch = 0.2665s, grad.norm=12.60878277
  6013: 4 [  705/ 1327], train_loss/perplexity = 4.77366066/118.3516922 secs/batch = 0.2658s, grad.norm=12.85512733
  6018: 4 [  710/ 1327], train_loss/perplexity = 4.66252804/105.9034729 secs/batch = 0.2647s, grad.norm=13.04314041
  6023: 4 [  715/ 1327], train_loss/perplexity = 4.62722683/102.2301712 secs/batch = 0.2639s, grad.norm=12.72504139
  6028: 4 [  720/ 1327], train_loss/perplexity = 4.65959215/105.5930099 secs/batch = 0.2652s, grad.norm=13.23249626
  6033: 4 [  725/ 1327], train_loss/perplexity = 4.62685490/102.1921539 secs/batch = 0.2660s, grad.norm=13.17020988
  6038: 4 [  730/ 1327], train_loss/perplexity = 4.74006796/114.4419785 secs/batch = 0.2627s, grad.norm=13.08935738
  6043: 4 [  735/ 1327], train_loss/perplexity = 4.86424017/129.5724487 secs/batch = 0.2660s, grad.norm=13.13291931
  6048: 4 [  740/ 1327], train_loss/perplexity = 4.23352480/68.9598770 secs/batch = 0.2658s, grad.norm=12.22951698
  6053: 4 [  745/ 1327], train_loss/perplexity = 4.79663467/121.1021805 secs/batch = 0.2656s, grad.norm=12.93277359
  6058: 4 [  750/ 1327], train_loss/perplexity = 4.61055946/100.5403824 secs/batch = 0.2662s, grad.norm=13.17352486
  6063: 4 [  755/ 1327], train_loss/perplexity = 4.51934004/91.7750092 secs/batch = 0.2669s, grad.norm=13.02895546
  6068: 4 [  760/ 1327], train_loss/perplexity = 4.41299152/82.5159454 secs/batch = 0.2661s, grad.norm=12.97056866
  6073: 4 [  765/ 1327], train_loss/perplexity = 4.52383757/92.1886978 secs/batch = 0.2648s, grad.norm=12.97615719
  6078: 4 [  770/ 1327], train_loss/perplexity = 4.44149399/84.9016876 secs/batch = 0.2654s, grad.norm=12.71704674
  6083: 4 [  775/ 1327], train_loss/perplexity = 4.65356588/104.9585876 secs/batch = 0.2635s, grad.norm=12.97688293
  6088: 4 [  780/ 1327], train_loss/perplexity = 4.94956303/141.1132812 secs/batch = 0.2656s, grad.norm=12.55785942
  6093: 4 [  785/ 1327], train_loss/perplexity = 4.71910667/112.0680923 secs/batch = 0.2661s, grad.norm=13.19354820
  6098: 4 [  790/ 1327], train_loss/perplexity = 4.52941132/92.7039719 secs/batch = 0.2617s, grad.norm=12.85029888
  6103: 4 [  795/ 1327], train_loss/perplexity = 4.94064045/139.8597870 secs/batch = 0.2658s, grad.norm=12.57640076
  6108: 4 [  800/ 1327], train_loss/perplexity = 4.81149435/122.9151611 secs/batch = 0.2656s, grad.norm=13.42255497
  6113: 4 [  805/ 1327], train_loss/perplexity = 5.12766123/168.6222839 secs/batch = 0.2649s, grad.norm=12.82494545
  6118: 4 [  810/ 1327], train_loss/perplexity = 4.77993584/119.0967102 secs/batch = 0.2661s, grad.norm=12.53598118
  6123: 4 [  815/ 1327], train_loss/perplexity = 4.74835539/115.3943481 secs/batch = 0.2660s, grad.norm=12.16710567
  6128: 4 [  820/ 1327], train_loss/perplexity = 4.39986372/81.4397659 secs/batch = 0.2658s, grad.norm=12.13903046
  6133: 4 [  825/ 1327], train_loss/perplexity = 4.64431953/103.9925766 secs/batch = 0.2655s, grad.norm=12.35712337
  6138: 4 [  830/ 1327], train_loss/perplexity = 4.46906900/87.2754288 secs/batch = 0.2648s, grad.norm=13.23581600
  6143: 4 [  835/ 1327], train_loss/perplexity = 4.72636032/112.8839493 secs/batch = 0.2655s, grad.norm=12.90962982
  6148: 4 [  840/ 1327], train_loss/perplexity = 4.76158905/116.9315872 secs/batch = 0.2661s, grad.norm=12.93616199
  6153: 4 [  845/ 1327], train_loss/perplexity = 4.61709356/101.1994781 secs/batch = 0.2659s, grad.norm=13.37018490
  6158: 4 [  850/ 1327], train_loss/perplexity = 4.67348480/107.0702133 secs/batch = 0.2665s, grad.norm=12.35956764
  6163: 4 [  855/ 1327], train_loss/perplexity = 4.66504145/106.1699829 secs/batch = 0.2657s, grad.norm=13.36703873
  6168: 4 [  860/ 1327], train_loss/perplexity = 4.41373444/82.5772705 secs/batch = 0.2669s, grad.norm=12.74120998
  6173: 4 [  865/ 1327], train_loss/perplexity = 4.82266426/124.2958069 secs/batch = 0.2659s, grad.norm=12.39256668
  6178: 4 [  870/ 1327], train_loss/perplexity = 4.80378628/121.9713593 secs/batch = 0.2640s, grad.norm=13.12631512
  6183: 4 [  875/ 1327], train_loss/perplexity = 4.37416601/79.3736191 secs/batch = 0.2665s, grad.norm=12.66525078
  6188: 4 [  880/ 1327], train_loss/perplexity = 4.57213116/96.7500763 secs/batch = 0.2666s, grad.norm=12.41158581
  6193: 4 [  885/ 1327], train_loss/perplexity = 4.72548485/112.7851715 secs/batch = 0.2657s, grad.norm=12.31902790
  6198: 4 [  890/ 1327], train_loss/perplexity = 4.86080790/129.1284790 secs/batch = 0.2656s, grad.norm=12.72385597
  6203: 4 [  895/ 1327], train_loss/perplexity = 4.90223122/134.5897369 secs/batch = 0.2629s, grad.norm=12.32068825
  6208: 4 [  900/ 1327], train_loss/perplexity = 4.69920444/109.8597412 secs/batch = 0.2642s, grad.norm=13.40504551
  6213: 4 [  905/ 1327], train_loss/perplexity = 4.57139587/96.6789703 secs/batch = 0.2654s, grad.norm=12.31521797
  6218: 4 [  910/ 1327], train_loss/perplexity = 4.55941582/95.5276566 secs/batch = 0.2667s, grad.norm=13.36026192
  6223: 4 [  915/ 1327], train_loss/perplexity = 4.82522106/124.6140137 secs/batch = 0.2658s, grad.norm=13.10877800
  6228: 4 [  920/ 1327], train_loss/perplexity = 5.02289963/151.8509827 secs/batch = 0.2660s, grad.norm=12.60291481
  6233: 4 [  925/ 1327], train_loss/perplexity = 4.80253506/121.8188477 secs/batch = 0.2650s, grad.norm=12.74405193
  6238: 4 [  930/ 1327], train_loss/perplexity = 4.72922993/113.2083511 secs/batch = 0.2655s, grad.norm=12.41596699
  6243: 4 [  935/ 1327], train_loss/perplexity = 4.83895636/126.3374329 secs/batch = 0.2660s, grad.norm=12.54436111
  6248: 4 [  940/ 1327], train_loss/perplexity = 4.80354977/121.9425201 secs/batch = 0.2663s, grad.norm=12.78715897
  6253: 4 [  945/ 1327], train_loss/perplexity = 4.98757839/146.5810242 secs/batch = 0.2661s, grad.norm=12.70287418
  6258: 4 [  950/ 1327], train_loss/perplexity = 4.78249884/119.4023438 secs/batch = 0.2664s, grad.norm=12.39128780
  6263: 4 [  955/ 1327], train_loss/perplexity = 4.79584360/121.0064163 secs/batch = 0.2657s, grad.norm=12.61503124
  6268: 4 [  960/ 1327], train_loss/perplexity = 5.02323818/151.9023895 secs/batch = 0.2658s, grad.norm=12.18119907
  6273: 4 [  965/ 1327], train_loss/perplexity = 4.82182741/124.1918335 secs/batch = 0.2660s, grad.norm=12.21336365
  6278: 4 [  970/ 1327], train_loss/perplexity = 5.02063799/151.5079346 secs/batch = 0.2658s, grad.norm=12.58894825
  6283: 4 [  975/ 1327], train_loss/perplexity = 4.81159782/122.9278793 secs/batch = 0.2620s, grad.norm=13.04229164
  6288: 4 [  980/ 1327], train_loss/perplexity = 4.59986496/99.4708786 secs/batch = 0.2659s, grad.norm=12.28542805
  6293: 4 [  985/ 1327], train_loss/perplexity = 4.68603754/108.4227066 secs/batch = 0.2640s, grad.norm=13.19501781
  6298: 4 [  990/ 1327], train_loss/perplexity = 4.88841486/132.7429962 secs/batch = 0.2659s, grad.norm=12.01427841
  6303: 4 [  995/ 1327], train_loss/perplexity = 4.95465088/141.8330841 secs/batch = 0.2667s, grad.norm=12.03634262
  6308: 4 [ 1000/ 1327], train_loss/perplexity = 4.35449219/77.8272934 secs/batch = 0.2663s, grad.norm=11.73401451
  6313: 4 [ 1005/ 1327], train_loss/perplexity = 4.85698509/128.6357880 secs/batch = 0.2654s, grad.norm=12.24449730
  6318: 4 [ 1010/ 1327], train_loss/perplexity = 4.41068745/82.3260422 secs/batch = 0.2652s, grad.norm=11.84486771
  6323: 4 [ 1015/ 1327], train_loss/perplexity = 4.95796967/142.3045807 secs/batch = 0.2652s, grad.norm=12.06111813
  6328: 4 [ 1020/ 1327], train_loss/perplexity = 5.11042833/165.7413330 secs/batch = 0.2654s, grad.norm=12.41442013
  6333: 4 [ 1025/ 1327], train_loss/perplexity = 4.93369675/138.8920135 secs/batch = 0.2667s, grad.norm=11.98646545
  6338: 4 [ 1030/ 1327], train_loss/perplexity = 4.77590275/118.6173477 secs/batch = 0.2659s, grad.norm=12.35290909
  6343: 4 [ 1035/ 1327], train_loss/perplexity = 4.60652542/100.1356125 secs/batch = 0.2649s, grad.norm=11.84936523
  6348: 4 [ 1040/ 1327], train_loss/perplexity = 4.92322588/137.4452820 secs/batch = 0.2666s, grad.norm=12.38641739
  6353: 4 [ 1045/ 1327], train_loss/perplexity = 4.45333767/85.9132156 secs/batch = 0.2598s, grad.norm=11.79928398
  6358: 4 [ 1050/ 1327], train_loss/perplexity = 4.51519489/91.3953781 secs/batch = 0.2663s, grad.norm=12.41077614
  6363: 4 [ 1055/ 1327], train_loss/perplexity = 4.69588566/109.4957428 secs/batch = 0.2666s, grad.norm=13.35770416
  6368: 4 [ 1060/ 1327], train_loss/perplexity = 4.33399868/76.2485733 secs/batch = 0.2665s, grad.norm=13.79102993
  6373: 4 [ 1065/ 1327], train_loss/perplexity = 4.44865179/85.5115814 secs/batch = 0.2650s, grad.norm=12.77105331
  6378: 4 [ 1070/ 1327], train_loss/perplexity = 4.81707048/123.6024628 secs/batch = 0.2662s, grad.norm=13.18888569
  6383: 4 [ 1075/ 1327], train_loss/perplexity = 4.56428051/95.9934998 secs/batch = 0.2660s, grad.norm=12.74870396
  6388: 4 [ 1080/ 1327], train_loss/perplexity = 4.45334339/85.9137039 secs/batch = 0.2633s, grad.norm=12.59609795
  6393: 4 [ 1085/ 1327], train_loss/perplexity = 4.29513359/73.3420105 secs/batch = 0.2667s, grad.norm=13.08815956
  6398: 4 [ 1090/ 1327], train_loss/perplexity = 4.54913855/94.5509186 secs/batch = 0.2656s, grad.norm=13.79077053
  6403: 4 [ 1095/ 1327], train_loss/perplexity = 4.69541216/109.4439087 secs/batch = 0.2655s, grad.norm=13.30253696
  6408: 4 [ 1100/ 1327], train_loss/perplexity = 4.50619650/90.5766525 secs/batch = 0.2660s, grad.norm=15.74937153
  6413: 4 [ 1105/ 1327], train_loss/perplexity = 4.38391447/80.1511688 secs/batch = 0.2647s, grad.norm=13.20318604
  6418: 4 [ 1110/ 1327], train_loss/perplexity = 4.79734707/121.1884842 secs/batch = 0.2661s, grad.norm=13.19509697
  6423: 4 [ 1115/ 1327], train_loss/perplexity = 4.50516987/90.4837112 secs/batch = 0.2659s, grad.norm=12.96250725
  6428: 4 [ 1120/ 1327], train_loss/perplexity = 4.65824795/105.4511642 secs/batch = 0.2630s, grad.norm=12.53746223
  6433: 4 [ 1125/ 1327], train_loss/perplexity = 4.92389154/137.5368042 secs/batch = 0.2656s, grad.norm=13.18109512
  6438: 4 [ 1130/ 1327], train_loss/perplexity = 4.61063051/100.5475235 secs/batch = 0.2668s, grad.norm=12.99790478
  6443: 4 [ 1135/ 1327], train_loss/perplexity = 4.62361717/101.8618164 secs/batch = 0.2668s, grad.norm=12.59147930
  6448: 4 [ 1140/ 1327], train_loss/perplexity = 4.92565918/137.7801361 secs/batch = 0.2665s, grad.norm=12.79348850
  6453: 4 [ 1145/ 1327], train_loss/perplexity = 4.64696789/104.2683487 secs/batch = 0.2666s, grad.norm=12.92465687
  6458: 4 [ 1150/ 1327], train_loss/perplexity = 4.67136288/106.8432541 secs/batch = 0.2658s, grad.norm=12.93392658
  6463: 4 [ 1155/ 1327], train_loss/perplexity = 4.78710175/119.9532089 secs/batch = 0.2654s, grad.norm=12.70783138
  6468: 4 [ 1160/ 1327], train_loss/perplexity = 4.72819233/113.0909500 secs/batch = 0.2660s, grad.norm=12.76806545
  6473: 4 [ 1165/ 1327], train_loss/perplexity = 4.73672009/114.0594788 secs/batch = 0.2594s, grad.norm=12.71525383
  6478: 4 [ 1170/ 1327], train_loss/perplexity = 4.63089800/102.6061630 secs/batch = 0.2658s, grad.norm=13.99860668
  6483: 4 [ 1175/ 1327], train_loss/perplexity = 4.35212517/77.6432953 secs/batch = 0.2662s, grad.norm=13.99362278
  6488: 4 [ 1180/ 1327], train_loss/perplexity = 4.39554119/81.0885010 secs/batch = 0.2658s, grad.norm=13.51524639
  6493: 4 [ 1185/ 1327], train_loss/perplexity = 4.55880117/95.4689636 secs/batch = 0.2653s, grad.norm=12.98096561
  6498: 4 [ 1190/ 1327], train_loss/perplexity = 4.70546961/110.5501862 secs/batch = 0.2671s, grad.norm=12.94860172
  6503: 4 [ 1195/ 1327], train_loss/perplexity = 4.52022457/91.8562241 secs/batch = 0.2662s, grad.norm=13.25793648
  6508: 4 [ 1200/ 1327], train_loss/perplexity = 4.45734453/86.2581482 secs/batch = 0.2664s, grad.norm=12.78966427
  6513: 4 [ 1205/ 1327], train_loss/perplexity = 4.41883564/82.9995880 secs/batch = 0.2648s, grad.norm=12.79658985
  6518: 4 [ 1210/ 1327], train_loss/perplexity = 4.14058685/62.8396873 secs/batch = 0.2613s, grad.norm=13.49062061
  6523: 4 [ 1215/ 1327], train_loss/perplexity = 4.33363724/76.2210159 secs/batch = 0.2638s, grad.norm=12.47276878
  6528: 4 [ 1220/ 1327], train_loss/perplexity = 4.54169703/93.8499298 secs/batch = 0.2665s, grad.norm=13.85975361
  6533: 4 [ 1225/ 1327], train_loss/perplexity = 4.35246801/77.6699142 secs/batch = 0.2663s, grad.norm=13.66861820
  6538: 4 [ 1230/ 1327], train_loss/perplexity = 4.54309177/93.9809189 secs/batch = 0.2666s, grad.norm=13.26623631
  6543: 4 [ 1235/ 1327], train_loss/perplexity = 4.49572182/89.6328430 secs/batch = 0.2663s, grad.norm=12.58140469
  6548: 4 [ 1240/ 1327], train_loss/perplexity = 4.65853262/105.4811859 secs/batch = 0.2658s, grad.norm=13.01857758
  6553: 4 [ 1245/ 1327], train_loss/perplexity = 4.58201838/97.7114105 secs/batch = 0.2668s, grad.norm=12.77148628
  6558: 4 [ 1250/ 1327], train_loss/perplexity = 4.64193916/103.7453308 secs/batch = 0.2666s, grad.norm=12.35398197
  6563: 4 [ 1255/ 1327], train_loss/perplexity = 4.69870567/109.8049545 secs/batch = 0.2659s, grad.norm=12.29610252
  6568: 4 [ 1260/ 1327], train_loss/perplexity = 4.55261230/94.8799438 secs/batch = 0.2645s, grad.norm=13.90763092
  6573: 4 [ 1265/ 1327], train_loss/perplexity = 4.78592634/119.8123016 secs/batch = 0.2663s, grad.norm=13.05393314
  6578: 4 [ 1270/ 1327], train_loss/perplexity = 4.49069595/89.1834946 secs/batch = 0.2666s, grad.norm=13.19450092
  6583: 4 [ 1275/ 1327], train_loss/perplexity = 4.76378727/117.1889114 secs/batch = 0.2665s, grad.norm=13.20606518
  6588: 4 [ 1280/ 1327], train_loss/perplexity = 4.45793533/86.3091278 secs/batch = 0.2665s, grad.norm=12.96193027
  6593: 4 [ 1285/ 1327], train_loss/perplexity = 4.45331907/85.9116211 secs/batch = 0.2669s, grad.norm=12.98388004
  6598: 4 [ 1290/ 1327], train_loss/perplexity = 4.66768789/106.4513321 secs/batch = 0.2656s, grad.norm=13.14284992
  6603: 4 [ 1295/ 1327], train_loss/perplexity = 4.65241957/104.8383408 secs/batch = 0.2668s, grad.norm=13.21336174
  6608: 4 [ 1300/ 1327], train_loss/perplexity = 4.76724958/117.5953598 secs/batch = 0.2657s, grad.norm=12.31933784
  6613: 4 [ 1305/ 1327], train_loss/perplexity = 4.94899654/141.0333710 secs/batch = 0.2664s, grad.norm=13.15952015
  6618: 4 [ 1310/ 1327], train_loss/perplexity = 5.14294624/171.2194824 secs/batch = 0.2657s, grad.norm=12.17238331
  6623: 4 [ 1315/ 1327], train_loss/perplexity = 4.95458221/141.8233490 secs/batch = 0.2656s, grad.norm=12.42987442
  6628: 4 [ 1320/ 1327], train_loss/perplexity = 4.94798136/140.8902740 secs/batch = 0.2670s, grad.norm=12.38486385
  6633: 4 [ 1325/ 1327], train_loss/perplexity = 4.86748981/129.9942017 secs/batch = 0.2671s, grad.norm=12.51356888
Epoch training time: 352.5482339859009
	> validation loss = 5.08089638, perplexity = 160.91822815
	> validation loss = 4.94800949, perplexity = 140.89422607
	> validation loss = 4.89472675, perplexity = 133.58349609
	> validation loss = 4.95750856, perplexity = 142.23896790
	> validation loss = 5.15842247, perplexity = 173.88992310
	> validation loss = 5.02286625, perplexity = 151.84590149
	> validation loss = 4.91943455, perplexity = 136.92517090
	> validation loss = 4.87870073, perplexity = 131.45974731
	> validation loss = 4.74536467, perplexity = 115.04975128
	> validation loss = 4.79608059, perplexity = 121.03510284
	> validation loss = 4.77291775, perplexity = 118.26380157
	> validation loss = 4.96065378, perplexity = 142.68705750
	> validation loss = 4.85675669, perplexity = 128.60641479
	> validation loss = 4.73638964, perplexity = 114.02179718
	> validation loss = 4.61154556, perplexity = 100.63957214
	> validation loss = 4.59832525, perplexity = 99.31784058
	> validation loss = 5.05008554, perplexity = 156.03581238
	> validation loss = 4.64723206, perplexity = 104.29589844
	> validation loss = 5.06667995, perplexity = 158.64674377
	> validation loss = 4.90160751, perplexity = 134.50582886
	> validation loss = 4.71329355, perplexity = 111.41851807
at the end of epoch: 4
train loss = 4.79719339, perplexity = 121.16986340
validation loss = 4.86800664, perplexity = 130.06139948
Saved model cv/epoch004_4.8680.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 1.0
new learning rate is: 0.5
  6640: 5 [    5/ 1327], train_loss/perplexity = 4.91029930/135.6800232 secs/batch = 0.2632s, grad.norm=12.47392082
  6645: 5 [   10/ 1327], train_loss/perplexity = 4.39601421/81.1268692 secs/batch = 0.2671s, grad.norm=12.31910324
  6650: 5 [   15/ 1327], train_loss/perplexity = 4.74420071/114.9159164 secs/batch = 0.2653s, grad.norm=11.75536442
  6655: 5 [   20/ 1327], train_loss/perplexity = 4.89060020/133.0334015 secs/batch = 0.2658s, grad.norm=12.05331612
  6660: 5 [   25/ 1327], train_loss/perplexity = 4.80035305/121.5533218 secs/batch = 0.2659s, grad.norm=12.48345375
  6665: 5 [   30/ 1327], train_loss/perplexity = 4.79749155/121.2059937 secs/batch = 0.2658s, grad.norm=12.01988602
  6670: 5 [   35/ 1327], train_loss/perplexity = 4.61728144/101.2184906 secs/batch = 0.2654s, grad.norm=12.24040890
  6675: 5 [   40/ 1327], train_loss/perplexity = 4.62270212/101.7686539 secs/batch = 0.2667s, grad.norm=12.44693947
  6680: 5 [   45/ 1327], train_loss/perplexity = 4.40397263/81.7750854 secs/batch = 0.2655s, grad.norm=11.88588142
  6685: 5 [   50/ 1327], train_loss/perplexity = 4.62889671/102.4010239 secs/batch = 0.2643s, grad.norm=12.24917126
  6690: 5 [   55/ 1327], train_loss/perplexity = 4.53638220/93.3524551 secs/batch = 0.2620s, grad.norm=12.59877205
  6695: 5 [   60/ 1327], train_loss/perplexity = 4.91772747/136.6916199 secs/batch = 0.2661s, grad.norm=12.40556717
  6700: 5 [   65/ 1327], train_loss/perplexity = 4.44169617/84.9188538 secs/batch = 0.2670s, grad.norm=12.14180660
  6705: 5 [   70/ 1327], train_loss/perplexity = 4.31766319/75.0131302 secs/batch = 0.2659s, grad.norm=12.67102623
  6710: 5 [   75/ 1327], train_loss/perplexity = 4.10419226/60.5937805 secs/batch = 0.2649s, grad.norm=11.94080639
  6715: 5 [   80/ 1327], train_loss/perplexity = 4.55884409/95.4730606 secs/batch = 0.2676s, grad.norm=12.84639740
  6720: 5 [   85/ 1327], train_loss/perplexity = 4.63098431/102.6150208 secs/batch = 0.2635s, grad.norm=12.57288074
  6725: 5 [   90/ 1327], train_loss/perplexity = 4.57167053/96.7055283 secs/batch = 0.2672s, grad.norm=12.42901134
  6730: 5 [   95/ 1327], train_loss/perplexity = 4.48643303/88.8041153 secs/batch = 0.2654s, grad.norm=12.48386574
  6735: 5 [  100/ 1327], train_loss/perplexity = 4.80931902/122.6480713 secs/batch = 0.2657s, grad.norm=12.32261848
  6740: 5 [  105/ 1327], train_loss/perplexity = 4.63741112/103.2766266 secs/batch = 0.2661s, grad.norm=13.09151840
  6745: 5 [  110/ 1327], train_loss/perplexity = 4.48857117/88.9941940 secs/batch = 0.2660s, grad.norm=12.54364872
  6750: 5 [  115/ 1327], train_loss/perplexity = 4.42175341/83.2421112 secs/batch = 0.2657s, grad.norm=12.87678432
  6755: 5 [  120/ 1327], train_loss/perplexity = 4.48351192/88.5450897 secs/batch = 0.2655s, grad.norm=12.70421886
  6760: 5 [  125/ 1327], train_loss/perplexity = 4.61052990/100.5374069 secs/batch = 0.2664s, grad.norm=12.96778202
  6765: 5 [  130/ 1327], train_loss/perplexity = 4.46887970/87.2589111 secs/batch = 0.2652s, grad.norm=12.92744160
  6770: 5 [  135/ 1327], train_loss/perplexity = 4.55263615/94.8822021 secs/batch = 0.2609s, grad.norm=12.36041737
  6775: 5 [  140/ 1327], train_loss/perplexity = 4.83847952/126.2772064 secs/batch = 0.2661s, grad.norm=12.42643929
  6780: 5 [  145/ 1327], train_loss/perplexity = 4.75658083/116.3474350 secs/batch = 0.2662s, grad.norm=13.23553753
  6785: 5 [  150/ 1327], train_loss/perplexity = 4.74797440/115.3503952 secs/batch = 0.2657s, grad.norm=12.76827717
  6790: 5 [  155/ 1327], train_loss/perplexity = 4.97576714/144.8599091 secs/batch = 0.2660s, grad.norm=12.05466843
  6795: 5 [  160/ 1327], train_loss/perplexity = 4.56860924/96.4099350 secs/batch = 0.2674s, grad.norm=12.18234348
  6800: 5 [  165/ 1327], train_loss/perplexity = 4.80350018/121.9364700 secs/batch = 0.2652s, grad.norm=11.73922539
  6805: 5 [  170/ 1327], train_loss/perplexity = 4.63432360/102.9582520 secs/batch = 0.2654s, grad.norm=12.26384068
  6810: 5 [  175/ 1327], train_loss/perplexity = 4.84764147/127.4394684 secs/batch = 0.2604s, grad.norm=12.44294167
  6815: 5 [  180/ 1327], train_loss/perplexity = 4.70558023/110.5624161 secs/batch = 0.2650s, grad.norm=12.91513634
  6820: 5 [  185/ 1327], train_loss/perplexity = 4.95118141/141.3418427 secs/batch = 0.2663s, grad.norm=12.08490181
  6825: 5 [  190/ 1327], train_loss/perplexity = 4.50786734/90.7281189 secs/batch = 0.2660s, grad.norm=11.78180599
  6830: 5 [  195/ 1327], train_loss/perplexity = 4.77048445/117.9763794 secs/batch = 0.2655s, grad.norm=11.64357376
  6835: 5 [  200/ 1327], train_loss/perplexity = 4.69773817/109.6987686 secs/batch = 0.2652s, grad.norm=12.22319984
  6840: 5 [  205/ 1327], train_loss/perplexity = 4.84027481/126.5041122 secs/batch = 0.2654s, grad.norm=11.93732262
  6845: 5 [  210/ 1327], train_loss/perplexity = 4.73001766/113.2975616 secs/batch = 0.2665s, grad.norm=11.41759586
  6850: 5 [  215/ 1327], train_loss/perplexity = 4.88219309/131.9196625 secs/batch = 0.2663s, grad.norm=11.92460251
  6855: 5 [  220/ 1327], train_loss/perplexity = 4.84038687/126.5182877 secs/batch = 0.2659s, grad.norm=12.11613750
  6860: 5 [  225/ 1327], train_loss/perplexity = 4.99690056/147.9538727 secs/batch = 0.2667s, grad.norm=12.23608589
  6865: 5 [  230/ 1327], train_loss/perplexity = 4.75837946/116.5568848 secs/batch = 0.2660s, grad.norm=12.60280037
  6870: 5 [  235/ 1327], train_loss/perplexity = 4.62933207/102.4456177 secs/batch = 0.2652s, grad.norm=12.20713520
  6875: 5 [  240/ 1327], train_loss/perplexity = 4.45724726/86.2497559 secs/batch = 0.2647s, grad.norm=12.53148651
  6880: 5 [  245/ 1327], train_loss/perplexity = 4.75488329/116.1500931 secs/batch = 0.2659s, grad.norm=12.06642151
  6885: 5 [  250/ 1327], train_loss/perplexity = 4.49449158/89.5226440 secs/batch = 0.2670s, grad.norm=12.05016136
  6890: 5 [  255/ 1327], train_loss/perplexity = 4.52621365/92.4080048 secs/batch = 0.2669s, grad.norm=12.25015450
  6895: 5 [  260/ 1327], train_loss/perplexity = 4.85044193/127.7968521 secs/batch = 0.2659s, grad.norm=13.35435867
  6900: 5 [  265/ 1327], train_loss/perplexity = 4.92883015/138.2177277 secs/batch = 0.2664s, grad.norm=11.78472328
  6905: 5 [  270/ 1327], train_loss/perplexity = 4.97817326/145.2088776 secs/batch = 0.2666s, grad.norm=11.91205406
  6910: 5 [  275/ 1327], train_loss/perplexity = 4.99693680/147.9592285 secs/batch = 0.2657s, grad.norm=12.11005306
  6915: 5 [  280/ 1327], train_loss/perplexity = 4.76130629/116.8985291 secs/batch = 0.2661s, grad.norm=12.17218208
  6920: 5 [  285/ 1327], train_loss/perplexity = 5.06077909/157.7133484 secs/batch = 0.2664s, grad.norm=12.40751362
  6925: 5 [  290/ 1327], train_loss/perplexity = 4.74341726/114.8259201 secs/batch = 0.2642s, grad.norm=12.19767189
  6930: 5 [  295/ 1327], train_loss/perplexity = 4.53962708/93.6558685 secs/batch = 0.2666s, grad.norm=12.13941002
  6935: 5 [  300/ 1327], train_loss/perplexity = 4.11468554/61.2329559 secs/batch = 0.2658s, grad.norm=12.18713760
  6940: 5 [  305/ 1327], train_loss/perplexity = 4.61902761/101.3953857 secs/batch = 0.2665s, grad.norm=11.94536781
  6945: 5 [  310/ 1327], train_loss/perplexity = 4.62739182/102.2470398 secs/batch = 0.2640s, grad.norm=11.96961212
  6950: 5 [  315/ 1327], train_loss/perplexity = 4.14983988/63.4238434 secs/batch = 0.2656s, grad.norm=12.07401371
  6955: 5 [  320/ 1327], train_loss/perplexity = 4.18870544/65.9373779 secs/batch = 0.2647s, grad.norm=14.33421612
  6960: 5 [  325/ 1327], train_loss/perplexity = 4.14785528/63.2980995 secs/batch = 0.2674s, grad.norm=11.79337215
  6965: 5 [  330/ 1327], train_loss/perplexity = 4.63530731/103.0595856 secs/batch = 0.2676s, grad.norm=12.63653564
  6970: 5 [  335/ 1327], train_loss/perplexity = 4.07230282/58.6919632 secs/batch = 0.2656s, grad.norm=12.02035809
  6975: 5 [  340/ 1327], train_loss/perplexity = 4.82678795/124.8094254 secs/batch = 0.2657s, grad.norm=12.16730309
  6980: 5 [  345/ 1327], train_loss/perplexity = 4.66932106/106.6253281 secs/batch = 0.2656s, grad.norm=12.12077332
  6985: 5 [  350/ 1327], train_loss/perplexity = 4.72707891/112.9650955 secs/batch = 0.2660s, grad.norm=12.96660995
  6990: 5 [  355/ 1327], train_loss/perplexity = 4.72729063/112.9890213 secs/batch = 0.2658s, grad.norm=12.61646175
  6995: 5 [  360/ 1327], train_loss/perplexity = 4.87207508/130.5916290 secs/batch = 0.2661s, grad.norm=13.47476196
  7000: 5 [  365/ 1327], train_loss/perplexity = 4.82707500/124.8452530 secs/batch = 0.2659s, grad.norm=12.17920876
  7005: 5 [  370/ 1327], train_loss/perplexity = 4.80416775/122.0178986 secs/batch = 0.2652s, grad.norm=12.11639118
  7010: 5 [  375/ 1327], train_loss/perplexity = 4.23044682/68.7479401 secs/batch = 0.2666s, grad.norm=12.87794876
  7015: 5 [  380/ 1327], train_loss/perplexity = 4.33410072/76.2563553 secs/batch = 0.2663s, grad.norm=12.68701077
  7020: 5 [  385/ 1327], train_loss/perplexity = 4.51886272/91.7312164 secs/batch = 0.2646s, grad.norm=12.70303535
  7025: 5 [  390/ 1327], train_loss/perplexity = 4.64756250/104.3303680 secs/batch = 0.2650s, grad.norm=12.58863163
  7030: 5 [  395/ 1327], train_loss/perplexity = 4.73377609/113.7241821 secs/batch = 0.2616s, grad.norm=13.01584625
  7035: 5 [  400/ 1327], train_loss/perplexity = 4.57929087/97.4452667 secs/batch = 0.2657s, grad.norm=12.23378277
  7040: 5 [  405/ 1327], train_loss/perplexity = 4.88307858/132.0365295 secs/batch = 0.2651s, grad.norm=12.47871399
  7045: 5 [  410/ 1327], train_loss/perplexity = 4.63734913/103.2702255 secs/batch = 0.2635s, grad.norm=12.59454250
  7050: 5 [  415/ 1327], train_loss/perplexity = 4.43658018/84.4855194 secs/batch = 0.2664s, grad.norm=12.26206493
  7055: 5 [  420/ 1327], train_loss/perplexity = 4.20016623/66.6974182 secs/batch = 0.2659s, grad.norm=12.43867397
  7060: 5 [  425/ 1327], train_loss/perplexity = 4.48561049/88.7311020 secs/batch = 0.2663s, grad.norm=13.57335663
  7065: 5 [  430/ 1327], train_loss/perplexity = 4.73476601/113.8368149 secs/batch = 0.2656s, grad.norm=13.44941425
  7070: 5 [  435/ 1327], train_loss/perplexity = 4.73973036/114.4033508 secs/batch = 0.2651s, grad.norm=12.84502888
  7075: 5 [  440/ 1327], train_loss/perplexity = 4.34907103/77.4065247 secs/batch = 0.2658s, grad.norm=12.65630817
  7080: 5 [  445/ 1327], train_loss/perplexity = 4.65498066/105.1071854 secs/batch = 0.2652s, grad.norm=12.92687225
  7085: 5 [  450/ 1327], train_loss/perplexity = 4.51296425/91.1917343 secs/batch = 0.2662s, grad.norm=12.67182064
  7090: 5 [  455/ 1327], train_loss/perplexity = 4.45342207/85.9204636 secs/batch = 0.2660s, grad.norm=12.23603916
  7095: 5 [  460/ 1327], train_loss/perplexity = 4.47526073/87.8174973 secs/batch = 0.2656s, grad.norm=12.81327629
  7100: 5 [  465/ 1327], train_loss/perplexity = 4.33177614/76.0792923 secs/batch = 0.2673s, grad.norm=13.99478722
  7105: 5 [  470/ 1327], train_loss/perplexity = 4.90928650/135.5426636 secs/batch = 0.2662s, grad.norm=11.85085011
  7110: 5 [  475/ 1327], train_loss/perplexity = 4.39325523/80.9033508 secs/batch = 0.2611s, grad.norm=12.57598686
  7115: 5 [  480/ 1327], train_loss/perplexity = 4.61710119/101.2002487 secs/batch = 0.2668s, grad.norm=12.58637047
  7120: 5 [  485/ 1327], train_loss/perplexity = 4.51026058/90.9455109 secs/batch = 0.2582s, grad.norm=12.56456184
  7125: 5 [  490/ 1327], train_loss/perplexity = 4.42572498/83.5733719 secs/batch = 0.2665s, grad.norm=13.37747765
  7130: 5 [  495/ 1327], train_loss/perplexity = 4.37631416/79.5443039 secs/batch = 0.2656s, grad.norm=12.52254581
  7135: 5 [  500/ 1327], train_loss/perplexity = 4.61608934/101.0979004 secs/batch = 0.2656s, grad.norm=12.75016308
  7140: 5 [  505/ 1327], train_loss/perplexity = 4.75459814/116.1169815 secs/batch = 0.2662s, grad.norm=11.72706890
  7145: 5 [  510/ 1327], train_loss/perplexity = 5.09308720/162.8919678 secs/batch = 0.2670s, grad.norm=11.37271214
  7150: 5 [  515/ 1327], train_loss/perplexity = 4.70753384/110.7786255 secs/batch = 0.2664s, grad.norm=11.86371231
  7155: 5 [  520/ 1327], train_loss/perplexity = 4.87951374/131.5666656 secs/batch = 0.2635s, grad.norm=12.37674236
  7160: 5 [  525/ 1327], train_loss/perplexity = 4.43401146/84.2687836 secs/batch = 0.2614s, grad.norm=12.36515617
  7165: 5 [  530/ 1327], train_loss/perplexity = 4.49335289/89.4207611 secs/batch = 0.2662s, grad.norm=13.00650692
  7170: 5 [  535/ 1327], train_loss/perplexity = 4.63162565/102.6808548 secs/batch = 0.2662s, grad.norm=12.70357990
  7175: 5 [  540/ 1327], train_loss/perplexity = 4.69578838/109.4850922 secs/batch = 0.2663s, grad.norm=11.84613323
  7180: 5 [  545/ 1327], train_loss/perplexity = 4.69947577/109.8895493 secs/batch = 0.2659s, grad.norm=12.75147057
  7185: 5 [  550/ 1327], train_loss/perplexity = 4.66799545/106.4840775 secs/batch = 0.2642s, grad.norm=12.59566689
  7190: 5 [  555/ 1327], train_loss/perplexity = 4.47129583/87.4699936 secs/batch = 0.2664s, grad.norm=12.32853031
  7195: 5 [  560/ 1327], train_loss/perplexity = 4.57171535/96.7098618 secs/batch = 0.2620s, grad.norm=13.40109634
  7200: 5 [  565/ 1327], train_loss/perplexity = 4.53558159/93.2777481 secs/batch = 0.2643s, grad.norm=13.11083984
  7205: 5 [  570/ 1327], train_loss/perplexity = 4.48358870/88.5518875 secs/batch = 0.2669s, grad.norm=13.06177521
  7210: 5 [  575/ 1327], train_loss/perplexity = 4.25630665/70.5489426 secs/batch = 0.2657s, grad.norm=12.67641354
  7215: 5 [  580/ 1327], train_loss/perplexity = 4.69586277/109.4932327 secs/batch = 0.2582s, grad.norm=12.61066723
  7220: 5 [  585/ 1327], train_loss/perplexity = 4.24102116/69.4787674 secs/batch = 0.2670s, grad.norm=12.57232285
  7225: 5 [  590/ 1327], train_loss/perplexity = 4.61799335/101.2905731 secs/batch = 0.2664s, grad.norm=12.29199123
  7230: 5 [  595/ 1327], train_loss/perplexity = 4.54876995/94.5160751 secs/batch = 0.2658s, grad.norm=12.83496380
  7235: 5 [  600/ 1327], train_loss/perplexity = 4.86713648/129.9482727 secs/batch = 0.2659s, grad.norm=12.19500637
  7240: 5 [  605/ 1327], train_loss/perplexity = 4.69540691/109.4433365 secs/batch = 0.2644s, grad.norm=12.66569901
  7245: 5 [  610/ 1327], train_loss/perplexity = 4.87537289/131.0229950 secs/batch = 0.2663s, grad.norm=12.82622147
  7250: 5 [  615/ 1327], train_loss/perplexity = 4.33952475/76.6710892 secs/batch = 0.2651s, grad.norm=12.22320461
  7255: 5 [  620/ 1327], train_loss/perplexity = 4.78869295/120.1442337 secs/batch = 0.2661s, grad.norm=12.70739841
  7260: 5 [  625/ 1327], train_loss/perplexity = 4.79541874/120.9550171 secs/batch = 0.2666s, grad.norm=12.22380161
  7265: 5 [  630/ 1327], train_loss/perplexity = 4.85415602/128.2723846 secs/batch = 0.2656s, grad.norm=12.29608631
  7270: 5 [  635/ 1327], train_loss/perplexity = 4.59393835/98.8831024 secs/batch = 0.2660s, grad.norm=13.13091087
  7275: 5 [  640/ 1327], train_loss/perplexity = 4.54205465/93.8834991 secs/batch = 0.2663s, grad.norm=12.93968010
  7280: 5 [  645/ 1327], train_loss/perplexity = 4.84328175/126.8850708 secs/batch = 0.2659s, grad.norm=13.35186577
  7285: 5 [  650/ 1327], train_loss/perplexity = 4.29215288/73.1237259 secs/batch = 0.2663s, grad.norm=13.00871658
  7290: 5 [  655/ 1327], train_loss/perplexity = 4.51682091/91.5441055 secs/batch = 0.2649s, grad.norm=12.84395695
  7295: 5 [  660/ 1327], train_loss/perplexity = 4.40467358/81.8324280 secs/batch = 0.2651s, grad.norm=12.54101753
  7300: 5 [  665/ 1327], train_loss/perplexity = 4.64932585/104.5145035 secs/batch = 0.2665s, grad.norm=12.94946480
  7305: 5 [  670/ 1327], train_loss/perplexity = 4.54265499/93.9398804 secs/batch = 0.2666s, grad.norm=12.64111423
  7310: 5 [  675/ 1327], train_loss/perplexity = 4.31499672/74.8133774 secs/batch = 0.2650s, grad.norm=12.79496574
  7315: 5 [  680/ 1327], train_loss/perplexity = 4.53950357/93.6443024 secs/batch = 0.2657s, grad.norm=13.37467384
  7320: 5 [  685/ 1327], train_loss/perplexity = 4.37608767/79.5262909 secs/batch = 0.2644s, grad.norm=12.93155479
  7325: 5 [  690/ 1327], train_loss/perplexity = 4.83149290/125.3980255 secs/batch = 0.2671s, grad.norm=12.37458038
  7330: 5 [  695/ 1327], train_loss/perplexity = 4.58030844/97.5444794 secs/batch = 0.2656s, grad.norm=12.91793728
  7335: 5 [  700/ 1327], train_loss/perplexity = 4.84127045/126.6301270 secs/batch = 0.2662s, grad.norm=12.93125725
  7340: 5 [  705/ 1327], train_loss/perplexity = 4.50191689/90.1898499 secs/batch = 0.2658s, grad.norm=12.19155025
  7345: 5 [  710/ 1327], train_loss/perplexity = 4.48581314/88.7490845 secs/batch = 0.2660s, grad.norm=13.12879562
  7350: 5 [  715/ 1327], train_loss/perplexity = 4.40138912/81.5640945 secs/batch = 0.2669s, grad.norm=12.57440472
  7355: 5 [  720/ 1327], train_loss/perplexity = 4.43062401/83.9838104 secs/batch = 0.2660s, grad.norm=13.09751606
  7360: 5 [  725/ 1327], train_loss/perplexity = 4.40952682/82.2305450 secs/batch = 0.2667s, grad.norm=12.99889469
  7365: 5 [  730/ 1327], train_loss/perplexity = 4.53176355/92.9222870 secs/batch = 0.2652s, grad.norm=12.72204494
  7370: 5 [  735/ 1327], train_loss/perplexity = 4.68034029/107.8067551 secs/batch = 0.2660s, grad.norm=13.23862648
  7375: 5 [  740/ 1327], train_loss/perplexity = 4.05664396/57.7800751 secs/batch = 0.2661s, grad.norm=12.03785992
  7380: 5 [  745/ 1327], train_loss/perplexity = 4.54190159/93.8691330 secs/batch = 0.2666s, grad.norm=12.52604675
  7385: 5 [  750/ 1327], train_loss/perplexity = 4.37042809/79.0774765 secs/batch = 0.2672s, grad.norm=12.46558762
  7390: 5 [  755/ 1327], train_loss/perplexity = 4.34096718/76.7817688 secs/batch = 0.2636s, grad.norm=12.72306919
  7395: 5 [  760/ 1327], train_loss/perplexity = 4.19350767/66.2547836 secs/batch = 0.2643s, grad.norm=12.48381424
  7400: 5 [  765/ 1327], train_loss/perplexity = 4.23929787/69.3591385 secs/batch = 0.2662s, grad.norm=12.31774712
  7405: 5 [  770/ 1327], train_loss/perplexity = 4.26507854/71.1705093 secs/batch = 0.2651s, grad.norm=12.42475414
  7410: 5 [  775/ 1327], train_loss/perplexity = 4.42773342/83.7413940 secs/batch = 0.2666s, grad.norm=13.12340355
  7415: 5 [  780/ 1327], train_loss/perplexity = 4.78485966/119.6845627 secs/batch = 0.2652s, grad.norm=12.99344635
  7420: 5 [  785/ 1327], train_loss/perplexity = 4.54657602/94.3089447 secs/batch = 0.2656s, grad.norm=13.05911732
  7425: 5 [  790/ 1327], train_loss/perplexity = 4.31608725/74.8950119 secs/batch = 0.2658s, grad.norm=12.92853546
  7430: 5 [  795/ 1327], train_loss/perplexity = 4.75756693/116.4622192 secs/batch = 0.2668s, grad.norm=12.69786072
  7435: 5 [  800/ 1327], train_loss/perplexity = 4.55144310/94.7690735 secs/batch = 0.2653s, grad.norm=13.01124096
  7440: 5 [  805/ 1327], train_loss/perplexity = 4.94197178/140.0461121 secs/batch = 0.2659s, grad.norm=12.57095051
  7445: 5 [  810/ 1327], train_loss/perplexity = 4.55954218/95.5397263 secs/batch = 0.2653s, grad.norm=12.34468555
  7450: 5 [  815/ 1327], train_loss/perplexity = 4.49783993/89.8228989 secs/batch = 0.2680s, grad.norm=12.41845417
  7455: 5 [  820/ 1327], train_loss/perplexity = 4.16445303/64.3574677 secs/batch = 0.2643s, grad.norm=11.88262749
  7460: 5 [  825/ 1327], train_loss/perplexity = 4.41000748/82.2700806 secs/batch = 0.2664s, grad.norm=12.65762806
  7465: 5 [  830/ 1327], train_loss/perplexity = 4.22567892/68.4209366 secs/batch = 0.2676s, grad.norm=13.02764225
  7470: 5 [  835/ 1327], train_loss/perplexity = 4.52536821/92.3299179 secs/batch = 0.2663s, grad.norm=13.27592754
  7475: 5 [  840/ 1327], train_loss/perplexity = 4.53013372/92.7709656 secs/batch = 0.2656s, grad.norm=12.72992802
  7480: 5 [  845/ 1327], train_loss/perplexity = 4.39706230/81.2119446 secs/batch = 0.2668s, grad.norm=12.91989708
  7485: 5 [  850/ 1327], train_loss/perplexity = 4.45659590/86.1935959 secs/batch = 0.2659s, grad.norm=12.70967484
  7490: 5 [  855/ 1327], train_loss/perplexity = 4.42469168/83.4870605 secs/batch = 0.2672s, grad.norm=12.75567722
  7495: 5 [  860/ 1327], train_loss/perplexity = 4.20089817/66.7462540 secs/batch = 0.2665s, grad.norm=12.41391468
  7500: 5 [  865/ 1327], train_loss/perplexity = 4.65007925/104.5932770 secs/batch = 0.2667s, grad.norm=12.58755207
  7505: 5 [  870/ 1327], train_loss/perplexity = 4.55734825/95.3303528 secs/batch = 0.2659s, grad.norm=12.91640663
  7510: 5 [  875/ 1327], train_loss/perplexity = 4.10084152/60.3910866 secs/batch = 0.2660s, grad.norm=12.55982304
  7515: 5 [  880/ 1327], train_loss/perplexity = 4.36334991/78.5197296 secs/batch = 0.2662s, grad.norm=12.33897114
  7520: 5 [  885/ 1327], train_loss/perplexity = 4.50977850/90.9016800 secs/batch = 0.2673s, grad.norm=12.75679493
  7525: 5 [  890/ 1327], train_loss/perplexity = 4.66601372/106.2732620 secs/batch = 0.2645s, grad.norm=12.67839718
  7530: 5 [  895/ 1327], train_loss/perplexity = 4.72642517/112.8912735 secs/batch = 0.2659s, grad.norm=12.41028404
  7535: 5 [  900/ 1327], train_loss/perplexity = 4.48487663/88.6660080 secs/batch = 0.2664s, grad.norm=12.50369453
  7540: 5 [  905/ 1327], train_loss/perplexity = 4.30275249/73.9029312 secs/batch = 0.2646s, grad.norm=12.27248192
  7545: 5 [  910/ 1327], train_loss/perplexity = 4.38617706/80.3327255 secs/batch = 0.2653s, grad.norm=12.25677586
  7550: 5 [  915/ 1327], train_loss/perplexity = 4.64396048/103.9552460 secs/batch = 0.2653s, grad.norm=12.41208267
  7555: 5 [  920/ 1327], train_loss/perplexity = 4.82159472/124.1629410 secs/batch = 0.2655s, grad.norm=12.89615631
  7560: 5 [  925/ 1327], train_loss/perplexity = 4.58479929/97.9835205 secs/batch = 0.2659s, grad.norm=12.58624458
  7565: 5 [  930/ 1327], train_loss/perplexity = 4.53376293/93.1082611 secs/batch = 0.2671s, grad.norm=12.34100246
  7570: 5 [  935/ 1327], train_loss/perplexity = 4.65444088/105.0504684 secs/batch = 0.2660s, grad.norm=12.47535133
  7575: 5 [  940/ 1327], train_loss/perplexity = 4.58733034/98.2318344 secs/batch = 0.2673s, grad.norm=12.40008450
  7580: 5 [  945/ 1327], train_loss/perplexity = 4.78783321/120.0409851 secs/batch = 0.2662s, grad.norm=12.44177151
  7585: 5 [  950/ 1327], train_loss/perplexity = 4.55537605/95.1425247 secs/batch = 0.2651s, grad.norm=12.47139168
  7590: 5 [  955/ 1327], train_loss/perplexity = 4.58800030/98.2976685 secs/batch = 0.2666s, grad.norm=12.62065220
  7595: 5 [  960/ 1327], train_loss/perplexity = 4.89655781/133.8283234 secs/batch = 0.2662s, grad.norm=12.35523796
  7600: 5 [  965/ 1327], train_loss/perplexity = 4.61933947/101.4270172 secs/batch = 0.2661s, grad.norm=12.54808235
  7605: 5 [  970/ 1327], train_loss/perplexity = 4.83250237/125.5246735 secs/batch = 0.2636s, grad.norm=12.42832947
  7610: 5 [  975/ 1327], train_loss/perplexity = 4.54150629/93.8320312 secs/batch = 0.2662s, grad.norm=13.22463226
  7615: 5 [  980/ 1327], train_loss/perplexity = 4.33341265/76.2039032 secs/batch = 0.2663s, grad.norm=12.34514713
  7620: 5 [  985/ 1327], train_loss/perplexity = 4.48721790/88.8738480 secs/batch = 0.2667s, grad.norm=12.85487556
  7625: 5 [  990/ 1327], train_loss/perplexity = 4.70580482/110.5872498 secs/batch = 0.2658s, grad.norm=12.81602669
  7630: 5 [  995/ 1327], train_loss/perplexity = 4.73114491/113.4253464 secs/batch = 0.2659s, grad.norm=12.51951885
  7635: 5 [ 1000/ 1327], train_loss/perplexity = 4.17076683/64.7650986 secs/batch = 0.2662s, grad.norm=12.24897575
  7640: 5 [ 1005/ 1327], train_loss/perplexity = 4.64219952/103.7723465 secs/batch = 0.2659s, grad.norm=13.00587654
  7645: 5 [ 1010/ 1327], train_loss/perplexity = 4.23767376/69.2465820 secs/batch = 0.2636s, grad.norm=11.96905422
  7650: 5 [ 1015/ 1327], train_loss/perplexity = 4.69194269/109.0648499 secs/batch = 0.2641s, grad.norm=12.20878315
  7655: 5 [ 1020/ 1327], train_loss/perplexity = 4.82894087/125.0784149 secs/batch = 0.2656s, grad.norm=12.10324287
  7660: 5 [ 1025/ 1327], train_loss/perplexity = 4.77385092/118.3742142 secs/batch = 0.2662s, grad.norm=12.20199013
  7665: 5 [ 1030/ 1327], train_loss/perplexity = 4.51478243/91.3576889 secs/batch = 0.2664s, grad.norm=11.82776356
  7670: 5 [ 1035/ 1327], train_loss/perplexity = 4.43778515/84.5873871 secs/batch = 0.2673s, grad.norm=12.17182541
  7675: 5 [ 1040/ 1327], train_loss/perplexity = 4.71933746/112.0939636 secs/batch = 0.2660s, grad.norm=12.69524670
  7680: 5 [ 1045/ 1327], train_loss/perplexity = 4.15149403/63.5288429 secs/batch = 0.2662s, grad.norm=12.46203232
  7685: 5 [ 1050/ 1327], train_loss/perplexity = 4.32243538/75.3719635 secs/batch = 0.2667s, grad.norm=13.07947254
  7690: 5 [ 1055/ 1327], train_loss/perplexity = 4.49429274/89.5048447 secs/batch = 0.2642s, grad.norm=13.87442684
  7695: 5 [ 1060/ 1327], train_loss/perplexity = 4.09322357/59.9327774 secs/batch = 0.2657s, grad.norm=13.95485115
  7700: 5 [ 1065/ 1327], train_loss/perplexity = 4.23786736/69.2599869 secs/batch = 0.2661s, grad.norm=12.89938545
  7705: 5 [ 1070/ 1327], train_loss/perplexity = 4.59236002/98.7271500 secs/batch = 0.2664s, grad.norm=13.08012390
  7710: 5 [ 1075/ 1327], train_loss/perplexity = 4.36434460/78.5978699 secs/batch = 0.2675s, grad.norm=12.86638165
  7715: 5 [ 1080/ 1327], train_loss/perplexity = 4.25308228/70.3218307 secs/batch = 0.2671s, grad.norm=12.71853828
  7720: 5 [ 1085/ 1327], train_loss/perplexity = 4.08017731/59.1559563 secs/batch = 0.2638s, grad.norm=12.83678818
  7725: 5 [ 1090/ 1327], train_loss/perplexity = 4.29176903/73.0956650 secs/batch = 0.2665s, grad.norm=13.63904572
  7730: 5 [ 1095/ 1327], train_loss/perplexity = 4.47989798/88.2256699 secs/batch = 0.2661s, grad.norm=13.25489044
  7735: 5 [ 1100/ 1327], train_loss/perplexity = 4.22062969/68.0763397 secs/batch = 0.2621s, grad.norm=14.78683472
  7740: 5 [ 1105/ 1327], train_loss/perplexity = 4.16358089/64.3013687 secs/batch = 0.2661s, grad.norm=13.62557602
  7745: 5 [ 1110/ 1327], train_loss/perplexity = 4.52080250/91.9093246 secs/batch = 0.2669s, grad.norm=13.12226582
  7750: 5 [ 1115/ 1327], train_loss/perplexity = 4.28645420/72.7081985 secs/batch = 0.2663s, grad.norm=12.68301105
  7755: 5 [ 1120/ 1327], train_loss/perplexity = 4.46881580/87.2533340 secs/batch = 0.2664s, grad.norm=12.79277992
  7760: 5 [ 1125/ 1327], train_loss/perplexity = 4.71235180/111.3136368 secs/batch = 0.2665s, grad.norm=13.29236221
  7765: 5 [ 1130/ 1327], train_loss/perplexity = 4.34748554/77.2838898 secs/batch = 0.2668s, grad.norm=12.89077377
  7770: 5 [ 1135/ 1327], train_loss/perplexity = 4.39473295/81.0229874 secs/batch = 0.2659s, grad.norm=12.73024273
  7775: 5 [ 1140/ 1327], train_loss/perplexity = 4.68136215/107.9169693 secs/batch = 0.2653s, grad.norm=13.46882057
  7780: 5 [ 1145/ 1327], train_loss/perplexity = 4.42423630/83.4490509 secs/batch = 0.2623s, grad.norm=12.67921734
  7785: 5 [ 1150/ 1327], train_loss/perplexity = 4.40767574/82.0784683 secs/batch = 0.2639s, grad.norm=12.84939384
  7790: 5 [ 1155/ 1327], train_loss/perplexity = 4.49124289/89.2322845 secs/batch = 0.2613s, grad.norm=13.24328995
  7795: 5 [ 1160/ 1327], train_loss/perplexity = 4.46814060/87.1944427 secs/batch = 0.2669s, grad.norm=12.82637787
  7800: 5 [ 1165/ 1327], train_loss/perplexity = 4.47432804/87.7356262 secs/batch = 0.2621s, grad.norm=12.75847912
  7805: 5 [ 1170/ 1327], train_loss/perplexity = 4.33462238/76.2961426 secs/batch = 0.2659s, grad.norm=12.70702267
  7810: 5 [ 1175/ 1327], train_loss/perplexity = 4.11236715/61.0911598 secs/batch = 0.2661s, grad.norm=13.22418022
  7815: 5 [ 1180/ 1327], train_loss/perplexity = 4.18046141/65.3960190 secs/batch = 0.2637s, grad.norm=13.39172268
  7820: 5 [ 1185/ 1327], train_loss/perplexity = 4.38315916/80.0906525 secs/batch = 0.2659s, grad.norm=13.11732101
  7825: 5 [ 1190/ 1327], train_loss/perplexity = 4.50327206/90.3121567 secs/batch = 0.2670s, grad.norm=13.17252350
  7830: 5 [ 1195/ 1327], train_loss/perplexity = 4.29907036/73.6313095 secs/batch = 0.2668s, grad.norm=12.84612751
  7835: 5 [ 1200/ 1327], train_loss/perplexity = 4.18524456/65.7095718 secs/batch = 0.2672s, grad.norm=12.97896576
  7840: 5 [ 1205/ 1327], train_loss/perplexity = 4.23696804/69.1977310 secs/batch = 0.2670s, grad.norm=13.25965309
  7845: 5 [ 1210/ 1327], train_loss/perplexity = 3.88196707/48.5195618 secs/batch = 0.2672s, grad.norm=13.31865978
  7850: 5 [ 1215/ 1327], train_loss/perplexity = 4.11876583/61.4833145 secs/batch = 0.2667s, grad.norm=12.71848106
  7855: 5 [ 1220/ 1327], train_loss/perplexity = 4.27303982/71.7393799 secs/batch = 0.2647s, grad.norm=13.45034218
  7860: 5 [ 1225/ 1327], train_loss/perplexity = 4.03353786/56.4603081 secs/batch = 0.2669s, grad.norm=13.70033932
  7865: 5 [ 1230/ 1327], train_loss/perplexity = 4.28324509/72.4752502 secs/batch = 0.2662s, grad.norm=13.02875519
  7870: 5 [ 1235/ 1327], train_loss/perplexity = 4.23138237/68.8122940 secs/batch = 0.2657s, grad.norm=12.91306496
  7875: 5 [ 1240/ 1327], train_loss/perplexity = 4.42766666/83.7358017 secs/batch = 0.2666s, grad.norm=13.33824730
  7880: 5 [ 1245/ 1327], train_loss/perplexity = 4.40098190/81.5308838 secs/batch = 0.2664s, grad.norm=12.83071899
  7885: 5 [ 1250/ 1327], train_loss/perplexity = 4.49150991/89.2561111 secs/batch = 0.2649s, grad.norm=12.62063789
  7890: 5 [ 1255/ 1327], train_loss/perplexity = 4.50561619/90.5241089 secs/batch = 0.2653s, grad.norm=12.40720844
  7895: 5 [ 1260/ 1327], train_loss/perplexity = 4.37994623/79.8337402 secs/batch = 0.2672s, grad.norm=13.77362251
  7900: 5 [ 1265/ 1327], train_loss/perplexity = 4.52268982/92.0829544 secs/batch = 0.2672s, grad.norm=13.74131107
  7905: 5 [ 1270/ 1327], train_loss/perplexity = 4.25044727/70.1367722 secs/batch = 0.2656s, grad.norm=13.34780121
  7910: 5 [ 1275/ 1327], train_loss/perplexity = 4.49816227/89.8518524 secs/batch = 0.2667s, grad.norm=13.51813984
  7915: 5 [ 1280/ 1327], train_loss/perplexity = 4.28403473/72.5325012 secs/batch = 0.2658s, grad.norm=13.05105114
  7920: 5 [ 1285/ 1327], train_loss/perplexity = 4.21170950/67.4717865 secs/batch = 0.2656s, grad.norm=13.08540249
  7925: 5 [ 1290/ 1327], train_loss/perplexity = 4.46181822/86.6449051 secs/batch = 0.2666s, grad.norm=13.12011623
  7930: 5 [ 1295/ 1327], train_loss/perplexity = 4.51271772/91.1692581 secs/batch = 0.2624s, grad.norm=13.31157303
  7935: 5 [ 1300/ 1327], train_loss/perplexity = 4.58811760/98.3091965 secs/batch = 0.2663s, grad.norm=12.52711678
  7940: 5 [ 1305/ 1327], train_loss/perplexity = 4.73249674/113.5787888 secs/batch = 0.2648s, grad.norm=13.11023140
  7945: 5 [ 1310/ 1327], train_loss/perplexity = 4.88281155/132.0012665 secs/batch = 0.2650s, grad.norm=13.02333355
  7950: 5 [ 1315/ 1327], train_loss/perplexity = 4.70579338/110.5859833 secs/batch = 0.2665s, grad.norm=13.02198124
  7955: 5 [ 1320/ 1327], train_loss/perplexity = 4.72554970/112.7924805 secs/batch = 0.2655s, grad.norm=13.06736946
  7960: 5 [ 1325/ 1327], train_loss/perplexity = 4.65605736/105.2204132 secs/batch = 0.2666s, grad.norm=12.91471100
Epoch training time: 352.61197113990784
	> validation loss = 4.80039883, perplexity = 121.55889130
	> validation loss = 4.71479082, perplexity = 111.58546448
	> validation loss = 4.64541245, perplexity = 104.10629272
	> validation loss = 4.65206528, perplexity = 104.80120850
	> validation loss = 4.85914755, perplexity = 128.91426086
	> validation loss = 4.77307320, perplexity = 118.28218842
	> validation loss = 4.72819901, perplexity = 113.09169769
	> validation loss = 4.57893085, perplexity = 97.41019440
	> validation loss = 4.37705946, perplexity = 79.60361481
	> validation loss = 4.51025820, perplexity = 90.94529724
	> validation loss = 4.58420563, perplexity = 97.92536926
	> validation loss = 4.70148659, perplexity = 110.11074066
	> validation loss = 4.62291861, perplexity = 101.79068756
	> validation loss = 4.45115280, perplexity = 85.72570801
	> validation loss = 4.30664635, perplexity = 74.19126129
	> validation loss = 4.35074377, perplexity = 77.53610992
	> validation loss = 4.79652166, perplexity = 121.08850098
	> validation loss = 4.39271355, perplexity = 80.85953522
	> validation loss = 4.76823568, perplexity = 117.71138000
	> validation loss = 4.62247229, perplexity = 101.74526215
	> validation loss = 4.43277311, perplexity = 84.16448975
at the end of epoch: 5
train loss = 4.55577008, perplexity = 95.18002279
validation loss = 4.60842875, perplexity = 100.32638796
Saved model cv/epoch005_4.6084.model
  7967: 6 [    5/ 1327], train_loss/perplexity = 4.73573112/113.9467392 secs/batch = 0.2651s, grad.norm=12.68454075
  7972: 6 [   10/ 1327], train_loss/perplexity = 4.29985332/73.6889877 secs/batch = 0.2658s, grad.norm=12.61396122
  7977: 6 [   15/ 1327], train_loss/perplexity = 4.66392136/106.0511322 secs/batch = 0.2664s, grad.norm=12.08153152
  7982: 6 [   20/ 1327], train_loss/perplexity = 4.76718569/117.5878448 secs/batch = 0.2578s, grad.norm=12.62979507
  7987: 6 [   25/ 1327], train_loss/perplexity = 4.67173481/106.8830032 secs/batch = 0.2657s, grad.norm=13.34586620
  7992: 6 [   30/ 1327], train_loss/perplexity = 4.63005781/102.5199890 secs/batch = 0.2662s, grad.norm=12.76410961
  7997: 6 [   35/ 1327], train_loss/perplexity = 4.48616171/88.7800293 secs/batch = 0.2645s, grad.norm=12.69034958
  8002: 6 [   40/ 1327], train_loss/perplexity = 4.44944096/85.5790863 secs/batch = 0.2666s, grad.norm=12.72561073
  8007: 6 [   45/ 1327], train_loss/perplexity = 4.27312136/71.7452316 secs/batch = 0.2644s, grad.norm=12.18948841
  8012: 6 [   50/ 1327], train_loss/perplexity = 4.48132133/88.3513336 secs/batch = 0.2653s, grad.norm=12.99847317
  8017: 6 [   55/ 1327], train_loss/perplexity = 4.36604977/78.7320099 secs/batch = 0.2664s, grad.norm=13.01772881
  8022: 6 [   60/ 1327], train_loss/perplexity = 4.77548552/118.5678635 secs/batch = 0.2653s, grad.norm=13.19779587
  8027: 6 [   65/ 1327], train_loss/perplexity = 4.23503876/69.0643539 secs/batch = 0.2664s, grad.norm=12.33408833
  8032: 6 [   70/ 1327], train_loss/perplexity = 4.11663151/61.3522301 secs/batch = 0.2666s, grad.norm=12.65856647
  8037: 6 [   75/ 1327], train_loss/perplexity = 3.94973588/51.9216499 secs/batch = 0.2667s, grad.norm=13.12211990
  8042: 6 [   80/ 1327], train_loss/perplexity = 4.43254471/84.1452713 secs/batch = 0.2640s, grad.norm=13.01838875
  8047: 6 [   85/ 1327], train_loss/perplexity = 4.44221973/84.9633255 secs/batch = 0.2640s, grad.norm=13.12947178
  8052: 6 [   90/ 1327], train_loss/perplexity = 4.49951649/89.9736176 secs/batch = 0.2664s, grad.norm=13.25413799
  8057: 6 [   95/ 1327], train_loss/perplexity = 4.36739922/78.8383255 secs/batch = 0.2658s, grad.norm=12.97802448
  8062: 6 [  100/ 1327], train_loss/perplexity = 4.68122482/107.9021530 secs/batch = 0.2673s, grad.norm=13.17629910
  8067: 6 [  105/ 1327], train_loss/perplexity = 4.48140049/88.3583298 secs/batch = 0.2625s, grad.norm=13.49093056
  8072: 6 [  110/ 1327], train_loss/perplexity = 4.30134392/73.7989044 secs/batch = 0.2659s, grad.norm=12.93158722
  8077: 6 [  115/ 1327], train_loss/perplexity = 4.29899073/73.6254501 secs/batch = 0.2669s, grad.norm=13.56425667
  8082: 6 [  120/ 1327], train_loss/perplexity = 4.37952757/79.8003235 secs/batch = 0.2664s, grad.norm=13.56910610
  8087: 6 [  125/ 1327], train_loss/perplexity = 4.48755026/88.9033890 secs/batch = 0.2666s, grad.norm=13.48001099
  8092: 6 [  130/ 1327], train_loss/perplexity = 4.35295820/77.7080002 secs/batch = 0.2662s, grad.norm=13.45753098
  8097: 6 [  135/ 1327], train_loss/perplexity = 4.39805746/81.2928009 secs/batch = 0.2670s, grad.norm=13.07455826
  8102: 6 [  140/ 1327], train_loss/perplexity = 4.71200085/111.2745819 secs/batch = 0.2644s, grad.norm=13.41750908
  8107: 6 [  145/ 1327], train_loss/perplexity = 4.58266163/97.7742844 secs/batch = 0.2661s, grad.norm=13.81595612
  8112: 6 [  150/ 1327], train_loss/perplexity = 4.58449554/97.9537582 secs/batch = 0.2665s, grad.norm=13.15126801
  8117: 6 [  155/ 1327], train_loss/perplexity = 4.86637402/129.8492279 secs/batch = 0.2586s, grad.norm=12.51489544
  8122: 6 [  160/ 1327], train_loss/perplexity = 4.44445419/85.1533890 secs/batch = 0.2664s, grad.norm=12.20901394
  8127: 6 [  165/ 1327], train_loss/perplexity = 4.62322235/101.8216095 secs/batch = 0.2660s, grad.norm=12.63276768
  8132: 6 [  170/ 1327], train_loss/perplexity = 4.43367386/84.2403336 secs/batch = 0.2664s, grad.norm=12.58414936
  8137: 6 [  175/ 1327], train_loss/perplexity = 4.73634863/114.0171204 secs/batch = 0.2666s, grad.norm=12.76067352
  8142: 6 [  180/ 1327], train_loss/perplexity = 4.58739567/98.2382507 secs/batch = 0.2670s, grad.norm=13.20811081
  8147: 6 [  185/ 1327], train_loss/perplexity = 4.88118649/131.7869415 secs/batch = 0.2615s, grad.norm=13.34362316
  8152: 6 [  190/ 1327], train_loss/perplexity = 4.36619282/78.7432709 secs/batch = 0.2671s, grad.norm=12.24872303
  8157: 6 [  195/ 1327], train_loss/perplexity = 4.71016836/111.0708618 secs/batch = 0.2654s, grad.norm=12.44217396
  8162: 6 [  200/ 1327], train_loss/perplexity = 4.55368996/94.9822464 secs/batch = 0.2665s, grad.norm=12.95889568
  8167: 6 [  205/ 1327], train_loss/perplexity = 4.66059256/105.6986923 secs/batch = 0.2659s, grad.norm=12.73842144
  8172: 6 [  210/ 1327], train_loss/perplexity = 4.62328959/101.8284531 secs/batch = 0.2633s, grad.norm=11.99178696
  8177: 6 [  215/ 1327], train_loss/perplexity = 4.76691532/117.5560608 secs/batch = 0.2626s, grad.norm=12.65110016
  8182: 6 [  220/ 1327], train_loss/perplexity = 4.65914297/105.5455856 secs/batch = 0.2667s, grad.norm=12.50179958
  8187: 6 [  225/ 1327], train_loss/perplexity = 4.81546545/123.4042358 secs/batch = 0.2603s, grad.norm=12.92172432
  8192: 6 [  230/ 1327], train_loss/perplexity = 4.68034458/107.8072128 secs/batch = 0.2651s, grad.norm=13.14233208
  8197: 6 [  235/ 1327], train_loss/perplexity = 4.51055431/90.9722290 secs/batch = 0.2665s, grad.norm=13.18151569
  8202: 6 [  240/ 1327], train_loss/perplexity = 4.36116028/78.3479843 secs/batch = 0.2655s, grad.norm=13.05669880
  8207: 6 [  245/ 1327], train_loss/perplexity = 4.57030773/96.5738220 secs/batch = 0.2643s, grad.norm=12.47275925
  8212: 6 [  250/ 1327], train_loss/perplexity = 4.41095066/82.3477097 secs/batch = 0.2654s, grad.norm=12.30733013
  8217: 6 [  255/ 1327], train_loss/perplexity = 4.40290451/81.6877899 secs/batch = 0.2663s, grad.norm=12.42236042
  8222: 6 [  260/ 1327], train_loss/perplexity = 4.65295124/104.8940964 secs/batch = 0.2667s, grad.norm=13.43121243
  8227: 6 [  265/ 1327], train_loss/perplexity = 4.83435965/125.7580261 secs/batch = 0.2662s, grad.norm=12.39590073
  8232: 6 [  270/ 1327], train_loss/perplexity = 4.92963028/138.3283539 secs/batch = 0.2668s, grad.norm=12.62750340
  8237: 6 [  275/ 1327], train_loss/perplexity = 4.89048338/133.0178528 secs/batch = 0.2645s, grad.norm=12.75054932
  8242: 6 [  280/ 1327], train_loss/perplexity = 4.62810898/102.3203888 secs/batch = 0.2667s, grad.norm=12.65040588
  8247: 6 [  285/ 1327], train_loss/perplexity = 4.90915251/135.5245056 secs/batch = 0.2676s, grad.norm=12.72757435
  8252: 6 [  290/ 1327], train_loss/perplexity = 4.67954445/107.7209854 secs/batch = 0.2665s, grad.norm=13.24047661
  8257: 6 [  295/ 1327], train_loss/perplexity = 4.40395117/81.7733307 secs/batch = 0.2664s, grad.norm=12.59752464
  8262: 6 [  300/ 1327], train_loss/perplexity = 3.94540596/51.6973190 secs/batch = 0.2658s, grad.norm=12.44352245
  8267: 6 [  305/ 1327], train_loss/perplexity = 4.48408985/88.5962753 secs/batch = 0.2659s, grad.norm=12.73715019
  8272: 6 [  310/ 1327], train_loss/perplexity = 4.51323891/91.2167816 secs/batch = 0.2639s, grad.norm=12.47959423
  8277: 6 [  315/ 1327], train_loss/perplexity = 4.00649309/54.9538155 secs/batch = 0.2650s, grad.norm=12.55898666
  8282: 6 [  320/ 1327], train_loss/perplexity = 4.00225258/54.7212753 secs/batch = 0.2666s, grad.norm=13.62273216
  8287: 6 [  325/ 1327], train_loss/perplexity = 4.02706528/56.0960426 secs/batch = 0.2666s, grad.norm=12.57572079
  8292: 6 [  330/ 1327], train_loss/perplexity = 4.53193760/92.9384613 secs/batch = 0.2676s, grad.norm=13.47979927
  8297: 6 [  335/ 1327], train_loss/perplexity = 4.00266409/54.7437973 secs/batch = 0.2664s, grad.norm=12.45009232
  8302: 6 [  340/ 1327], train_loss/perplexity = 4.74106550/114.5561981 secs/batch = 0.2660s, grad.norm=12.57999802
  8307: 6 [  345/ 1327], train_loss/perplexity = 4.51331568/91.2237854 secs/batch = 0.2654s, grad.norm=12.53200817
  8312: 6 [  350/ 1327], train_loss/perplexity = 4.59169579/98.6615982 secs/batch = 0.2667s, grad.norm=13.37376595
  8317: 6 [  355/ 1327], train_loss/perplexity = 4.56893539/96.4413834 secs/batch = 0.2663s, grad.norm=12.69801044
  8322: 6 [  360/ 1327], train_loss/perplexity = 4.76291180/117.0863647 secs/batch = 0.2660s, grad.norm=13.72997284
  8327: 6 [  365/ 1327], train_loss/perplexity = 4.62529993/102.0333710 secs/batch = 0.2664s, grad.norm=12.72485447
  8332: 6 [  370/ 1327], train_loss/perplexity = 4.70188141/110.1542206 secs/batch = 0.2659s, grad.norm=12.69028187
  8337: 6 [  375/ 1327], train_loss/perplexity = 4.09401417/59.9801788 secs/batch = 0.2663s, grad.norm=13.35404491
  8342: 6 [  380/ 1327], train_loss/perplexity = 4.17780781/65.2227173 secs/batch = 0.2660s, grad.norm=13.08326912
  8347: 6 [  385/ 1327], train_loss/perplexity = 4.47288752/87.6093292 secs/batch = 0.2655s, grad.norm=13.50047970
  8352: 6 [  390/ 1327], train_loss/perplexity = 4.47060347/87.4094543 secs/batch = 0.2650s, grad.norm=12.83873177
  8357: 6 [  395/ 1327], train_loss/perplexity = 4.61018991/100.5032349 secs/batch = 0.2655s, grad.norm=12.92348957
  8362: 6 [  400/ 1327], train_loss/perplexity = 4.52411270/92.2140656 secs/batch = 0.2671s, grad.norm=12.78199387
  8367: 6 [  405/ 1327], train_loss/perplexity = 4.79282284/120.6414413 secs/batch = 0.2662s, grad.norm=13.24057865
  8372: 6 [  410/ 1327], train_loss/perplexity = 4.52586460/92.3757629 secs/batch = 0.2660s, grad.norm=13.17389870
  8377: 6 [  415/ 1327], train_loss/perplexity = 4.36844730/78.9209976 secs/batch = 0.2657s, grad.norm=12.79601479
  8382: 6 [  420/ 1327], train_loss/perplexity = 4.09347200/59.9476700 secs/batch = 0.2664s, grad.norm=13.20530319
  8387: 6 [  425/ 1327], train_loss/perplexity = 4.39222336/80.8199081 secs/batch = 0.2644s, grad.norm=13.98715782
  8392: 6 [  430/ 1327], train_loss/perplexity = 4.61949396/101.4426880 secs/batch = 0.2656s, grad.norm=13.62405682
  8397: 6 [  435/ 1327], train_loss/perplexity = 4.61757946/101.2486572 secs/batch = 0.2662s, grad.norm=13.58662510
  8402: 6 [  440/ 1327], train_loss/perplexity = 4.16277456/64.2495422 secs/batch = 0.2640s, grad.norm=13.04714203
  8407: 6 [  445/ 1327], train_loss/perplexity = 4.56452227/96.0167160 secs/batch = 0.2667s, grad.norm=13.39461327
  8412: 6 [  450/ 1327], train_loss/perplexity = 4.43727684/84.5444031 secs/batch = 0.2657s, grad.norm=13.20424461
  8417: 6 [  455/ 1327], train_loss/perplexity = 4.39623308/81.1446304 secs/batch = 0.2647s, grad.norm=13.03181553
  8422: 6 [  460/ 1327], train_loss/perplexity = 4.46808481/87.1895752 secs/batch = 0.2636s, grad.norm=13.75357819
  8427: 6 [  465/ 1327], train_loss/perplexity = 4.19891071/66.6137314 secs/batch = 0.2657s, grad.norm=14.50875759
  8432: 6 [  470/ 1327], train_loss/perplexity = 4.87196636/130.5774231 secs/batch = 0.2641s, grad.norm=12.87099934
  8437: 6 [  475/ 1327], train_loss/perplexity = 4.30173016/73.8274155 secs/batch = 0.2652s, grad.norm=13.11845016
  8442: 6 [  480/ 1327], train_loss/perplexity = 4.49898863/89.9261398 secs/batch = 0.2681s, grad.norm=13.31709862
  8447: 6 [  485/ 1327], train_loss/perplexity = 4.39481735/81.0298309 secs/batch = 0.2666s, grad.norm=13.08928585
  8452: 6 [  490/ 1327], train_loss/perplexity = 4.31344604/74.6974564 secs/batch = 0.2666s, grad.norm=13.81215763
  8457: 6 [  495/ 1327], train_loss/perplexity = 4.32836676/75.8203506 secs/batch = 0.2655s, grad.norm=13.30007648
  8462: 6 [  500/ 1327], train_loss/perplexity = 4.53248358/92.9892197 secs/batch = 0.2663s, grad.norm=13.18614006
  8467: 6 [  505/ 1327], train_loss/perplexity = 4.62254906/101.7530746 secs/batch = 0.2656s, grad.norm=12.10576153
  8472: 6 [  510/ 1327], train_loss/perplexity = 4.98251772/145.8411102 secs/batch = 0.2647s, grad.norm=12.13693333
  8477: 6 [  515/ 1327], train_loss/perplexity = 4.61776638/101.2675858 secs/batch = 0.2661s, grad.norm=12.12714195
  8482: 6 [  520/ 1327], train_loss/perplexity = 4.79666567/121.1059341 secs/batch = 0.2658s, grad.norm=12.74004364
  8487: 6 [  525/ 1327], train_loss/perplexity = 4.30697393/74.2155685 secs/batch = 0.2662s, grad.norm=12.85950565
  8492: 6 [  530/ 1327], train_loss/perplexity = 4.37063265/79.0936508 secs/batch = 0.2618s, grad.norm=13.65036392
  8497: 6 [  535/ 1327], train_loss/perplexity = 4.52955055/92.7168808 secs/batch = 0.2652s, grad.norm=13.28645325
  8502: 6 [  540/ 1327], train_loss/perplexity = 4.57297754/96.8320007 secs/batch = 0.2661s, grad.norm=12.49956512
  8507: 6 [  545/ 1327], train_loss/perplexity = 4.59314442/98.8046265 secs/batch = 0.2668s, grad.norm=13.44513893
  8512: 6 [  550/ 1327], train_loss/perplexity = 4.56533527/96.0948105 secs/batch = 0.2660s, grad.norm=13.44999981
  8517: 6 [  555/ 1327], train_loss/perplexity = 4.42326021/83.3676376 secs/batch = 0.2647s, grad.norm=12.69827461
  8522: 6 [  560/ 1327], train_loss/perplexity = 4.47873211/88.1228714 secs/batch = 0.2665s, grad.norm=13.90122509
  8527: 6 [  565/ 1327], train_loss/perplexity = 4.39476633/81.0256958 secs/batch = 0.2638s, grad.norm=14.10691738
  8532: 6 [  570/ 1327], train_loss/perplexity = 4.38704157/80.4021988 secs/batch = 0.2666s, grad.norm=14.21448135
  8537: 6 [  575/ 1327], train_loss/perplexity = 4.22433567/68.3290939 secs/batch = 0.2637s, grad.norm=13.68531322
  8542: 6 [  580/ 1327], train_loss/perplexity = 4.60935736/100.4195938 secs/batch = 0.2653s, grad.norm=13.38385677
  8547: 6 [  585/ 1327], train_loss/perplexity = 4.14157152/62.9015961 secs/batch = 0.2597s, grad.norm=12.70983505
  8552: 6 [  590/ 1327], train_loss/perplexity = 4.56040096/95.6218109 secs/batch = 0.2656s, grad.norm=12.91071129
  8557: 6 [  595/ 1327], train_loss/perplexity = 4.46214437/86.6731720 secs/batch = 0.2655s, grad.norm=13.23588943
  8562: 6 [  600/ 1327], train_loss/perplexity = 4.73084974/113.3918762 secs/batch = 0.2651s, grad.norm=12.66812134
  8567: 6 [  605/ 1327], train_loss/perplexity = 4.60924530/100.4083405 secs/batch = 0.2671s, grad.norm=13.01832962
  8572: 6 [  610/ 1327], train_loss/perplexity = 4.77641916/118.6786194 secs/batch = 0.2661s, grad.norm=13.36350632
  8577: 6 [  615/ 1327], train_loss/perplexity = 4.25391674/70.3805389 secs/batch = 0.2665s, grad.norm=12.85151768
  8582: 6 [  620/ 1327], train_loss/perplexity = 4.68458700/108.2655487 secs/batch = 0.2642s, grad.norm=12.96989918
  8587: 6 [  625/ 1327], train_loss/perplexity = 4.61358452/100.8449860 secs/batch = 0.2669s, grad.norm=12.77709866
  8592: 6 [  630/ 1327], train_loss/perplexity = 4.72181654/112.3721924 secs/batch = 0.2663s, grad.norm=12.87970448
  8597: 6 [  635/ 1327], train_loss/perplexity = 4.46723032/87.1151047 secs/batch = 0.2651s, grad.norm=13.45587158
  8602: 6 [  640/ 1327], train_loss/perplexity = 4.48067999/88.2946930 secs/batch = 0.2683s, grad.norm=13.51834679
  8607: 6 [  645/ 1327], train_loss/perplexity = 4.75188732/115.8026352 secs/batch = 0.2668s, grad.norm=13.97203350
  8612: 6 [  650/ 1327], train_loss/perplexity = 4.27576542/71.9351807 secs/batch = 0.2662s, grad.norm=13.59965706
  8617: 6 [  655/ 1327], train_loss/perplexity = 4.44398308/85.1132812 secs/batch = 0.2660s, grad.norm=13.77535725
  8622: 6 [  660/ 1327], train_loss/perplexity = 4.31371260/74.7173691 secs/batch = 0.2666s, grad.norm=13.07552910
  8627: 6 [  665/ 1327], train_loss/perplexity = 4.55238771/94.8586349 secs/batch = 0.2609s, grad.norm=13.32427120
  8632: 6 [  670/ 1327], train_loss/perplexity = 4.41335440/82.5458908 secs/batch = 0.2668s, grad.norm=13.38310337
  8637: 6 [  675/ 1327], train_loss/perplexity = 4.24506617/69.7603760 secs/batch = 0.2665s, grad.norm=13.43035793
  8642: 6 [  680/ 1327], train_loss/perplexity = 4.41487217/82.6712723 secs/batch = 0.2656s, grad.norm=13.80859184
  8647: 6 [  685/ 1327], train_loss/perplexity = 4.26742554/71.3377457 secs/batch = 0.2662s, grad.norm=13.31543350
  8652: 6 [  690/ 1327], train_loss/perplexity = 4.68955994/108.8052902 secs/batch = 0.2668s, grad.norm=12.83523750
  8657: 6 [  695/ 1327], train_loss/perplexity = 4.47920609/88.1646500 secs/batch = 0.2675s, grad.norm=13.33253002
  8662: 6 [  700/ 1327], train_loss/perplexity = 4.74630070/115.1574936 secs/batch = 0.2652s, grad.norm=13.43453884
  8667: 6 [  705/ 1327], train_loss/perplexity = 4.46863699/87.2377396 secs/batch = 0.2658s, grad.norm=12.76364136
  8672: 6 [  710/ 1327], train_loss/perplexity = 4.42699242/83.6793671 secs/batch = 0.2642s, grad.norm=13.85406208
  8677: 6 [  715/ 1327], train_loss/perplexity = 4.33358765/76.2172394 secs/batch = 0.2659s, grad.norm=13.22240353
  8682: 6 [  720/ 1327], train_loss/perplexity = 4.25921154/70.7541733 secs/batch = 0.2659s, grad.norm=13.28655243
  8687: 6 [  725/ 1327], train_loss/perplexity = 4.28459597/72.5732193 secs/batch = 0.2655s, grad.norm=13.51164436
  8692: 6 [  730/ 1327], train_loss/perplexity = 4.46835423/87.2130737 secs/batch = 0.2679s, grad.norm=13.41722107
  8697: 6 [  735/ 1327], train_loss/perplexity = 4.53281546/93.0200882 secs/batch = 0.2663s, grad.norm=13.72120667
  8702: 6 [  740/ 1327], train_loss/perplexity = 3.96270847/52.5995979 secs/batch = 0.2652s, grad.norm=13.11133766
  8707: 6 [  745/ 1327], train_loss/perplexity = 4.44439793/85.1485977 secs/batch = 0.2661s, grad.norm=13.32094002
  8712: 6 [  750/ 1327], train_loss/perplexity = 4.35083675/77.5433197 secs/batch = 0.2657s, grad.norm=13.42095184
  8717: 6 [  755/ 1327], train_loss/perplexity = 4.18919182/65.9694519 secs/batch = 0.2663s, grad.norm=12.94720936
  8722: 6 [  760/ 1327], train_loss/perplexity = 4.09885120/60.2710075 secs/batch = 0.2668s, grad.norm=12.53767300
  8727: 6 [  765/ 1327], train_loss/perplexity = 4.24586773/69.8163147 secs/batch = 0.2665s, grad.norm=13.00285625
  8732: 6 [  770/ 1327], train_loss/perplexity = 4.16863585/64.6272278 secs/batch = 0.2654s, grad.norm=13.58542728
  8737: 6 [  775/ 1327], train_loss/perplexity = 4.26952648/71.4877777 secs/batch = 0.2671s, grad.norm=13.62134075
  8742: 6 [  780/ 1327], train_loss/perplexity = 4.70246696/110.2187424 secs/batch = 0.2677s, grad.norm=13.71697617
  8747: 6 [  785/ 1327], train_loss/perplexity = 4.48410225/88.5973740 secs/batch = 0.2659s, grad.norm=13.96997166
  8752: 6 [  790/ 1327], train_loss/perplexity = 4.22750473/68.5459747 secs/batch = 0.2666s, grad.norm=13.68727589
  8757: 6 [  795/ 1327], train_loss/perplexity = 4.63870335/103.4101715 secs/batch = 0.2667s, grad.norm=13.11748981
  8762: 6 [  800/ 1327], train_loss/perplexity = 4.51624155/91.4910889 secs/batch = 0.2678s, grad.norm=13.84198189
  8767: 6 [  805/ 1327], train_loss/perplexity = 4.82093334/124.0808487 secs/batch = 0.2662s, grad.norm=13.26937962
  8772: 6 [  810/ 1327], train_loss/perplexity = 4.43365574/84.2388077 secs/batch = 0.2667s, grad.norm=12.54878616
  8777: 6 [  815/ 1327], train_loss/perplexity = 4.39423943/80.9830170 secs/batch = 0.2671s, grad.norm=12.91547012
  8782: 6 [  820/ 1327], train_loss/perplexity = 4.17151451/64.8135376 secs/batch = 0.2653s, grad.norm=12.57164574
  8787: 6 [  825/ 1327], train_loss/perplexity = 4.36774588/78.8656616 secs/batch = 0.2658s, grad.norm=13.12030029
  8792: 6 [  830/ 1327], train_loss/perplexity = 4.13235950/62.3248062 secs/batch = 0.2660s, grad.norm=13.38280487
  8797: 6 [  835/ 1327], train_loss/perplexity = 4.42390680/83.4215622 secs/batch = 0.2662s, grad.norm=13.90739727
  8802: 6 [  840/ 1327], train_loss/perplexity = 4.47272444/87.5950470 secs/batch = 0.2675s, grad.norm=13.09253311
  8807: 6 [  845/ 1327], train_loss/perplexity = 4.30320072/73.9360657 secs/batch = 0.2620s, grad.norm=13.81273174
  8812: 6 [  850/ 1327], train_loss/perplexity = 4.38522577/80.2563400 secs/batch = 0.2655s, grad.norm=13.37417316
  8817: 6 [  855/ 1327], train_loss/perplexity = 4.43132734/84.0429001 secs/batch = 0.2675s, grad.norm=13.41342640
  8822: 6 [  860/ 1327], train_loss/perplexity = 4.09949732/60.3099632 secs/batch = 0.2605s, grad.norm=12.91110325
  8827: 6 [  865/ 1327], train_loss/perplexity = 4.65400982/105.0051956 secs/batch = 0.2601s, grad.norm=13.22157764
  8832: 6 [  870/ 1327], train_loss/perplexity = 4.47748327/88.0128860 secs/batch = 0.2672s, grad.norm=13.65079117
  8837: 6 [  875/ 1327], train_loss/perplexity = 4.04069996/56.8661346 secs/batch = 0.2669s, grad.norm=12.96478939
  8842: 6 [  880/ 1327], train_loss/perplexity = 4.24523926/69.7724533 secs/batch = 0.2657s, grad.norm=12.97545815
  8847: 6 [  885/ 1327], train_loss/perplexity = 4.36947012/79.0017624 secs/batch = 0.2655s, grad.norm=12.98979092
  8852: 6 [  890/ 1327], train_loss/perplexity = 4.59474468/98.9628677 secs/batch = 0.2666s, grad.norm=13.14622974
  8857: 6 [  895/ 1327], train_loss/perplexity = 4.60203362/99.6868362 secs/batch = 0.2675s, grad.norm=12.93065357
  8862: 6 [  900/ 1327], train_loss/perplexity = 4.38164663/79.9696045 secs/batch = 0.2667s, grad.norm=13.08586597
  8867: 6 [  905/ 1327], train_loss/perplexity = 4.27024126/71.5388947 secs/batch = 0.2666s, grad.norm=13.35748768
  8872: 6 [  910/ 1327], train_loss/perplexity = 4.26308537/71.0287933 secs/batch = 0.2662s, grad.norm=12.75104618
  8877: 6 [  915/ 1327], train_loss/perplexity = 4.51766157/91.6211014 secs/batch = 0.2666s, grad.norm=13.01758957
  8882: 6 [  920/ 1327], train_loss/perplexity = 4.70894146/110.9346695 secs/batch = 0.2662s, grad.norm=13.17031288
  8887: 6 [  925/ 1327], train_loss/perplexity = 4.49733591/89.7776337 secs/batch = 0.2669s, grad.norm=13.33817387
  8892: 6 [  930/ 1327], train_loss/perplexity = 4.46721268/87.1135712 secs/batch = 0.2667s, grad.norm=12.79805851
  8897: 6 [  935/ 1327], train_loss/perplexity = 4.57816696/97.3358078 secs/batch = 0.2662s, grad.norm=13.10665607
  8902: 6 [  940/ 1327], train_loss/perplexity = 4.57094812/96.6356888 secs/batch = 0.2689s, grad.norm=13.19613075
  8907: 6 [  945/ 1327], train_loss/perplexity = 4.78158569/119.2933655 secs/batch = 0.2668s, grad.norm=13.36759853
  8912: 6 [  950/ 1327], train_loss/perplexity = 4.51513720/91.3901062 secs/batch = 0.2687s, grad.norm=13.16662884
  8917: 6 [  955/ 1327], train_loss/perplexity = 4.48508501/88.6844864 secs/batch = 0.2648s, grad.norm=13.13298702
  8922: 6 [  960/ 1327], train_loss/perplexity = 4.75936699/116.6720505 secs/batch = 0.2628s, grad.norm=13.15420341
  8927: 6 [  965/ 1327], train_loss/perplexity = 4.54078960/93.7648087 secs/batch = 0.2679s, grad.norm=12.94959545
  8932: 6 [  970/ 1327], train_loss/perplexity = 4.77330828/118.3099976 secs/batch = 0.2615s, grad.norm=13.53612041
  8937: 6 [  975/ 1327], train_loss/perplexity = 4.45778275/86.2959595 secs/batch = 0.2661s, grad.norm=13.84923744
  8942: 6 [  980/ 1327], train_loss/perplexity = 4.26814604/71.3891602 secs/batch = 0.2667s, grad.norm=13.06764793
  8947: 6 [  985/ 1327], train_loss/perplexity = 4.40758181/82.0707626 secs/batch = 0.2684s, grad.norm=13.59805107
  8952: 6 [  990/ 1327], train_loss/perplexity = 4.70703030/110.7228546 secs/batch = 0.2652s, grad.norm=13.65777397
  8957: 6 [  995/ 1327], train_loss/perplexity = 4.66342878/105.9989090 secs/batch = 0.2661s, grad.norm=13.09453964
  8962: 6 [ 1000/ 1327], train_loss/perplexity = 4.15540695/63.7779121 secs/batch = 0.2680s, grad.norm=13.01487160
  8967: 6 [ 1005/ 1327], train_loss/perplexity = 4.57902622/97.4194794 secs/batch = 0.2658s, grad.norm=13.53181171
  8972: 6 [ 1010/ 1327], train_loss/perplexity = 4.18244123/65.5256195 secs/batch = 0.2619s, grad.norm=12.65954208
  8977: 6 [ 1015/ 1327], train_loss/perplexity = 4.65077305/104.6658630 secs/batch = 0.2684s, grad.norm=12.80866337
  8982: 6 [ 1020/ 1327], train_loss/perplexity = 4.83438253/125.7609024 secs/batch = 0.2660s, grad.norm=13.11186314
  8987: 6 [ 1025/ 1327], train_loss/perplexity = 4.67732859/107.4825592 secs/batch = 0.2655s, grad.norm=12.87328720
  8992: 6 [ 1030/ 1327], train_loss/perplexity = 4.39950037/81.4101868 secs/batch = 0.2610s, grad.norm=12.44834137
  8997: 6 [ 1035/ 1327], train_loss/perplexity = 4.36194468/78.4094696 secs/batch = 0.2683s, grad.norm=12.64398766
  9002: 6 [ 1040/ 1327], train_loss/perplexity = 4.58890533/98.3866730 secs/batch = 0.2605s, grad.norm=13.17485905
  9007: 6 [ 1045/ 1327], train_loss/perplexity = 4.15230274/63.5802422 secs/batch = 0.2664s, grad.norm=12.96307850
  9012: 6 [ 1050/ 1327], train_loss/perplexity = 4.25255632/70.2848511 secs/batch = 0.2659s, grad.norm=13.92715263
  9017: 6 [ 1055/ 1327], train_loss/perplexity = 4.32041073/75.2195129 secs/batch = 0.2664s, grad.norm=14.34332561
  9022: 6 [ 1060/ 1327], train_loss/perplexity = 4.00497341/54.8703651 secs/batch = 0.2662s, grad.norm=14.22074413
  9027: 6 [ 1065/ 1327], train_loss/perplexity = 4.15929413/64.0263138 secs/batch = 0.2657s, grad.norm=13.76117992
  9032: 6 [ 1070/ 1327], train_loss/perplexity = 4.47680664/87.9533539 secs/batch = 0.2676s, grad.norm=13.55655575
  9037: 6 [ 1075/ 1327], train_loss/perplexity = 4.30423832/74.0128174 secs/batch = 0.2676s, grad.norm=13.64046955
  9042: 6 [ 1080/ 1327], train_loss/perplexity = 4.21866131/67.9424667 secs/batch = 0.2650s, grad.norm=13.38343430
  9047: 6 [ 1085/ 1327], train_loss/perplexity = 4.01160288/55.2353363 secs/batch = 0.2673s, grad.norm=13.38974190
  9052: 6 [ 1090/ 1327], train_loss/perplexity = 4.16961622/64.6906204 secs/batch = 0.2665s, grad.norm=13.82676506
  9057: 6 [ 1095/ 1327], train_loss/perplexity = 4.40307617/81.7018127 secs/batch = 0.2663s, grad.norm=13.69915104
  9062: 6 [ 1100/ 1327], train_loss/perplexity = 4.15005445/63.4374542 secs/batch = 0.2667s, grad.norm=15.65659332
  9067: 6 [ 1105/ 1327], train_loss/perplexity = 4.10091257/60.3953781 secs/batch = 0.2650s, grad.norm=14.13533688
  9072: 6 [ 1110/ 1327], train_loss/perplexity = 4.45965910/86.4580307 secs/batch = 0.2666s, grad.norm=13.90410328
  9077: 6 [ 1115/ 1327], train_loss/perplexity = 4.19674158/66.4693909 secs/batch = 0.2661s, grad.norm=12.98812199
  9082: 6 [ 1120/ 1327], train_loss/perplexity = 4.42769289/83.7379990 secs/batch = 0.2672s, grad.norm=13.41341019
  9087: 6 [ 1125/ 1327], train_loss/perplexity = 4.67358828/107.0812912 secs/batch = 0.2667s, grad.norm=14.09899044
  9092: 6 [ 1130/ 1327], train_loss/perplexity = 4.32455063/75.5315628 secs/batch = 0.2659s, grad.norm=13.56438351
  9097: 6 [ 1135/ 1327], train_loss/perplexity = 4.30834341/74.3172760 secs/batch = 0.2637s, grad.norm=13.42846870
  9102: 6 [ 1140/ 1327], train_loss/perplexity = 4.56198359/95.7732697 secs/batch = 0.2671s, grad.norm=14.34386444
  9107: 6 [ 1145/ 1327], train_loss/perplexity = 4.32850742/75.8310165 secs/batch = 0.2674s, grad.norm=12.88589191
  9112: 6 [ 1150/ 1327], train_loss/perplexity = 4.29200935/73.1132278 secs/batch = 0.2616s, grad.norm=13.42627525
  9117: 6 [ 1155/ 1327], train_loss/perplexity = 4.48729992/88.8811340 secs/batch = 0.2663s, grad.norm=13.71285915
  9122: 6 [ 1160/ 1327], train_loss/perplexity = 4.40249491/81.6543350 secs/batch = 0.2657s, grad.norm=13.57196236
  9127: 6 [ 1165/ 1327], train_loss/perplexity = 4.41489124/82.6728516 secs/batch = 0.2670s, grad.norm=13.33178806
  9132: 6 [ 1170/ 1327], train_loss/perplexity = 4.30939865/74.3957367 secs/batch = 0.2660s, grad.norm=13.25464058
  9137: 6 [ 1175/ 1327], train_loss/perplexity = 4.09640169/60.1235542 secs/batch = 0.2618s, grad.norm=13.67213345
  9142: 6 [ 1180/ 1327], train_loss/perplexity = 4.05529976/57.7024574 secs/batch = 0.2656s, grad.norm=13.58774471
  9147: 6 [ 1185/ 1327], train_loss/perplexity = 4.35004711/77.4821167 secs/batch = 0.2644s, grad.norm=13.78003883
  9152: 6 [ 1190/ 1327], train_loss/perplexity = 4.38758945/80.4462662 secs/batch = 0.2646s, grad.norm=13.76337147
  9157: 6 [ 1195/ 1327], train_loss/perplexity = 4.19385958/66.2781067 secs/batch = 0.2663s, grad.norm=13.42395401
  9162: 6 [ 1200/ 1327], train_loss/perplexity = 4.17957115/65.3378296 secs/batch = 0.2674s, grad.norm=13.29422092
  9167: 6 [ 1205/ 1327], train_loss/perplexity = 4.12566662/61.9090652 secs/batch = 0.2654s, grad.norm=13.68166924
  9172: 6 [ 1210/ 1327], train_loss/perplexity = 3.78970242/44.2432327 secs/batch = 0.2662s, grad.norm=13.73899269
  9177: 6 [ 1215/ 1327], train_loss/perplexity = 3.97361231/53.1762733 secs/batch = 0.2679s, grad.norm=12.85907078
  9182: 6 [ 1220/ 1327], train_loss/perplexity = 4.26832581/71.4019928 secs/batch = 0.2661s, grad.norm=14.01039314
  9187: 6 [ 1225/ 1327], train_loss/perplexity = 3.93627167/51.2272530 secs/batch = 0.2676s, grad.norm=14.31063557
  9192: 6 [ 1230/ 1327], train_loss/perplexity = 4.18816042/65.9014511 secs/batch = 0.2673s, grad.norm=13.31345272
  9197: 6 [ 1235/ 1327], train_loss/perplexity = 4.17501926/65.0410919 secs/batch = 0.2663s, grad.norm=13.35290623
  9202: 6 [ 1240/ 1327], train_loss/perplexity = 4.37534809/79.4674988 secs/batch = 0.2685s, grad.norm=14.12958622
  9207: 6 [ 1245/ 1327], train_loss/perplexity = 4.24767399/69.9425354 secs/batch = 0.2663s, grad.norm=13.18539524
  9212: 6 [ 1250/ 1327], train_loss/perplexity = 4.43059731/83.9815674 secs/batch = 0.2666s, grad.norm=13.13836575
  9217: 6 [ 1255/ 1327], train_loss/perplexity = 4.49465466/89.5372391 secs/batch = 0.2668s, grad.norm=12.98890114
  9222: 6 [ 1260/ 1327], train_loss/perplexity = 4.25081730/70.1627350 secs/batch = 0.2673s, grad.norm=13.95345020
  9227: 6 [ 1265/ 1327], train_loss/perplexity = 4.47840023/88.0936279 secs/batch = 0.2659s, grad.norm=13.75771332
  9232: 6 [ 1270/ 1327], train_loss/perplexity = 4.15187359/63.5529594 secs/batch = 0.2665s, grad.norm=13.59389305
  9237: 6 [ 1275/ 1327], train_loss/perplexity = 4.42666197/83.6517181 secs/batch = 0.2671s, grad.norm=14.34417152
  9242: 6 [ 1280/ 1327], train_loss/perplexity = 4.23006725/68.7218552 secs/batch = 0.2625s, grad.norm=14.09458828
  9247: 6 [ 1285/ 1327], train_loss/perplexity = 4.08989191/59.7334366 secs/batch = 0.2670s, grad.norm=13.15522957
  9252: 6 [ 1290/ 1327], train_loss/perplexity = 4.35571957/77.9228745 secs/batch = 0.2655s, grad.norm=13.36451721
  9257: 6 [ 1295/ 1327], train_loss/perplexity = 4.32226944/75.3594589 secs/batch = 0.2650s, grad.norm=13.43514252
  9262: 6 [ 1300/ 1327], train_loss/perplexity = 4.50333977/90.3182678 secs/batch = 0.2674s, grad.norm=12.67322731
  9267: 6 [ 1305/ 1327], train_loss/perplexity = 4.57849789/97.3680267 secs/batch = 0.2662s, grad.norm=13.86778736
  9272: 6 [ 1310/ 1327], train_loss/perplexity = 4.86624813/129.8328857 secs/batch = 0.2677s, grad.norm=13.80628204
  9277: 6 [ 1315/ 1327], train_loss/perplexity = 4.65585423/105.1990433 secs/batch = 0.2661s, grad.norm=13.53574944
  9282: 6 [ 1320/ 1327], train_loss/perplexity = 4.64980793/104.5649033 secs/batch = 0.2676s, grad.norm=13.30932426
  9287: 6 [ 1325/ 1327], train_loss/perplexity = 4.62713861/102.2211533 secs/batch = 0.2643s, grad.norm=13.90594673
Epoch training time: 352.7628719806671
	> validation loss = 4.75754261, perplexity = 116.45938873
	> validation loss = 4.69023037, perplexity = 108.87825775
	> validation loss = 4.63562107, perplexity = 103.09192657
	> validation loss = 4.63272524, perplexity = 102.79382324
	> validation loss = 4.82952738, perplexity = 125.15179443
	> validation loss = 4.75625896, perplexity = 116.30998993
	> validation loss = 4.70114040, perplexity = 110.07263184
	> validation loss = 4.53040791, perplexity = 92.79640198
	> validation loss = 4.33549118, perplexity = 76.36245728
	> validation loss = 4.46821165, perplexity = 87.20063782
	> validation loss = 4.56963778, perplexity = 96.50914764
	> validation loss = 4.62843418, perplexity = 102.35366821
	> validation loss = 4.60696077, perplexity = 100.17922211
	> validation loss = 4.38390064, perplexity = 80.15006256
	> validation loss = 4.28142405, perplexity = 72.34338379
	> validation loss = 4.29866838, perplexity = 73.60172272
	> validation loss = 4.74836588, perplexity = 115.39556122
	> validation loss = 4.32732964, perplexity = 75.74176025
	> validation loss = 4.74511766, perplexity = 115.02133942
	> validation loss = 4.59183121, perplexity = 98.67495728
	> validation loss = 4.40927458, perplexity = 82.20980835
at the end of epoch: 6
train loss = 4.49507131, perplexity = 89.57455620
validation loss = 4.57264592, perplexity = 96.79989631
Saved model cv/epoch006_4.5726.model
  9294: 7 [    5/ 1327], train_loss/perplexity = 4.70549297/110.5527725 secs/batch = 0.2666s, grad.norm=13.66527557
  9299: 7 [   10/ 1327], train_loss/perplexity = 4.22954178/68.6857529 secs/batch = 0.2643s, grad.norm=13.32660198
  9304: 7 [   15/ 1327], train_loss/perplexity = 4.53497744/93.2214127 secs/batch = 0.2672s, grad.norm=12.58177376
  9309: 7 [   20/ 1327], train_loss/perplexity = 4.69845104/109.7770004 secs/batch = 0.2640s, grad.norm=12.99123192
  9314: 7 [   25/ 1327], train_loss/perplexity = 4.60934830/100.4186859 secs/batch = 0.2665s, grad.norm=13.77176189
  9319: 7 [   30/ 1327], train_loss/perplexity = 4.59283686/98.7742386 secs/batch = 0.2652s, grad.norm=14.00591183
  9324: 7 [   35/ 1327], train_loss/perplexity = 4.38235378/80.0261765 secs/batch = 0.2673s, grad.norm=13.36042881
  9329: 7 [   40/ 1327], train_loss/perplexity = 4.36662340/78.7771835 secs/batch = 0.2632s, grad.norm=13.13236618
  9334: 7 [   45/ 1327], train_loss/perplexity = 4.20828152/67.2408905 secs/batch = 0.2665s, grad.norm=13.17136860
  9339: 7 [   50/ 1327], train_loss/perplexity = 4.38813305/80.4900055 secs/batch = 0.2668s, grad.norm=13.84853363
  9344: 7 [   55/ 1327], train_loss/perplexity = 4.27947235/72.2023315 secs/batch = 0.2630s, grad.norm=13.69968510
  9349: 7 [   60/ 1327], train_loss/perplexity = 4.61832047/101.3237152 secs/batch = 0.2662s, grad.norm=13.66636848
  9354: 7 [   65/ 1327], train_loss/perplexity = 4.14640570/63.2064095 secs/batch = 0.2657s, grad.norm=13.27651691
  9359: 7 [   70/ 1327], train_loss/perplexity = 4.07157946/58.6495247 secs/batch = 0.2659s, grad.norm=13.29128838
  9364: 7 [   75/ 1327], train_loss/perplexity = 3.86557746/47.7308273 secs/batch = 0.2669s, grad.norm=12.87283802
  9369: 7 [   80/ 1327], train_loss/perplexity = 4.34402466/77.0168839 secs/batch = 0.2602s, grad.norm=13.60085201
  9374: 7 [   85/ 1327], train_loss/perplexity = 4.36259699/78.4606323 secs/batch = 0.2659s, grad.norm=13.72448063
  9379: 7 [   90/ 1327], train_loss/perplexity = 4.40651274/81.9830704 secs/batch = 0.2657s, grad.norm=13.69733143
  9384: 7 [   95/ 1327], train_loss/perplexity = 4.31228018/74.6104202 secs/batch = 0.2667s, grad.norm=13.84723854
  9389: 7 [  100/ 1327], train_loss/perplexity = 4.54880190/94.5190964 secs/batch = 0.2675s, grad.norm=13.58227539
  9394: 7 [  105/ 1327], train_loss/perplexity = 4.40302515/81.6976395 secs/batch = 0.2664s, grad.norm=13.99539948
  9399: 7 [  110/ 1327], train_loss/perplexity = 4.21623135/67.7775726 secs/batch = 0.2677s, grad.norm=13.40002632
  9404: 7 [  115/ 1327], train_loss/perplexity = 4.21366978/67.6041794 secs/batch = 0.2683s, grad.norm=14.33707428
  9409: 7 [  120/ 1327], train_loss/perplexity = 4.28137922/72.3401413 secs/batch = 0.2668s, grad.norm=13.97856903
  9414: 7 [  125/ 1327], train_loss/perplexity = 4.39444637/80.9997711 secs/batch = 0.2668s, grad.norm=13.96098137
  9419: 7 [  130/ 1327], train_loss/perplexity = 4.27946377/72.2017136 secs/batch = 0.2667s, grad.norm=14.04911232
  9424: 7 [  135/ 1327], train_loss/perplexity = 4.36438751/78.6012421 secs/batch = 0.2660s, grad.norm=13.51275539
  9429: 7 [  140/ 1327], train_loss/perplexity = 4.63880301/103.4204788 secs/batch = 0.2677s, grad.norm=14.02341080
  9434: 7 [  145/ 1327], train_loss/perplexity = 4.51484013/91.3629608 secs/batch = 0.2663s, grad.norm=14.40155792
  9439: 7 [  150/ 1327], train_loss/perplexity = 4.53106546/92.8574448 secs/batch = 0.2686s, grad.norm=14.15447140
  9444: 7 [  155/ 1327], train_loss/perplexity = 4.74702835/115.2413177 secs/batch = 0.2617s, grad.norm=13.33634949
  9449: 7 [  160/ 1327], train_loss/perplexity = 4.37327290/79.3027573 secs/batch = 0.2668s, grad.norm=12.80259514
  9454: 7 [  165/ 1327], train_loss/perplexity = 4.52172422/91.9940796 secs/batch = 0.2602s, grad.norm=13.04512501
  9459: 7 [  170/ 1327], train_loss/perplexity = 4.38053370/79.8806534 secs/batch = 0.2670s, grad.norm=13.17621613
  9464: 7 [  175/ 1327], train_loss/perplexity = 4.67269230/106.9853897 secs/batch = 0.2652s, grad.norm=13.24154377
  9469: 7 [  180/ 1327], train_loss/perplexity = 4.57804394/97.3238373 secs/batch = 0.2638s, grad.norm=14.25938416
  9474: 7 [  185/ 1327], train_loss/perplexity = 4.82872915/125.0519409 secs/batch = 0.2658s, grad.norm=13.29630375
  9479: 7 [  190/ 1327], train_loss/perplexity = 4.32590389/75.6338501 secs/batch = 0.2653s, grad.norm=12.45737457
  9484: 7 [  195/ 1327], train_loss/perplexity = 4.60266733/99.7500305 secs/batch = 0.2676s, grad.norm=12.65761757
  9489: 7 [  200/ 1327], train_loss/perplexity = 4.48009539/88.2430878 secs/batch = 0.2669s, grad.norm=13.34636688
  9494: 7 [  205/ 1327], train_loss/perplexity = 4.66955662/106.6504440 secs/batch = 0.2666s, grad.norm=13.21817875
  9499: 7 [  210/ 1327], train_loss/perplexity = 4.51686239/91.5479050 secs/batch = 0.2660s, grad.norm=12.62175465
  9504: 7 [  215/ 1327], train_loss/perplexity = 4.70645237/110.6588821 secs/batch = 0.2665s, grad.norm=13.22252655
  9509: 7 [  220/ 1327], train_loss/perplexity = 4.57050323/96.5927048 secs/batch = 0.2645s, grad.norm=12.86733055
  9514: 7 [  225/ 1327], train_loss/perplexity = 4.78935242/120.2234879 secs/batch = 0.2665s, grad.norm=13.62933159
  9519: 7 [  230/ 1327], train_loss/perplexity = 4.52728558/92.5071182 secs/batch = 0.2640s, grad.norm=13.76897049
  9524: 7 [  235/ 1327], train_loss/perplexity = 4.39206219/80.8068848 secs/batch = 0.2656s, grad.norm=13.04429531
  9529: 7 [  240/ 1327], train_loss/perplexity = 4.24995899/70.1025391 secs/batch = 0.2664s, grad.norm=13.34597683
  9534: 7 [  245/ 1327], train_loss/perplexity = 4.54788685/94.4326477 secs/batch = 0.2646s, grad.norm=13.23845482
  9539: 7 [  250/ 1327], train_loss/perplexity = 4.36670971/78.7839813 secs/batch = 0.2671s, grad.norm=12.87158394
  9544: 7 [  255/ 1327], train_loss/perplexity = 4.34307146/76.9435043 secs/batch = 0.2646s, grad.norm=13.64168358
  9549: 7 [  260/ 1327], train_loss/perplexity = 4.67195415/106.9064484 secs/batch = 0.2659s, grad.norm=14.35076904
  9554: 7 [  265/ 1327], train_loss/perplexity = 4.84916115/127.6332779 secs/batch = 0.2667s, grad.norm=12.98031139
  9559: 7 [  270/ 1327], train_loss/perplexity = 4.83758926/126.1648331 secs/batch = 0.2658s, grad.norm=13.53707409
  9564: 7 [  275/ 1327], train_loss/perplexity = 4.86965227/130.2756042 secs/batch = 0.2664s, grad.norm=13.26480579
  9569: 7 [  280/ 1327], train_loss/perplexity = 4.56768608/96.3209763 secs/batch = 0.2663s, grad.norm=13.39945507
  9574: 7 [  285/ 1327], train_loss/perplexity = 4.83542871/125.8925400 secs/batch = 0.2667s, grad.norm=13.58194542
  9579: 7 [  290/ 1327], train_loss/perplexity = 4.57331181/96.8643723 secs/batch = 0.2660s, grad.norm=13.59077263
  9584: 7 [  295/ 1327], train_loss/perplexity = 4.36695051/78.8029556 secs/batch = 0.2655s, grad.norm=13.30390263
  9589: 7 [  300/ 1327], train_loss/perplexity = 3.90913129/49.8556213 secs/batch = 0.2666s, grad.norm=13.20419788
  9594: 7 [  305/ 1327], train_loss/perplexity = 4.40673923/82.0016403 secs/batch = 0.2671s, grad.norm=13.42733765
  9599: 7 [  310/ 1327], train_loss/perplexity = 4.44747496/85.4110031 secs/batch = 0.2664s, grad.norm=13.11509228
  9604: 7 [  315/ 1327], train_loss/perplexity = 3.89490438/49.1513519 secs/batch = 0.2661s, grad.norm=13.20934963
  9609: 7 [  320/ 1327], train_loss/perplexity = 3.93859887/51.3466072 secs/batch = 0.2672s, grad.norm=14.43825531
  9614: 7 [  325/ 1327], train_loss/perplexity = 3.93443084/51.1330376 secs/batch = 0.2656s, grad.norm=12.85501194
  9619: 7 [  330/ 1327], train_loss/perplexity = 4.47087002/87.4327545 secs/batch = 0.2655s, grad.norm=13.85451317
  9624: 7 [  335/ 1327], train_loss/perplexity = 3.96048021/52.4825211 secs/batch = 0.2655s, grad.norm=13.26356030
  9629: 7 [  340/ 1327], train_loss/perplexity = 4.64322710/103.8790359 secs/batch = 0.2611s, grad.norm=13.30448437
  9634: 7 [  345/ 1327], train_loss/perplexity = 4.41919565/83.0294724 secs/batch = 0.2662s, grad.norm=12.98326206
  9639: 7 [  350/ 1327], train_loss/perplexity = 4.51551199/91.4243622 secs/batch = 0.2660s, grad.norm=14.17752457
  9644: 7 [  355/ 1327], train_loss/perplexity = 4.50938606/90.8660126 secs/batch = 0.2681s, grad.norm=13.55639076
  9649: 7 [  360/ 1327], train_loss/perplexity = 4.63044548/102.5597458 secs/batch = 0.2671s, grad.norm=14.36668110
  9654: 7 [  365/ 1327], train_loss/perplexity = 4.64727020/104.2998810 secs/batch = 0.2662s, grad.norm=13.21357822
  9659: 7 [  370/ 1327], train_loss/perplexity = 4.63245106/102.7656403 secs/batch = 0.2658s, grad.norm=13.27320385
  9664: 7 [  375/ 1327], train_loss/perplexity = 4.04178715/56.9279900 secs/batch = 0.2656s, grad.norm=13.79202366
  9669: 7 [  380/ 1327], train_loss/perplexity = 4.11888313/61.4905281 secs/batch = 0.2649s, grad.norm=13.85408401
  9674: 7 [  385/ 1327], train_loss/perplexity = 4.38082790/79.9041595 secs/batch = 0.2660s, grad.norm=13.75180244
  9679: 7 [  390/ 1327], train_loss/perplexity = 4.48129845/88.3493118 secs/batch = 0.2655s, grad.norm=13.37492275
  9684: 7 [  395/ 1327], train_loss/perplexity = 4.54857492/94.4976425 secs/batch = 0.2658s, grad.norm=14.14546585
  9689: 7 [  400/ 1327], train_loss/perplexity = 4.44069290/84.8337021 secs/batch = 0.2660s, grad.norm=13.04940891
  9694: 7 [  405/ 1327], train_loss/perplexity = 4.71154070/111.2233887 secs/batch = 0.2664s, grad.norm=14.02893734
  9699: 7 [  410/ 1327], train_loss/perplexity = 4.33522749/76.3423233 secs/batch = 0.2665s, grad.norm=13.52457047
  9704: 7 [  415/ 1327], train_loss/perplexity = 4.32833862/75.8182220 secs/batch = 0.2644s, grad.norm=13.25978374
  9709: 7 [  420/ 1327], train_loss/perplexity = 4.00826931/55.0515099 secs/batch = 0.2667s, grad.norm=13.39916897
  9714: 7 [  425/ 1327], train_loss/perplexity = 4.28989220/72.9586029 secs/batch = 0.2666s, grad.norm=15.24219227
  9719: 7 [  430/ 1327], train_loss/perplexity = 4.53090906/92.8429260 secs/batch = 0.2668s, grad.norm=14.24256706
  9724: 7 [  435/ 1327], train_loss/perplexity = 4.55567551/95.1710205 secs/batch = 0.2648s, grad.norm=13.89547825
  9729: 7 [  440/ 1327], train_loss/perplexity = 4.16247320/64.2301788 secs/batch = 0.2665s, grad.norm=13.78625107
  9734: 7 [  445/ 1327], train_loss/perplexity = 4.47382212/87.6912460 secs/batch = 0.2658s, grad.norm=13.71771431
  9739: 7 [  450/ 1327], train_loss/perplexity = 4.44692755/85.3642654 secs/batch = 0.2654s, grad.norm=14.07890606
  9744: 7 [  455/ 1327], train_loss/perplexity = 4.31448078/74.7747879 secs/batch = 0.2678s, grad.norm=13.43138123
  9749: 7 [  460/ 1327], train_loss/perplexity = 4.31613731/74.8987579 secs/batch = 0.2673s, grad.norm=14.01740456
  9754: 7 [  465/ 1327], train_loss/perplexity = 4.02069139/55.7396317 secs/batch = 0.2667s, grad.norm=14.50229836
  9759: 7 [  470/ 1327], train_loss/perplexity = 4.72127962/112.3118744 secs/batch = 0.2660s, grad.norm=13.31627655
  9764: 7 [  475/ 1327], train_loss/perplexity = 4.28324986/72.4755936 secs/batch = 0.2660s, grad.norm=13.91582584
  9769: 7 [  480/ 1327], train_loss/perplexity = 4.42497873/83.5110321 secs/batch = 0.2664s, grad.norm=14.08650112
  9774: 7 [  485/ 1327], train_loss/perplexity = 4.32954550/75.9097748 secs/batch = 0.2659s, grad.norm=13.54115677
  9779: 7 [  490/ 1327], train_loss/perplexity = 4.22677803/68.4961853 secs/batch = 0.2638s, grad.norm=14.50300217
  9784: 7 [  495/ 1327], train_loss/perplexity = 4.29868889/73.6032257 secs/batch = 0.2660s, grad.norm=13.64807129
  9789: 7 [  500/ 1327], train_loss/perplexity = 4.52194834/92.0147018 secs/batch = 0.2661s, grad.norm=13.94996071
  9794: 7 [  505/ 1327], train_loss/perplexity = 4.56249762/95.8225098 secs/batch = 0.2609s, grad.norm=12.61712551
  9799: 7 [  510/ 1327], train_loss/perplexity = 4.94681549/140.7261047 secs/batch = 0.2667s, grad.norm=12.75992298
  9804: 7 [  515/ 1327], train_loss/perplexity = 4.55676937/95.2751846 secs/batch = 0.2681s, grad.norm=12.96911144
  9809: 7 [  520/ 1327], train_loss/perplexity = 4.70417166/110.4067917 secs/batch = 0.2639s, grad.norm=13.50481892
  9814: 7 [  525/ 1327], train_loss/perplexity = 4.24213791/69.5563965 secs/batch = 0.2650s, grad.norm=13.56475067
  9819: 7 [  530/ 1327], train_loss/perplexity = 4.32861567/75.8392258 secs/batch = 0.2666s, grad.norm=14.18541718
  9824: 7 [  535/ 1327], train_loss/perplexity = 4.43994570/84.7703400 secs/batch = 0.2613s, grad.norm=13.60978508
  9829: 7 [  540/ 1327], train_loss/perplexity = 4.57296610/96.8308945 secs/batch = 0.2663s, grad.norm=13.29479027
  9834: 7 [  545/ 1327], train_loss/perplexity = 4.56934357/96.4807587 secs/batch = 0.2646s, grad.norm=13.95073891
  9839: 7 [  550/ 1327], train_loss/perplexity = 4.49493408/89.5622635 secs/batch = 0.2645s, grad.norm=13.84905529
  9844: 7 [  555/ 1327], train_loss/perplexity = 4.29493237/73.3272552 secs/batch = 0.2664s, grad.norm=13.14155006
  9849: 7 [  560/ 1327], train_loss/perplexity = 4.39512968/81.0551376 secs/batch = 0.2657s, grad.norm=14.19184399
  9854: 7 [  565/ 1327], train_loss/perplexity = 4.29094505/73.0354614 secs/batch = 0.2664s, grad.norm=14.16723824
  9859: 7 [  570/ 1327], train_loss/perplexity = 4.33182907/76.0833206 secs/batch = 0.2655s, grad.norm=14.39048195
  9864: 7 [  575/ 1327], train_loss/perplexity = 4.10887384/60.8781204 secs/batch = 0.2655s, grad.norm=14.02011776
  9869: 7 [  580/ 1327], train_loss/perplexity = 4.55601025/95.2028885 secs/batch = 0.2670s, grad.norm=13.83831787
  9874: 7 [  585/ 1327], train_loss/perplexity = 4.10627651/60.7202034 secs/batch = 0.2649s, grad.norm=13.73118401
  9879: 7 [  590/ 1327], train_loss/perplexity = 4.46007442/86.4939499 secs/batch = 0.2638s, grad.norm=13.43889618
  9884: 7 [  595/ 1327], train_loss/perplexity = 4.43644619/84.4742050 secs/batch = 0.2669s, grad.norm=14.07041740
  9889: 7 [  600/ 1327], train_loss/perplexity = 4.68494320/108.3041229 secs/batch = 0.2664s, grad.norm=13.58246613
  9894: 7 [  605/ 1327], train_loss/perplexity = 4.48642159/88.8031006 secs/batch = 0.2667s, grad.norm=13.47912884
  9899: 7 [  610/ 1327], train_loss/perplexity = 4.66619873/106.2929230 secs/batch = 0.2692s, grad.norm=13.86896801
  9904: 7 [  615/ 1327], train_loss/perplexity = 4.23275137/68.9065628 secs/batch = 0.2651s, grad.norm=13.56931782
  9909: 7 [  620/ 1327], train_loss/perplexity = 4.63121939/102.6391449 secs/batch = 0.2661s, grad.norm=13.78419781
  9914: 7 [  625/ 1327], train_loss/perplexity = 4.62066078/101.5611191 secs/batch = 0.2651s, grad.norm=13.22826290
  9919: 7 [  630/ 1327], train_loss/perplexity = 4.71945810/112.1074829 secs/batch = 0.2650s, grad.norm=13.69906998
  9924: 7 [  635/ 1327], train_loss/perplexity = 4.44222450/84.9637299 secs/batch = 0.2667s, grad.norm=14.00054646
  9929: 7 [  640/ 1327], train_loss/perplexity = 4.43775797/84.5850830 secs/batch = 0.2665s, grad.norm=13.82586956
  9934: 7 [  645/ 1327], train_loss/perplexity = 4.70409775/110.3986359 secs/batch = 0.2627s, grad.norm=14.31229019
  9939: 7 [  650/ 1327], train_loss/perplexity = 4.19952488/66.6546555 secs/batch = 0.2657s, grad.norm=13.81316376
  9944: 7 [  655/ 1327], train_loss/perplexity = 4.36094189/78.3308792 secs/batch = 0.2662s, grad.norm=13.76807690
  9949: 7 [  660/ 1327], train_loss/perplexity = 4.26972723/71.5021286 secs/batch = 0.2658s, grad.norm=13.40768814
  9954: 7 [  665/ 1327], train_loss/perplexity = 4.44891214/85.5338440 secs/batch = 0.2657s, grad.norm=13.88210106
  9959: 7 [  670/ 1327], train_loss/perplexity = 4.32339334/75.4442062 secs/batch = 0.2647s, grad.norm=14.00882816
  9964: 7 [  675/ 1327], train_loss/perplexity = 4.19106102/66.0928802 secs/batch = 0.2659s, grad.norm=14.09511185
  9969: 7 [  680/ 1327], train_loss/perplexity = 4.38779020/80.4624176 secs/batch = 0.2667s, grad.norm=14.53768158
  9974: 7 [  685/ 1327], train_loss/perplexity = 4.14744663/63.2722359 secs/batch = 0.2670s, grad.norm=13.74980927
  9979: 7 [  690/ 1327], train_loss/perplexity = 4.58867407/98.3639221 secs/batch = 0.2661s, grad.norm=13.33762646
  9984: 7 [  695/ 1327], train_loss/perplexity = 4.43335629/84.2135849 secs/batch = 0.2617s, grad.norm=13.81055355
  9989: 7 [  700/ 1327], train_loss/perplexity = 4.65279388/104.8775940 secs/batch = 0.2668s, grad.norm=13.85025311
  9994: 7 [  705/ 1327], train_loss/perplexity = 4.37403345/79.3630905 secs/batch = 0.2648s, grad.norm=13.23756123
  9999: 7 [  710/ 1327], train_loss/perplexity = 4.32667351/75.6920776 secs/batch = 0.2670s, grad.norm=14.32660961
 10004: 7 [  715/ 1327], train_loss/perplexity = 4.21329832/67.5790710 secs/batch = 0.2611s, grad.norm=13.97872353
 10009: 7 [  720/ 1327], train_loss/perplexity = 4.26599121/71.2354965 secs/batch = 0.2650s, grad.norm=14.34489918
 10014: 7 [  725/ 1327], train_loss/perplexity = 4.25161457/70.2186966 secs/batch = 0.2666s, grad.norm=14.40435314
 10019: 7 [  730/ 1327], train_loss/perplexity = 4.38181973/79.9834518 secs/batch = 0.2644s, grad.norm=14.08376503
 10024: 7 [  735/ 1327], train_loss/perplexity = 4.48182297/88.3956680 secs/batch = 0.2644s, grad.norm=14.50947380
 10029: 7 [  740/ 1327], train_loss/perplexity = 3.96036005/52.4762154 secs/batch = 0.2600s, grad.norm=13.28709507
 10034: 7 [  745/ 1327], train_loss/perplexity = 4.37969971/79.8140640 secs/batch = 0.2663s, grad.norm=13.53481197
 10039: 7 [  750/ 1327], train_loss/perplexity = 4.25495052/70.4533310 secs/batch = 0.2663s, grad.norm=13.65756321
 10044: 7 [  755/ 1327], train_loss/perplexity = 4.18150139/65.4640656 secs/batch = 0.2691s, grad.norm=14.06781006
 10049: 7 [  760/ 1327], train_loss/perplexity = 4.05948448/57.9444313 secs/batch = 0.2652s, grad.norm=12.83905220
 10054: 7 [  765/ 1327], train_loss/perplexity = 4.07049847/58.5861588 secs/batch = 0.2657s, grad.norm=13.21508789
 10059: 7 [  770/ 1327], train_loss/perplexity = 4.08631706/59.5202789 secs/batch = 0.2669s, grad.norm=13.82454205
 10064: 7 [  775/ 1327], train_loss/perplexity = 4.24004555/69.4110107 secs/batch = 0.2682s, grad.norm=14.11133575
 10069: 7 [  780/ 1327], train_loss/perplexity = 4.63807678/103.3453979 secs/batch = 0.2668s, grad.norm=14.12782097
 10074: 7 [  785/ 1327], train_loss/perplexity = 4.42014408/83.1082611 secs/batch = 0.2671s, grad.norm=14.22010708
 10079: 7 [  790/ 1327], train_loss/perplexity = 4.17512608/65.0480423 secs/batch = 0.2648s, grad.norm=13.81626797
 10084: 7 [  795/ 1327], train_loss/perplexity = 4.52444744/92.2449417 secs/batch = 0.2638s, grad.norm=13.72849178
 10089: 7 [  800/ 1327], train_loss/perplexity = 4.41178274/82.4162598 secs/batch = 0.2665s, grad.norm=13.95217228
 10094: 7 [  805/ 1327], train_loss/perplexity = 4.74017429/114.4541473 secs/batch = 0.2661s, grad.norm=13.78645325
 10099: 7 [  810/ 1327], train_loss/perplexity = 4.37042999/79.0776291 secs/batch = 0.2671s, grad.norm=13.54833412
 10104: 7 [  815/ 1327], train_loss/perplexity = 4.32716846/75.7295532 secs/batch = 0.2661s, grad.norm=13.51359749
 10109: 7 [  820/ 1327], train_loss/perplexity = 4.09035110/59.7608719 secs/batch = 0.2663s, grad.norm=13.12159634
 10114: 7 [  825/ 1327], train_loss/perplexity = 4.27635336/71.9774857 secs/batch = 0.2662s, grad.norm=13.90854740
 10119: 7 [  830/ 1327], train_loss/perplexity = 4.06991959/58.5522537 secs/batch = 0.2659s, grad.norm=14.02915287
 10124: 7 [  835/ 1327], train_loss/perplexity = 4.42160273/83.2295761 secs/batch = 0.2656s, grad.norm=14.59957504
 10129: 7 [  840/ 1327], train_loss/perplexity = 4.40257978/81.6612625 secs/batch = 0.2656s, grad.norm=13.82002831
 10134: 7 [  845/ 1327], train_loss/perplexity = 4.27287197/71.7273407 secs/batch = 0.2673s, grad.norm=13.99869633
 10139: 7 [  850/ 1327], train_loss/perplexity = 4.32581377/75.6270294 secs/batch = 0.2660s, grad.norm=13.83046722
 10144: 7 [  855/ 1327], train_loss/perplexity = 4.32514048/75.5761261 secs/batch = 0.2660s, grad.norm=13.91101646
 10149: 7 [  860/ 1327], train_loss/perplexity = 4.09870434/60.2621574 secs/batch = 0.2655s, grad.norm=13.47089863
 10154: 7 [  865/ 1327], train_loss/perplexity = 4.51732063/91.5898666 secs/batch = 0.2664s, grad.norm=13.77825260
 10159: 7 [  870/ 1327], train_loss/perplexity = 4.47397661/87.7047958 secs/batch = 0.2672s, grad.norm=14.23927689
 10164: 7 [  875/ 1327], train_loss/perplexity = 3.99612284/54.3868752 secs/batch = 0.2647s, grad.norm=13.75780678
 10169: 7 [  880/ 1327], train_loss/perplexity = 4.19345474/66.2512741 secs/batch = 0.2656s, grad.norm=13.70930004
 10174: 7 [  885/ 1327], train_loss/perplexity = 4.37426949/79.3818283 secs/batch = 0.2652s, grad.norm=13.45009995
 10179: 7 [  890/ 1327], train_loss/perplexity = 4.47981024/88.2179337 secs/batch = 0.2652s, grad.norm=13.75007343
 10184: 7 [  895/ 1327], train_loss/perplexity = 4.51538801/91.4130249 secs/batch = 0.2665s, grad.norm=13.45533371
 10189: 7 [  900/ 1327], train_loss/perplexity = 4.33817387/76.5675888 secs/batch = 0.2657s, grad.norm=13.12051964
 10194: 7 [  905/ 1327], train_loss/perplexity = 4.16126251/64.1524658 secs/batch = 0.2662s, grad.norm=13.45314407
 10199: 7 [  910/ 1327], train_loss/perplexity = 4.23974085/69.3898697 secs/batch = 0.2668s, grad.norm=13.51144123
 10204: 7 [  915/ 1327], train_loss/perplexity = 4.48842573/88.9812546 secs/batch = 0.2674s, grad.norm=13.12966537
 10209: 7 [  920/ 1327], train_loss/perplexity = 4.64163446/103.7137222 secs/batch = 0.2657s, grad.norm=13.55845928
 10214: 7 [  925/ 1327], train_loss/perplexity = 4.44848013/85.4969025 secs/batch = 0.2664s, grad.norm=13.47566986
 10219: 7 [  930/ 1327], train_loss/perplexity = 4.47480536/87.7775116 secs/batch = 0.2662s, grad.norm=12.86443138
 10224: 7 [  935/ 1327], train_loss/perplexity = 4.51124048/91.0346756 secs/batch = 0.2665s, grad.norm=13.17381287
 10229: 7 [  940/ 1327], train_loss/perplexity = 4.48230124/88.4379578 secs/batch = 0.2666s, grad.norm=13.18365860
 10234: 7 [  945/ 1327], train_loss/perplexity = 4.65224600/104.8201447 secs/batch = 0.2659s, grad.norm=13.49771118
 10239: 7 [  950/ 1327], train_loss/perplexity = 4.45277977/85.8652954 secs/batch = 0.2650s, grad.norm=13.28877258
 10244: 7 [  955/ 1327], train_loss/perplexity = 4.44721889/85.3891373 secs/batch = 0.2589s, grad.norm=13.84109402
 10249: 7 [  960/ 1327], train_loss/perplexity = 4.68137503/107.9183578 secs/batch = 0.2656s, grad.norm=13.43947697
 10254: 7 [  965/ 1327], train_loss/perplexity = 4.45106792/85.7184372 secs/batch = 0.2669s, grad.norm=13.71026325
 10259: 7 [  970/ 1327], train_loss/perplexity = 4.67301321/107.0197296 secs/batch = 0.2661s, grad.norm=13.75057507
 10264: 7 [  975/ 1327], train_loss/perplexity = 4.44995546/85.6231308 secs/batch = 0.2684s, grad.norm=14.14668655
 10269: 7 [  980/ 1327], train_loss/perplexity = 4.18164539/65.4734955 secs/batch = 0.2656s, grad.norm=13.50353527
 10274: 7 [  985/ 1327], train_loss/perplexity = 4.32994366/75.9400101 secs/batch = 0.2665s, grad.norm=13.80342388
 10279: 7 [  990/ 1327], train_loss/perplexity = 4.60434055/99.9170685 secs/batch = 0.2661s, grad.norm=13.72458172
 10284: 7 [  995/ 1327], train_loss/perplexity = 4.58857203/98.3538818 secs/batch = 0.2661s, grad.norm=13.21078587
 10289: 7 [ 1000/ 1327], train_loss/perplexity = 4.04268980/56.9794006 secs/batch = 0.2619s, grad.norm=13.18584728
 10294: 7 [ 1005/ 1327], train_loss/perplexity = 4.52075291/91.9047699 secs/batch = 0.2670s, grad.norm=13.85944271
 10299: 7 [ 1010/ 1327], train_loss/perplexity = 4.12630653/61.9486961 secs/batch = 0.2664s, grad.norm=13.23103714
 10304: 7 [ 1015/ 1327], train_loss/perplexity = 4.60379553/99.8626251 secs/batch = 0.2663s, grad.norm=13.30453396
 10309: 7 [ 1020/ 1327], train_loss/perplexity = 4.73541689/113.9109344 secs/batch = 0.2610s, grad.norm=13.44076347
 10314: 7 [ 1025/ 1327], train_loss/perplexity = 4.65644741/105.2614670 secs/batch = 0.2656s, grad.norm=13.46373844
 10319: 7 [ 1030/ 1327], train_loss/perplexity = 4.28449917/72.5661926 secs/batch = 0.2663s, grad.norm=13.03915405
 10324: 7 [ 1035/ 1327], train_loss/perplexity = 4.28752327/72.7859726 secs/batch = 0.2652s, grad.norm=13.30809689
 10329: 7 [ 1040/ 1327], train_loss/perplexity = 4.53071976/92.8253479 secs/batch = 0.2672s, grad.norm=14.03099346
 10334: 7 [ 1045/ 1327], train_loss/perplexity = 4.10624504/60.7182961 secs/batch = 0.2683s, grad.norm=13.54182816
 10339: 7 [ 1050/ 1327], train_loss/perplexity = 4.17966318/65.3438416 secs/batch = 0.2673s, grad.norm=13.77932358
 10344: 7 [ 1055/ 1327], train_loss/perplexity = 4.28363895/72.5037994 secs/batch = 0.2653s, grad.norm=14.71828747
 10349: 7 [ 1060/ 1327], train_loss/perplexity = 3.89852810/49.3297882 secs/batch = 0.2661s, grad.norm=14.95851135
 10354: 7 [ 1065/ 1327], train_loss/perplexity = 4.11684942/61.3656006 secs/batch = 0.2656s, grad.norm=14.57091713
 10359: 7 [ 1070/ 1327], train_loss/perplexity = 4.36843920/78.9203568 secs/batch = 0.2659s, grad.norm=14.11417007
 10364: 7 [ 1075/ 1327], train_loss/perplexity = 4.28226423/72.4041977 secs/batch = 0.2661s, grad.norm=13.97134686
 10369: 7 [ 1080/ 1327], train_loss/perplexity = 4.12625837/61.9457092 secs/batch = 0.2666s, grad.norm=13.70448494
 10374: 7 [ 1085/ 1327], train_loss/perplexity = 3.91998792/50.3998375 secs/batch = 0.2668s, grad.norm=13.84122372
 10379: 7 [ 1090/ 1327], train_loss/perplexity = 4.15842009/63.9703751 secs/batch = 0.2652s, grad.norm=14.64961052
 10384: 7 [ 1095/ 1327], train_loss/perplexity = 4.32519627/75.5803452 secs/batch = 0.2654s, grad.norm=14.30364037
 10389: 7 [ 1100/ 1327], train_loss/perplexity = 4.06228399/58.1068764 secs/batch = 0.2657s, grad.norm=15.78525257
 10394: 7 [ 1105/ 1327], train_loss/perplexity = 4.08208799/59.2690926 secs/batch = 0.2664s, grad.norm=14.27553082
 10399: 7 [ 1110/ 1327], train_loss/perplexity = 4.39232349/80.8280029 secs/batch = 0.2672s, grad.norm=14.80044270
 10404: 7 [ 1115/ 1327], train_loss/perplexity = 4.13085651/62.2312012 secs/batch = 0.2667s, grad.norm=13.65827656
 10409: 7 [ 1120/ 1327], train_loss/perplexity = 4.36292839/78.4866409 secs/batch = 0.2665s, grad.norm=13.66589737
 10414: 7 [ 1125/ 1327], train_loss/perplexity = 4.57360792/96.8930588 secs/batch = 0.2674s, grad.norm=14.50364876
 10419: 7 [ 1130/ 1327], train_loss/perplexity = 4.21242332/67.5199661 secs/batch = 0.2632s, grad.norm=13.91089916
 10424: 7 [ 1135/ 1327], train_loss/perplexity = 4.25723076/70.6141663 secs/batch = 0.2667s, grad.norm=13.74007607
 10429: 7 [ 1140/ 1327], train_loss/perplexity = 4.55944061/95.5300293 secs/batch = 0.2665s, grad.norm=14.57813168
 10434: 7 [ 1145/ 1327], train_loss/perplexity = 4.30713129/74.2272491 secs/batch = 0.2670s, grad.norm=13.58321381
 10439: 7 [ 1150/ 1327], train_loss/perplexity = 4.30869150/74.3431473 secs/batch = 0.2673s, grad.norm=14.33460140
 10444: 7 [ 1155/ 1327], train_loss/perplexity = 4.41677332/82.8285904 secs/batch = 0.2664s, grad.norm=14.50163555
 10449: 7 [ 1160/ 1327], train_loss/perplexity = 4.34524298/77.1107712 secs/batch = 0.2659s, grad.norm=14.13347149
 10454: 7 [ 1165/ 1327], train_loss/perplexity = 4.33132553/76.0450211 secs/batch = 0.2647s, grad.norm=13.75574017
 10459: 7 [ 1170/ 1327], train_loss/perplexity = 4.23124409/68.8027725 secs/batch = 0.2647s, grad.norm=13.76382065
 10464: 7 [ 1175/ 1327], train_loss/perplexity = 3.98095846/53.5683517 secs/batch = 0.2658s, grad.norm=13.88714027
 10469: 7 [ 1180/ 1327], train_loss/perplexity = 4.06263351/58.1271896 secs/batch = 0.2651s, grad.norm=13.97243690
 10474: 7 [ 1185/ 1327], train_loss/perplexity = 4.29665041/73.4533463 secs/batch = 0.2671s, grad.norm=14.22878742
 10479: 7 [ 1190/ 1327], train_loss/perplexity = 4.32262802/75.3864822 secs/batch = 0.2656s, grad.norm=13.84507656
 10484: 7 [ 1195/ 1327], train_loss/perplexity = 4.16287851/64.2562180 secs/batch = 0.2669s, grad.norm=13.92540836
 10489: 7 [ 1200/ 1327], train_loss/perplexity = 4.07902145/59.0876198 secs/batch = 0.2663s, grad.norm=13.91860771
 10494: 7 [ 1205/ 1327], train_loss/perplexity = 4.12945843/62.1442566 secs/batch = 0.2659s, grad.norm=14.36049938
 10499: 7 [ 1210/ 1327], train_loss/perplexity = 3.74655604/42.3748932 secs/batch = 0.2642s, grad.norm=14.08736610
 10504: 7 [ 1215/ 1327], train_loss/perplexity = 4.01863384/55.6250610 secs/batch = 0.2637s, grad.norm=13.87045193
 10509: 7 [ 1220/ 1327], train_loss/perplexity = 4.15716982/63.8904457 secs/batch = 0.2639s, grad.norm=14.06247711
 10514: 7 [ 1225/ 1327], train_loss/perplexity = 3.88683772/48.7564621 secs/batch = 0.2659s, grad.norm=14.72077179
 10519: 7 [ 1230/ 1327], train_loss/perplexity = 4.11369514/61.1723404 secs/batch = 0.2665s, grad.norm=13.93109322
 10524: 7 [ 1235/ 1327], train_loss/perplexity = 4.09608364/60.1044350 secs/batch = 0.2637s, grad.norm=13.88749218
 10529: 7 [ 1240/ 1327], train_loss/perplexity = 4.32461500/75.5364227 secs/batch = 0.2664s, grad.norm=14.56533909
 10534: 7 [ 1245/ 1327], train_loss/perplexity = 4.20355558/66.9238586 secs/batch = 0.2658s, grad.norm=13.81469631
 10539: 7 [ 1250/ 1327], train_loss/perplexity = 4.31963253/75.1610031 secs/batch = 0.2659s, grad.norm=13.31297684
 10544: 7 [ 1255/ 1327], train_loss/perplexity = 4.39623594/81.1448593 secs/batch = 0.2664s, grad.norm=13.42979336
 10549: 7 [ 1260/ 1327], train_loss/perplexity = 4.17264462/64.8868256 secs/batch = 0.2661s, grad.norm=14.39088249
 10554: 7 [ 1265/ 1327], train_loss/perplexity = 4.44304180/85.0332031 secs/batch = 0.2662s, grad.norm=14.00361633
 10559: 7 [ 1270/ 1327], train_loss/perplexity = 4.10714674/60.7730675 secs/batch = 0.2661s, grad.norm=14.08498192
 10564: 7 [ 1275/ 1327], train_loss/perplexity = 4.30368137/73.9716110 secs/batch = 0.2668s, grad.norm=14.41839504
 10569: 7 [ 1280/ 1327], train_loss/perplexity = 4.16369343/64.3086014 secs/batch = 0.2673s, grad.norm=14.34754658
 10574: 7 [ 1285/ 1327], train_loss/perplexity = 4.07505083/58.8534737 secs/batch = 0.2664s, grad.norm=14.37910843
 10579: 7 [ 1290/ 1327], train_loss/perplexity = 4.31070805/74.4932175 secs/batch = 0.2668s, grad.norm=13.95819855
 10584: 7 [ 1295/ 1327], train_loss/perplexity = 4.29220486/73.1275253 secs/batch = 0.2658s, grad.norm=14.41076469
 10589: 7 [ 1300/ 1327], train_loss/perplexity = 4.40418625/81.7925568 secs/batch = 0.2668s, grad.norm=13.20910835
 10594: 7 [ 1305/ 1327], train_loss/perplexity = 4.58204126/97.7136536 secs/batch = 0.2661s, grad.norm=14.72848034
 10599: 7 [ 1310/ 1327], train_loss/perplexity = 4.79382944/120.7629395 secs/batch = 0.2673s, grad.norm=14.29024792
 10604: 7 [ 1315/ 1327], train_loss/perplexity = 4.56826496/96.3767471 secs/batch = 0.2639s, grad.norm=14.32546616
 10609: 7 [ 1320/ 1327], train_loss/perplexity = 4.57477188/97.0059052 secs/batch = 0.2661s, grad.norm=13.87063313
 10614: 7 [ 1325/ 1327], train_loss/perplexity = 4.48854542/88.9919052 secs/batch = 0.2666s, grad.norm=14.27364731
Epoch training time: 352.8773145675659
	> validation loss = 4.73179865, perplexity = 113.49952698
	> validation loss = 4.63935947, perplexity = 103.47804260
	> validation loss = 4.61612940, perplexity = 101.10195160
	> validation loss = 4.62344170, perplexity = 101.84394836
	> validation loss = 4.77264452, perplexity = 118.23149109
	> validation loss = 4.73299742, perplexity = 113.63566589
	> validation loss = 4.65473175, perplexity = 105.08103180
	> validation loss = 4.49634409, perplexity = 89.68863678
	> validation loss = 4.30497456, perplexity = 74.06732941
	> validation loss = 4.43128633, perplexity = 84.03945160
	> validation loss = 4.53235865, perplexity = 92.97760773
	> validation loss = 4.61383724, perplexity = 100.87047577
	> validation loss = 4.56606579, perplexity = 96.16503143
	> validation loss = 4.35534286, perplexity = 77.89352417
	> validation loss = 4.26112461, perplexity = 70.88966370
	> validation loss = 4.27810955, perplexity = 72.10400391
	> validation loss = 4.72357607, perplexity = 112.57009125
	> validation loss = 4.29395962, perplexity = 73.25595856
	> validation loss = 4.71839094, perplexity = 111.98791504
	> validation loss = 4.55325031, perplexity = 94.94049072
	> validation loss = 4.36188364, perplexity = 78.40467834
at the end of epoch: 7
train loss = 4.43604180, perplexity = 84.44004856
validation loss = 4.54588131, perplexity = 94.24344810
Saved model cv/epoch007_4.5459.model
 10621: 8 [    5/ 1327], train_loss/perplexity = 4.55749130/95.3439865 secs/batch = 0.2662s, grad.norm=14.01401711
 10626: 8 [   10/ 1327], train_loss/perplexity = 4.17042637/64.7430496 secs/batch = 0.2649s, grad.norm=13.89936638
 10631: 8 [   15/ 1327], train_loss/perplexity = 4.49223423/89.3207855 secs/batch = 0.2668s, grad.norm=13.29950428
 10636: 8 [   20/ 1327], train_loss/perplexity = 4.62061453/101.5564194 secs/batch = 0.2672s, grad.norm=13.30312061
 10641: 8 [   25/ 1327], train_loss/perplexity = 4.47718716/87.9868317 secs/batch = 0.2662s, grad.norm=14.36444950
 10646: 8 [   30/ 1327], train_loss/perplexity = 4.52056503/91.8875046 secs/batch = 0.2653s, grad.norm=14.14383125
 10651: 8 [   35/ 1327], train_loss/perplexity = 4.34063959/76.7566147 secs/batch = 0.2624s, grad.norm=13.63629723
 10656: 8 [   40/ 1327], train_loss/perplexity = 4.27602196/71.9536362 secs/batch = 0.2662s, grad.norm=13.98086452
 10661: 8 [   45/ 1327], train_loss/perplexity = 4.11338615/61.1534424 secs/batch = 0.2608s, grad.norm=13.24903393
 10666: 8 [   50/ 1327], train_loss/perplexity = 4.34178400/76.8445053 secs/batch = 0.2670s, grad.norm=13.72495174
 10671: 8 [   55/ 1327], train_loss/perplexity = 4.30646276/74.1776428 secs/batch = 0.2594s, grad.norm=14.35926247
 10676: 8 [   60/ 1327], train_loss/perplexity = 4.55479240/95.0870132 secs/batch = 0.2653s, grad.norm=14.16657352
 10681: 8 [   65/ 1327], train_loss/perplexity = 4.13594532/62.5486908 secs/batch = 0.2656s, grad.norm=13.46293068
 10686: 8 [   70/ 1327], train_loss/perplexity = 3.97348332/53.1694145 secs/batch = 0.2664s, grad.norm=13.81798077
 10691: 8 [   75/ 1327], train_loss/perplexity = 3.82329559/45.7547493 secs/batch = 0.2650s, grad.norm=13.40863132
 10696: 8 [   80/ 1327], train_loss/perplexity = 4.21364546/67.6025314 secs/batch = 0.2672s, grad.norm=13.73294544
 10701: 8 [   85/ 1327], train_loss/perplexity = 4.31546783/74.8486328 secs/batch = 0.2657s, grad.norm=14.04960632
 10706: 8 [   90/ 1327], train_loss/perplexity = 4.32328939/75.4363632 secs/batch = 0.2673s, grad.norm=14.17637062
 10711: 8 [   95/ 1327], train_loss/perplexity = 4.23289442/68.9164200 secs/batch = 0.2661s, grad.norm=13.94227886
 10716: 8 [  100/ 1327], train_loss/perplexity = 4.46790028/87.1734924 secs/batch = 0.2677s, grad.norm=13.82015705
 10721: 8 [  105/ 1327], train_loss/perplexity = 4.31623697/74.9062195 secs/batch = 0.2653s, grad.norm=14.54565430
 10726: 8 [  110/ 1327], train_loss/perplexity = 4.18461609/65.6682816 secs/batch = 0.2652s, grad.norm=13.80461597
 10731: 8 [  115/ 1327], train_loss/perplexity = 4.14990091/63.4277153 secs/batch = 0.2658s, grad.norm=14.54941559
 10736: 8 [  120/ 1327], train_loss/perplexity = 4.22583771/68.4318085 secs/batch = 0.2661s, grad.norm=14.49920559
 10741: 8 [  125/ 1327], train_loss/perplexity = 4.33288193/76.1634674 secs/batch = 0.2657s, grad.norm=14.65741634
 10746: 8 [  130/ 1327], train_loss/perplexity = 4.32780457/75.7777405 secs/batch = 0.2673s, grad.norm=15.10931110
 10751: 8 [  135/ 1327], train_loss/perplexity = 4.26335573/71.0480042 secs/batch = 0.2656s, grad.norm=14.02092075
 10756: 8 [  140/ 1327], train_loss/perplexity = 4.51144123/91.0529556 secs/batch = 0.2682s, grad.norm=13.85898781
 10761: 8 [  145/ 1327], train_loss/perplexity = 4.43858767/84.6552963 secs/batch = 0.2668s, grad.norm=14.99024677
 10766: 8 [  150/ 1327], train_loss/perplexity = 4.46369934/86.8080521 secs/batch = 0.2654s, grad.norm=14.55684948
 10771: 8 [  155/ 1327], train_loss/perplexity = 4.70837069/110.8713684 secs/batch = 0.2671s, grad.norm=14.15953922
 10776: 8 [  160/ 1327], train_loss/perplexity = 4.29546547/73.3663559 secs/batch = 0.2649s, grad.norm=13.04244518
 10781: 8 [  165/ 1327], train_loss/perplexity = 4.50773811/90.7163925 secs/batch = 0.2674s, grad.norm=13.65750504
 10786: 8 [  170/ 1327], train_loss/perplexity = 4.29684067/73.4673157 secs/batch = 0.2659s, grad.norm=13.50762177
 10791: 8 [  175/ 1327], train_loss/perplexity = 4.63315487/102.8379898 secs/batch = 0.2651s, grad.norm=14.15202618
 10796: 8 [  180/ 1327], train_loss/perplexity = 4.49335814/89.4212341 secs/batch = 0.2620s, grad.norm=14.30106258
 10801: 8 [  185/ 1327], train_loss/perplexity = 4.72448349/112.6722870 secs/batch = 0.2671s, grad.norm=13.83079052
 10806: 8 [  190/ 1327], train_loss/perplexity = 4.33797598/76.5524368 secs/batch = 0.2671s, grad.norm=13.12404156
 10811: 8 [  195/ 1327], train_loss/perplexity = 4.58391857/97.8972626 secs/batch = 0.2671s, grad.norm=13.28371429
 10816: 8 [  200/ 1327], train_loss/perplexity = 4.42339802/83.3791275 secs/batch = 0.2657s, grad.norm=13.73321915
 10821: 8 [  205/ 1327], train_loss/perplexity = 4.62988520/102.5022964 secs/batch = 0.2677s, grad.norm=13.85045052
 10826: 8 [  210/ 1327], train_loss/perplexity = 4.47171783/87.5069199 secs/batch = 0.2679s, grad.norm=12.94164181
 10831: 8 [  215/ 1327], train_loss/perplexity = 4.59143066/98.6354446 secs/batch = 0.2659s, grad.norm=13.35207272
 10836: 8 [  220/ 1327], train_loss/perplexity = 4.49902725/89.9296112 secs/batch = 0.2639s, grad.norm=13.55154037
 10841: 8 [  225/ 1327], train_loss/perplexity = 4.66851616/106.5395355 secs/batch = 0.2664s, grad.norm=13.95049095
 10846: 8 [  230/ 1327], train_loss/perplexity = 4.48424530/88.6100540 secs/batch = 0.2664s, grad.norm=14.56054592
 10851: 8 [  235/ 1327], train_loss/perplexity = 4.37366056/79.3335037 secs/batch = 0.2670s, grad.norm=13.74770069
 10856: 8 [  240/ 1327], train_loss/perplexity = 4.25160885/70.2182922 secs/batch = 0.2658s, grad.norm=14.07166862
 10861: 8 [  245/ 1327], train_loss/perplexity = 4.44037628/84.8068466 secs/batch = 0.2638s, grad.norm=13.74757290
 10866: 8 [  250/ 1327], train_loss/perplexity = 4.26155472/70.9201584 secs/batch = 0.2680s, grad.norm=13.37773514
 10871: 8 [  255/ 1327], train_loss/perplexity = 4.32119846/75.2787933 secs/batch = 0.2673s, grad.norm=14.03731251
 10876: 8 [  260/ 1327], train_loss/perplexity = 4.54393005/94.0597382 secs/batch = 0.2671s, grad.norm=14.62126350
 10881: 8 [  265/ 1327], train_loss/perplexity = 4.71999645/112.1678543 secs/batch = 0.2676s, grad.norm=13.62941074
 10886: 8 [  270/ 1327], train_loss/perplexity = 4.76529312/117.3655167 secs/batch = 0.2662s, grad.norm=13.75120163
 10891: 8 [  275/ 1327], train_loss/perplexity = 4.76054239/116.8092651 secs/batch = 0.2659s, grad.norm=13.67696381
 10896: 8 [  280/ 1327], train_loss/perplexity = 4.57631493/97.1557083 secs/batch = 0.2661s, grad.norm=13.89386940
 10901: 8 [  285/ 1327], train_loss/perplexity = 4.77009678/117.9306564 secs/batch = 0.2658s, grad.norm=13.61750984
 10906: 8 [  290/ 1327], train_loss/perplexity = 4.47492313/87.7878494 secs/batch = 0.2668s, grad.norm=14.16557789
 10911: 8 [  295/ 1327], train_loss/perplexity = 4.26378107/71.0782242 secs/batch = 0.2659s, grad.norm=13.78477955
 10916: 8 [  300/ 1327], train_loss/perplexity = 3.78890634/44.2080269 secs/batch = 0.2666s, grad.norm=13.43855858
 10921: 8 [  305/ 1327], train_loss/perplexity = 4.33446789/76.2843552 secs/batch = 0.2649s, grad.norm=13.57277489
 10926: 8 [  310/ 1327], train_loss/perplexity = 4.31640482/74.9188004 secs/batch = 0.2668s, grad.norm=13.70147324
 10931: 8 [  315/ 1327], train_loss/perplexity = 3.84654045/46.8307686 secs/batch = 0.2628s, grad.norm=14.10154533
 10936: 8 [  320/ 1327], train_loss/perplexity = 3.82580423/45.8696747 secs/batch = 0.2654s, grad.norm=14.76700592
 10941: 8 [  325/ 1327], train_loss/perplexity = 3.87191892/48.0344734 secs/batch = 0.2680s, grad.norm=13.46151829
 10946: 8 [  330/ 1327], train_loss/perplexity = 4.38202667/80.0000000 secs/batch = 0.2678s, grad.norm=13.95824337
 10951: 8 [  335/ 1327], train_loss/perplexity = 3.82456541/45.8128853 secs/batch = 0.2669s, grad.norm=13.29354382
 10956: 8 [  340/ 1327], train_loss/perplexity = 4.61101294/100.5859833 secs/batch = 0.2618s, grad.norm=13.75327110
 10961: 8 [  345/ 1327], train_loss/perplexity = 4.42170000/83.2376709 secs/batch = 0.2616s, grad.norm=13.43349552
 10966: 8 [  350/ 1327], train_loss/perplexity = 4.45416546/85.9843597 secs/batch = 0.2666s, grad.norm=14.55851841
 10971: 8 [  355/ 1327], train_loss/perplexity = 4.39654493/81.1699371 secs/batch = 0.2682s, grad.norm=13.83756351
 10976: 8 [  360/ 1327], train_loss/perplexity = 4.55205631/94.8272018 secs/batch = 0.2668s, grad.norm=14.74244595
 10981: 8 [  365/ 1327], train_loss/perplexity = 4.53449059/93.1760406 secs/batch = 0.2682s, grad.norm=14.09447193
 10986: 8 [  370/ 1327], train_loss/perplexity = 4.57397223/96.9283676 secs/batch = 0.2665s, grad.norm=13.99447441
 10991: 8 [  375/ 1327], train_loss/perplexity = 3.98640847/53.8610992 secs/batch = 0.2662s, grad.norm=14.08228207
 10996: 8 [  380/ 1327], train_loss/perplexity = 4.04940844/57.3635139 secs/batch = 0.2662s, grad.norm=14.23297596
 11001: 8 [  385/ 1327], train_loss/perplexity = 4.28201532/72.3861771 secs/batch = 0.2655s, grad.norm=14.53876686
 11006: 8 [  390/ 1327], train_loss/perplexity = 4.34887218/77.3911285 secs/batch = 0.2665s, grad.norm=13.94235134
 11011: 8 [  395/ 1327], train_loss/perplexity = 4.43282223/84.1686249 secs/batch = 0.2612s, grad.norm=13.96256065
 11016: 8 [  400/ 1327], train_loss/perplexity = 4.38784599/80.4669037 secs/batch = 0.2645s, grad.norm=13.61162567
 11021: 8 [  405/ 1327], train_loss/perplexity = 4.66121387/105.7643890 secs/batch = 0.2671s, grad.norm=14.22352600
 11026: 8 [  410/ 1327], train_loss/perplexity = 4.30397654/73.9934464 secs/batch = 0.2663s, grad.norm=14.03363228
 11031: 8 [  415/ 1327], train_loss/perplexity = 4.21387577/67.6181030 secs/batch = 0.2667s, grad.norm=13.45123196
 11036: 8 [  420/ 1327], train_loss/perplexity = 3.96600151/52.7730942 secs/batch = 0.2661s, grad.norm=14.31898975
 11041: 8 [  425/ 1327], train_loss/perplexity = 4.26692390/71.3019638 secs/batch = 0.2674s, grad.norm=15.26382637
 11046: 8 [  430/ 1327], train_loss/perplexity = 4.54422808/94.0877686 secs/batch = 0.2665s, grad.norm=14.95208168
 11051: 8 [  435/ 1327], train_loss/perplexity = 4.44249439/84.9866638 secs/batch = 0.2660s, grad.norm=14.66422176
 11056: 8 [  440/ 1327], train_loss/perplexity = 4.08872128/59.6635513 secs/batch = 0.2657s, grad.norm=14.23224354
 11061: 8 [  445/ 1327], train_loss/perplexity = 4.41820765/82.9474792 secs/batch = 0.2661s, grad.norm=14.38442326
 11066: 8 [  450/ 1327], train_loss/perplexity = 4.31651831/74.9272995 secs/batch = 0.2673s, grad.norm=14.09362793
 11071: 8 [  455/ 1327], train_loss/perplexity = 4.32421970/75.5065689 secs/batch = 0.2670s, grad.norm=14.65899658
 11076: 8 [  460/ 1327], train_loss/perplexity = 4.23787594/69.2605820 secs/batch = 0.2666s, grad.norm=14.52921772
 11081: 8 [  465/ 1327], train_loss/perplexity = 4.01286268/55.3049660 secs/batch = 0.2651s, grad.norm=15.09915161
 11086: 8 [  470/ 1327], train_loss/perplexity = 4.69404697/109.2946014 secs/batch = 0.2667s, grad.norm=13.99089813
 11091: 8 [  475/ 1327], train_loss/perplexity = 4.23547316/69.0943604 secs/batch = 0.2676s, grad.norm=14.26999378
 11096: 8 [  480/ 1327], train_loss/perplexity = 4.34601116/77.1700287 secs/batch = 0.2650s, grad.norm=14.36165237
 11101: 8 [  485/ 1327], train_loss/perplexity = 4.31639051/74.9177246 secs/batch = 0.2656s, grad.norm=14.42641735
 11106: 8 [  490/ 1327], train_loss/perplexity = 4.18443489/65.6563873 secs/batch = 0.2640s, grad.norm=15.42239952
 11111: 8 [  495/ 1327], train_loss/perplexity = 4.19878197/66.6051559 secs/batch = 0.2607s, grad.norm=14.03049850
 11116: 8 [  500/ 1327], train_loss/perplexity = 4.37109089/79.1299057 secs/batch = 0.2668s, grad.norm=14.37475872
 11121: 8 [  505/ 1327], train_loss/perplexity = 4.46908855/87.2771378 secs/batch = 0.2638s, grad.norm=13.05931377
 11126: 8 [  510/ 1327], train_loss/perplexity = 4.82520008/124.6113968 secs/batch = 0.2658s, grad.norm=13.33850574
 11131: 8 [  515/ 1327], train_loss/perplexity = 4.45240688/85.8332825 secs/batch = 0.2659s, grad.norm=13.82831383
 11136: 8 [  520/ 1327], train_loss/perplexity = 4.67184162/106.8944168 secs/batch = 0.2661s, grad.norm=14.20877743
 11141: 8 [  525/ 1327], train_loss/perplexity = 4.19444275/66.3167648 secs/batch = 0.2663s, grad.norm=14.37857533
 11146: 8 [  530/ 1327], train_loss/perplexity = 4.24923801/70.0520096 secs/batch = 0.2649s, grad.norm=14.61188698
 11151: 8 [  535/ 1327], train_loss/perplexity = 4.34832144/77.3485184 secs/batch = 0.2640s, grad.norm=14.18040276
 11156: 8 [  540/ 1327], train_loss/perplexity = 4.45432568/85.9981384 secs/batch = 0.2676s, grad.norm=13.70472240
 11161: 8 [  545/ 1327], train_loss/perplexity = 4.46236515/86.6923065 secs/batch = 0.2662s, grad.norm=14.31204605
 11166: 8 [  550/ 1327], train_loss/perplexity = 4.39911985/81.3792114 secs/batch = 0.2659s, grad.norm=14.08033466
 11171: 8 [  555/ 1327], train_loss/perplexity = 4.27149773/71.6288376 secs/batch = 0.2620s, grad.norm=13.76163197
 11176: 8 [  560/ 1327], train_loss/perplexity = 4.39414358/80.9752502 secs/batch = 0.2673s, grad.norm=15.37308979
 11181: 8 [  565/ 1327], train_loss/perplexity = 4.25662041/70.5710754 secs/batch = 0.2618s, grad.norm=15.14332867
 11186: 8 [  570/ 1327], train_loss/perplexity = 4.27543354/71.9113083 secs/batch = 0.2665s, grad.norm=15.04665947
 11191: 8 [  575/ 1327], train_loss/perplexity = 4.13154888/62.2743034 secs/batch = 0.2663s, grad.norm=14.61456776
 11196: 8 [  580/ 1327], train_loss/perplexity = 4.49714375/89.7603836 secs/batch = 0.2659s, grad.norm=14.41400814
 11201: 8 [  585/ 1327], train_loss/perplexity = 3.99009800/54.0601883 secs/batch = 0.2670s, grad.norm=14.00337505
 11206: 8 [  590/ 1327], train_loss/perplexity = 4.41860008/82.9800415 secs/batch = 0.2666s, grad.norm=14.19490528
 11211: 8 [  595/ 1327], train_loss/perplexity = 4.38683796/80.3858337 secs/batch = 0.2664s, grad.norm=14.76106453
 11216: 8 [  600/ 1327], train_loss/perplexity = 4.63807917/103.3456497 secs/batch = 0.2666s, grad.norm=13.84375763
 11221: 8 [  605/ 1327], train_loss/perplexity = 4.47227383/87.5555801 secs/batch = 0.2657s, grad.norm=13.82798958
 11226: 8 [  610/ 1327], train_loss/perplexity = 4.60058117/99.5421524 secs/batch = 0.2658s, grad.norm=14.04680538
 11231: 8 [  615/ 1327], train_loss/perplexity = 4.14823055/63.3218575 secs/batch = 0.2648s, grad.norm=13.72651768
 11236: 8 [  620/ 1327], train_loss/perplexity = 4.58514452/98.0173492 secs/batch = 0.2653s, grad.norm=14.06273079
 11241: 8 [  625/ 1327], train_loss/perplexity = 4.53532457/93.2537766 secs/batch = 0.2639s, grad.norm=13.74383163
 11246: 8 [  630/ 1327], train_loss/perplexity = 4.66304255/105.9579773 secs/batch = 0.2665s, grad.norm=14.17400742
 11251: 8 [  635/ 1327], train_loss/perplexity = 4.34097672/76.7824936 secs/batch = 0.2662s, grad.norm=13.94800091
 11256: 8 [  640/ 1327], train_loss/perplexity = 4.34576035/77.1506729 secs/batch = 0.2652s, grad.norm=14.17938709
 11261: 8 [  645/ 1327], train_loss/perplexity = 4.57963133/97.4784470 secs/batch = 0.2668s, grad.norm=15.05631542
 11266: 8 [  650/ 1327], train_loss/perplexity = 4.17006779/64.7198410 secs/batch = 0.2659s, grad.norm=14.54035091
 11271: 8 [  655/ 1327], train_loss/perplexity = 4.30502558/74.0711136 secs/batch = 0.2668s, grad.norm=14.53158283
 11276: 8 [  660/ 1327], train_loss/perplexity = 4.19214678/66.1646805 secs/batch = 0.2652s, grad.norm=14.23294163
 11281: 8 [  665/ 1327], train_loss/perplexity = 4.37270451/79.2576981 secs/batch = 0.2658s, grad.norm=14.38382912
 11286: 8 [  670/ 1327], train_loss/perplexity = 4.32994556/75.9401550 secs/batch = 0.2655s, grad.norm=14.50836277
 11291: 8 [  675/ 1327], train_loss/perplexity = 4.12829828/62.0722046 secs/batch = 0.2668s, grad.norm=14.65135670
 11296: 8 [  680/ 1327], train_loss/perplexity = 4.31708860/74.9700394 secs/batch = 0.2672s, grad.norm=14.95142651
 11301: 8 [  685/ 1327], train_loss/perplexity = 4.07820082/59.0391502 secs/batch = 0.2652s, grad.norm=14.19174099
 11306: 8 [  690/ 1327], train_loss/perplexity = 4.54760742/94.4062653 secs/batch = 0.2661s, grad.norm=13.77504539
 11311: 8 [  695/ 1327], train_loss/perplexity = 4.31318140/74.6776886 secs/batch = 0.2658s, grad.norm=14.07990265
 11316: 8 [  700/ 1327], train_loss/perplexity = 4.57034206/96.5771408 secs/batch = 0.2667s, grad.norm=14.65125465
 11321: 8 [  705/ 1327], train_loss/perplexity = 4.32711983/75.7258682 secs/batch = 0.2653s, grad.norm=13.83878517
 11326: 8 [  710/ 1327], train_loss/perplexity = 4.26853561/71.4169769 secs/batch = 0.2669s, grad.norm=14.85684013
 11331: 8 [  715/ 1327], train_loss/perplexity = 4.14275980/62.9763832 secs/batch = 0.2665s, grad.norm=14.41224289
 11336: 8 [  720/ 1327], train_loss/perplexity = 4.11609030/61.3190346 secs/batch = 0.2662s, grad.norm=14.84419632
 11341: 8 [  725/ 1327], train_loss/perplexity = 4.23231411/68.8764343 secs/batch = 0.2668s, grad.norm=14.98020935
 11346: 8 [  730/ 1327], train_loss/perplexity = 4.30117989/73.7868042 secs/batch = 0.2636s, grad.norm=14.72863007
 11351: 8 [  735/ 1327], train_loss/perplexity = 4.36951494/79.0053024 secs/batch = 0.2650s, grad.norm=14.96843433
 11356: 8 [  740/ 1327], train_loss/perplexity = 3.93823886/51.3281250 secs/batch = 0.2659s, grad.norm=14.05468559
 11361: 8 [  745/ 1327], train_loss/perplexity = 4.33720589/76.4935074 secs/batch = 0.2665s, grad.norm=14.69746971
 11366: 8 [  750/ 1327], train_loss/perplexity = 4.19385719/66.2779465 secs/batch = 0.2642s, grad.norm=14.27641010
 11371: 8 [  755/ 1327], train_loss/perplexity = 4.09476995/60.0255280 secs/batch = 0.2658s, grad.norm=14.40634918
 11376: 8 [  760/ 1327], train_loss/perplexity = 3.99678636/54.4229736 secs/batch = 0.2662s, grad.norm=13.85923862
 11381: 8 [  765/ 1327], train_loss/perplexity = 4.05065441/57.4350319 secs/batch = 0.2664s, grad.norm=14.34697723
 11386: 8 [  770/ 1327], train_loss/perplexity = 4.05992794/57.9701347 secs/batch = 0.2674s, grad.norm=14.25029945
 11391: 8 [  775/ 1327], train_loss/perplexity = 4.12552309/61.9001808 secs/batch = 0.2659s, grad.norm=14.61852741
 11396: 8 [  780/ 1327], train_loss/perplexity = 4.54311228/93.9828491 secs/batch = 0.2670s, grad.norm=14.42384338
 11401: 8 [  785/ 1327], train_loss/perplexity = 4.37575436/79.4997864 secs/batch = 0.2658s, grad.norm=15.48257065
 11406: 8 [  790/ 1327], train_loss/perplexity = 4.12313986/61.7528343 secs/batch = 0.2665s, grad.norm=14.90477467
 11411: 8 [  795/ 1327], train_loss/perplexity = 4.50521755/90.4880295 secs/batch = 0.2640s, grad.norm=14.65579128
 11416: 8 [  800/ 1327], train_loss/perplexity = 4.38425350/80.1783447 secs/batch = 0.2657s, grad.norm=14.92215157
 11421: 8 [  805/ 1327], train_loss/perplexity = 4.68518400/108.3302002 secs/batch = 0.2612s, grad.norm=14.55062962
 11426: 8 [  810/ 1327], train_loss/perplexity = 4.32996130/75.9413452 secs/batch = 0.2672s, grad.norm=13.80521774
 11431: 8 [  815/ 1327], train_loss/perplexity = 4.24531078/69.7774429 secs/batch = 0.2643s, grad.norm=13.94830990
 11436: 8 [  820/ 1327], train_loss/perplexity = 4.04036188/56.8469124 secs/batch = 0.2649s, grad.norm=13.55559444
 11441: 8 [  825/ 1327], train_loss/perplexity = 4.22653055/68.4792328 secs/batch = 0.2667s, grad.norm=14.00169659
 11446: 8 [  830/ 1327], train_loss/perplexity = 3.99804235/54.4913712 secs/batch = 0.2640s, grad.norm=14.36115074
 11451: 8 [  835/ 1327], train_loss/perplexity = 4.26853418/71.4168777 secs/batch = 0.2658s, grad.norm=14.55910110
 11456: 8 [  840/ 1327], train_loss/perplexity = 4.37635374/79.5474548 secs/batch = 0.2660s, grad.norm=14.40816879
 11461: 8 [  845/ 1327], train_loss/perplexity = 4.22674036/68.4936066 secs/batch = 0.2671s, grad.norm=14.60998535
 11466: 8 [  850/ 1327], train_loss/perplexity = 4.23566103/69.1073456 secs/batch = 0.2670s, grad.norm=14.19826508
 11471: 8 [  855/ 1327], train_loss/perplexity = 4.33524084/76.3433456 secs/batch = 0.2661s, grad.norm=14.64748764
 11476: 8 [  860/ 1327], train_loss/perplexity = 4.00716496/54.9907494 secs/batch = 0.2641s, grad.norm=13.91552544
 11481: 8 [  865/ 1327], train_loss/perplexity = 4.49158525/89.2628403 secs/batch = 0.2670s, grad.norm=14.19940281
 11486: 8 [  870/ 1327], train_loss/perplexity = 4.35946894/78.2155838 secs/batch = 0.2643s, grad.norm=14.75723362
 11491: 8 [  875/ 1327], train_loss/perplexity = 3.90207005/49.5048218 secs/batch = 0.2668s, grad.norm=14.04968262
 11496: 8 [  880/ 1327], train_loss/perplexity = 4.12044191/61.5864525 secs/batch = 0.2667s, grad.norm=13.98390865
 11501: 8 [  885/ 1327], train_loss/perplexity = 4.27522993/71.8966675 secs/batch = 0.2670s, grad.norm=14.04623127
 11506: 8 [  890/ 1327], train_loss/perplexity = 4.45172882/85.7751083 secs/batch = 0.2669s, grad.norm=14.05452728
 11511: 8 [  895/ 1327], train_loss/perplexity = 4.46522236/86.9403610 secs/batch = 0.2662s, grad.norm=13.41530323
 11516: 8 [  900/ 1327], train_loss/perplexity = 4.18924904/65.9732285 secs/batch = 0.2662s, grad.norm=13.55639458
 11521: 8 [  905/ 1327], train_loss/perplexity = 4.16718960/64.5338287 secs/batch = 0.2662s, grad.norm=13.83235645
 11526: 8 [  910/ 1327], train_loss/perplexity = 4.20468760/66.9996643 secs/batch = 0.2662s, grad.norm=13.60523987
 11531: 8 [  915/ 1327], train_loss/perplexity = 4.45414448/85.9825592 secs/batch = 0.2645s, grad.norm=13.59081650
 11536: 8 [  920/ 1327], train_loss/perplexity = 4.60979128/100.4631805 secs/batch = 0.2646s, grad.norm=14.47181129
 11541: 8 [  925/ 1327], train_loss/perplexity = 4.36598206/78.7266769 secs/batch = 0.2672s, grad.norm=13.87936020
 11546: 8 [  930/ 1327], train_loss/perplexity = 4.33967113/76.6823196 secs/batch = 0.2651s, grad.norm=13.69466972
 11551: 8 [  935/ 1327], train_loss/perplexity = 4.45878649/86.3826218 secs/batch = 0.2661s, grad.norm=13.62193298
 11556: 8 [  940/ 1327], train_loss/perplexity = 4.45704794/86.2325668 secs/batch = 0.2660s, grad.norm=14.05821896
 11561: 8 [  945/ 1327], train_loss/perplexity = 4.59690142/99.1765289 secs/batch = 0.2658s, grad.norm=14.01416874
 11566: 8 [  950/ 1327], train_loss/perplexity = 4.40845346/82.1423264 secs/batch = 0.2659s, grad.norm=14.51854515
 11571: 8 [  955/ 1327], train_loss/perplexity = 4.30692387/74.2118530 secs/batch = 0.2665s, grad.norm=14.18232632
 11576: 8 [  960/ 1327], train_loss/perplexity = 4.61542511/101.0307693 secs/batch = 0.2671s, grad.norm=13.94775581
 11581: 8 [  965/ 1327], train_loss/perplexity = 4.42861128/83.8149414 secs/batch = 0.2665s, grad.norm=13.94542217
 11586: 8 [  970/ 1327], train_loss/perplexity = 4.62765884/102.2743454 secs/batch = 0.2660s, grad.norm=14.05154610
 11591: 8 [  975/ 1327], train_loss/perplexity = 4.36167097/78.3880081 secs/batch = 0.2645s, grad.norm=15.03344727
 11596: 8 [  980/ 1327], train_loss/perplexity = 4.18589354/65.7522278 secs/batch = 0.2664s, grad.norm=13.83704281
 11601: 8 [  985/ 1327], train_loss/perplexity = 4.30807257/74.2971497 secs/batch = 0.2592s, grad.norm=14.39058590
 11606: 8 [  990/ 1327], train_loss/perplexity = 4.56259537/95.8318787 secs/batch = 0.2661s, grad.norm=14.51339817
 11611: 8 [  995/ 1327], train_loss/perplexity = 4.50778961/90.7210693 secs/batch = 0.2656s, grad.norm=13.83465481
 11616: 8 [ 1000/ 1327], train_loss/perplexity = 4.01358700/55.3450394 secs/batch = 0.2641s, grad.norm=13.79373646
 11621: 8 [ 1005/ 1327], train_loss/perplexity = 4.48107338/88.3294296 secs/batch = 0.2654s, grad.norm=14.19779873
 11626: 8 [ 1010/ 1327], train_loss/perplexity = 4.04848385/57.3105011 secs/batch = 0.2661s, grad.norm=13.67010403
 11631: 8 [ 1015/ 1327], train_loss/perplexity = 4.55420303/95.0309906 secs/batch = 0.2651s, grad.norm=14.04033566
 11636: 8 [ 1020/ 1327], train_loss/perplexity = 4.68359661/108.1583786 secs/batch = 0.2654s, grad.norm=13.86290073
 11641: 8 [ 1025/ 1327], train_loss/perplexity = 4.56977654/96.5225372 secs/batch = 0.2656s, grad.norm=14.07081699
 11646: 8 [ 1030/ 1327], train_loss/perplexity = 4.30249119/73.8836212 secs/batch = 0.2660s, grad.norm=13.38478756
 11651: 8 [ 1035/ 1327], train_loss/perplexity = 4.29387093/73.2494659 secs/batch = 0.2663s, grad.norm=13.54836750
 11656: 8 [ 1040/ 1327], train_loss/perplexity = 4.47740030/88.0055847 secs/batch = 0.2658s, grad.norm=14.42867756
 11661: 8 [ 1045/ 1327], train_loss/perplexity = 4.02948475/56.2319298 secs/batch = 0.2660s, grad.norm=13.82308006
 11666: 8 [ 1050/ 1327], train_loss/perplexity = 4.10299444/60.5212440 secs/batch = 0.2658s, grad.norm=14.44037724
 11671: 8 [ 1055/ 1327], train_loss/perplexity = 4.21457100/67.6651306 secs/batch = 0.2676s, grad.norm=14.70131111
 11676: 8 [ 1060/ 1327], train_loss/perplexity = 3.95216846/52.0481071 secs/batch = 0.2660s, grad.norm=15.13835907
 11681: 8 [ 1065/ 1327], train_loss/perplexity = 4.02874660/56.1904373 secs/batch = 0.2658s, grad.norm=14.64649487
 11686: 8 [ 1070/ 1327], train_loss/perplexity = 4.36104965/78.3393173 secs/batch = 0.2654s, grad.norm=14.77923965
 11691: 8 [ 1075/ 1327], train_loss/perplexity = 4.17600155/65.1050110 secs/batch = 0.2665s, grad.norm=14.40724945
 11696: 8 [ 1080/ 1327], train_loss/perplexity = 4.06480646/58.2536316 secs/batch = 0.2658s, grad.norm=14.05563068
 11701: 8 [ 1085/ 1327], train_loss/perplexity = 3.96354055/52.6433830 secs/batch = 0.2655s, grad.norm=14.50747776
 11706: 8 [ 1090/ 1327], train_loss/perplexity = 4.09045601/59.7671394 secs/batch = 0.2657s, grad.norm=14.65947342
 11711: 8 [ 1095/ 1327], train_loss/perplexity = 4.28356409/72.4983673 secs/batch = 0.2643s, grad.norm=14.67932129
 11716: 8 [ 1100/ 1327], train_loss/perplexity = 4.01344061/55.3369370 secs/batch = 0.2656s, grad.norm=15.91531658
 11721: 8 [ 1105/ 1327], train_loss/perplexity = 3.97724724/53.3699188 secs/batch = 0.2663s, grad.norm=14.47839546
 11726: 8 [ 1110/ 1327], train_loss/perplexity = 4.35223818/77.6520691 secs/batch = 0.2659s, grad.norm=14.75897694
 11731: 8 [ 1115/ 1327], train_loss/perplexity = 4.02393770/55.9208717 secs/batch = 0.2658s, grad.norm=13.91502666
 11736: 8 [ 1120/ 1327], train_loss/perplexity = 4.33284187/76.1604156 secs/batch = 0.2662s, grad.norm=14.07992935
 11741: 8 [ 1125/ 1327], train_loss/perplexity = 4.50792789/90.7336121 secs/batch = 0.2663s, grad.norm=14.69411755
 11746: 8 [ 1130/ 1327], train_loss/perplexity = 4.19960022/66.6596756 secs/batch = 0.2612s, grad.norm=14.34547997
 11751: 8 [ 1135/ 1327], train_loss/perplexity = 4.21700144/67.8297882 secs/batch = 0.2676s, grad.norm=14.31476498
 11756: 8 [ 1140/ 1327], train_loss/perplexity = 4.43630505/84.4622803 secs/batch = 0.2664s, grad.norm=15.09973049
 11761: 8 [ 1145/ 1327], train_loss/perplexity = 4.22343445/68.2675400 secs/batch = 0.2661s, grad.norm=14.28540802
 11766: 8 [ 1150/ 1327], train_loss/perplexity = 4.20061970/66.7276688 secs/batch = 0.2669s, grad.norm=14.60715103
 11771: 8 [ 1155/ 1327], train_loss/perplexity = 4.30834675/74.3175201 secs/batch = 0.2641s, grad.norm=14.70894623
 11776: 8 [ 1160/ 1327], train_loss/perplexity = 4.28638506/72.7031784 secs/batch = 0.2658s, grad.norm=14.32675838
 11781: 8 [ 1165/ 1327], train_loss/perplexity = 4.26795197/71.3753052 secs/batch = 0.2644s, grad.norm=14.24097824
 11786: 8 [ 1170/ 1327], train_loss/perplexity = 4.24275589/69.5993958 secs/batch = 0.2655s, grad.norm=14.80553818
 11791: 8 [ 1175/ 1327], train_loss/perplexity = 3.94820333/51.8421402 secs/batch = 0.2661s, grad.norm=14.71621799
 11796: 8 [ 1180/ 1327], train_loss/perplexity = 4.04795504/57.2802010 secs/batch = 0.2665s, grad.norm=14.55492401
 11801: 8 [ 1185/ 1327], train_loss/perplexity = 4.18968964/66.0023041 secs/batch = 0.2663s, grad.norm=14.83864021
 11806: 8 [ 1190/ 1327], train_loss/perplexity = 4.30486441/74.0591736 secs/batch = 0.2636s, grad.norm=14.77188778
 11811: 8 [ 1195/ 1327], train_loss/perplexity = 4.09820318/60.2319641 secs/batch = 0.2676s, grad.norm=14.18255234
 11816: 8 [ 1200/ 1327], train_loss/perplexity = 4.04534149/57.1306915 secs/batch = 0.2648s, grad.norm=14.23562622
 11821: 8 [ 1205/ 1327], train_loss/perplexity = 4.03280640/56.4190216 secs/batch = 0.2636s, grad.norm=15.02150059
 11826: 8 [ 1210/ 1327], train_loss/perplexity = 3.65804052/38.7852707 secs/batch = 0.2653s, grad.norm=14.73008060
 11831: 8 [ 1215/ 1327], train_loss/perplexity = 3.96044540/52.4806976 secs/batch = 0.2658s, grad.norm=14.37682915
 11836: 8 [ 1220/ 1327], train_loss/perplexity = 4.08952522/59.7115364 secs/batch = 0.2649s, grad.norm=14.94780827
 11841: 8 [ 1225/ 1327], train_loss/perplexity = 3.82424021/45.7979889 secs/batch = 0.2652s, grad.norm=15.54917812
 11846: 8 [ 1230/ 1327], train_loss/perplexity = 4.11329842/61.1480789 secs/batch = 0.2664s, grad.norm=14.40710735
 11851: 8 [ 1235/ 1327], train_loss/perplexity = 4.05103207/57.4567261 secs/batch = 0.2675s, grad.norm=14.62909794
 11856: 8 [ 1240/ 1327], train_loss/perplexity = 4.26817036/71.3908997 secs/batch = 0.2661s, grad.norm=15.03063011
 11861: 8 [ 1245/ 1327], train_loss/perplexity = 4.13763762/62.6546326 secs/batch = 0.2639s, grad.norm=14.76379299
 11866: 8 [ 1250/ 1327], train_loss/perplexity = 4.31639385/74.9179764 secs/batch = 0.2667s, grad.norm=14.05313492
 11871: 8 [ 1255/ 1327], train_loss/perplexity = 4.29891539/73.6199036 secs/batch = 0.2651s, grad.norm=14.03857803
 11876: 8 [ 1260/ 1327], train_loss/perplexity = 4.15593433/63.8115578 secs/batch = 0.2658s, grad.norm=15.32235050
 11881: 8 [ 1265/ 1327], train_loss/perplexity = 4.37972307/79.8159256 secs/batch = 0.2666s, grad.norm=14.99810314
 11886: 8 [ 1270/ 1327], train_loss/perplexity = 4.04093552/56.8795280 secs/batch = 0.2652s, grad.norm=15.31029797
 11891: 8 [ 1275/ 1327], train_loss/perplexity = 4.29956055/73.6674118 secs/batch = 0.2660s, grad.norm=15.29141426
 11896: 8 [ 1280/ 1327], train_loss/perplexity = 4.10900450/60.8860741 secs/batch = 0.2662s, grad.norm=14.68313503
 11901: 8 [ 1285/ 1327], train_loss/perplexity = 3.99065185/54.0901375 secs/batch = 0.2665s, grad.norm=14.38351154
 11906: 8 [ 1290/ 1327], train_loss/perplexity = 4.23390484/68.9860840 secs/batch = 0.2666s, grad.norm=14.11374950
 11911: 8 [ 1295/ 1327], train_loss/perplexity = 4.26766348/71.3547211 secs/batch = 0.2665s, grad.norm=14.34462357
 11916: 8 [ 1300/ 1327], train_loss/perplexity = 4.34552431/77.1324692 secs/batch = 0.2617s, grad.norm=13.90814590
 11921: 8 [ 1305/ 1327], train_loss/perplexity = 4.47549152/87.8377609 secs/batch = 0.2674s, grad.norm=14.97960949
 11926: 8 [ 1310/ 1327], train_loss/perplexity = 4.75618410/116.3012848 secs/batch = 0.2657s, grad.norm=14.47809696
 11931: 8 [ 1315/ 1327], train_loss/perplexity = 4.51945162/91.7852478 secs/batch = 0.2659s, grad.norm=14.59855556
 11936: 8 [ 1320/ 1327], train_loss/perplexity = 4.48165417/88.3807449 secs/batch = 0.2654s, grad.norm=14.56446362
 11941: 8 [ 1325/ 1327], train_loss/perplexity = 4.41410780/82.6081085 secs/batch = 0.2655s, grad.norm=14.68130493
Epoch training time: 352.96685004234314
	> validation loss = 4.70807791, perplexity = 110.83891296
	> validation loss = 4.62643290, perplexity = 102.14904022
	> validation loss = 4.61616564, perplexity = 101.10561371
	> validation loss = 4.59786510, perplexity = 99.27215576
	> validation loss = 4.75376940, perplexity = 116.02079010
	> validation loss = 4.71623945, perplexity = 111.74723053
	> validation loss = 4.66646767, perplexity = 106.32151794
	> validation loss = 4.47661400, perplexity = 87.93641663
	> validation loss = 4.27504110, perplexity = 71.88309479
	> validation loss = 4.41741705, perplexity = 82.88192749
	> validation loss = 4.55144310, perplexity = 94.76907349
	> validation loss = 4.58683395, perplexity = 98.18308258
	> validation loss = 4.53729010, perplexity = 93.43724823
	> validation loss = 4.32339382, perplexity = 75.44423676
	> validation loss = 4.23079491, perplexity = 68.77188110
	> validation loss = 4.27212620, perplexity = 71.67386627
	> validation loss = 4.68540668, perplexity = 108.35433197
	> validation loss = 4.27188969, perplexity = 71.65691376
	> validation loss = 4.68677950, perplexity = 108.50318146
	> validation loss = 4.54060745, perplexity = 93.74772644
	> validation loss = 4.34606123, perplexity = 77.17389679
at the end of epoch: 8
train loss = 4.37447130, perplexity = 79.39785045
validation loss = 4.52703086, perplexity = 92.48355668
Saved model cv/epoch008_4.5270.model
 11948: 9 [    5/ 1327], train_loss/perplexity = 4.54590988/94.2461395 secs/batch = 0.2623s, grad.norm=14.47010326
 11953: 9 [   10/ 1327], train_loss/perplexity = 4.09220028/59.8714790 secs/batch = 0.2652s, grad.norm=13.99724865
 11958: 9 [   15/ 1327], train_loss/perplexity = 4.41073036/82.3295746 secs/batch = 0.2660s, grad.norm=13.72081661
 11963: 9 [   20/ 1327], train_loss/perplexity = 4.61379910/100.8666229 secs/batch = 0.2657s, grad.norm=14.01651955
 11968: 9 [   25/ 1327], train_loss/perplexity = 4.48652220/88.8120346 secs/batch = 0.2662s, grad.norm=14.76329041
 11973: 9 [   30/ 1327], train_loss/perplexity = 4.51455545/91.3369522 secs/batch = 0.2654s, grad.norm=14.35557365
 11978: 9 [   35/ 1327], train_loss/perplexity = 4.29962635/73.6722641 secs/batch = 0.2662s, grad.norm=13.64706230
 11983: 9 [   40/ 1327], train_loss/perplexity = 4.21384144/67.6157837 secs/batch = 0.2657s, grad.norm=14.07911873
 11988: 9 [   45/ 1327], train_loss/perplexity = 4.00955439/55.1223030 secs/batch = 0.2663s, grad.norm=13.33526993
 11993: 9 [   50/ 1327], train_loss/perplexity = 4.26982641/71.5092239 secs/batch = 0.2658s, grad.norm=14.00624275
 11998: 9 [   55/ 1327], train_loss/perplexity = 4.19830322/66.5732727 secs/batch = 0.2663s, grad.norm=14.74999714
 12003: 9 [   60/ 1327], train_loss/perplexity = 4.51727009/91.5852356 secs/batch = 0.2632s, grad.norm=14.72649765
 12008: 9 [   65/ 1327], train_loss/perplexity = 4.10307360/60.5260353 secs/batch = 0.2666s, grad.norm=14.04535675
 12013: 9 [   70/ 1327], train_loss/perplexity = 3.88940310/48.8817024 secs/batch = 0.2656s, grad.norm=14.20376778
 12018: 9 [   75/ 1327], train_loss/perplexity = 3.77840829/43.7463531 secs/batch = 0.2670s, grad.norm=14.08884430
 12023: 9 [   80/ 1327], train_loss/perplexity = 4.20274019/66.8693161 secs/batch = 0.2667s, grad.norm=14.41482067
 12028: 9 [   85/ 1327], train_loss/perplexity = 4.24196148/69.5441284 secs/batch = 0.2656s, grad.norm=14.91051006
 12033: 9 [   90/ 1327], train_loss/perplexity = 4.26211166/70.9596710 secs/batch = 0.2652s, grad.norm=14.64307022
 12038: 9 [   95/ 1327], train_loss/perplexity = 4.20908403/67.2948685 secs/batch = 0.2654s, grad.norm=14.58971214
 12043: 9 [  100/ 1327], train_loss/perplexity = 4.48131800/88.3510437 secs/batch = 0.2659s, grad.norm=14.56190491
 12048: 9 [  105/ 1327], train_loss/perplexity = 4.23885393/69.3283539 secs/batch = 0.2664s, grad.norm=15.03062439
 12053: 9 [  110/ 1327], train_loss/perplexity = 4.14562607/63.1571503 secs/batch = 0.2666s, grad.norm=14.35140896
 12058: 9 [  115/ 1327], train_loss/perplexity = 4.03849983/56.7411575 secs/batch = 0.2647s, grad.norm=14.57770348
 12063: 9 [  120/ 1327], train_loss/perplexity = 4.12024546/61.5743561 secs/batch = 0.2660s, grad.norm=14.86435699
 12068: 9 [  125/ 1327], train_loss/perplexity = 4.27470779/71.8591385 secs/batch = 0.2655s, grad.norm=15.16965294
 12073: 9 [  130/ 1327], train_loss/perplexity = 4.21461916/67.6683884 secs/batch = 0.2654s, grad.norm=15.25479412
 12078: 9 [  135/ 1327], train_loss/perplexity = 4.17606449/65.1091080 secs/batch = 0.2658s, grad.norm=14.34494114
 12083: 9 [  140/ 1327], train_loss/perplexity = 4.43858290/84.6548920 secs/batch = 0.2667s, grad.norm=14.77054310
 12088: 9 [  145/ 1327], train_loss/perplexity = 4.32755756/75.7590256 secs/batch = 0.2653s, grad.norm=15.21152878
 12093: 9 [  150/ 1327], train_loss/perplexity = 4.37512493/79.4497681 secs/batch = 0.2652s, grad.norm=15.11145115
 12098: 9 [  155/ 1327], train_loss/perplexity = 4.65093327/104.6826401 secs/batch = 0.2660s, grad.norm=14.44556808
 12103: 9 [  160/ 1327], train_loss/perplexity = 4.33061981/75.9913712 secs/batch = 0.2659s, grad.norm=13.52820873
 12108: 9 [  165/ 1327], train_loss/perplexity = 4.42471600/83.4890900 secs/batch = 0.2667s, grad.norm=14.09461975
 12113: 9 [  170/ 1327], train_loss/perplexity = 4.29237843/73.1402206 secs/batch = 0.2659s, grad.norm=14.24125481
 12118: 9 [  175/ 1327], train_loss/perplexity = 4.53765011/93.4708939 secs/batch = 0.2664s, grad.norm=14.61718655
 12123: 9 [  180/ 1327], train_loss/perplexity = 4.44964504/85.5965576 secs/batch = 0.2658s, grad.norm=14.88734627
 12128: 9 [  185/ 1327], train_loss/perplexity = 4.64682913/104.2538834 secs/batch = 0.2662s, grad.norm=14.59723377
 12133: 9 [  190/ 1327], train_loss/perplexity = 4.19946194/66.6504593 secs/batch = 0.2615s, grad.norm=13.57888508
 12138: 9 [  195/ 1327], train_loss/perplexity = 4.51320791/91.2139587 secs/batch = 0.2622s, grad.norm=13.96734333
 12143: 9 [  200/ 1327], train_loss/perplexity = 4.29477167/73.3154755 secs/batch = 0.2631s, grad.norm=14.46118736
 12148: 9 [  205/ 1327], train_loss/perplexity = 4.56895590/96.4433594 secs/batch = 0.2651s, grad.norm=14.36030674
 12153: 9 [  210/ 1327], train_loss/perplexity = 4.43449020/84.3091354 secs/batch = 0.2646s, grad.norm=13.74939919
 12158: 9 [  215/ 1327], train_loss/perplexity = 4.56068802/95.6492691 secs/batch = 0.2654s, grad.norm=14.09213638
 12163: 9 [  220/ 1327], train_loss/perplexity = 4.44603205/85.2878571 secs/batch = 0.2661s, grad.norm=13.93197727
 12168: 9 [  225/ 1327], train_loss/perplexity = 4.64509344/104.0730896 secs/batch = 0.2645s, grad.norm=14.24468994
 12173: 9 [  230/ 1327], train_loss/perplexity = 4.47595501/87.8784866 secs/batch = 0.2655s, grad.norm=14.72073078
 12178: 9 [  235/ 1327], train_loss/perplexity = 4.34667015/77.2209015 secs/batch = 0.2665s, grad.norm=14.63017273
 12183: 9 [  240/ 1327], train_loss/perplexity = 4.13566494/62.5311584 secs/batch = 0.2659s, grad.norm=14.92279053
 12188: 9 [  245/ 1327], train_loss/perplexity = 4.48055220/88.2834091 secs/batch = 0.2656s, grad.norm=14.51778698
 12193: 9 [  250/ 1327], train_loss/perplexity = 4.22765207/68.5560760 secs/batch = 0.2667s, grad.norm=13.87399292
 12198: 9 [  255/ 1327], train_loss/perplexity = 4.22185516/68.1598129 secs/batch = 0.2664s, grad.norm=14.36380196
 12203: 9 [  260/ 1327], train_loss/perplexity = 4.52504778/92.3003387 secs/batch = 0.2662s, grad.norm=15.59817886
 12208: 9 [  265/ 1327], train_loss/perplexity = 4.61650848/101.1402817 secs/batch = 0.2664s, grad.norm=14.00772190
 12213: 9 [  270/ 1327], train_loss/perplexity = 4.65457964/105.0650482 secs/batch = 0.2605s, grad.norm=14.13951778
 12218: 9 [  275/ 1327], train_loss/perplexity = 4.68144131/107.9255142 secs/batch = 0.2666s, grad.norm=14.21658421
 12223: 9 [  280/ 1327], train_loss/perplexity = 4.44244289/84.9822922 secs/batch = 0.2662s, grad.norm=13.97691059
 12228: 9 [  285/ 1327], train_loss/perplexity = 4.75253391/115.8775330 secs/batch = 0.2662s, grad.norm=14.47475624
 12233: 9 [  290/ 1327], train_loss/perplexity = 4.43100786/84.0160522 secs/batch = 0.2660s, grad.norm=14.33783722
 12238: 9 [  295/ 1327], train_loss/perplexity = 4.21513844/67.7035370 secs/batch = 0.2658s, grad.norm=14.47508907
 12243: 9 [  300/ 1327], train_loss/perplexity = 3.74264359/42.2094269 secs/batch = 0.2664s, grad.norm=14.01375580
 12248: 9 [  305/ 1327], train_loss/perplexity = 4.34144115/76.8181686 secs/batch = 0.2644s, grad.norm=14.42448616
 12253: 9 [  310/ 1327], train_loss/perplexity = 4.30697203/74.2154236 secs/batch = 0.2662s, grad.norm=14.24660683
 12258: 9 [  315/ 1327], train_loss/perplexity = 3.81085825/45.1892052 secs/batch = 0.2655s, grad.norm=14.16656685
 12263: 9 [  320/ 1327], train_loss/perplexity = 3.79806447/44.6147461 secs/batch = 0.2656s, grad.norm=15.08154106
 12268: 9 [  325/ 1327], train_loss/perplexity = 3.82887483/46.0107384 secs/batch = 0.2620s, grad.norm=13.68036556
 12273: 9 [  330/ 1327], train_loss/perplexity = 4.41611481/82.7740631 secs/batch = 0.2653s, grad.norm=14.70542240
 12278: 9 [  335/ 1327], train_loss/perplexity = 3.83750677/46.4096184 secs/batch = 0.2655s, grad.norm=14.18137360
 12283: 9 [  340/ 1327], train_loss/perplexity = 4.57996368/97.5108566 secs/batch = 0.2649s, grad.norm=14.29538918
 12288: 9 [  345/ 1327], train_loss/perplexity = 4.36215115/78.4256592 secs/batch = 0.2667s, grad.norm=14.13338661
 12293: 9 [  350/ 1327], train_loss/perplexity = 4.32907009/75.8736954 secs/batch = 0.2640s, grad.norm=14.90985966
 12298: 9 [  355/ 1327], train_loss/perplexity = 4.39123249/80.7398682 secs/batch = 0.2654s, grad.norm=14.63032818
 12303: 9 [  360/ 1327], train_loss/perplexity = 4.53558016/93.2776184 secs/batch = 0.2659s, grad.norm=16.40957832
 12308: 9 [  365/ 1327], train_loss/perplexity = 4.39823723/81.3074188 secs/batch = 0.2656s, grad.norm=14.57241535
 12313: 9 [  370/ 1327], train_loss/perplexity = 4.52214241/92.0325546 secs/batch = 0.2629s, grad.norm=14.58888245
 12318: 9 [  375/ 1327], train_loss/perplexity = 3.91617918/50.2082405 secs/batch = 0.2653s, grad.norm=14.31014252
 12323: 9 [  380/ 1327], train_loss/perplexity = 3.99659395/54.4125023 secs/batch = 0.2658s, grad.norm=14.80983162
 12328: 9 [  385/ 1327], train_loss/perplexity = 4.19085693/66.0793915 secs/batch = 0.2663s, grad.norm=15.20040417
 12333: 9 [  390/ 1327], train_loss/perplexity = 4.37710905/79.6075592 secs/batch = 0.2663s, grad.norm=14.91580009
 12338: 9 [  395/ 1327], train_loss/perplexity = 4.33291245/76.1657944 secs/batch = 0.2662s, grad.norm=14.48633099
 12343: 9 [  400/ 1327], train_loss/perplexity = 4.26426888/71.1129074 secs/batch = 0.2656s, grad.norm=14.24050903
 12348: 9 [  405/ 1327], train_loss/perplexity = 4.61710024/101.2001495 secs/batch = 0.2659s, grad.norm=14.88236713
 12353: 9 [  410/ 1327], train_loss/perplexity = 4.25637007/70.5534134 secs/batch = 0.2658s, grad.norm=14.48281765
 12358: 9 [  415/ 1327], train_loss/perplexity = 4.19974089/66.6690521 secs/batch = 0.2637s, grad.norm=13.94011211
 12363: 9 [  420/ 1327], train_loss/perplexity = 3.89800310/49.3038940 secs/batch = 0.2658s, grad.norm=14.84774017
 12368: 9 [  425/ 1327], train_loss/perplexity = 4.24997330/70.1035385 secs/batch = 0.2674s, grad.norm=15.35741425
 12373: 9 [  430/ 1327], train_loss/perplexity = 4.45689344/86.2192459 secs/batch = 0.2653s, grad.norm=15.37696743
 12378: 9 [  435/ 1327], train_loss/perplexity = 4.48950768/89.0775833 secs/batch = 0.2650s, grad.norm=15.39022350
 12383: 9 [  440/ 1327], train_loss/perplexity = 4.02790165/56.1429787 secs/batch = 0.2642s, grad.norm=14.95745850
 12388: 9 [  445/ 1327], train_loss/perplexity = 4.37938452/79.7889099 secs/batch = 0.2658s, grad.norm=15.17196560
 12393: 9 [  450/ 1327], train_loss/perplexity = 4.26975441/71.5040741 secs/batch = 0.2660s, grad.norm=14.64430904
 12398: 9 [  455/ 1327], train_loss/perplexity = 4.25387621/70.3776855 secs/batch = 0.2648s, grad.norm=14.62231445
 12403: 9 [  460/ 1327], train_loss/perplexity = 4.24568272/69.8033981 secs/batch = 0.2661s, grad.norm=15.21472549
 12408: 9 [  465/ 1327], train_loss/perplexity = 3.97409010/53.2016869 secs/batch = 0.2664s, grad.norm=16.47414780
 12413: 9 [  470/ 1327], train_loss/perplexity = 4.62861538/102.3722229 secs/batch = 0.2664s, grad.norm=14.36793327
 12418: 9 [  475/ 1327], train_loss/perplexity = 4.18099403/65.4308624 secs/batch = 0.2608s, grad.norm=14.67896557
 12423: 9 [  480/ 1327], train_loss/perplexity = 4.32326508/75.4345245 secs/batch = 0.2666s, grad.norm=14.99350071
 12428: 9 [  485/ 1327], train_loss/perplexity = 4.16397285/64.3265762 secs/batch = 0.2658s, grad.norm=14.67314911
 12433: 9 [  490/ 1327], train_loss/perplexity = 4.12784672/62.0441818 secs/batch = 0.2657s, grad.norm=15.56718063
 12438: 9 [  495/ 1327], train_loss/perplexity = 4.18317842/65.5739441 secs/batch = 0.2615s, grad.norm=14.57065296
 12443: 9 [  500/ 1327], train_loss/perplexity = 4.32698059/75.7153244 secs/batch = 0.2668s, grad.norm=14.96675301
 12448: 9 [  505/ 1327], train_loss/perplexity = 4.44512653/85.2106628 secs/batch = 0.2650s, grad.norm=13.85992241
 12453: 9 [  510/ 1327], train_loss/perplexity = 4.78836870/120.1052780 secs/batch = 0.2647s, grad.norm=13.49278164
 12458: 9 [  515/ 1327], train_loss/perplexity = 4.43482876/84.3376846 secs/batch = 0.2658s, grad.norm=13.76671124
 12463: 9 [  520/ 1327], train_loss/perplexity = 4.54751301/94.3973541 secs/batch = 0.2661s, grad.norm=14.60693359
 12468: 9 [  525/ 1327], train_loss/perplexity = 4.19483852/66.3430176 secs/batch = 0.2658s, grad.norm=14.85262871
 12473: 9 [  530/ 1327], train_loss/perplexity = 4.19710016/66.4932327 secs/batch = 0.2652s, grad.norm=15.42897320
 12478: 9 [  535/ 1327], train_loss/perplexity = 4.31717253/74.9763336 secs/batch = 0.2639s, grad.norm=14.81811619
 12483: 9 [  540/ 1327], train_loss/perplexity = 4.41174412/82.4130783 secs/batch = 0.2668s, grad.norm=14.40726566
 12488: 9 [  545/ 1327], train_loss/perplexity = 4.44247341/84.9848862 secs/batch = 0.2659s, grad.norm=15.06918335
 12493: 9 [  550/ 1327], train_loss/perplexity = 4.38546848/80.2758255 secs/batch = 0.2634s, grad.norm=14.51628685
 12498: 9 [  555/ 1327], train_loss/perplexity = 4.16945648/64.6802902 secs/batch = 0.2660s, grad.norm=14.42575741
 12503: 9 [  560/ 1327], train_loss/perplexity = 4.30349398/73.9577484 secs/batch = 0.2654s, grad.norm=15.67294121
 12508: 9 [  565/ 1327], train_loss/perplexity = 4.19706392/66.4908218 secs/batch = 0.2658s, grad.norm=15.10351944
 12513: 9 [  570/ 1327], train_loss/perplexity = 4.20761538/67.1961136 secs/batch = 0.2644s, grad.norm=15.93857288
 12518: 9 [  575/ 1327], train_loss/perplexity = 4.00960636/55.1251678 secs/batch = 0.2640s, grad.norm=15.02729511
 12523: 9 [  580/ 1327], train_loss/perplexity = 4.43792725/84.5994034 secs/batch = 0.2670s, grad.norm=14.91592884
 12528: 9 [  585/ 1327], train_loss/perplexity = 3.95972562/52.4429359 secs/batch = 0.2679s, grad.norm=14.13596058
 12533: 9 [  590/ 1327], train_loss/perplexity = 4.35605097/77.9487076 secs/batch = 0.2646s, grad.norm=14.54574585
 12538: 9 [  595/ 1327], train_loss/perplexity = 4.26513672/71.1746521 secs/batch = 0.2647s, grad.norm=14.87112331
 12543: 9 [  600/ 1327], train_loss/perplexity = 4.55372620/94.9856873 secs/batch = 0.2660s, grad.norm=14.04004192
 12548: 9 [  605/ 1327], train_loss/perplexity = 4.44598532/85.2838669 secs/batch = 0.2629s, grad.norm=14.14196396
 12553: 9 [  610/ 1327], train_loss/perplexity = 4.64298010/103.8533783 secs/batch = 0.2656s, grad.norm=15.33903885
 12558: 9 [  615/ 1327], train_loss/perplexity = 4.15562677/63.7919350 secs/batch = 0.2667s, grad.norm=14.65125942
 12563: 9 [  620/ 1327], train_loss/perplexity = 4.50247049/90.2397919 secs/batch = 0.2651s, grad.norm=14.59939957
 12568: 9 [  625/ 1327], train_loss/perplexity = 4.48070240/88.2966690 secs/batch = 0.2613s, grad.norm=14.32284451
 12573: 9 [  630/ 1327], train_loss/perplexity = 4.59919310/99.4040756 secs/batch = 0.2660s, grad.norm=14.41759014
 12578: 9 [  635/ 1327], train_loss/perplexity = 4.27030230/71.5432587 secs/batch = 0.2663s, grad.norm=14.78880596
 12583: 9 [  640/ 1327], train_loss/perplexity = 4.27745342/72.0567093 secs/batch = 0.2658s, grad.norm=14.80797005
 12588: 9 [  645/ 1327], train_loss/perplexity = 4.56723738/96.2777634 secs/batch = 0.2651s, grad.norm=15.52208138
 12593: 9 [  650/ 1327], train_loss/perplexity = 4.03882074/56.7593689 secs/batch = 0.2660s, grad.norm=14.90664387
 12598: 9 [  655/ 1327], train_loss/perplexity = 4.25278759/70.3011093 secs/batch = 0.2665s, grad.norm=14.75585938
 12603: 9 [  660/ 1327], train_loss/perplexity = 4.13097239/62.2384148 secs/batch = 0.2659s, grad.norm=14.60496044
 12608: 9 [  665/ 1327], train_loss/perplexity = 4.34966850/77.4527817 secs/batch = 0.2665s, grad.norm=14.78612232
 12613: 9 [  670/ 1327], train_loss/perplexity = 4.24893475/70.0307693 secs/batch = 0.2653s, grad.norm=14.80373478
 12618: 9 [  675/ 1327], train_loss/perplexity = 4.08751202/59.5914459 secs/batch = 0.2643s, grad.norm=14.73249340
 12623: 9 [  680/ 1327], train_loss/perplexity = 4.22848082/68.6129150 secs/batch = 0.2663s, grad.norm=15.34209251
 12628: 9 [  685/ 1327], train_loss/perplexity = 4.07127905/58.6319084 secs/batch = 0.2664s, grad.norm=14.56519222
 12633: 9 [  690/ 1327], train_loss/perplexity = 4.51650810/91.5154800 secs/batch = 0.2657s, grad.norm=14.49676228
 12638: 9 [  695/ 1327], train_loss/perplexity = 4.33148146/76.0568771 secs/batch = 0.2662s, grad.norm=14.86820889
 12643: 9 [  700/ 1327], train_loss/perplexity = 4.55414772/95.0257339 secs/batch = 0.2671s, grad.norm=14.94531059
 12648: 9 [  705/ 1327], train_loss/perplexity = 4.27390385/71.8013916 secs/batch = 0.2681s, grad.norm=14.34636974
 12653: 9 [  710/ 1327], train_loss/perplexity = 4.16459942/64.3668900 secs/batch = 0.2652s, grad.norm=15.17396450
 12658: 9 [  715/ 1327], train_loss/perplexity = 4.07340670/58.7567902 secs/batch = 0.2679s, grad.norm=14.54492760
 12663: 9 [  720/ 1327], train_loss/perplexity = 4.07272530/58.7167664 secs/batch = 0.2670s, grad.norm=15.51729870
 12668: 9 [  725/ 1327], train_loss/perplexity = 4.14180565/62.9163246 secs/batch = 0.2663s, grad.norm=14.87944984
 12673: 9 [  730/ 1327], train_loss/perplexity = 4.23767376/69.2465820 secs/batch = 0.2657s, grad.norm=14.95916557
 12678: 9 [  735/ 1327], train_loss/perplexity = 4.35665035/77.9954376 secs/batch = 0.2661s, grad.norm=15.45728970
 12683: 9 [  740/ 1327], train_loss/perplexity = 3.83011341/46.0677643 secs/batch = 0.2663s, grad.norm=14.01004887
 12688: 9 [  745/ 1327], train_loss/perplexity = 4.33236647/76.1242218 secs/batch = 0.2653s, grad.norm=15.13864517
 12693: 9 [  750/ 1327], train_loss/perplexity = 4.22924614/68.6654510 secs/batch = 0.2657s, grad.norm=14.88849735
 12698: 9 [  755/ 1327], train_loss/perplexity = 4.07504511/58.8531342 secs/batch = 0.2668s, grad.norm=14.39682579
 12703: 9 [  760/ 1327], train_loss/perplexity = 3.96219301/52.5724907 secs/batch = 0.2661s, grad.norm=13.95183372
 12708: 9 [  765/ 1327], train_loss/perplexity = 4.00665474/54.9626999 secs/batch = 0.2646s, grad.norm=14.07291412
 12713: 9 [  770/ 1327], train_loss/perplexity = 3.99682498/54.4250755 secs/batch = 0.2664s, grad.norm=14.91003513
 12718: 9 [  775/ 1327], train_loss/perplexity = 4.13392067/62.4221802 secs/batch = 0.2664s, grad.norm=15.14531326
 12723: 9 [  780/ 1327], train_loss/perplexity = 4.55381918/94.9945145 secs/batch = 0.2664s, grad.norm=15.21263027
 12728: 9 [  785/ 1327], train_loss/perplexity = 4.30192900/73.8420944 secs/batch = 0.2662s, grad.norm=15.21938324
 12733: 9 [  790/ 1327], train_loss/perplexity = 4.08502197/59.4432449 secs/batch = 0.2595s, grad.norm=14.94564247
 12738: 9 [  795/ 1327], train_loss/perplexity = 4.53952646/93.6464462 secs/batch = 0.2656s, grad.norm=14.88136292
 12743: 9 [  800/ 1327], train_loss/perplexity = 4.32398081/75.4885330 secs/batch = 0.2652s, grad.norm=14.57897854
 12748: 9 [  805/ 1327], train_loss/perplexity = 4.73199368/113.5216599 secs/batch = 0.2654s, grad.norm=15.00636387
 12753: 9 [  810/ 1327], train_loss/perplexity = 4.31061363/74.4861832 secs/batch = 0.2652s, grad.norm=14.17280483
 12758: 9 [  815/ 1327], train_loss/perplexity = 4.21502113/67.6955948 secs/batch = 0.2654s, grad.norm=14.49189281
 12763: 9 [  820/ 1327], train_loss/perplexity = 3.99255085/54.1929512 secs/batch = 0.2661s, grad.norm=14.10253620
 12768: 9 [  825/ 1327], train_loss/perplexity = 4.22278547/68.2232513 secs/batch = 0.2661s, grad.norm=14.44723511
 12773: 9 [  830/ 1327], train_loss/perplexity = 3.96543694/52.7433090 secs/batch = 0.2659s, grad.norm=15.33971310
 12778: 9 [  835/ 1327], train_loss/perplexity = 4.32215977/75.3511963 secs/batch = 0.2622s, grad.norm=15.34875298
 12783: 9 [  840/ 1327], train_loss/perplexity = 4.28841448/72.8508682 secs/batch = 0.2669s, grad.norm=15.01903439
 12788: 9 [  845/ 1327], train_loss/perplexity = 4.18760014/65.8645325 secs/batch = 0.2644s, grad.norm=15.22019386
 12793: 9 [  850/ 1327], train_loss/perplexity = 4.20361757/66.9280090 secs/batch = 0.2670s, grad.norm=14.40477943
 12798: 9 [  855/ 1327], train_loss/perplexity = 4.21920538/67.9794464 secs/batch = 0.2669s, grad.norm=15.04792023
 12803: 9 [  860/ 1327], train_loss/perplexity = 3.91708660/50.2538223 secs/batch = 0.2664s, grad.norm=14.06215668
 12808: 9 [  865/ 1327], train_loss/perplexity = 4.45230484/85.8245316 secs/batch = 0.2644s, grad.norm=14.54358864
 12813: 9 [  870/ 1327], train_loss/perplexity = 4.28540421/72.6318970 secs/batch = 0.2635s, grad.norm=15.31689548
 12818: 9 [  875/ 1327], train_loss/perplexity = 3.90942836/49.8704376 secs/batch = 0.2664s, grad.norm=14.98826027
 12823: 9 [  880/ 1327], train_loss/perplexity = 4.06443501/58.2319984 secs/batch = 0.2647s, grad.norm=14.29159355
 12828: 9 [  885/ 1327], train_loss/perplexity = 4.26131582/70.9032211 secs/batch = 0.2656s, grad.norm=14.48142242
 12833: 9 [  890/ 1327], train_loss/perplexity = 4.37951088/79.7989960 secs/batch = 0.2657s, grad.norm=14.82502365
 12838: 9 [  895/ 1327], train_loss/perplexity = 4.43508053/84.3589172 secs/batch = 0.2656s, grad.norm=14.40725422
 12843: 9 [  900/ 1327], train_loss/perplexity = 4.24978399/70.0902710 secs/batch = 0.2641s, grad.norm=14.24946117
 12848: 9 [  905/ 1327], train_loss/perplexity = 4.14449215/63.0855751 secs/batch = 0.2669s, grad.norm=14.13231564
 12853: 9 [  910/ 1327], train_loss/perplexity = 4.16023493/64.0865784 secs/batch = 0.2660s, grad.norm=14.36669159
 12858: 9 [  915/ 1327], train_loss/perplexity = 4.38572931/80.2967606 secs/batch = 0.2667s, grad.norm=14.27692604
 12863: 9 [  920/ 1327], train_loss/perplexity = 4.59273911/98.7645874 secs/batch = 0.2660s, grad.norm=14.76724339
 12868: 9 [  925/ 1327], train_loss/perplexity = 4.35835886/78.1288071 secs/batch = 0.2642s, grad.norm=14.45687103
 12873: 9 [  930/ 1327], train_loss/perplexity = 4.34510708/77.1002960 secs/batch = 0.2659s, grad.norm=14.31083107
 12878: 9 [  935/ 1327], train_loss/perplexity = 4.42204189/83.2661362 secs/batch = 0.2640s, grad.norm=14.56635475
 12883: 9 [  940/ 1327], train_loss/perplexity = 4.38232708/80.0240402 secs/batch = 0.2659s, grad.norm=14.12985802
 12888: 9 [  945/ 1327], train_loss/perplexity = 4.55881786/95.4705505 secs/batch = 0.2654s, grad.norm=13.94931698
 12893: 9 [  950/ 1327], train_loss/perplexity = 4.34818745/77.3381577 secs/batch = 0.2657s, grad.norm=14.43046570
 12898: 9 [  955/ 1327], train_loss/perplexity = 4.31649303/74.9254074 secs/batch = 0.2657s, grad.norm=14.77626991
 12903: 9 [  960/ 1327], train_loss/perplexity = 4.62964535/102.4777145 secs/batch = 0.2657s, grad.norm=14.73082733
 12908: 9 [  965/ 1327], train_loss/perplexity = 4.36213827/78.4246521 secs/batch = 0.2653s, grad.norm=14.66200352
 12913: 9 [  970/ 1327], train_loss/perplexity = 4.56367588/95.9354782 secs/batch = 0.2657s, grad.norm=14.79854488
 12918: 9 [  975/ 1327], train_loss/perplexity = 4.31373405/74.7189713 secs/batch = 0.2657s, grad.norm=15.74015903
 12923: 9 [  980/ 1327], train_loss/perplexity = 4.11336565/61.1521873 secs/batch = 0.2659s, grad.norm=14.33588314
 12928: 9 [  985/ 1327], train_loss/perplexity = 4.23126793/68.8044128 secs/batch = 0.2656s, grad.norm=14.65864944
 12933: 9 [  990/ 1327], train_loss/perplexity = 4.46668148/87.0673065 secs/batch = 0.2651s, grad.norm=14.84981537
 12938: 9 [  995/ 1327], train_loss/perplexity = 4.49421263/89.4976730 secs/batch = 0.2616s, grad.norm=14.18446445
 12943: 9 [ 1000/ 1327], train_loss/perplexity = 3.96732616/52.8430481 secs/batch = 0.2669s, grad.norm=14.27859116
 12948: 9 [ 1005/ 1327], train_loss/perplexity = 4.39599752/81.1255188 secs/batch = 0.2658s, grad.norm=14.44168663
 12953: 9 [ 1010/ 1327], train_loss/perplexity = 4.07077122/58.6021385 secs/batch = 0.2663s, grad.norm=14.04785538
 12958: 9 [ 1015/ 1327], train_loss/perplexity = 4.55318689/94.9344711 secs/batch = 0.2672s, grad.norm=14.51730919
 12963: 9 [ 1020/ 1327], train_loss/perplexity = 4.59761477/99.2473068 secs/batch = 0.2665s, grad.norm=14.28057861
 12968: 9 [ 1025/ 1327], train_loss/perplexity = 4.53786278/93.4907761 secs/batch = 0.2652s, grad.norm=14.68484402
 12973: 9 [ 1030/ 1327], train_loss/perplexity = 4.25403786/70.3890610 secs/batch = 0.2664s, grad.norm=13.96408081
 12978: 9 [ 1035/ 1327], train_loss/perplexity = 4.25722504/70.6137619 secs/batch = 0.2654s, grad.norm=14.66913986
 12983: 9 [ 1040/ 1327], train_loss/perplexity = 4.43488979/84.3428268 secs/batch = 0.2664s, grad.norm=15.01823711
 12988: 9 [ 1045/ 1327], train_loss/perplexity = 3.97977710/53.5051079 secs/batch = 0.2656s, grad.norm=14.16644859
 12993: 9 [ 1050/ 1327], train_loss/perplexity = 4.07373762/58.7762375 secs/batch = 0.2653s, grad.norm=15.04047775
 12998: 9 [ 1055/ 1327], train_loss/perplexity = 4.20046902/66.7176132 secs/batch = 0.2660s, grad.norm=15.34770012
 13003: 9 [ 1060/ 1327], train_loss/perplexity = 3.76976013/43.3696594 secs/batch = 0.2658s, grad.norm=15.23677826
 13008: 9 [ 1065/ 1327], train_loss/perplexity = 3.98754978/53.9226036 secs/batch = 0.2647s, grad.norm=15.08058167
 13013: 9 [ 1070/ 1327], train_loss/perplexity = 4.27250767/71.7012100 secs/batch = 0.2658s, grad.norm=15.19772530
 13018: 9 [ 1075/ 1327], train_loss/perplexity = 4.09982824/60.3299255 secs/batch = 0.2664s, grad.norm=14.61032391
 13023: 9 [ 1080/ 1327], train_loss/perplexity = 4.05662298/57.7788620 secs/batch = 0.2670s, grad.norm=15.14802742
 13028: 9 [ 1085/ 1327], train_loss/perplexity = 3.83835196/46.4488602 secs/batch = 0.2665s, grad.norm=14.81237507
 13033: 9 [ 1090/ 1327], train_loss/perplexity = 4.02284098/55.8595772 secs/batch = 0.2658s, grad.norm=15.39841938
 13038: 9 [ 1095/ 1327], train_loss/perplexity = 4.24355936/69.6553421 secs/batch = 0.2648s, grad.norm=15.57969379
 13043: 9 [ 1100/ 1327], train_loss/perplexity = 3.91627765/50.2131844 secs/batch = 0.2661s, grad.norm=16.14669037
 13048: 9 [ 1105/ 1327], train_loss/perplexity = 3.97963190/53.4973373 secs/batch = 0.2661s, grad.norm=14.93776703
 13053: 9 [ 1110/ 1327], train_loss/perplexity = 4.24516964/69.7675934 secs/batch = 0.2659s, grad.norm=15.73741245
 13058: 9 [ 1115/ 1327], train_loss/perplexity = 4.05928135/57.9326630 secs/batch = 0.2650s, grad.norm=14.83261204
 13063: 9 [ 1120/ 1327], train_loss/perplexity = 4.23698568/69.1989517 secs/batch = 0.2652s, grad.norm=14.40210724
 13068: 9 [ 1125/ 1327], train_loss/perplexity = 4.44534779/85.2295151 secs/batch = 0.2659s, grad.norm=15.40047550
 13073: 9 [ 1130/ 1327], train_loss/perplexity = 4.11011648/60.9538155 secs/batch = 0.2660s, grad.norm=14.95613766
 13078: 9 [ 1135/ 1327], train_loss/perplexity = 4.12102938/61.6226425 secs/batch = 0.2656s, grad.norm=14.84738064
 13083: 9 [ 1140/ 1327], train_loss/perplexity = 4.46249485/86.7035522 secs/batch = 0.2579s, grad.norm=16.19334602
 13088: 9 [ 1145/ 1327], train_loss/perplexity = 4.18211508/65.5042572 secs/batch = 0.2670s, grad.norm=14.74970436
 13093: 9 [ 1150/ 1327], train_loss/perplexity = 4.19771862/66.5343704 secs/batch = 0.2635s, grad.norm=14.91064453
 13098: 9 [ 1155/ 1327], train_loss/perplexity = 4.21937990/67.9913101 secs/batch = 0.2652s, grad.norm=14.73088455
 13103: 9 [ 1160/ 1327], train_loss/perplexity = 4.20484400/67.0101395 secs/batch = 0.2585s, grad.norm=14.84325123
 13108: 9 [ 1165/ 1327], train_loss/perplexity = 4.22008753/68.0394363 secs/batch = 0.2630s, grad.norm=14.67364979
 13113: 9 [ 1170/ 1327], train_loss/perplexity = 4.13336086/62.3872452 secs/batch = 0.2659s, grad.norm=15.09039307
 13118: 9 [ 1175/ 1327], train_loss/perplexity = 3.95194650/52.0365562 secs/batch = 0.2648s, grad.norm=15.04525280
 13123: 9 [ 1180/ 1327], train_loss/perplexity = 3.93950009/51.3929024 secs/batch = 0.2664s, grad.norm=14.88063431
 13128: 9 [ 1185/ 1327], train_loss/perplexity = 4.12494469/61.8643875 secs/batch = 0.2667s, grad.norm=15.16123009
 13133: 9 [ 1190/ 1327], train_loss/perplexity = 4.24775362/69.9481049 secs/batch = 0.2652s, grad.norm=15.19910812
 13138: 9 [ 1195/ 1327], train_loss/perplexity = 4.04179430/56.9283981 secs/batch = 0.2652s, grad.norm=14.88366318
 13143: 9 [ 1200/ 1327], train_loss/perplexity = 3.96589375/52.7674103 secs/batch = 0.2658s, grad.norm=15.29596519
 13148: 9 [ 1205/ 1327], train_loss/perplexity = 3.99145555/54.1336250 secs/batch = 0.2654s, grad.norm=15.03672028
 13153: 9 [ 1210/ 1327], train_loss/perplexity = 3.61603713/37.1898956 secs/batch = 0.2635s, grad.norm=15.01885605
 13158: 9 [ 1215/ 1327], train_loss/perplexity = 3.87487912/48.1768761 secs/batch = 0.2655s, grad.norm=14.46496677
 13163: 9 [ 1220/ 1327], train_loss/perplexity = 4.02584410/56.0275803 secs/batch = 0.2660s, grad.norm=15.28137493
 13168: 9 [ 1225/ 1327], train_loss/perplexity = 3.72042108/41.2817726 secs/batch = 0.2634s, grad.norm=15.60113525
 13173: 9 [ 1230/ 1327], train_loss/perplexity = 4.05046892/57.4243774 secs/batch = 0.2654s, grad.norm=14.93228054
 13178: 9 [ 1235/ 1327], train_loss/perplexity = 4.07254171/58.7059860 secs/batch = 0.2666s, grad.norm=15.00710583
 13183: 9 [ 1240/ 1327], train_loss/perplexity = 4.23994446/69.4039993 secs/batch = 0.2662s, grad.norm=15.02376270
 13188: 9 [ 1245/ 1327], train_loss/perplexity = 4.16039753/64.0970001 secs/batch = 0.2655s, grad.norm=14.60069847
 13193: 9 [ 1250/ 1327], train_loss/perplexity = 4.29839087/73.5812988 secs/batch = 0.2650s, grad.norm=14.43826294
 13198: 9 [ 1255/ 1327], train_loss/perplexity = 4.24799919/69.9652863 secs/batch = 0.2671s, grad.norm=14.06162262
 13203: 9 [ 1260/ 1327], train_loss/perplexity = 4.06486177/58.2568550 secs/batch = 0.2660s, grad.norm=15.67957497
 13208: 9 [ 1265/ 1327], train_loss/perplexity = 4.29574108/73.3865814 secs/batch = 0.2616s, grad.norm=15.34549141
 13213: 9 [ 1270/ 1327], train_loss/perplexity = 4.02874374/56.1902771 secs/batch = 0.2638s, grad.norm=15.48226357
 13218: 9 [ 1275/ 1327], train_loss/perplexity = 4.17402554/64.9764938 secs/batch = 0.2661s, grad.norm=15.58880138
 13223: 9 [ 1280/ 1327], train_loss/perplexity = 4.06252670/58.1209793 secs/batch = 0.2632s, grad.norm=15.80618572
 13228: 9 [ 1285/ 1327], train_loss/perplexity = 3.88532758/48.6828880 secs/batch = 0.2651s, grad.norm=14.63979721
 13233: 9 [ 1290/ 1327], train_loss/perplexity = 4.26519966/71.1791306 secs/batch = 0.2610s, grad.norm=15.06202412
 13238: 9 [ 1295/ 1327], train_loss/perplexity = 4.15081644/63.4858131 secs/batch = 0.2635s, grad.norm=14.90662575
 13243: 9 [ 1300/ 1327], train_loss/perplexity = 4.38267279/80.0517120 secs/batch = 0.2649s, grad.norm=14.72630978
 13248: 9 [ 1305/ 1327], train_loss/perplexity = 4.47057390/87.4068680 secs/batch = 0.2658s, grad.norm=15.78686810
 13253: 9 [ 1310/ 1327], train_loss/perplexity = 4.69237661/109.1121902 secs/batch = 0.2657s, grad.norm=15.22852898
 13258: 9 [ 1315/ 1327], train_loss/perplexity = 4.50692844/90.6429749 secs/batch = 0.2665s, grad.norm=15.06009102
 13263: 9 [ 1320/ 1327], train_loss/perplexity = 4.46291113/86.7396545 secs/batch = 0.2655s, grad.norm=14.69130611
 13268: 9 [ 1325/ 1327], train_loss/perplexity = 4.40476179/81.8396454 secs/batch = 0.2654s, grad.norm=14.86041164
Epoch training time: 352.4997043609619
	> validation loss = 4.69533634, perplexity = 109.43560791
	> validation loss = 4.59497070, perplexity = 98.98523712
	> validation loss = 4.59596586, perplexity = 99.08379364
	> validation loss = 4.58689928, perplexity = 98.18949890
	> validation loss = 4.74939346, perplexity = 115.51419830
	> validation loss = 4.69927788, perplexity = 109.86780548
	> validation loss = 4.62794065, perplexity = 102.30316925
	> validation loss = 4.46454096, perplexity = 86.88114166
	> validation loss = 4.27991199, perplexity = 72.23408508
	> validation loss = 4.39025545, perplexity = 80.66101837
	> validation loss = 4.52362776, perplexity = 92.16935730
	> validation loss = 4.58479261, perplexity = 97.98286438
	> validation loss = 4.53259897, perplexity = 92.99995422
	> validation loss = 4.31479645, perplexity = 74.79839325
	> validation loss = 4.20561695, perplexity = 67.06195831
	> validation loss = 4.26886559, perplexity = 71.44054413
	> validation loss = 4.66716766, perplexity = 106.39596558
	> validation loss = 4.24356222, perplexity = 69.65554047
	> validation loss = 4.68434858, perplexity = 108.23973846
	> validation loss = 4.53725529, perplexity = 93.43399811
	> validation loss = 4.32257557, perplexity = 75.38253021
at the end of epoch: 9
train loss = 4.32627535, perplexity = 75.66194658
validation loss = 4.51143661, perplexity = 91.05253199
Saved model cv/epoch009_4.5114.model
 13275: 10 [    5/ 1327], train_loss/perplexity = 4.42285633/83.3339767 secs/batch = 0.2646s, grad.norm=14.40313530
 13280: 10 [   10/ 1327], train_loss/perplexity = 4.02810478/56.1543846 secs/batch = 0.2645s, grad.norm=14.44965935
 13285: 10 [   15/ 1327], train_loss/perplexity = 4.41322851/82.5354996 secs/batch = 0.2658s, grad.norm=13.93570805
 13290: 10 [   20/ 1327], train_loss/perplexity = 4.54446363/94.1099396 secs/batch = 0.2659s, grad.norm=14.54861164
 13295: 10 [   25/ 1327], train_loss/perplexity = 4.38650990/80.3594666 secs/batch = 0.2660s, grad.norm=15.22867489
 13300: 10 [   30/ 1327], train_loss/perplexity = 4.47110271/87.4531021 secs/batch = 0.2636s, grad.norm=15.28370857
 13305: 10 [   35/ 1327], train_loss/perplexity = 4.21160269/67.4645767 secs/batch = 0.2659s, grad.norm=14.06825066
 13310: 10 [   40/ 1327], train_loss/perplexity = 4.21499014/67.6934967 secs/batch = 0.2657s, grad.norm=14.98145580
 13315: 10 [   45/ 1327], train_loss/perplexity = 4.01979256/55.6895523 secs/batch = 0.2664s, grad.norm=14.05160522
 13320: 10 [   50/ 1327], train_loss/perplexity = 4.19257879/66.1932678 secs/batch = 0.2653s, grad.norm=14.95727062
 13325: 10 [   55/ 1327], train_loss/perplexity = 4.19791937/66.5477219 secs/batch = 0.2666s, grad.norm=15.58345795
 13330: 10 [   60/ 1327], train_loss/perplexity = 4.49347639/89.4318085 secs/batch = 0.2660s, grad.norm=15.00254440
 13335: 10 [   65/ 1327], train_loss/perplexity = 4.07340908/58.7569275 secs/batch = 0.2656s, grad.norm=14.65107536
 13340: 10 [   70/ 1327], train_loss/perplexity = 3.83289909/46.1962700 secs/batch = 0.2659s, grad.norm=14.50749683
 13345: 10 [   75/ 1327], train_loss/perplexity = 3.67876530/39.5974731 secs/batch = 0.2660s, grad.norm=13.98618507
 13350: 10 [   80/ 1327], train_loss/perplexity = 4.16973782/64.6984863 secs/batch = 0.2661s, grad.norm=14.59164715
 13355: 10 [   85/ 1327], train_loss/perplexity = 4.17394066/64.9709778 secs/batch = 0.2658s, grad.norm=15.24033165
 13360: 10 [   90/ 1327], train_loss/perplexity = 4.23933601/69.3617783 secs/batch = 0.2658s, grad.norm=15.19968891
 13365: 10 [   95/ 1327], train_loss/perplexity = 4.12268686/61.7248650 secs/batch = 0.2653s, grad.norm=14.62693119
 13370: 10 [  100/ 1327], train_loss/perplexity = 4.37694025/79.5941238 secs/batch = 0.2630s, grad.norm=15.06129360
 13375: 10 [  105/ 1327], train_loss/perplexity = 4.24447060/69.7188416 secs/batch = 0.2665s, grad.norm=15.80423069
 13380: 10 [  110/ 1327], train_loss/perplexity = 4.02234602/55.8319359 secs/batch = 0.2640s, grad.norm=14.53414917
 13385: 10 [  115/ 1327], train_loss/perplexity = 4.02591610/56.0316162 secs/batch = 0.2657s, grad.norm=15.25283241
 13390: 10 [  120/ 1327], train_loss/perplexity = 4.14792061/63.3022346 secs/batch = 0.2653s, grad.norm=15.42693043
 13395: 10 [  125/ 1327], train_loss/perplexity = 4.13114071/62.2488899 secs/batch = 0.2656s, grad.norm=15.19704247
 13400: 10 [  130/ 1327], train_loss/perplexity = 4.09109879/59.8055687 secs/batch = 0.2621s, grad.norm=15.84573364
 13405: 10 [  135/ 1327], train_loss/perplexity = 4.16196203/64.1973572 secs/batch = 0.2660s, grad.norm=14.79270172
 13410: 10 [  140/ 1327], train_loss/perplexity = 4.40998125/82.2679214 secs/batch = 0.2648s, grad.norm=14.86584091
 13415: 10 [  145/ 1327], train_loss/perplexity = 4.32484293/75.5536423 secs/batch = 0.2668s, grad.norm=16.00813866
 13420: 10 [  150/ 1327], train_loss/perplexity = 4.30375385/73.9769745 secs/batch = 0.2659s, grad.norm=15.75127506
 13425: 10 [  155/ 1327], train_loss/perplexity = 4.61542702/101.0309601 secs/batch = 0.2658s, grad.norm=14.84573269
 13430: 10 [  160/ 1327], train_loss/perplexity = 4.21137619/67.4493027 secs/batch = 0.2662s, grad.norm=14.37013912
 13435: 10 [  165/ 1327], train_loss/perplexity = 4.43393469/84.2623138 secs/batch = 0.2666s, grad.norm=14.79030800
 13440: 10 [  170/ 1327], train_loss/perplexity = 4.22658634/68.4830551 secs/batch = 0.2656s, grad.norm=14.52470493
 13445: 10 [  175/ 1327], train_loss/perplexity = 4.53343534/93.0777664 secs/batch = 0.2672s, grad.norm=14.92508030
 13450: 10 [  180/ 1327], train_loss/perplexity = 4.33621931/76.4180832 secs/batch = 0.2609s, grad.norm=15.20108509
 13455: 10 [  185/ 1327], train_loss/perplexity = 4.63793182/103.3304214 secs/batch = 0.2634s, grad.norm=14.88048744
 13460: 10 [  190/ 1327], train_loss/perplexity = 4.19423056/66.3026962 secs/batch = 0.2668s, grad.norm=13.95920372
 13465: 10 [  195/ 1327], train_loss/perplexity = 4.46311569/86.7574005 secs/batch = 0.2648s, grad.norm=13.94985294
 13470: 10 [  200/ 1327], train_loss/perplexity = 4.30544662/74.1023026 secs/batch = 0.2654s, grad.norm=15.08285046
 13475: 10 [  205/ 1327], train_loss/perplexity = 4.54800177/94.4434967 secs/batch = 0.2635s, grad.norm=14.95404816
 13480: 10 [  210/ 1327], train_loss/perplexity = 4.39229965/80.8260803 secs/batch = 0.2598s, grad.norm=14.11133957
 13485: 10 [  215/ 1327], train_loss/perplexity = 4.56699514/96.2544403 secs/batch = 0.2677s, grad.norm=14.29812336
 13490: 10 [  220/ 1327], train_loss/perplexity = 4.40918732/82.2026291 secs/batch = 0.2680s, grad.norm=14.44438839
 13495: 10 [  225/ 1327], train_loss/perplexity = 4.60614872/100.0979004 secs/batch = 0.2663s, grad.norm=14.86340046
 13500: 10 [  230/ 1327], train_loss/perplexity = 4.41471386/82.6581879 secs/batch = 0.2664s, grad.norm=15.16782570
 13505: 10 [  235/ 1327], train_loss/perplexity = 4.31143713/74.5475464 secs/batch = 0.2663s, grad.norm=14.53901863
 13510: 10 [  240/ 1327], train_loss/perplexity = 4.07555008/58.8828621 secs/batch = 0.2643s, grad.norm=14.97870541
 13515: 10 [  245/ 1327], train_loss/perplexity = 4.34823036/77.3414764 secs/batch = 0.2669s, grad.norm=14.95551300
 13520: 10 [  250/ 1327], train_loss/perplexity = 4.19062996/66.0643921 secs/batch = 0.2649s, grad.norm=14.28148746
 13525: 10 [  255/ 1327], train_loss/perplexity = 4.20088339/66.7452698 secs/batch = 0.2659s, grad.norm=14.73341370
 13530: 10 [  260/ 1327], train_loss/perplexity = 4.45752001/86.2732849 secs/batch = 0.2659s, grad.norm=15.98268414
 13535: 10 [  265/ 1327], train_loss/perplexity = 4.58253908/97.7623062 secs/batch = 0.2660s, grad.norm=14.61802959
 13540: 10 [  270/ 1327], train_loss/perplexity = 4.64932632/104.5145493 secs/batch = 0.2638s, grad.norm=14.76968670
 13545: 10 [  275/ 1327], train_loss/perplexity = 4.62715244/102.2225647 secs/batch = 0.2663s, grad.norm=14.42254829
 13550: 10 [  280/ 1327], train_loss/perplexity = 4.37280607/79.2657471 secs/batch = 0.2645s, grad.norm=14.47487164
 13555: 10 [  285/ 1327], train_loss/perplexity = 4.70558548/110.5629959 secs/batch = 0.2647s, grad.norm=14.51672268
 13560: 10 [  290/ 1327], train_loss/perplexity = 4.38104630/79.9216080 secs/batch = 0.2648s, grad.norm=15.08208179
 13565: 10 [  295/ 1327], train_loss/perplexity = 4.18167257/65.4752731 secs/batch = 0.2658s, grad.norm=15.07405758
 13570: 10 [  300/ 1327], train_loss/perplexity = 3.74010611/42.1024590 secs/batch = 0.2625s, grad.norm=14.51989937
 13575: 10 [  305/ 1327], train_loss/perplexity = 4.25829411/70.6892929 secs/batch = 0.2658s, grad.norm=14.86624718
 13580: 10 [  310/ 1327], train_loss/perplexity = 4.21151972/67.4589844 secs/batch = 0.2602s, grad.norm=14.64235592
 13585: 10 [  315/ 1327], train_loss/perplexity = 3.78109741/43.8641510 secs/batch = 0.2658s, grad.norm=14.86386395
 13590: 10 [  320/ 1327], train_loss/perplexity = 3.77557659/43.6226540 secs/batch = 0.2664s, grad.norm=16.56820869
 13595: 10 [  325/ 1327], train_loss/perplexity = 3.76390886/43.1166344 secs/batch = 0.2664s, grad.norm=14.30909157
 13600: 10 [  330/ 1327], train_loss/perplexity = 4.33244753/76.1303864 secs/batch = 0.2650s, grad.norm=14.86651993
 13605: 10 [  335/ 1327], train_loss/perplexity = 3.81425667/45.3430405 secs/batch = 0.2656s, grad.norm=14.31681824
 13610: 10 [  340/ 1327], train_loss/perplexity = 4.50047874/90.0602341 secs/batch = 0.2671s, grad.norm=14.54938507
 13615: 10 [  345/ 1327], train_loss/perplexity = 4.24183941/69.5356369 secs/batch = 0.2642s, grad.norm=14.16898251
 13620: 10 [  350/ 1327], train_loss/perplexity = 4.29450178/73.2956848 secs/batch = 0.2652s, grad.norm=15.29716301
 13625: 10 [  355/ 1327], train_loss/perplexity = 4.27359486/71.7792053 secs/batch = 0.2654s, grad.norm=14.76820564
 13630: 10 [  360/ 1327], train_loss/perplexity = 4.47316504/87.6336517 secs/batch = 0.2659s, grad.norm=16.16165161
 13635: 10 [  365/ 1327], train_loss/perplexity = 4.38871479/80.5368423 secs/batch = 0.2661s, grad.norm=15.28205109
 13640: 10 [  370/ 1327], train_loss/perplexity = 4.45941782/86.4371719 secs/batch = 0.2661s, grad.norm=15.24338245
 13645: 10 [  375/ 1327], train_loss/perplexity = 3.84078670/46.5620918 secs/batch = 0.2656s, grad.norm=14.83397865
 13650: 10 [  380/ 1327], train_loss/perplexity = 3.90557528/49.6786499 secs/batch = 0.2669s, grad.norm=15.32167339
 13655: 10 [  385/ 1327], train_loss/perplexity = 4.16662312/64.4972839 secs/batch = 0.2647s, grad.norm=15.15522766
 13660: 10 [  390/ 1327], train_loss/perplexity = 4.22181559/68.1571198 secs/batch = 0.2640s, grad.norm=14.94711685
 13665: 10 [  395/ 1327], train_loss/perplexity = 4.26183891/70.9403152 secs/batch = 0.2637s, grad.norm=14.92680359
 13670: 10 [  400/ 1327], train_loss/perplexity = 4.23488712/69.0538864 secs/batch = 0.2651s, grad.norm=14.67750645
 13675: 10 [  405/ 1327], train_loss/perplexity = 4.54573345/94.2295151 secs/batch = 0.2653s, grad.norm=14.66494083
 13680: 10 [  410/ 1327], train_loss/perplexity = 4.19947672/66.6514435 secs/batch = 0.2610s, grad.norm=14.78376198
 13685: 10 [  415/ 1327], train_loss/perplexity = 4.15335512/63.6471863 secs/batch = 0.2633s, grad.norm=14.25206280
 13690: 10 [  420/ 1327], train_loss/perplexity = 3.80857658/45.0862160 secs/batch = 0.2647s, grad.norm=14.91843319
 13695: 10 [  425/ 1327], train_loss/perplexity = 4.15480757/63.7396965 secs/batch = 0.2655s, grad.norm=15.70963001
 13700: 10 [  430/ 1327], train_loss/perplexity = 4.31328487/74.6854172 secs/batch = 0.2647s, grad.norm=15.25427055
 13705: 10 [  435/ 1327], train_loss/perplexity = 4.36162996/78.3847961 secs/batch = 0.2662s, grad.norm=15.63723850
 13710: 10 [  440/ 1327], train_loss/perplexity = 3.94093561/51.4667320 secs/batch = 0.2664s, grad.norm=14.94515514
 13715: 10 [  445/ 1327], train_loss/perplexity = 4.27162886/71.6382294 secs/batch = 0.2595s, grad.norm=15.74428272
 13720: 10 [  450/ 1327], train_loss/perplexity = 4.18647623/65.7905502 secs/batch = 0.2666s, grad.norm=14.82745361
 13725: 10 [  455/ 1327], train_loss/perplexity = 4.19389582/66.2805023 secs/batch = 0.2663s, grad.norm=14.76185131
 13730: 10 [  460/ 1327], train_loss/perplexity = 4.13002920/62.1797371 secs/batch = 0.2666s, grad.norm=15.00490093
 13735: 10 [  465/ 1327], train_loss/perplexity = 3.87268782/48.0714188 secs/batch = 0.2640s, grad.norm=16.28254318
 13740: 10 [  470/ 1327], train_loss/perplexity = 4.61446714/100.9340286 secs/batch = 0.2657s, grad.norm=15.16021252
 13745: 10 [  475/ 1327], train_loss/perplexity = 4.06539726/58.2880592 secs/batch = 0.2655s, grad.norm=15.13004112
 13750: 10 [  480/ 1327], train_loss/perplexity = 4.26188087/70.9432907 secs/batch = 0.2636s, grad.norm=14.94847393
 13755: 10 [  485/ 1327], train_loss/perplexity = 4.09767437/60.2001228 secs/batch = 0.2643s, grad.norm=14.78096771
 13760: 10 [  490/ 1327], train_loss/perplexity = 4.02343178/55.8925896 secs/batch = 0.2663s, grad.norm=16.19725037
 13765: 10 [  495/ 1327], train_loss/perplexity = 4.13073635/62.2237244 secs/batch = 0.2641s, grad.norm=14.93698502
 13770: 10 [  500/ 1327], train_loss/perplexity = 4.27435160/71.8335495 secs/batch = 0.2670s, grad.norm=14.85671616
 13775: 10 [  505/ 1327], train_loss/perplexity = 4.41132736/82.3787384 secs/batch = 0.2664s, grad.norm=14.05178928
 13780: 10 [  510/ 1327], train_loss/perplexity = 4.76033974/116.7855988 secs/batch = 0.2646s, grad.norm=14.17681599
 13785: 10 [  515/ 1327], train_loss/perplexity = 4.33777857/76.5373306 secs/batch = 0.2650s, grad.norm=14.26592064
 13790: 10 [  520/ 1327], train_loss/perplexity = 4.53952026/93.6458664 secs/batch = 0.2666s, grad.norm=14.97010899
 13795: 10 [  525/ 1327], train_loss/perplexity = 4.12005615/61.5626984 secs/batch = 0.2646s, grad.norm=14.84581566
 13800: 10 [  530/ 1327], train_loss/perplexity = 4.16207075/64.2043381 secs/batch = 0.2661s, grad.norm=15.33655548
 13805: 10 [  535/ 1327], train_loss/perplexity = 4.27213764/71.6746902 secs/batch = 0.2657s, grad.norm=15.21717834
 13810: 10 [  540/ 1327], train_loss/perplexity = 4.41623640/82.7841339 secs/batch = 0.2626s, grad.norm=14.74691963
 13815: 10 [  545/ 1327], train_loss/perplexity = 4.34427929/77.0364990 secs/batch = 0.2661s, grad.norm=15.15682793
 13820: 10 [  550/ 1327], train_loss/perplexity = 4.32365561/75.4639893 secs/batch = 0.2656s, grad.norm=15.13611889
 13825: 10 [  555/ 1327], train_loss/perplexity = 4.18595743/65.7564316 secs/batch = 0.2655s, grad.norm=14.36004353
 13830: 10 [  560/ 1327], train_loss/perplexity = 4.29912090/73.6350327 secs/batch = 0.2663s, grad.norm=15.71405125
 13835: 10 [  565/ 1327], train_loss/perplexity = 4.09510851/60.0458527 secs/batch = 0.2663s, grad.norm=15.93957615
 13840: 10 [  570/ 1327], train_loss/perplexity = 4.17179871/64.8319626 secs/batch = 0.2656s, grad.norm=16.31822968
 13845: 10 [  575/ 1327], train_loss/perplexity = 3.99416757/54.2806358 secs/batch = 0.2661s, grad.norm=15.36303520
 13850: 10 [  580/ 1327], train_loss/perplexity = 4.32392216/75.4841080 secs/batch = 0.2648s, grad.norm=15.54752350
 13855: 10 [  585/ 1327], train_loss/perplexity = 3.93636847/51.2322121 secs/batch = 0.2659s, grad.norm=14.91138554
 13860: 10 [  590/ 1327], train_loss/perplexity = 4.28865767/72.8685913 secs/batch = 0.2653s, grad.norm=14.75191879
 13865: 10 [  595/ 1327], train_loss/perplexity = 4.27494240/71.8759995 secs/batch = 0.2673s, grad.norm=15.12246513
 13870: 10 [  600/ 1327], train_loss/perplexity = 4.43880463/84.6736679 secs/batch = 0.2664s, grad.norm=14.28403759
 13875: 10 [  605/ 1327], train_loss/perplexity = 4.34625816/77.1890945 secs/batch = 0.2663s, grad.norm=14.85780525
 13880: 10 [  610/ 1327], train_loss/perplexity = 4.54192114/93.8709641 secs/batch = 0.2664s, grad.norm=15.23354053
 13885: 10 [  615/ 1327], train_loss/perplexity = 4.07327843/58.7492523 secs/batch = 0.2625s, grad.norm=14.35132027
 13890: 10 [  620/ 1327], train_loss/perplexity = 4.49152565/89.2575150 secs/batch = 0.2663s, grad.norm=14.74796009
 13895: 10 [  625/ 1327], train_loss/perplexity = 4.41679859/82.8306885 secs/batch = 0.2660s, grad.norm=14.58904648
 13900: 10 [  630/ 1327], train_loss/perplexity = 4.50185394/90.1841736 secs/batch = 0.2668s, grad.norm=14.79345703
 13905: 10 [  635/ 1327], train_loss/perplexity = 4.24373960/69.6678925 secs/batch = 0.2665s, grad.norm=15.42124462
 13910: 10 [  640/ 1327], train_loss/perplexity = 4.18624020/65.7750244 secs/batch = 0.2577s, grad.norm=15.04063988
 13915: 10 [  645/ 1327], train_loss/perplexity = 4.51912594/91.7553635 secs/batch = 0.2658s, grad.norm=16.17409515
 13920: 10 [  650/ 1327], train_loss/perplexity = 4.02464104/55.9602165 secs/batch = 0.2661s, grad.norm=15.54566383
 13925: 10 [  655/ 1327], train_loss/perplexity = 4.18475819/65.6776199 secs/batch = 0.2661s, grad.norm=15.18182755
 13930: 10 [  660/ 1327], train_loss/perplexity = 4.08369446/59.3643837 secs/batch = 0.2650s, grad.norm=15.10746861
 13935: 10 [  665/ 1327], train_loss/perplexity = 4.30017138/73.7124252 secs/batch = 0.2658s, grad.norm=14.71473026
 13940: 10 [  670/ 1327], train_loss/perplexity = 4.20445347/66.9839783 secs/batch = 0.2663s, grad.norm=15.24511337
 13945: 10 [  675/ 1327], train_loss/perplexity = 3.98971319/54.0393867 secs/batch = 0.2671s, grad.norm=15.13552475
 13950: 10 [  680/ 1327], train_loss/perplexity = 4.19548512/66.3859253 secs/batch = 0.2661s, grad.norm=16.24298668
 13955: 10 [  685/ 1327], train_loss/perplexity = 4.00703096/54.9833794 secs/batch = 0.2592s, grad.norm=15.25458908
 13960: 10 [  690/ 1327], train_loss/perplexity = 4.44196796/84.9419403 secs/batch = 0.2656s, grad.norm=14.87420177
 13965: 10 [  695/ 1327], train_loss/perplexity = 4.24858522/70.0063019 secs/batch = 0.2662s, grad.norm=15.34456921
 13970: 10 [  700/ 1327], train_loss/perplexity = 4.44519424/85.2164307 secs/batch = 0.2653s, grad.norm=15.62342072
 13975: 10 [  705/ 1327], train_loss/perplexity = 4.22029018/68.0532303 secs/batch = 0.2653s, grad.norm=14.73739338
 13980: 10 [  710/ 1327], train_loss/perplexity = 4.21054840/67.3934860 secs/batch = 0.2662s, grad.norm=16.04407883
 13985: 10 [  715/ 1327], train_loss/perplexity = 4.02788973/56.1423111 secs/batch = 0.2650s, grad.norm=15.67411327
 13990: 10 [  720/ 1327], train_loss/perplexity = 4.08331728/59.3419991 secs/batch = 0.2659s, grad.norm=15.59880066
 13995: 10 [  725/ 1327], train_loss/perplexity = 4.10196114/60.4587402 secs/batch = 0.2659s, grad.norm=15.61509514
 14000: 10 [  730/ 1327], train_loss/perplexity = 4.21210194/67.4982681 secs/batch = 0.2662s, grad.norm=15.26404667
 14005: 10 [  735/ 1327], train_loss/perplexity = 4.28594398/72.6711121 secs/batch = 0.2660s, grad.norm=15.83990669
 14010: 10 [  740/ 1327], train_loss/perplexity = 3.80526280/44.9370575 secs/batch = 0.2666s, grad.norm=14.24438667
 14015: 10 [  745/ 1327], train_loss/perplexity = 4.25492191/70.4513168 secs/batch = 0.2663s, grad.norm=15.49266624
 14020: 10 [  750/ 1327], train_loss/perplexity = 4.16794491/64.5825958 secs/batch = 0.2668s, grad.norm=15.14312744
 14025: 10 [  755/ 1327], train_loss/perplexity = 4.02942085/56.2283363 secs/batch = 0.2659s, grad.norm=14.91232967
 14030: 10 [  760/ 1327], train_loss/perplexity = 3.87963772/48.4066734 secs/batch = 0.2654s, grad.norm=14.22543907
 14035: 10 [  765/ 1327], train_loss/perplexity = 3.99709988/54.4400368 secs/batch = 0.2656s, grad.norm=14.95089722
 14040: 10 [  770/ 1327], train_loss/perplexity = 4.02479172/55.9686508 secs/batch = 0.2677s, grad.norm=15.53004932
 14045: 10 [  775/ 1327], train_loss/perplexity = 4.10763264/60.8026047 secs/batch = 0.2661s, grad.norm=15.45229435
 14050: 10 [  780/ 1327], train_loss/perplexity = 4.44946766/85.5813751 secs/batch = 0.2662s, grad.norm=15.13511467
 14055: 10 [  785/ 1327], train_loss/perplexity = 4.24615479/69.8363571 secs/batch = 0.2632s, grad.norm=15.55600739
 14060: 10 [  790/ 1327], train_loss/perplexity = 4.02392864/55.9203644 secs/batch = 0.2651s, grad.norm=15.37098598
 14065: 10 [  795/ 1327], train_loss/perplexity = 4.39376354/80.9444809 secs/batch = 0.2668s, grad.norm=15.19245911
 14070: 10 [  800/ 1327], train_loss/perplexity = 4.27865314/72.1432114 secs/batch = 0.2653s, grad.norm=15.34340668
 14075: 10 [  805/ 1327], train_loss/perplexity = 4.59519386/99.0073242 secs/batch = 0.2615s, grad.norm=15.49721336
 14080: 10 [  810/ 1327], train_loss/perplexity = 4.19920206/66.6331406 secs/batch = 0.2656s, grad.norm=14.41556168
 14085: 10 [  815/ 1327], train_loss/perplexity = 4.13425112/62.4428101 secs/batch = 0.2665s, grad.norm=15.09711838
 14090: 10 [  820/ 1327], train_loss/perplexity = 3.98389626/53.7259560 secs/batch = 0.2663s, grad.norm=14.26454830
 14095: 10 [  825/ 1327], train_loss/perplexity = 4.16760111/64.5603943 secs/batch = 0.2658s, grad.norm=14.96107197
 14100: 10 [  830/ 1327], train_loss/perplexity = 3.89628792/49.2194023 secs/batch = 0.2663s, grad.norm=15.65245628
 14105: 10 [  835/ 1327], train_loss/perplexity = 4.18433571/65.6498795 secs/batch = 0.2668s, grad.norm=15.62441730
 14110: 10 [  840/ 1327], train_loss/perplexity = 4.26024008/70.8269882 secs/batch = 0.2662s, grad.norm=15.39933681
 14115: 10 [  845/ 1327], train_loss/perplexity = 4.11927557/61.5146637 secs/batch = 0.2664s, grad.norm=15.26480007
 14120: 10 [  850/ 1327], train_loss/perplexity = 4.22079229/68.0874100 secs/batch = 0.2659s, grad.norm=15.08178806
 14125: 10 [  855/ 1327], train_loss/perplexity = 4.09697008/60.1577377 secs/batch = 0.2664s, grad.norm=15.23587227
 14130: 10 [  860/ 1327], train_loss/perplexity = 3.90746641/49.7726898 secs/batch = 0.2671s, grad.norm=14.56622601
 14135: 10 [  865/ 1327], train_loss/perplexity = 4.38627338/80.3404617 secs/batch = 0.2658s, grad.norm=15.22538280
 14140: 10 [  870/ 1327], train_loss/perplexity = 4.29026365/72.9857101 secs/batch = 0.2656s, grad.norm=16.04927063
 14145: 10 [  875/ 1327], train_loss/perplexity = 3.78015924/43.8230209 secs/batch = 0.2655s, grad.norm=14.74242973
 14150: 10 [  880/ 1327], train_loss/perplexity = 4.00374889/54.8032150 secs/batch = 0.2654s, grad.norm=14.75684166
 14155: 10 [  885/ 1327], train_loss/perplexity = 4.21666765/67.8071518 secs/batch = 0.2637s, grad.norm=14.98087788
 14160: 10 [  890/ 1327], train_loss/perplexity = 4.39331007/80.9077835 secs/batch = 0.2657s, grad.norm=15.90169048
 14165: 10 [  895/ 1327], train_loss/perplexity = 4.33580685/76.3865662 secs/batch = 0.2658s, grad.norm=14.38569641
 14170: 10 [  900/ 1327], train_loss/perplexity = 4.15835714/63.9663506 secs/batch = 0.2661s, grad.norm=14.61941433
 14175: 10 [  905/ 1327], train_loss/perplexity = 4.06169748/58.0728035 secs/batch = 0.2663s, grad.norm=14.41146564
 14180: 10 [  910/ 1327], train_loss/perplexity = 4.11904860/61.5007019 secs/batch = 0.2657s, grad.norm=14.11496830
 14185: 10 [  915/ 1327], train_loss/perplexity = 4.32600975/75.6418533 secs/batch = 0.2666s, grad.norm=14.73077393
 14190: 10 [  920/ 1327], train_loss/perplexity = 4.49053097/89.1687775 secs/batch = 0.2658s, grad.norm=15.04479885
 14195: 10 [  925/ 1327], train_loss/perplexity = 4.25562096/70.5005798 secs/batch = 0.2657s, grad.norm=14.65424442
 14200: 10 [  930/ 1327], train_loss/perplexity = 4.35256481/77.6774368 secs/batch = 0.2668s, grad.norm=14.51015186
 14205: 10 [  935/ 1327], train_loss/perplexity = 4.39805937/81.2929535 secs/batch = 0.2673s, grad.norm=14.71406364
 14210: 10 [  940/ 1327], train_loss/perplexity = 4.28841686/72.8510437 secs/batch = 0.2648s, grad.norm=14.73987293
 14215: 10 [  945/ 1327], train_loss/perplexity = 4.53591919/93.3092422 secs/batch = 0.2665s, grad.norm=14.98768520
 14220: 10 [  950/ 1327], train_loss/perplexity = 4.29608583/73.4118881 secs/batch = 0.2646s, grad.norm=15.11331081
 14225: 10 [  955/ 1327], train_loss/perplexity = 4.30389786/73.9876251 secs/batch = 0.2654s, grad.norm=15.27037907
 14230: 10 [  960/ 1327], train_loss/perplexity = 4.61224222/100.7097092 secs/batch = 0.2656s, grad.norm=15.21456528
 14235: 10 [  965/ 1327], train_loss/perplexity = 4.36747885/78.8446045 secs/batch = 0.2663s, grad.norm=15.31288528
 14240: 10 [  970/ 1327], train_loss/perplexity = 4.53925419/93.6209488 secs/batch = 0.2672s, grad.norm=14.86912823
 14245: 10 [  975/ 1327], train_loss/perplexity = 4.18605757/65.7630157 secs/batch = 0.2609s, grad.norm=15.55131626
 14250: 10 [  980/ 1327], train_loss/perplexity = 4.09898949/60.2793427 secs/batch = 0.2670s, grad.norm=14.81730652
 14255: 10 [  985/ 1327], train_loss/perplexity = 4.21923018/67.9811325 secs/batch = 0.2649s, grad.norm=15.34246445
 14260: 10 [  990/ 1327], train_loss/perplexity = 4.44835186/85.4859314 secs/batch = 0.2661s, grad.norm=15.61361599
 14265: 10 [  995/ 1327], train_loss/perplexity = 4.46295929/86.7438278 secs/batch = 0.2649s, grad.norm=14.66729736
 14270: 10 [ 1000/ 1327], train_loss/perplexity = 3.87245202/48.0600853 secs/batch = 0.2669s, grad.norm=14.33181477
 14275: 10 [ 1005/ 1327], train_loss/perplexity = 4.36314392/78.5035553 secs/batch = 0.2663s, grad.norm=15.05246162
 14280: 10 [ 1010/ 1327], train_loss/perplexity = 3.96113896/52.5171051 secs/batch = 0.2663s, grad.norm=14.30458546
 14285: 10 [ 1015/ 1327], train_loss/perplexity = 4.53396702/93.1272659 secs/batch = 0.2657s, grad.norm=14.95758533
 14290: 10 [ 1020/ 1327], train_loss/perplexity = 4.63073397/102.5893326 secs/batch = 0.2651s, grad.norm=15.05424690
 14295: 10 [ 1025/ 1327], train_loss/perplexity = 4.47330952/87.6463089 secs/batch = 0.2632s, grad.norm=14.67750645
 14300: 10 [ 1030/ 1327], train_loss/perplexity = 4.23210478/68.8620224 secs/batch = 0.2602s, grad.norm=14.31695366
 14305: 10 [ 1035/ 1327], train_loss/perplexity = 4.19643974/66.4493332 secs/batch = 0.2661s, grad.norm=14.92133999
 14310: 10 [ 1040/ 1327], train_loss/perplexity = 4.41655111/82.8101883 secs/batch = 0.2664s, grad.norm=15.24235153
 14315: 10 [ 1045/ 1327], train_loss/perplexity = 3.92726254/50.7678108 secs/batch = 0.2668s, grad.norm=14.52730083
 14320: 10 [ 1050/ 1327], train_loss/perplexity = 4.04629230/57.1850395 secs/batch = 0.2660s, grad.norm=15.11869049
 14325: 10 [ 1055/ 1327], train_loss/perplexity = 4.09378719/59.9665680 secs/batch = 0.2653s, grad.norm=15.86148167
 14330: 10 [ 1060/ 1327], train_loss/perplexity = 3.74491382/42.3053627 secs/batch = 0.2672s, grad.norm=15.92493725
 14335: 10 [ 1065/ 1327], train_loss/perplexity = 3.94650173/51.7540016 secs/batch = 0.2669s, grad.norm=15.48247433
 14340: 10 [ 1070/ 1327], train_loss/perplexity = 4.25397062/70.3843307 secs/batch = 0.2664s, grad.norm=15.48174858
 14345: 10 [ 1075/ 1327], train_loss/perplexity = 4.04154634/56.9142838 secs/batch = 0.2584s, grad.norm=15.09537315
 14350: 10 [ 1080/ 1327], train_loss/perplexity = 3.99994850/54.5953369 secs/batch = 0.2666s, grad.norm=15.00322247
 14355: 10 [ 1085/ 1327], train_loss/perplexity = 3.82595301/45.8764992 secs/batch = 0.2658s, grad.norm=15.21212482
 14360: 10 [ 1090/ 1327], train_loss/perplexity = 4.03720903/56.6679649 secs/batch = 0.2668s, grad.norm=15.88473606
 14365: 10 [ 1095/ 1327], train_loss/perplexity = 4.17296982/64.9079285 secs/batch = 0.2665s, grad.norm=15.77521420
 14370: 10 [ 1100/ 1327], train_loss/perplexity = 3.86185908/47.5536766 secs/batch = 0.2658s, grad.norm=16.51538277
 14375: 10 [ 1105/ 1327], train_loss/perplexity = 3.93541884/51.1835823 secs/batch = 0.2648s, grad.norm=16.02770996
 14380: 10 [ 1110/ 1327], train_loss/perplexity = 4.18956709/65.9942169 secs/batch = 0.2656s, grad.norm=15.85138226
 14385: 10 [ 1115/ 1327], train_loss/perplexity = 4.04287624/56.9900246 secs/batch = 0.2655s, grad.norm=14.90233517
 14390: 10 [ 1120/ 1327], train_loss/perplexity = 4.30469179/74.0463867 secs/batch = 0.2653s, grad.norm=15.32567501
 14395: 10 [ 1125/ 1327], train_loss/perplexity = 4.41957235/83.0607605 secs/batch = 0.2665s, grad.norm=15.79921341
 14400: 10 [ 1130/ 1327], train_loss/perplexity = 4.05719376/57.8118477 secs/batch = 0.2659s, grad.norm=15.27195644
 14405: 10 [ 1135/ 1327], train_loss/perplexity = 4.12922478/62.1297417 secs/batch = 0.2656s, grad.norm=15.10505009
 14410: 10 [ 1140/ 1327], train_loss/perplexity = 4.41208839/82.4414520 secs/batch = 0.2658s, grad.norm=16.32665825
 14415: 10 [ 1145/ 1327], train_loss/perplexity = 4.12680960/61.9798660 secs/batch = 0.2659s, grad.norm=14.98088646
 14420: 10 [ 1150/ 1327], train_loss/perplexity = 4.12807226/62.0581741 secs/batch = 0.2663s, grad.norm=15.11014175
 14425: 10 [ 1155/ 1327], train_loss/perplexity = 4.24349689/69.6509857 secs/batch = 0.2658s, grad.norm=15.59495544
 14430: 10 [ 1160/ 1327], train_loss/perplexity = 4.15630770/63.8353882 secs/batch = 0.2670s, grad.norm=15.29716015
 14435: 10 [ 1165/ 1327], train_loss/perplexity = 4.15360069/63.6628189 secs/batch = 0.2647s, grad.norm=15.41145420
 14440: 10 [ 1170/ 1327], train_loss/perplexity = 4.05669641/57.7831039 secs/batch = 0.2659s, grad.norm=15.43017197
 14445: 10 [ 1175/ 1327], train_loss/perplexity = 3.90231609/49.5170021 secs/batch = 0.2662s, grad.norm=15.21822739
 14450: 10 [ 1180/ 1327], train_loss/perplexity = 3.89795637/49.3015938 secs/batch = 0.2606s, grad.norm=15.51735592
 14455: 10 [ 1185/ 1327], train_loss/perplexity = 4.05917454/57.9264755 secs/batch = 0.2664s, grad.norm=15.06164932
 14460: 10 [ 1190/ 1327], train_loss/perplexity = 4.16056728/64.1078796 secs/batch = 0.2656s, grad.norm=15.55723000
 14465: 10 [ 1195/ 1327], train_loss/perplexity = 4.04869652/57.3226891 secs/batch = 0.2660s, grad.norm=15.25345421
 14470: 10 [ 1200/ 1327], train_loss/perplexity = 3.93765259/51.2980423 secs/batch = 0.2637s, grad.norm=15.28818607
 14475: 10 [ 1205/ 1327], train_loss/perplexity = 3.94879961/51.8730621 secs/batch = 0.2659s, grad.norm=15.69400883
 14480: 10 [ 1210/ 1327], train_loss/perplexity = 3.65634394/38.7195244 secs/batch = 0.2661s, grad.norm=15.59002590
 14485: 10 [ 1215/ 1327], train_loss/perplexity = 3.84981370/46.9843102 secs/batch = 0.2620s, grad.norm=15.00007153
 14490: 10 [ 1220/ 1327], train_loss/perplexity = 3.98049235/53.5433884 secs/batch = 0.2657s, grad.norm=15.85213280
 14495: 10 [ 1225/ 1327], train_loss/perplexity = 3.67326379/39.3802261 secs/batch = 0.2652s, grad.norm=16.18084526
 14500: 10 [ 1230/ 1327], train_loss/perplexity = 3.99784994/54.4808884 secs/batch = 0.2654s, grad.norm=15.71965790
 14505: 10 [ 1235/ 1327], train_loss/perplexity = 3.96408749/52.6721840 secs/batch = 0.2654s, grad.norm=15.04261303
 14510: 10 [ 1240/ 1327], train_loss/perplexity = 4.23616600/69.1422501 secs/batch = 0.2654s, grad.norm=16.14521027
 14515: 10 [ 1245/ 1327], train_loss/perplexity = 4.07737541/58.9904404 secs/batch = 0.2652s, grad.norm=15.06310558
 14520: 10 [ 1250/ 1327], train_loss/perplexity = 4.18842459/65.9188614 secs/batch = 0.2656s, grad.norm=15.02488232
 14525: 10 [ 1255/ 1327], train_loss/perplexity = 4.25525570/70.4748383 secs/batch = 0.2646s, grad.norm=14.73079109
 14530: 10 [ 1260/ 1327], train_loss/perplexity = 4.03201675/56.3744888 secs/batch = 0.2651s, grad.norm=15.99048996
 14535: 10 [ 1265/ 1327], train_loss/perplexity = 4.30524445/74.0873260 secs/batch = 0.2667s, grad.norm=15.84783745
 14540: 10 [ 1270/ 1327], train_loss/perplexity = 4.00489235/54.8659172 secs/batch = 0.2673s, grad.norm=15.60092831
 14545: 10 [ 1275/ 1327], train_loss/perplexity = 4.20552158/67.0555649 secs/batch = 0.2673s, grad.norm=16.11896896
 14550: 10 [ 1280/ 1327], train_loss/perplexity = 3.99806595/54.4926567 secs/batch = 0.2644s, grad.norm=15.48432446
 14555: 10 [ 1285/ 1327], train_loss/perplexity = 3.90275955/49.5389671 secs/batch = 0.2667s, grad.norm=15.20592403
 14560: 10 [ 1290/ 1327], train_loss/perplexity = 4.16454077/64.3631210 secs/batch = 0.2655s, grad.norm=15.03976345
 14565: 10 [ 1295/ 1327], train_loss/perplexity = 4.12514544/61.8768082 secs/batch = 0.2669s, grad.norm=14.90744686
 14570: 10 [ 1300/ 1327], train_loss/perplexity = 4.28444815/72.5624924 secs/batch = 0.2637s, grad.norm=14.38772011
 14575: 10 [ 1305/ 1327], train_loss/perplexity = 4.34379053/76.9988556 secs/batch = 0.2608s, grad.norm=15.61151314
 14580: 10 [ 1310/ 1327], train_loss/perplexity = 4.64047956/103.5940170 secs/batch = 0.2674s, grad.norm=15.36870956
 14585: 10 [ 1315/ 1327], train_loss/perplexity = 4.43733978/84.5497208 secs/batch = 0.2652s, grad.norm=15.56005001
 14590: 10 [ 1320/ 1327], train_loss/perplexity = 4.41128397/82.3751602 secs/batch = 0.2660s, grad.norm=15.52858925
 14595: 10 [ 1325/ 1327], train_loss/perplexity = 4.36633539/78.7545013 secs/batch = 0.2657s, grad.norm=15.31972599
Epoch training time: 352.5586667060852
	> validation loss = 4.67290115, perplexity = 107.00773621
	> validation loss = 4.58537054, perplexity = 98.03950500
	> validation loss = 4.57728195, perplexity = 97.24970245
	> validation loss = 4.57737446, perplexity = 97.25870514
	> validation loss = 4.74274254, perplexity = 114.74847412
	> validation loss = 4.70877266, perplexity = 110.91594696
	> validation loss = 4.62974358, perplexity = 102.48777771
	> validation loss = 4.45897388, perplexity = 86.39881134
	> validation loss = 4.24752188, perplexity = 69.93190002
	> validation loss = 4.33709240, perplexity = 76.48482513
	> validation loss = 4.50765705, perplexity = 90.70904541
	> validation loss = 4.55163002, perplexity = 94.78678894
	> validation loss = 4.53502560, perplexity = 93.22589874
	> validation loss = 4.31973124, perplexity = 75.16842651
	> validation loss = 4.20210791, perplexity = 66.82704926
	> validation loss = 4.24929619, perplexity = 70.05609131
	> validation loss = 4.65574503, perplexity = 105.18756104
	> validation loss = 4.23769999, perplexity = 69.24839783
	> validation loss = 4.67406034, perplexity = 107.13185120
	> validation loss = 4.54737377, perplexity = 94.38420868
	> validation loss = 4.33322334, perplexity = 76.18947601
at the end of epoch: 10
train loss = 4.28439889, perplexity = 72.55891752
validation loss = 4.50020728, perplexity = 90.03579209
Saved model cv/epoch010_4.5002.model
 14602: 11 [    5/ 1327], train_loss/perplexity = 4.32224560/75.3576584 secs/batch = 0.2661s, grad.norm=15.36711597
 14607: 11 [   10/ 1327], train_loss/perplexity = 4.01066065/55.1833153 secs/batch = 0.2617s, grad.norm=15.22937107
 14612: 11 [   15/ 1327], train_loss/perplexity = 4.36134386/78.3623734 secs/batch = 0.2662s, grad.norm=14.30925369
 14617: 11 [   20/ 1327], train_loss/perplexity = 4.49789190/89.8275681 secs/batch = 0.2662s, grad.norm=14.94563961
 14622: 11 [   25/ 1327], train_loss/perplexity = 4.35323477/77.7294922 secs/batch = 0.2636s, grad.norm=15.77826691
 14627: 11 [   30/ 1327], train_loss/perplexity = 4.44865513/85.5118637 secs/batch = 0.2673s, grad.norm=15.16636944
 14632: 11 [   35/ 1327], train_loss/perplexity = 4.21061420/67.3979263 secs/batch = 0.2660s, grad.norm=14.53997612
 14637: 11 [   40/ 1327], train_loss/perplexity = 4.21471024/67.6745529 secs/batch = 0.2629s, grad.norm=14.93083477
 14642: 11 [   45/ 1327], train_loss/perplexity = 3.92668700/50.7386017 secs/batch = 0.2642s, grad.norm=14.26483059
 14647: 11 [   50/ 1327], train_loss/perplexity = 4.20774412/67.2047653 secs/batch = 0.2658s, grad.norm=15.39487648
 14652: 11 [   55/ 1327], train_loss/perplexity = 4.08749580/59.5904770 secs/batch = 0.2645s, grad.norm=15.40673828
 14657: 11 [   60/ 1327], train_loss/perplexity = 4.44762993/85.4242401 secs/batch = 0.2598s, grad.norm=15.50328445
 14662: 11 [   65/ 1327], train_loss/perplexity = 4.04351950/57.0266953 secs/batch = 0.2661s, grad.norm=14.92082977
 14667: 11 [   70/ 1327], train_loss/perplexity = 3.82135916/45.6662331 secs/batch = 0.2658s, grad.norm=15.11271572
 14672: 11 [   75/ 1327], train_loss/perplexity = 3.66747475/39.1529083 secs/batch = 0.2661s, grad.norm=14.59065914
 14677: 11 [   80/ 1327], train_loss/perplexity = 4.07486963/58.8428078 secs/batch = 0.2668s, grad.norm=15.23605728
 14682: 11 [   85/ 1327], train_loss/perplexity = 4.13632059/62.5721703 secs/batch = 0.2668s, grad.norm=15.45029163
 14687: 11 [   90/ 1327], train_loss/perplexity = 4.20359945/66.9267960 secs/batch = 0.2672s, grad.norm=15.67682743
 14692: 11 [   95/ 1327], train_loss/perplexity = 4.09515476/60.0486336 secs/batch = 0.2586s, grad.norm=15.27323151
 14697: 11 [  100/ 1327], train_loss/perplexity = 4.35734797/78.0498657 secs/batch = 0.2660s, grad.norm=15.16213512
 14702: 11 [  105/ 1327], train_loss/perplexity = 4.12640667/61.9548988 secs/batch = 0.2611s, grad.norm=16.04604149
 14707: 11 [  110/ 1327], train_loss/perplexity = 4.05837250/57.8800354 secs/batch = 0.2655s, grad.norm=15.33144474
 14712: 11 [  115/ 1327], train_loss/perplexity = 3.98794699/53.9440269 secs/batch = 0.2666s, grad.norm=15.52629662
 14717: 11 [  120/ 1327], train_loss/perplexity = 4.05702543/57.8021202 secs/batch = 0.2664s, grad.norm=15.63441181
 14722: 11 [  125/ 1327], train_loss/perplexity = 4.09645748/60.1269073 secs/batch = 0.2653s, grad.norm=15.87831974
 14727: 11 [  130/ 1327], train_loss/perplexity = 4.10805035/60.8280067 secs/batch = 0.2642s, grad.norm=16.53948784
 14732: 11 [  135/ 1327], train_loss/perplexity = 4.09123516/59.8137245 secs/batch = 0.2678s, grad.norm=15.26577759
 14737: 11 [  140/ 1327], train_loss/perplexity = 4.33566618/76.3758240 secs/batch = 0.2642s, grad.norm=15.88756180
 14742: 11 [  145/ 1327], train_loss/perplexity = 4.22808647/68.5858688 secs/batch = 0.2661s, grad.norm=15.95858097
 14747: 11 [  150/ 1327], train_loss/perplexity = 4.28846979/72.8548965 secs/batch = 0.2663s, grad.norm=15.98514080
 14752: 11 [  155/ 1327], train_loss/perplexity = 4.48093700/88.3173904 secs/batch = 0.2672s, grad.norm=15.37054634
 14757: 11 [  160/ 1327], train_loss/perplexity = 4.18775177/65.8745270 secs/batch = 0.2660s, grad.norm=14.59886265
 14762: 11 [  165/ 1327], train_loss/perplexity = 4.36691380/78.8000641 secs/batch = 0.2658s, grad.norm=15.15925312
 14767: 11 [  170/ 1327], train_loss/perplexity = 4.16893196/64.6463699 secs/batch = 0.2612s, grad.norm=14.92289639
 14772: 11 [  175/ 1327], train_loss/perplexity = 4.41572237/82.7415924 secs/batch = 0.2662s, grad.norm=15.36570072
 14777: 11 [  180/ 1327], train_loss/perplexity = 4.32515860/75.5774994 secs/batch = 0.2668s, grad.norm=16.08191109
 14782: 11 [  185/ 1327], train_loss/perplexity = 4.65349674/104.9513321 secs/batch = 0.2654s, grad.norm=15.56373596
 14787: 11 [  190/ 1327], train_loss/perplexity = 4.13337612/62.3881989 secs/batch = 0.2667s, grad.norm=14.31505299
 14792: 11 [  195/ 1327], train_loss/perplexity = 4.40206337/81.6191025 secs/batch = 0.2663s, grad.norm=14.65963745
 14797: 11 [  200/ 1327], train_loss/perplexity = 4.27987862/72.2316742 secs/batch = 0.2663s, grad.norm=15.32356358
 14802: 11 [  205/ 1327], train_loss/perplexity = 4.52661943/92.4455109 secs/batch = 0.2669s, grad.norm=15.11612988
 14807: 11 [  210/ 1327], train_loss/perplexity = 4.32255173/75.3807373 secs/batch = 0.2666s, grad.norm=14.49813271
 14812: 11 [  215/ 1327], train_loss/perplexity = 4.45551205/86.1002274 secs/batch = 0.2658s, grad.norm=15.00073528
 14817: 11 [  220/ 1327], train_loss/perplexity = 4.40637732/81.9719696 secs/batch = 0.2653s, grad.norm=14.92006588
 14822: 11 [  225/ 1327], train_loss/perplexity = 4.58394384/97.8997345 secs/batch = 0.2656s, grad.norm=15.05290127
 14827: 11 [  230/ 1327], train_loss/perplexity = 4.34941769/77.4333572 secs/batch = 0.2658s, grad.norm=15.54272652
 14832: 11 [  235/ 1327], train_loss/perplexity = 4.24435139/69.7105331 secs/batch = 0.2656s, grad.norm=15.13441372
 14837: 11 [  240/ 1327], train_loss/perplexity = 4.06222486/58.1034393 secs/batch = 0.2658s, grad.norm=15.65142822
 14842: 11 [  245/ 1327], train_loss/perplexity = 4.32129002/75.2856827 secs/batch = 0.2659s, grad.norm=15.13516617
 14847: 11 [  250/ 1327], train_loss/perplexity = 4.21932745/67.9877472 secs/batch = 0.2630s, grad.norm=14.67309570
 14852: 11 [  255/ 1327], train_loss/perplexity = 4.17804098/65.2379227 secs/batch = 0.2661s, grad.norm=15.50333023
 14857: 11 [  260/ 1327], train_loss/perplexity = 4.37418699/79.3752823 secs/batch = 0.2661s, grad.norm=15.60324287
 14862: 11 [  265/ 1327], train_loss/perplexity = 4.59520435/99.0083694 secs/batch = 0.2661s, grad.norm=15.26479912
 14867: 11 [  270/ 1327], train_loss/perplexity = 4.57047462/96.5899429 secs/batch = 0.2665s, grad.norm=14.92464733
 14872: 11 [  275/ 1327], train_loss/perplexity = 4.59205341/98.6968842 secs/batch = 0.2668s, grad.norm=14.72982216
 14877: 11 [  280/ 1327], train_loss/perplexity = 4.30012035/73.7086639 secs/batch = 0.2672s, grad.norm=15.03472137
 14882: 11 [  285/ 1327], train_loss/perplexity = 4.60488510/99.9714966 secs/batch = 0.2626s, grad.norm=14.97613621
 14887: 11 [  290/ 1327], train_loss/perplexity = 4.34213448/76.8714447 secs/batch = 0.2661s, grad.norm=15.87355423
 14892: 11 [  295/ 1327], train_loss/perplexity = 4.09398317/59.9783211 secs/batch = 0.2650s, grad.norm=15.14410973
 14897: 11 [  300/ 1327], train_loss/perplexity = 3.62403393/37.4884872 secs/batch = 0.2643s, grad.norm=14.77094460
 14902: 11 [  305/ 1327], train_loss/perplexity = 4.18967533/66.0013580 secs/batch = 0.2670s, grad.norm=15.08246326
 14907: 11 [  310/ 1327], train_loss/perplexity = 4.19523048/66.3690262 secs/batch = 0.2618s, grad.norm=15.00102425
 14912: 11 [  315/ 1327], train_loss/perplexity = 3.74963975/42.5057678 secs/batch = 0.2668s, grad.norm=14.85725403
 14917: 11 [  320/ 1327], train_loss/perplexity = 3.70052719/40.4686317 secs/batch = 0.2649s, grad.norm=16.00060463
 14922: 11 [  325/ 1327], train_loss/perplexity = 3.80953979/45.1296654 secs/batch = 0.2652s, grad.norm=15.01789379
 14927: 11 [  330/ 1327], train_loss/perplexity = 4.22158766/68.1415863 secs/batch = 0.2648s, grad.norm=15.42008400
 14932: 11 [  335/ 1327], train_loss/perplexity = 3.75938821/42.9221573 secs/batch = 0.2663s, grad.norm=14.77742577
 14937: 11 [  340/ 1327], train_loss/perplexity = 4.41546249/82.7200928 secs/batch = 0.2673s, grad.norm=14.85546207
 14942: 11 [  345/ 1327], train_loss/perplexity = 4.20497561/67.0189590 secs/batch = 0.2654s, grad.norm=15.05689621
 14947: 11 [  350/ 1327], train_loss/perplexity = 4.19780350/66.5400162 secs/batch = 0.2667s, grad.norm=15.66089344
 14952: 11 [  355/ 1327], train_loss/perplexity = 4.25292635/70.3108673 secs/batch = 0.2643s, grad.norm=15.40008450
 14957: 11 [  360/ 1327], train_loss/perplexity = 4.35961056/78.2266617 secs/batch = 0.2665s, grad.norm=16.63281631
 14962: 11 [  365/ 1327], train_loss/perplexity = 4.40122128/81.5503998 secs/batch = 0.2663s, grad.norm=15.13388252
 14967: 11 [  370/ 1327], train_loss/perplexity = 4.42930508/83.8731079 secs/batch = 0.2671s, grad.norm=15.03984833
 14972: 11 [  375/ 1327], train_loss/perplexity = 3.83522463/46.3038292 secs/batch = 0.2637s, grad.norm=14.95677280
 14977: 11 [  380/ 1327], train_loss/perplexity = 3.96779251/52.8676987 secs/batch = 0.2646s, grad.norm=15.90160370
 14982: 11 [  385/ 1327], train_loss/perplexity = 4.07574558/58.8943748 secs/batch = 0.2656s, grad.norm=15.63160706
 14987: 11 [  390/ 1327], train_loss/perplexity = 4.27079725/71.5786819 secs/batch = 0.2601s, grad.norm=15.51005077
 14992: 11 [  395/ 1327], train_loss/perplexity = 4.28357172/72.4989243 secs/batch = 0.2658s, grad.norm=15.46598816
 14997: 11 [  400/ 1327], train_loss/perplexity = 4.25069237/70.1539688 secs/batch = 0.2660s, grad.norm=15.16354847
 15002: 11 [  405/ 1327], train_loss/perplexity = 4.48121405/88.3418579 secs/batch = 0.2662s, grad.norm=15.90823269
 15007: 11 [  410/ 1327], train_loss/perplexity = 4.19149351/66.1214676 secs/batch = 0.2658s, grad.norm=15.40864944
 15012: 11 [  415/ 1327], train_loss/perplexity = 4.05263233/57.5487442 secs/batch = 0.2662s, grad.norm=15.00801754
 15017: 11 [  420/ 1327], train_loss/perplexity = 3.80410337/44.8849869 secs/batch = 0.2666s, grad.norm=15.41688251
 15022: 11 [  425/ 1327], train_loss/perplexity = 4.13662434/62.5911789 secs/batch = 0.2654s, grad.norm=16.02995300
 15027: 11 [  430/ 1327], train_loss/perplexity = 4.31832027/75.0624390 secs/batch = 0.2670s, grad.norm=15.60406017
 15032: 11 [  435/ 1327], train_loss/perplexity = 4.30167055/73.8230133 secs/batch = 0.2660s, grad.norm=15.83798027
 15037: 11 [  440/ 1327], train_loss/perplexity = 3.86110687/47.5179176 secs/batch = 0.2659s, grad.norm=15.65042877
 15042: 11 [  445/ 1327], train_loss/perplexity = 4.29371834/73.2382889 secs/batch = 0.2655s, grad.norm=16.02437019
 15047: 11 [  450/ 1327], train_loss/perplexity = 4.19547462/66.3852310 secs/batch = 0.2654s, grad.norm=15.74584198
 15052: 11 [  455/ 1327], train_loss/perplexity = 4.13079548/62.2274055 secs/batch = 0.2659s, grad.norm=14.94161701
 15057: 11 [  460/ 1327], train_loss/perplexity = 4.13109207/62.2458611 secs/batch = 0.2659s, grad.norm=16.16667175
 15062: 11 [  465/ 1327], train_loss/perplexity = 3.83490610/46.2890816 secs/batch = 0.2645s, grad.norm=16.18563652
 15067: 11 [  470/ 1327], train_loss/perplexity = 4.55181313/94.8041458 secs/batch = 0.2660s, grad.norm=14.76986694
 15072: 11 [  475/ 1327], train_loss/perplexity = 4.02315044/55.8768654 secs/batch = 0.2663s, grad.norm=15.42248058
 15077: 11 [  480/ 1327], train_loss/perplexity = 4.19292545/66.2162170 secs/batch = 0.2647s, grad.norm=15.35692883
 15082: 11 [  485/ 1327], train_loss/perplexity = 4.09359741/59.9551888 secs/batch = 0.2639s, grad.norm=15.37544441
 15087: 11 [  490/ 1327], train_loss/perplexity = 4.00199890/54.7073936 secs/batch = 0.2626s, grad.norm=16.37054062
 15092: 11 [  495/ 1327], train_loss/perplexity = 4.03718233/56.6664505 secs/batch = 0.2661s, grad.norm=15.19207764
 15097: 11 [  500/ 1327], train_loss/perplexity = 4.24443483/69.7163467 secs/batch = 0.2655s, grad.norm=16.30725861
 15102: 11 [  505/ 1327], train_loss/perplexity = 4.41192722/82.4281693 secs/batch = 0.2651s, grad.norm=14.54742241
 15107: 11 [  510/ 1327], train_loss/perplexity = 4.67286396/107.0037613 secs/batch = 0.2654s, grad.norm=14.60759258
 15112: 11 [  515/ 1327], train_loss/perplexity = 4.32946491/75.9036636 secs/batch = 0.2643s, grad.norm=14.90945148
 15117: 11 [  520/ 1327], train_loss/perplexity = 4.56580305/96.1397705 secs/batch = 0.2648s, grad.norm=15.55900288
 15122: 11 [  525/ 1327], train_loss/perplexity = 4.07651186/58.9395218 secs/batch = 0.2647s, grad.norm=15.38954735
 15127: 11 [  530/ 1327], train_loss/perplexity = 4.13916445/62.7503700 secs/batch = 0.2657s, grad.norm=15.84072399
 15132: 11 [  535/ 1327], train_loss/perplexity = 4.23182249/68.8425827 secs/batch = 0.2657s, grad.norm=15.53245449
 15137: 11 [  540/ 1327], train_loss/perplexity = 4.28722858/72.7645264 secs/batch = 0.2651s, grad.norm=15.16775131
 15142: 11 [  545/ 1327], train_loss/perplexity = 4.25928688/70.7595062 secs/batch = 0.2655s, grad.norm=15.58953381
 15147: 11 [  550/ 1327], train_loss/perplexity = 4.27990437/72.2335281 secs/batch = 0.2657s, grad.norm=15.49336338
 15152: 11 [  555/ 1327], train_loss/perplexity = 4.13419962/62.4395943 secs/batch = 0.2655s, grad.norm=15.28339958
 15157: 11 [  560/ 1327], train_loss/perplexity = 4.29116964/73.0518646 secs/batch = 0.2660s, grad.norm=16.50196266
 15162: 11 [  565/ 1327], train_loss/perplexity = 4.04974031/57.3825531 secs/batch = 0.2653s, grad.norm=16.00079727
 15167: 11 [  570/ 1327], train_loss/perplexity = 4.14785528/63.2980995 secs/batch = 0.2650s, grad.norm=16.55207634
 15172: 11 [  575/ 1327], train_loss/perplexity = 3.96014738/52.4650574 secs/batch = 0.2653s, grad.norm=16.09452820
 15177: 11 [  580/ 1327], train_loss/perplexity = 4.33009100/75.9511948 secs/batch = 0.2585s, grad.norm=15.92741013
 15182: 11 [  585/ 1327], train_loss/perplexity = 3.88455272/48.6451797 secs/batch = 0.2660s, grad.norm=15.20207882
 15187: 11 [  590/ 1327], train_loss/perplexity = 4.27999496/72.2400742 secs/batch = 0.2609s, grad.norm=15.41829300
 15192: 11 [  595/ 1327], train_loss/perplexity = 4.24647093/69.8584442 secs/batch = 0.2651s, grad.norm=15.73103428
 15197: 11 [  600/ 1327], train_loss/perplexity = 4.43786287/84.5939636 secs/batch = 0.2656s, grad.norm=14.58487988
 15202: 11 [  605/ 1327], train_loss/perplexity = 4.34447861/77.0518494 secs/batch = 0.2661s, grad.norm=15.32916546
 15207: 11 [  610/ 1327], train_loss/perplexity = 4.43423367/84.2875061 secs/batch = 0.2650s, grad.norm=15.27004147
 15212: 11 [  615/ 1327], train_loss/perplexity = 4.02720833/56.1040688 secs/batch = 0.2650s, grad.norm=15.01091480
 15217: 11 [  620/ 1327], train_loss/perplexity = 4.40823174/82.1241150 secs/batch = 0.2662s, grad.norm=15.75551891
 15222: 11 [  625/ 1327], train_loss/perplexity = 4.44192505/84.9382935 secs/batch = 0.2671s, grad.norm=15.30780697
 15227: 11 [  630/ 1327], train_loss/perplexity = 4.51020813/90.9407425 secs/batch = 0.2661s, grad.norm=15.28650475
 15232: 11 [  635/ 1327], train_loss/perplexity = 4.23016834/68.7287979 secs/batch = 0.2634s, grad.norm=15.49940395
 15237: 11 [  640/ 1327], train_loss/perplexity = 4.14208984/62.9342079 secs/batch = 0.2647s, grad.norm=15.41596031
 15242: 11 [  645/ 1327], train_loss/perplexity = 4.47911119/88.1562805 secs/batch = 0.2660s, grad.norm=16.53735542
 15247: 11 [  650/ 1327], train_loss/perplexity = 3.99299669/54.2171173 secs/batch = 0.2662s, grad.norm=15.92465210
 15252: 11 [  655/ 1327], train_loss/perplexity = 4.12856531/62.0887794 secs/batch = 0.2655s, grad.norm=15.27048969
 15257: 11 [  660/ 1327], train_loss/perplexity = 4.08571863/59.4846687 secs/batch = 0.2652s, grad.norm=15.59152794
 15262: 11 [  665/ 1327], train_loss/perplexity = 4.23752785/69.2364807 secs/batch = 0.2658s, grad.norm=15.61704254
 15267: 11 [  670/ 1327], train_loss/perplexity = 4.15171719/63.5430222 secs/batch = 0.2660s, grad.norm=15.65422630
 15272: 11 [  675/ 1327], train_loss/perplexity = 3.97250223/53.1172752 secs/batch = 0.2637s, grad.norm=16.32282639
 15277: 11 [  680/ 1327], train_loss/perplexity = 4.21041870/67.3847504 secs/batch = 0.2655s, grad.norm=16.34392166
 15282: 11 [  685/ 1327], train_loss/perplexity = 3.90547323/49.6735802 secs/batch = 0.2618s, grad.norm=15.41059971
 15287: 11 [  690/ 1327], train_loss/perplexity = 4.39434814/80.9918213 secs/batch = 0.2658s, grad.norm=15.44450188
 15292: 11 [  695/ 1327], train_loss/perplexity = 4.21580696/67.7488174 secs/batch = 0.2651s, grad.norm=15.42759800
 15297: 11 [  700/ 1327], train_loss/perplexity = 4.43221426/84.1174698 secs/batch = 0.2629s, grad.norm=15.96697426
 15302: 11 [  705/ 1327], train_loss/perplexity = 4.13238382/62.3263206 secs/batch = 0.2652s, grad.norm=14.86209965
 15307: 11 [  710/ 1327], train_loss/perplexity = 4.13392735/62.4225960 secs/batch = 0.2652s, grad.norm=15.93011951
 15312: 11 [  715/ 1327], train_loss/perplexity = 4.05194283/57.5090790 secs/batch = 0.2627s, grad.norm=16.04483604
 15317: 11 [  720/ 1327], train_loss/perplexity = 4.00173044/54.6927109 secs/batch = 0.2649s, grad.norm=15.83010864
 15322: 11 [  725/ 1327], train_loss/perplexity = 4.01364708/55.3483620 secs/batch = 0.2655s, grad.norm=15.64009285
 15327: 11 [  730/ 1327], train_loss/perplexity = 4.15697289/63.8778648 secs/batch = 0.2659s, grad.norm=15.95530605
 15332: 11 [  735/ 1327], train_loss/perplexity = 4.27297115/71.7344513 secs/batch = 0.2632s, grad.norm=16.52119064
 15337: 11 [  740/ 1327], train_loss/perplexity = 3.74845743/42.4555397 secs/batch = 0.2654s, grad.norm=14.96034718
 15342: 11 [  745/ 1327], train_loss/perplexity = 4.22485876/68.3648453 secs/batch = 0.2663s, grad.norm=15.95639896
 15347: 11 [  750/ 1327], train_loss/perplexity = 4.12911892/62.1231651 secs/batch = 0.2652s, grad.norm=15.36941910
 15352: 11 [  755/ 1327], train_loss/perplexity = 3.95303488/52.0932236 secs/batch = 0.2649s, grad.norm=15.15180874
 15357: 11 [  760/ 1327], train_loss/perplexity = 3.85260725/47.1157455 secs/batch = 0.2645s, grad.norm=14.86779594
 15362: 11 [  765/ 1327], train_loss/perplexity = 3.91366863/50.0823479 secs/batch = 0.2670s, grad.norm=14.97599220
 15367: 11 [  770/ 1327], train_loss/perplexity = 3.91403389/50.1006470 secs/batch = 0.2632s, grad.norm=15.28681564
 15372: 11 [  775/ 1327], train_loss/perplexity = 3.97877717/53.4516335 secs/batch = 0.2654s, grad.norm=15.65070629
 15377: 11 [  780/ 1327], train_loss/perplexity = 4.43466568/84.3239288 secs/batch = 0.2655s, grad.norm=15.63601112
 15382: 11 [  785/ 1327], train_loss/perplexity = 4.26212502/70.9606171 secs/batch = 0.2586s, grad.norm=16.39822578
 15387: 11 [  790/ 1327], train_loss/perplexity = 3.95772123/52.3379250 secs/batch = 0.2634s, grad.norm=16.02807045
 15392: 11 [  795/ 1327], train_loss/perplexity = 4.37240791/79.2341919 secs/batch = 0.2658s, grad.norm=15.92483997
 15397: 11 [  800/ 1327], train_loss/perplexity = 4.28455687/72.5703812 secs/batch = 0.2656s, grad.norm=15.80591488
 15402: 11 [  805/ 1327], train_loss/perplexity = 4.57461071/96.9902725 secs/batch = 0.2660s, grad.norm=15.07712269
 15407: 11 [  810/ 1327], train_loss/perplexity = 4.18713522/65.8339233 secs/batch = 0.2656s, grad.norm=14.59741974
 15412: 11 [  815/ 1327], train_loss/perplexity = 4.06381416/58.1958580 secs/batch = 0.2669s, grad.norm=14.91472816
 15417: 11 [  820/ 1327], train_loss/perplexity = 3.91228151/50.0129280 secs/batch = 0.2659s, grad.norm=14.33771038
 15422: 11 [  825/ 1327], train_loss/perplexity = 4.13157415/62.2758789 secs/batch = 0.2656s, grad.norm=15.68063831
 15427: 11 [  830/ 1327], train_loss/perplexity = 3.82296753/45.7397423 secs/batch = 0.2660s, grad.norm=15.70162868
 15432: 11 [  835/ 1327], train_loss/perplexity = 4.18477106/65.6784592 secs/batch = 0.2655s, grad.norm=16.07524300
 15437: 11 [  840/ 1327], train_loss/perplexity = 4.18498659/65.6926193 secs/batch = 0.2630s, grad.norm=15.77355671
 15442: 11 [  845/ 1327], train_loss/perplexity = 4.01995134/55.6983948 secs/batch = 0.2653s, grad.norm=15.60263824
 15447: 11 [  850/ 1327], train_loss/perplexity = 4.15199804/63.5608711 secs/batch = 0.2653s, grad.norm=15.39603233
 15452: 11 [  855/ 1327], train_loss/perplexity = 4.07485247/58.8418007 secs/batch = 0.2650s, grad.norm=15.84728336
 15457: 11 [  860/ 1327], train_loss/perplexity = 3.84156179/46.5981941 secs/batch = 0.2652s, grad.norm=14.88831711
 15462: 11 [  865/ 1327], train_loss/perplexity = 4.26145983/70.9134293 secs/batch = 0.2655s, grad.norm=15.62995911
 15467: 11 [  870/ 1327], train_loss/perplexity = 4.19249582/66.1877747 secs/batch = 0.2648s, grad.norm=15.89653206
 15472: 11 [  875/ 1327], train_loss/perplexity = 3.73382807/41.8389626 secs/batch = 0.2659s, grad.norm=14.89129543
 15477: 11 [  880/ 1327], train_loss/perplexity = 4.03390026/56.4807701 secs/batch = 0.2653s, grad.norm=15.23541355
 15482: 11 [  885/ 1327], train_loss/perplexity = 4.16504097/64.3953171 secs/batch = 0.2647s, grad.norm=15.08974266
 15487: 11 [  890/ 1327], train_loss/perplexity = 4.29007387/72.9718552 secs/batch = 0.2656s, grad.norm=15.12435246
 15492: 11 [  895/ 1327], train_loss/perplexity = 4.32390404/75.4827423 secs/batch = 0.2651s, grad.norm=15.24462414
 15497: 11 [  900/ 1327], train_loss/perplexity = 4.16754580/64.5568237 secs/batch = 0.2656s, grad.norm=15.04160213
 15502: 11 [  905/ 1327], train_loss/perplexity = 3.96030045/52.4730873 secs/batch = 0.2642s, grad.norm=14.80810356
 15507: 11 [  910/ 1327], train_loss/perplexity = 4.09381151/59.9680252 secs/batch = 0.2652s, grad.norm=14.72459126
 15512: 11 [  915/ 1327], train_loss/perplexity = 4.32798052/75.7910767 secs/batch = 0.2633s, grad.norm=15.23819923
 15517: 11 [  920/ 1327], train_loss/perplexity = 4.45546484/86.0961609 secs/batch = 0.2654s, grad.norm=15.40821648
 15522: 11 [  925/ 1327], train_loss/perplexity = 4.21477127/67.6786804 secs/batch = 0.2636s, grad.norm=15.66557693
 15527: 11 [  930/ 1327], train_loss/perplexity = 4.24002600/69.4096527 secs/batch = 0.2660s, grad.norm=15.08691120
 15532: 11 [  935/ 1327], train_loss/perplexity = 4.32828522/75.8141708 secs/batch = 0.2654s, grad.norm=15.11342525
 15537: 11 [  940/ 1327], train_loss/perplexity = 4.28759480/72.7911835 secs/batch = 0.2649s, grad.norm=14.91992760
 15542: 11 [  945/ 1327], train_loss/perplexity = 4.43974257/84.7531204 secs/batch = 0.2647s, grad.norm=14.86445618
 15547: 11 [  950/ 1327], train_loss/perplexity = 4.25334120/70.3400421 secs/batch = 0.2652s, grad.norm=15.29176807
 15552: 11 [  955/ 1327], train_loss/perplexity = 4.24089432/69.4699554 secs/batch = 0.2662s, grad.norm=15.77816868
 15557: 11 [  960/ 1327], train_loss/perplexity = 4.53024435/92.7812271 secs/batch = 0.2639s, grad.norm=15.61127758
 15562: 11 [  965/ 1327], train_loss/perplexity = 4.27742386/72.0545807 secs/batch = 0.2660s, grad.norm=15.25868988
 15567: 11 [  970/ 1327], train_loss/perplexity = 4.52277184/92.0905075 secs/batch = 0.2644s, grad.norm=15.39418983
 15572: 11 [  975/ 1327], train_loss/perplexity = 4.24105215/69.4809189 secs/batch = 0.2661s, grad.norm=16.60927200
 15577: 11 [  980/ 1327], train_loss/perplexity = 4.05995607/57.9717636 secs/batch = 0.2652s, grad.norm=15.19721413
 15582: 11 [  985/ 1327], train_loss/perplexity = 4.13151264/62.2720451 secs/batch = 0.2662s, grad.norm=15.72982788
 15587: 11 [  990/ 1327], train_loss/perplexity = 4.37913704/79.7691650 secs/batch = 0.2647s, grad.norm=15.65582561
 15592: 11 [  995/ 1327], train_loss/perplexity = 4.37567377/79.4933853 secs/batch = 0.2648s, grad.norm=14.89984608
 15597: 11 [ 1000/ 1327], train_loss/perplexity = 3.90124869/49.4641762 secs/batch = 0.2613s, grad.norm=14.73558426
 15602: 11 [ 1005/ 1327], train_loss/perplexity = 4.40700960/82.0238113 secs/batch = 0.2619s, grad.norm=15.60985470
 15607: 11 [ 1010/ 1327], train_loss/perplexity = 3.97857332/53.4407387 secs/batch = 0.2662s, grad.norm=14.77114582
 15612: 11 [ 1015/ 1327], train_loss/perplexity = 4.51856804/91.7041855 secs/batch = 0.2664s, grad.norm=15.47605896
 15617: 11 [ 1020/ 1327], train_loss/perplexity = 4.51076794/90.9916687 secs/batch = 0.2662s, grad.norm=15.36075211
 15622: 11 [ 1025/ 1327], train_loss/perplexity = 4.49351835/89.4355621 secs/batch = 0.2661s, grad.norm=15.42794991
 15627: 11 [ 1030/ 1327], train_loss/perplexity = 4.14867496/63.3500023 secs/batch = 0.2657s, grad.norm=14.58692646
 15632: 11 [ 1035/ 1327], train_loss/perplexity = 4.13510036/62.4958649 secs/batch = 0.2651s, grad.norm=15.11885357
 15637: 11 [ 1040/ 1327], train_loss/perplexity = 4.36684513/78.7946548 secs/batch = 0.2647s, grad.norm=15.61207294
 15642: 11 [ 1045/ 1327], train_loss/perplexity = 3.88262391/48.5514412 secs/batch = 0.2651s, grad.norm=14.53744793
 15647: 11 [ 1050/ 1327], train_loss/perplexity = 3.98980618/54.0444145 secs/batch = 0.2652s, grad.norm=15.31062222
 15652: 11 [ 1055/ 1327], train_loss/perplexity = 4.06939220/58.5213814 secs/batch = 0.2657s, grad.norm=16.07494736
 15657: 11 [ 1060/ 1327], train_loss/perplexity = 3.75742102/42.8378067 secs/batch = 0.2654s, grad.norm=16.41985893
 15662: 11 [ 1065/ 1327], train_loss/perplexity = 3.86411238/47.6609497 secs/batch = 0.2653s, grad.norm=15.55515099
 15667: 11 [ 1070/ 1327], train_loss/perplexity = 4.26602364/71.2378006 secs/batch = 0.2637s, grad.norm=16.01955605
 15672: 11 [ 1075/ 1327], train_loss/perplexity = 3.94296026/51.5710411 secs/batch = 0.2655s, grad.norm=15.50390339
 15677: 11 [ 1080/ 1327], train_loss/perplexity = 3.93385124/51.1034126 secs/batch = 0.2655s, grad.norm=15.50647545
 15682: 11 [ 1085/ 1327], train_loss/perplexity = 3.85376930/47.1705284 secs/batch = 0.2653s, grad.norm=15.67733192
 15687: 11 [ 1090/ 1327], train_loss/perplexity = 3.97518873/53.2601662 secs/batch = 0.2651s, grad.norm=16.44835854
 15692: 11 [ 1095/ 1327], train_loss/perplexity = 4.09506702/60.0433617 secs/batch = 0.2649s, grad.norm=16.02993393
 15697: 11 [ 1100/ 1327], train_loss/perplexity = 3.85388684/47.1760750 secs/batch = 0.2648s, grad.norm=17.00958061
 15702: 11 [ 1105/ 1327], train_loss/perplexity = 3.88272285/48.5562477 secs/batch = 0.2593s, grad.norm=16.04483795
 15707: 11 [ 1110/ 1327], train_loss/perplexity = 4.16529369/64.4115982 secs/batch = 0.2655s, grad.norm=16.23215485
 15712: 11 [ 1115/ 1327], train_loss/perplexity = 3.91680932/50.2398911 secs/batch = 0.2651s, grad.norm=15.14625359
 15717: 11 [ 1120/ 1327], train_loss/perplexity = 4.22907591/68.6537628 secs/batch = 0.2653s, grad.norm=15.42112827
 15722: 11 [ 1125/ 1327], train_loss/perplexity = 4.37692928/79.5932465 secs/batch = 0.2656s, grad.norm=16.18445969
 15727: 11 [ 1130/ 1327], train_loss/perplexity = 4.07625437/58.9243469 secs/batch = 0.2662s, grad.norm=15.48480892
 15732: 11 [ 1135/ 1327], train_loss/perplexity = 4.04913521/57.3478432 secs/batch = 0.2659s, grad.norm=15.77130032
 15737: 11 [ 1140/ 1327], train_loss/perplexity = 4.27125120/71.6111832 secs/batch = 0.2662s, grad.norm=16.23353386
 15742: 11 [ 1145/ 1327], train_loss/perplexity = 4.12849474/62.0844002 secs/batch = 0.2669s, grad.norm=15.61794472
 15747: 11 [ 1150/ 1327], train_loss/perplexity = 4.09771919/60.2028198 secs/batch = 0.2665s, grad.norm=15.86302090
 15752: 11 [ 1155/ 1327], train_loss/perplexity = 4.13013744/62.1864700 secs/batch = 0.2649s, grad.norm=15.83445072
 15757: 11 [ 1160/ 1327], train_loss/perplexity = 4.13353920/62.3983727 secs/batch = 0.2658s, grad.norm=15.76753712
 15762: 11 [ 1165/ 1327], train_loss/perplexity = 4.24881124/70.0221252 secs/batch = 0.2624s, grad.norm=16.26112175
 15767: 11 [ 1170/ 1327], train_loss/perplexity = 4.01751280/55.5627365 secs/batch = 0.2659s, grad.norm=15.07493401
 15772: 11 [ 1175/ 1327], train_loss/perplexity = 3.84890628/46.9416924 secs/batch = 0.2642s, grad.norm=15.85169029
 15777: 11 [ 1180/ 1327], train_loss/perplexity = 3.87571812/48.2173119 secs/batch = 0.2635s, grad.norm=15.95168495
 15782: 11 [ 1185/ 1327], train_loss/perplexity = 4.05863142/57.8950233 secs/batch = 0.2656s, grad.norm=15.86171341
 15787: 11 [ 1190/ 1327], train_loss/perplexity = 4.20159483/66.7927704 secs/batch = 0.2658s, grad.norm=16.73056221
 15792: 11 [ 1195/ 1327], train_loss/perplexity = 3.95193815/52.0361214 secs/batch = 0.2648s, grad.norm=15.64665794
 15797: 11 [ 1200/ 1327], train_loss/perplexity = 3.88899732/48.8618698 secs/batch = 0.2652s, grad.norm=15.43215752
 15802: 11 [ 1205/ 1327], train_loss/perplexity = 3.93424344/51.1234589 secs/batch = 0.2650s, grad.norm=16.24109840
 15807: 11 [ 1210/ 1327], train_loss/perplexity = 3.55582762/35.0167885 secs/batch = 0.2660s, grad.norm=15.81113815
 15812: 11 [ 1215/ 1327], train_loss/perplexity = 3.78013778/43.8220787 secs/batch = 0.2655s, grad.norm=15.39229870
 15817: 11 [ 1220/ 1327], train_loss/perplexity = 3.95043468/51.9579468 secs/batch = 0.2657s, grad.norm=16.43006134
 15822: 11 [ 1225/ 1327], train_loss/perplexity = 3.65798521/38.7831230 secs/batch = 0.2656s, grad.norm=16.53216934
 15827: 11 [ 1230/ 1327], train_loss/perplexity = 3.97436166/53.2161369 secs/batch = 0.2654s, grad.norm=15.24282932
 15832: 11 [ 1235/ 1327], train_loss/perplexity = 3.85423517/47.1925087 secs/batch = 0.2664s, grad.norm=15.25394344
 15837: 11 [ 1240/ 1327], train_loss/perplexity = 4.12415648/61.8156433 secs/batch = 0.2656s, grad.norm=16.22665024
 15842: 11 [ 1245/ 1327], train_loss/perplexity = 4.05349350/57.5983238 secs/batch = 0.2660s, grad.norm=15.08926678
 15847: 11 [ 1250/ 1327], train_loss/perplexity = 4.18077230/65.4163513 secs/batch = 0.2656s, grad.norm=15.36490822
 15852: 11 [ 1255/ 1327], train_loss/perplexity = 4.22893858/68.6443329 secs/batch = 0.2589s, grad.norm=14.94842529
 15857: 11 [ 1260/ 1327], train_loss/perplexity = 3.94291019/51.5684586 secs/batch = 0.2647s, grad.norm=15.96071815
 15862: 11 [ 1265/ 1327], train_loss/perplexity = 4.18177414/65.4819260 secs/batch = 0.2602s, grad.norm=15.63226223
 15867: 11 [ 1270/ 1327], train_loss/perplexity = 3.94716406/51.7882881 secs/batch = 0.2658s, grad.norm=15.69050884
 15872: 11 [ 1275/ 1327], train_loss/perplexity = 4.14522600/63.1318893 secs/batch = 0.2656s, grad.norm=15.89223957
 15877: 11 [ 1280/ 1327], train_loss/perplexity = 3.99278450/54.2056160 secs/batch = 0.2666s, grad.norm=16.00012970
 15882: 11 [ 1285/ 1327], train_loss/perplexity = 3.89209461/49.0134430 secs/batch = 0.2660s, grad.norm=15.76449108
 15887: 11 [ 1290/ 1327], train_loss/perplexity = 4.11273813/61.1138268 secs/batch = 0.2648s, grad.norm=15.40379620
 15892: 11 [ 1295/ 1327], train_loss/perplexity = 4.16760826/64.5608521 secs/batch = 0.2642s, grad.norm=15.52948093
 15897: 11 [ 1300/ 1327], train_loss/perplexity = 4.28238916/72.4132385 secs/batch = 0.2635s, grad.norm=15.27032948
 15902: 11 [ 1305/ 1327], train_loss/perplexity = 4.32326841/75.4347763 secs/batch = 0.2660s, grad.norm=16.20474052
 15907: 11 [ 1310/ 1327], train_loss/perplexity = 4.54203463/93.8816223 secs/batch = 0.2650s, grad.norm=15.88731384
 15912: 11 [ 1315/ 1327], train_loss/perplexity = 4.35051346/77.5182571 secs/batch = 0.2641s, grad.norm=16.06557465
 15917: 11 [ 1320/ 1327], train_loss/perplexity = 4.40821743/82.1229401 secs/batch = 0.2654s, grad.norm=15.68138885
 15922: 11 [ 1325/ 1327], train_loss/perplexity = 4.31249523/74.6264648 secs/batch = 0.2650s, grad.norm=16.06915665
Epoch training time: 352.04691529273987
	> validation loss = 4.64214230, perplexity = 103.76641083
	> validation loss = 4.59184647, perplexity = 98.67646790
	> validation loss = 4.55988550, perplexity = 95.57253265
	> validation loss = 4.58458424, perplexity = 97.96244812
	> validation loss = 4.70426941, perplexity = 110.41758728
	> validation loss = 4.67117786, perplexity = 106.82349396
	> validation loss = 4.61922646, perplexity = 101.41555023
	> validation loss = 4.42772150, perplexity = 83.74039459
	> validation loss = 4.24290848, perplexity = 69.61001587
	> validation loss = 4.33533096, perplexity = 76.35022736
	> validation loss = 4.52140808, perplexity = 91.96500397
	> validation loss = 4.55020905, perplexity = 94.65219116
	> validation loss = 4.50698853, perplexity = 90.64842224
	> validation loss = 4.27479124, perplexity = 71.86513519
	> validation loss = 4.20962906, perplexity = 67.33155823
	> validation loss = 4.24460840, perplexity = 69.72844696
	> validation loss = 4.65590858, perplexity = 105.20476532
	> validation loss = 4.21942520, perplexity = 67.99439240
	> validation loss = 4.66454697, perplexity = 106.11750031
	> validation loss = 4.52115011, perplexity = 91.94127655
	> validation loss = 4.31664610, perplexity = 74.93687439
at the end of epoch: 11
train loss = 4.23925961, perplexity = 69.35648198
validation loss = 4.48680280, perplexity = 88.83696185
Saved model cv/epoch011_4.4868.model
 15929: 12 [    5/ 1327], train_loss/perplexity = 4.36499834/78.6492691 secs/batch = 0.2660s, grad.norm=15.68471050
 15934: 12 [   10/ 1327], train_loss/perplexity = 3.94431973/51.6411972 secs/batch = 0.2652s, grad.norm=15.01060581
 15939: 12 [   15/ 1327], train_loss/perplexity = 4.27919245/72.1821289 secs/batch = 0.2650s, grad.norm=14.91975689
 15944: 12 [   20/ 1327], train_loss/perplexity = 4.44448996/85.1564331 secs/batch = 0.2661s, grad.norm=15.07642937
 15949: 12 [   25/ 1327], train_loss/perplexity = 4.30290222/73.9139938 secs/batch = 0.2620s, grad.norm=15.85649776
 15954: 12 [   30/ 1327], train_loss/perplexity = 4.32509041/75.5723419 secs/batch = 0.2626s, grad.norm=15.62985611
 15959: 12 [   35/ 1327], train_loss/perplexity = 4.17990685/65.3597641 secs/batch = 0.2661s, grad.norm=15.63968658
 15964: 12 [   40/ 1327], train_loss/perplexity = 4.12260580/61.7198639 secs/batch = 0.2655s, grad.norm=15.62878513
 15969: 12 [   45/ 1327], train_loss/perplexity = 3.92454386/50.6299782 secs/batch = 0.2651s, grad.norm=14.71386147
 15974: 12 [   50/ 1327], train_loss/perplexity = 4.12944078/62.1431618 secs/batch = 0.2648s, grad.norm=15.46880627
 15979: 12 [   55/ 1327], train_loss/perplexity = 4.03140926/56.3402519 secs/batch = 0.2646s, grad.norm=15.78345776
 15984: 12 [   60/ 1327], train_loss/perplexity = 4.37822247/79.6962433 secs/batch = 0.2633s, grad.norm=16.44944954
 15989: 12 [   65/ 1327], train_loss/perplexity = 4.01205206/55.2601509 secs/batch = 0.2655s, grad.norm=15.54715633
 15994: 12 [   70/ 1327], train_loss/perplexity = 3.80872083/45.0927200 secs/batch = 0.2662s, grad.norm=15.57311440
 15999: 12 [   75/ 1327], train_loss/perplexity = 3.55585027/35.0175819 secs/batch = 0.2653s, grad.norm=15.03260994
 16004: 12 [   80/ 1327], train_loss/perplexity = 4.02227974/55.8282356 secs/batch = 0.2658s, grad.norm=15.47867584
 16009: 12 [   85/ 1327], train_loss/perplexity = 4.13330317/62.3836479 secs/batch = 0.2653s, grad.norm=15.95442772
 16014: 12 [   90/ 1327], train_loss/perplexity = 4.15848923/63.9747963 secs/batch = 0.2651s, grad.norm=15.86742496
 16019: 12 [   95/ 1327], train_loss/perplexity = 4.07926035/59.1017380 secs/batch = 0.2660s, grad.norm=15.57887936
 16024: 12 [  100/ 1327], train_loss/perplexity = 4.30463123/74.0419083 secs/batch = 0.2662s, grad.norm=15.56652164
 16029: 12 [  105/ 1327], train_loss/perplexity = 4.08828783/59.6376953 secs/batch = 0.2653s, grad.norm=16.67218399
 16034: 12 [  110/ 1327], train_loss/perplexity = 3.93889904/51.3620224 secs/batch = 0.2650s, grad.norm=15.64910793
 16039: 12 [  115/ 1327], train_loss/perplexity = 3.90840411/49.8193817 secs/batch = 0.2634s, grad.norm=16.11058617
 16044: 12 [  120/ 1327], train_loss/perplexity = 4.05638599/57.7651711 secs/batch = 0.2654s, grad.norm=16.25425911
 16049: 12 [  125/ 1327], train_loss/perplexity = 4.09229851/59.8773613 secs/batch = 0.2666s, grad.norm=16.14832878
 16054: 12 [  130/ 1327], train_loss/perplexity = 4.05525732/57.7000084 secs/batch = 0.2655s, grad.norm=16.54013824
 16059: 12 [  135/ 1327], train_loss/perplexity = 4.08465624/59.4215088 secs/batch = 0.2666s, grad.norm=15.53594589
 16064: 12 [  140/ 1327], train_loss/perplexity = 4.37193441/79.1966858 secs/batch = 0.2671s, grad.norm=16.39905739
 16069: 12 [  145/ 1327], train_loss/perplexity = 4.21045446/67.3871613 secs/batch = 0.2658s, grad.norm=16.59104347
 16074: 12 [  150/ 1327], train_loss/perplexity = 4.28400040/72.5300064 secs/batch = 0.2679s, grad.norm=16.51804733
 16079: 12 [  155/ 1327], train_loss/perplexity = 4.50900126/90.8310547 secs/batch = 0.2671s, grad.norm=15.65535831
 16084: 12 [  160/ 1327], train_loss/perplexity = 4.15985966/64.0625305 secs/batch = 0.2652s, grad.norm=15.23047829
 16089: 12 [  165/ 1327], train_loss/perplexity = 4.35250378/77.6726913 secs/batch = 0.2654s, grad.norm=15.49605274
 16094: 12 [  170/ 1327], train_loss/perplexity = 4.16660833/64.4963303 secs/batch = 0.2660s, grad.norm=15.06546593
 16099: 12 [  175/ 1327], train_loss/perplexity = 4.34331703/76.9624023 secs/batch = 0.2640s, grad.norm=15.80515099
 16104: 12 [  180/ 1327], train_loss/perplexity = 4.23266220/68.9004135 secs/batch = 0.2656s, grad.norm=16.06489182
 16109: 12 [  185/ 1327], train_loss/perplexity = 4.55104065/94.7309418 secs/batch = 0.2633s, grad.norm=15.89782238
 16114: 12 [  190/ 1327], train_loss/perplexity = 4.14213848/62.9372673 secs/batch = 0.2655s, grad.norm=14.80083942
 16119: 12 [  195/ 1327], train_loss/perplexity = 4.36815548/78.8979721 secs/batch = 0.2658s, grad.norm=14.83935833
 16124: 12 [  200/ 1327], train_loss/perplexity = 4.19101429/66.0897903 secs/batch = 0.2664s, grad.norm=15.93812084
 16129: 12 [  205/ 1327], train_loss/perplexity = 4.46818876/87.1986389 secs/batch = 0.2656s, grad.norm=15.94641209
 16134: 12 [  210/ 1327], train_loss/perplexity = 4.30705643/74.2216873 secs/batch = 0.2660s, grad.norm=15.19201279
 16139: 12 [  215/ 1327], train_loss/perplexity = 4.47156191/87.4932709 secs/batch = 0.2677s, grad.norm=15.14965153
 16144: 12 [  220/ 1327], train_loss/perplexity = 4.32263613/75.3871002 secs/batch = 0.2667s, grad.norm=15.29226398
 16149: 12 [  225/ 1327], train_loss/perplexity = 4.47968197/88.2066193 secs/batch = 0.2656s, grad.norm=15.66281319
 16154: 12 [  230/ 1327], train_loss/perplexity = 4.24779987/69.9513397 secs/batch = 0.2657s, grad.norm=16.23353004
 16159: 12 [  235/ 1327], train_loss/perplexity = 4.23408842/68.9987488 secs/batch = 0.2657s, grad.norm=15.60783863
 16164: 12 [  240/ 1327], train_loss/perplexity = 3.94288969/51.5674019 secs/batch = 0.2638s, grad.norm=15.67013550
 16169: 12 [  245/ 1327], train_loss/perplexity = 4.30519104/74.0833664 secs/batch = 0.2607s, grad.norm=15.91267490
 16174: 12 [  250/ 1327], train_loss/perplexity = 4.09275293/59.9045792 secs/batch = 0.2660s, grad.norm=15.13805676
 16179: 12 [  255/ 1327], train_loss/perplexity = 4.09632969/60.1192245 secs/batch = 0.2632s, grad.norm=15.75751019
 16184: 12 [  260/ 1327], train_loss/perplexity = 4.29430485/73.2812576 secs/batch = 0.2659s, grad.norm=16.03473473
 16189: 12 [  265/ 1327], train_loss/perplexity = 4.46019173/86.5040894 secs/batch = 0.2656s, grad.norm=15.22540760
 16194: 12 [  270/ 1327], train_loss/perplexity = 4.53469419/93.1950150 secs/batch = 0.2660s, grad.norm=15.22947979
 16199: 12 [  275/ 1327], train_loss/perplexity = 4.56122541/95.7006836 secs/batch = 0.2660s, grad.norm=15.39088726
 16204: 12 [  280/ 1327], train_loss/perplexity = 4.31553173/74.8534164 secs/batch = 0.2647s, grad.norm=15.26811314
 16209: 12 [  285/ 1327], train_loss/perplexity = 4.58338690/97.8452225 secs/batch = 0.2652s, grad.norm=15.72692680
 16214: 12 [  290/ 1327], train_loss/perplexity = 4.28734827/72.7732391 secs/batch = 0.2659s, grad.norm=15.85870171
 16219: 12 [  295/ 1327], train_loss/perplexity = 4.09721184/60.1722832 secs/batch = 0.2652s, grad.norm=15.81620407
 16224: 12 [  300/ 1327], train_loss/perplexity = 3.61439824/37.1289978 secs/batch = 0.2650s, grad.norm=15.01480961
 16229: 12 [  305/ 1327], train_loss/perplexity = 4.14236593/62.9515839 secs/batch = 0.2649s, grad.norm=15.35876942
 16234: 12 [  310/ 1327], train_loss/perplexity = 4.06925201/58.5131798 secs/batch = 0.2634s, grad.norm=15.44339371
 16239: 12 [  315/ 1327], train_loss/perplexity = 3.66645598/39.1130409 secs/batch = 0.2651s, grad.norm=15.01503372
 16244: 12 [  320/ 1327], train_loss/perplexity = 3.62073016/37.3648415 secs/batch = 0.2653s, grad.norm=16.03485489
 16249: 12 [  325/ 1327], train_loss/perplexity = 3.68324590/39.7752914 secs/batch = 0.2636s, grad.norm=15.49908352
 16254: 12 [  330/ 1327], train_loss/perplexity = 4.27481556/71.8668823 secs/batch = 0.2665s, grad.norm=16.18127060
 16259: 12 [  335/ 1327], train_loss/perplexity = 3.72355175/41.4112167 secs/batch = 0.2651s, grad.norm=15.05857468
 16264: 12 [  340/ 1327], train_loss/perplexity = 4.40826797/82.1270905 secs/batch = 0.2650s, grad.norm=15.49755478
 16269: 12 [  345/ 1327], train_loss/perplexity = 4.24313736/69.6259537 secs/batch = 0.2658s, grad.norm=15.67813873
 16274: 12 [  350/ 1327], train_loss/perplexity = 4.17835569/65.2584610 secs/batch = 0.2656s, grad.norm=16.35460854
 16279: 12 [  355/ 1327], train_loss/perplexity = 4.17063141/64.7563248 secs/batch = 0.2658s, grad.norm=15.59564114
 16284: 12 [  360/ 1327], train_loss/perplexity = 4.36246204/78.4500427 secs/batch = 0.2659s, grad.norm=16.88431549
 16289: 12 [  365/ 1327], train_loss/perplexity = 4.35149956/77.5947342 secs/batch = 0.2655s, grad.norm=15.98446941
 16294: 12 [  370/ 1327], train_loss/perplexity = 4.43125629/84.0369263 secs/batch = 0.2670s, grad.norm=15.99253178
 16299: 12 [  375/ 1327], train_loss/perplexity = 3.79895496/44.6544952 secs/batch = 0.2663s, grad.norm=15.63124084
 16304: 12 [  380/ 1327], train_loss/perplexity = 3.91900539/50.3503418 secs/batch = 0.2660s, grad.norm=16.28418541
 16309: 12 [  385/ 1327], train_loss/perplexity = 4.05616903/57.7526398 secs/batch = 0.2648s, grad.norm=16.36067963
 16314: 12 [  390/ 1327], train_loss/perplexity = 4.15169144/63.5413857 secs/batch = 0.2665s, grad.norm=15.47038841
 16319: 12 [  395/ 1327], train_loss/perplexity = 4.21083212/67.4126129 secs/batch = 0.2655s, grad.norm=15.57319736
 16324: 12 [  400/ 1327], train_loss/perplexity = 4.19249249/66.1875534 secs/batch = 0.2660s, grad.norm=15.50022030
 16329: 12 [  405/ 1327], train_loss/perplexity = 4.48882675/89.0169449 secs/batch = 0.2659s, grad.norm=16.18164635
 16334: 12 [  410/ 1327], train_loss/perplexity = 4.14030600/62.8220406 secs/batch = 0.2655s, grad.norm=15.65060425
 16339: 12 [  415/ 1327], train_loss/perplexity = 4.07309341/58.7383842 secs/batch = 0.2629s, grad.norm=15.22190571
 16344: 12 [  420/ 1327], train_loss/perplexity = 3.73630285/41.9426346 secs/batch = 0.2654s, grad.norm=15.78557014
 16349: 12 [  425/ 1327], train_loss/perplexity = 3.95452476/52.1708946 secs/batch = 0.2651s, grad.norm=16.60120201
 16354: 12 [  430/ 1327], train_loss/perplexity = 4.21545124/67.7247162 secs/batch = 0.2657s, grad.norm=16.31222916
 16359: 12 [  435/ 1327], train_loss/perplexity = 4.25217342/70.2579498 secs/batch = 0.2654s, grad.norm=16.68564415
 16364: 12 [  440/ 1327], train_loss/perplexity = 3.87445021/48.1562157 secs/batch = 0.2652s, grad.norm=15.80451012
 16369: 12 [  445/ 1327], train_loss/perplexity = 4.20295858/66.8839188 secs/batch = 0.2648s, grad.norm=15.96230793
 16374: 12 [  450/ 1327], train_loss/perplexity = 4.11165094/61.0474205 secs/batch = 0.2632s, grad.norm=15.71074200
 16379: 12 [  455/ 1327], train_loss/perplexity = 4.08184099/59.2544556 secs/batch = 0.2653s, grad.norm=15.76539230
 16384: 12 [  460/ 1327], train_loss/perplexity = 4.07987022/59.1377945 secs/batch = 0.2656s, grad.norm=16.01984024
 16389: 12 [  465/ 1327], train_loss/perplexity = 3.81472969/45.3644905 secs/batch = 0.2639s, grad.norm=16.84560776
 16394: 12 [  470/ 1327], train_loss/perplexity = 4.49611187/89.6678162 secs/batch = 0.2653s, grad.norm=15.63745689
 16399: 12 [  475/ 1327], train_loss/perplexity = 3.95937729/52.4246712 secs/batch = 0.2642s, grad.norm=15.95974255
 16404: 12 [  480/ 1327], train_loss/perplexity = 4.11609364/61.3192368 secs/batch = 0.2640s, grad.norm=15.54680920
 16409: 12 [  485/ 1327], train_loss/perplexity = 4.12209797/61.6885262 secs/batch = 0.2651s, grad.norm=16.08661270
 16414: 12 [  490/ 1327], train_loss/perplexity = 3.90604615/49.7020493 secs/batch = 0.2623s, grad.norm=16.62915039
 16419: 12 [  495/ 1327], train_loss/perplexity = 3.98122501/53.5826340 secs/batch = 0.2652s, grad.norm=15.48761749
 16424: 12 [  500/ 1327], train_loss/perplexity = 4.16492653/64.3879547 secs/batch = 0.2646s, grad.norm=16.10863686
 16429: 12 [  505/ 1327], train_loss/perplexity = 4.28547716/72.6371994 secs/batch = 0.2624s, grad.norm=15.02859497
 16434: 12 [  510/ 1327], train_loss/perplexity = 4.61711550/101.2016907 secs/batch = 0.2667s, grad.norm=14.94277477
 16439: 12 [  515/ 1327], train_loss/perplexity = 4.22293568/68.2335052 secs/batch = 0.2658s, grad.norm=15.01899624
 16444: 12 [  520/ 1327], train_loss/perplexity = 4.41918850/83.0288773 secs/batch = 0.2660s, grad.norm=15.90063381
 16449: 12 [  525/ 1327], train_loss/perplexity = 4.08653688/59.5333633 secs/batch = 0.2656s, grad.norm=15.98632526
 16454: 12 [  530/ 1327], train_loss/perplexity = 4.07465363/58.8301010 secs/batch = 0.2654s, grad.norm=16.39106369
 16459: 12 [  535/ 1327], train_loss/perplexity = 4.18301201/65.5630341 secs/batch = 0.2614s, grad.norm=15.72517014
 16464: 12 [  540/ 1327], train_loss/perplexity = 4.24830008/69.9863434 secs/batch = 0.2656s, grad.norm=15.80331993
 16469: 12 [  545/ 1327], train_loss/perplexity = 4.23136759/68.8112717 secs/batch = 0.2655s, grad.norm=15.98436260
 16474: 12 [  550/ 1327], train_loss/perplexity = 4.25732327/70.6206970 secs/batch = 0.2643s, grad.norm=15.85522079
 16479: 12 [  555/ 1327], train_loss/perplexity = 4.08663750/59.5393524 secs/batch = 0.2653s, grad.norm=15.14927483
 16484: 12 [  560/ 1327], train_loss/perplexity = 4.18021536/65.3799286 secs/batch = 0.2653s, grad.norm=16.81293869
 16489: 12 [  565/ 1327], train_loss/perplexity = 4.05429077/57.6442642 secs/batch = 0.2654s, grad.norm=16.99969673
 16494: 12 [  570/ 1327], train_loss/perplexity = 4.02590656/56.0310822 secs/batch = 0.2654s, grad.norm=17.32565498
 16499: 12 [  575/ 1327], train_loss/perplexity = 3.89612579/49.2114258 secs/batch = 0.2657s, grad.norm=16.33153534
 16504: 12 [  580/ 1327], train_loss/perplexity = 4.27221966/71.6805649 secs/batch = 0.2657s, grad.norm=16.51131630
 16509: 12 [  585/ 1327], train_loss/perplexity = 3.79838848/44.6292076 secs/batch = 0.2658s, grad.norm=15.83427811
 16514: 12 [  590/ 1327], train_loss/perplexity = 4.21545506/67.7249756 secs/batch = 0.2654s, grad.norm=15.93789577
 16519: 12 [  595/ 1327], train_loss/perplexity = 4.18115234/65.4412231 secs/batch = 0.2653s, grad.norm=16.21783066
 16524: 12 [  600/ 1327], train_loss/perplexity = 4.41068268/82.3256454 secs/batch = 0.2640s, grad.norm=15.44703007
 16529: 12 [  605/ 1327], train_loss/perplexity = 4.23749161/69.2339706 secs/batch = 0.2658s, grad.norm=15.52508354
 16534: 12 [  610/ 1327], train_loss/perplexity = 4.39200211/80.8020325 secs/batch = 0.2664s, grad.norm=16.01441956
 16539: 12 [  615/ 1327], train_loss/perplexity = 4.02373362/55.9094620 secs/batch = 0.2647s, grad.norm=15.31783199
 16544: 12 [  620/ 1327], train_loss/perplexity = 4.47678185/87.9511795 secs/batch = 0.2656s, grad.norm=15.67789650
 16549: 12 [  625/ 1327], train_loss/perplexity = 4.29737234/73.5063934 secs/batch = 0.2659s, grad.norm=15.86265564
 16554: 12 [  630/ 1327], train_loss/perplexity = 4.46076393/86.5536041 secs/batch = 0.2653s, grad.norm=15.45268917
 16559: 12 [  635/ 1327], train_loss/perplexity = 4.14211702/62.9359169 secs/batch = 0.2654s, grad.norm=16.00097275
 16564: 12 [  640/ 1327], train_loss/perplexity = 4.14134598/62.8874092 secs/batch = 0.2653s, grad.norm=15.67929077
 16569: 12 [  645/ 1327], train_loss/perplexity = 4.38948488/80.5988922 secs/batch = 0.2611s, grad.norm=17.07969093
 16574: 12 [  650/ 1327], train_loss/perplexity = 3.92475152/50.6404915 secs/batch = 0.2659s, grad.norm=16.41273689
 16579: 12 [  655/ 1327], train_loss/perplexity = 4.08337784/59.3455925 secs/batch = 0.2657s, grad.norm=16.27557945
 16584: 12 [  660/ 1327], train_loss/perplexity = 4.05977535/57.9612885 secs/batch = 0.2658s, grad.norm=15.90103626
 16589: 12 [  665/ 1327], train_loss/perplexity = 4.14810896/63.3141556 secs/batch = 0.2655s, grad.norm=16.05378914
 16594: 12 [  670/ 1327], train_loss/perplexity = 4.16634417/64.4792938 secs/batch = 0.2658s, grad.norm=16.43140030
 16599: 12 [  675/ 1327], train_loss/perplexity = 3.94773173/51.8176956 secs/batch = 0.2661s, grad.norm=16.38856697
 16604: 12 [  680/ 1327], train_loss/perplexity = 4.08282185/59.3126068 secs/batch = 0.2654s, grad.norm=17.02007294
 16609: 12 [  685/ 1327], train_loss/perplexity = 3.88949680/48.8862801 secs/batch = 0.2657s, grad.norm=16.01742554
 16614: 12 [  690/ 1327], train_loss/perplexity = 4.37403250/79.3630219 secs/batch = 0.2654s, grad.norm=15.56434917
 16619: 12 [  695/ 1327], train_loss/perplexity = 4.21169281/67.4706573 secs/batch = 0.2666s, grad.norm=16.33858109
 16624: 12 [  700/ 1327], train_loss/perplexity = 4.37166405/79.1752701 secs/batch = 0.2672s, grad.norm=16.30284691
 16629: 12 [  705/ 1327], train_loss/perplexity = 4.14925575/63.3868065 secs/batch = 0.2657s, grad.norm=15.18726444
 16634: 12 [  710/ 1327], train_loss/perplexity = 4.05951118/57.9459801 secs/batch = 0.2651s, grad.norm=16.69803047
 16639: 12 [  715/ 1327], train_loss/perplexity = 3.91587830/50.1931381 secs/batch = 0.2618s, grad.norm=15.90089989
 16644: 12 [  720/ 1327], train_loss/perplexity = 3.91485047/50.1415749 secs/batch = 0.2668s, grad.norm=16.46699333
 16649: 12 [  725/ 1327], train_loss/perplexity = 3.98107481/53.5745850 secs/batch = 0.2660s, grad.norm=16.21318054
 16654: 12 [  730/ 1327], train_loss/perplexity = 4.14004946/62.8059273 secs/batch = 0.2663s, grad.norm=16.32180023
 16659: 12 [  735/ 1327], train_loss/perplexity = 4.17122650/64.7948761 secs/batch = 0.2626s, grad.norm=16.45920563
 16664: 12 [  740/ 1327], train_loss/perplexity = 3.70825338/40.7825127 secs/batch = 0.2666s, grad.norm=15.34300804
 16669: 12 [  745/ 1327], train_loss/perplexity = 4.15715027/63.8891945 secs/batch = 0.2656s, grad.norm=16.34833527
 16674: 12 [  750/ 1327], train_loss/perplexity = 4.01117277/55.2115822 secs/batch = 0.2665s, grad.norm=15.67710304
 16679: 12 [  755/ 1327], train_loss/perplexity = 4.00862265/55.0709648 secs/batch = 0.2656s, grad.norm=16.27109718
 16684: 12 [  760/ 1327], train_loss/perplexity = 3.84967923/46.9779930 secs/batch = 0.2669s, grad.norm=15.29930305
 16689: 12 [  765/ 1327], train_loss/perplexity = 3.89227200/49.0221367 secs/batch = 0.2663s, grad.norm=15.36396694
 16694: 12 [  770/ 1327], train_loss/perplexity = 3.96410084/52.6728859 secs/batch = 0.2660s, grad.norm=15.93116474
 16699: 12 [  775/ 1327], train_loss/perplexity = 4.01215172/55.2656593 secs/batch = 0.2680s, grad.norm=16.53561592
 16704: 12 [  780/ 1327], train_loss/perplexity = 4.32413387/75.5000916 secs/batch = 0.2654s, grad.norm=16.08125114
 16709: 12 [  785/ 1327], train_loss/perplexity = 4.18935490/65.9802094 secs/batch = 0.2657s, grad.norm=16.90876198
 16714: 12 [  790/ 1327], train_loss/perplexity = 3.95616984/52.2567902 secs/batch = 0.2697s, grad.norm=16.26460838
 16719: 12 [  795/ 1327], train_loss/perplexity = 4.32940006/75.8987350 secs/batch = 0.2666s, grad.norm=16.11485100
 16724: 12 [  800/ 1327], train_loss/perplexity = 4.23667288/69.1773071 secs/batch = 0.2639s, grad.norm=16.45787430
 16729: 12 [  805/ 1327], train_loss/perplexity = 4.55246544/94.8660049 secs/batch = 0.2662s, grad.norm=16.19638824
 16734: 12 [  810/ 1327], train_loss/perplexity = 4.12141991/61.6467133 secs/batch = 0.2659s, grad.norm=15.39401531
 16739: 12 [  815/ 1327], train_loss/perplexity = 4.07610083/58.9153023 secs/batch = 0.2633s, grad.norm=15.73701954
 16744: 12 [  820/ 1327], train_loss/perplexity = 3.86093426/47.5097160 secs/batch = 0.2656s, grad.norm=15.05402756
 16749: 12 [  825/ 1327], train_loss/perplexity = 4.07550859/58.8804207 secs/batch = 0.2654s, grad.norm=15.55983543
 16754: 12 [  830/ 1327], train_loss/perplexity = 3.85519648/47.2378960 secs/batch = 0.2666s, grad.norm=16.12353516
 16759: 12 [  835/ 1327], train_loss/perplexity = 4.08110762/59.2110176 secs/batch = 0.2648s, grad.norm=16.28866196
 16764: 12 [  840/ 1327], train_loss/perplexity = 4.13365841/62.4058113 secs/batch = 0.2676s, grad.norm=15.49739838
 16769: 12 [  845/ 1327], train_loss/perplexity = 3.99854231/54.5186195 secs/batch = 0.2661s, grad.norm=16.17025948
 16774: 12 [  850/ 1327], train_loss/perplexity = 4.14454603/63.0889740 secs/batch = 0.2679s, grad.norm=15.51150322
 16779: 12 [  855/ 1327], train_loss/perplexity = 4.11330414/61.1484261 secs/batch = 0.2657s, grad.norm=16.23996353
 16784: 12 [  860/ 1327], train_loss/perplexity = 3.86416388/47.6634026 secs/batch = 0.2650s, grad.norm=15.61129761
 16789: 12 [  865/ 1327], train_loss/perplexity = 4.31129646/74.5370636 secs/batch = 0.2659s, grad.norm=15.80475044
 16794: 12 [  870/ 1327], train_loss/perplexity = 4.16246843/64.2298737 secs/batch = 0.2670s, grad.norm=16.11178017
 16799: 12 [  875/ 1327], train_loss/perplexity = 3.74137592/42.1559525 secs/batch = 0.2652s, grad.norm=15.60613441
 16804: 12 [  880/ 1327], train_loss/perplexity = 3.94598675/51.7273560 secs/batch = 0.2662s, grad.norm=15.86095142
 16809: 12 [  885/ 1327], train_loss/perplexity = 4.15256596/63.5969772 secs/batch = 0.2646s, grad.norm=15.56822014
 16814: 12 [  890/ 1327], train_loss/perplexity = 4.33898067/76.6293869 secs/batch = 0.2660s, grad.norm=16.12029266
 16819: 12 [  895/ 1327], train_loss/perplexity = 4.29612541/73.4147873 secs/batch = 0.2666s, grad.norm=15.41366196
 16824: 12 [  900/ 1327], train_loss/perplexity = 4.12813091/62.0618172 secs/batch = 0.2648s, grad.norm=15.83372784
 16829: 12 [  905/ 1327], train_loss/perplexity = 3.96028638/52.4723511 secs/batch = 0.2637s, grad.norm=14.90976238
 16834: 12 [  910/ 1327], train_loss/perplexity = 4.00576973/54.9140778 secs/batch = 0.2659s, grad.norm=15.03670597
 16839: 12 [  915/ 1327], train_loss/perplexity = 4.24641371/69.8544464 secs/batch = 0.2668s, grad.norm=15.39375401
 16844: 12 [  920/ 1327], train_loss/perplexity = 4.43232536/84.1268158 secs/batch = 0.2678s, grad.norm=16.00288582
 16849: 12 [  925/ 1327], train_loss/perplexity = 4.17207956/64.8501740 secs/batch = 0.2615s, grad.norm=15.57624912
 16854: 12 [  930/ 1327], train_loss/perplexity = 4.22764063/68.5552902 secs/batch = 0.2650s, grad.norm=15.63285160
 16859: 12 [  935/ 1327], train_loss/perplexity = 4.34067249/76.7591400 secs/batch = 0.2656s, grad.norm=15.69233799
 16864: 12 [  940/ 1327], train_loss/perplexity = 4.30077887/73.7572174 secs/batch = 0.2662s, grad.norm=15.59958172
 16869: 12 [  945/ 1327], train_loss/perplexity = 4.46572590/86.9841461 secs/batch = 0.2653s, grad.norm=15.55729389
 16874: 12 [  950/ 1327], train_loss/perplexity = 4.21542215/67.7227478 secs/batch = 0.2660s, grad.norm=15.73130035
 16879: 12 [  955/ 1327], train_loss/perplexity = 4.23718357/69.2126465 secs/batch = 0.2658s, grad.norm=16.42124748
 16884: 12 [  960/ 1327], train_loss/perplexity = 4.50267696/90.2584305 secs/batch = 0.2664s, grad.norm=15.97959423
 16889: 12 [  965/ 1327], train_loss/perplexity = 4.17501354/65.0407181 secs/batch = 0.2666s, grad.norm=16.04382324
 16894: 12 [  970/ 1327], train_loss/perplexity = 4.41780901/82.9144211 secs/batch = 0.2655s, grad.norm=15.62272167
 16899: 12 [  975/ 1327], train_loss/perplexity = 4.11213064/61.0767097 secs/batch = 0.2642s, grad.norm=16.76654625
 16904: 12 [  980/ 1327], train_loss/perplexity = 3.92791319/50.8008537 secs/batch = 0.2657s, grad.norm=15.57517052
 16909: 12 [  985/ 1327], train_loss/perplexity = 4.10150433/60.4311256 secs/batch = 0.2634s, grad.norm=16.00813103
 16914: 12 [  990/ 1327], train_loss/perplexity = 4.31933498/75.1386414 secs/batch = 0.2621s, grad.norm=16.04336357
 16919: 12 [  995/ 1327], train_loss/perplexity = 4.28662491/72.7206116 secs/batch = 0.2663s, grad.norm=15.54586506
 16924: 12 [ 1000/ 1327], train_loss/perplexity = 3.85419631/47.1906738 secs/batch = 0.2644s, grad.norm=15.29700279
 16929: 12 [ 1005/ 1327], train_loss/perplexity = 4.25270319/70.2951736 secs/batch = 0.2658s, grad.norm=15.76917839
 16934: 12 [ 1010/ 1327], train_loss/perplexity = 3.93610001/51.2184601 secs/batch = 0.2674s, grad.norm=15.08803558
 16939: 12 [ 1015/ 1327], train_loss/perplexity = 4.42377710/83.4107437 secs/batch = 0.2666s, grad.norm=15.77123260
 16944: 12 [ 1020/ 1327], train_loss/perplexity = 4.48054123/88.2824402 secs/batch = 0.2674s, grad.norm=15.64571095
 16949: 12 [ 1025/ 1327], train_loss/perplexity = 4.48840284/88.9792175 secs/batch = 0.2615s, grad.norm=15.82124996
 16954: 12 [ 1030/ 1327], train_loss/perplexity = 4.20072603/66.7347641 secs/batch = 0.2636s, grad.norm=15.27435970
 16959: 12 [ 1035/ 1327], train_loss/perplexity = 4.09129047/59.8170357 secs/batch = 0.2649s, grad.norm=15.66920185
 16964: 12 [ 1040/ 1327], train_loss/perplexity = 4.32142830/75.2960968 secs/batch = 0.2650s, grad.norm=16.09733009
 16969: 12 [ 1045/ 1327], train_loss/perplexity = 3.86606503/47.7541046 secs/batch = 0.2659s, grad.norm=15.16483307
 16974: 12 [ 1050/ 1327], train_loss/perplexity = 3.94685555/51.7723160 secs/batch = 0.2652s, grad.norm=15.78088760
 16979: 12 [ 1055/ 1327], train_loss/perplexity = 4.08941889/59.7051849 secs/batch = 0.2658s, grad.norm=16.92473984
 16984: 12 [ 1060/ 1327], train_loss/perplexity = 3.60762548/36.8783798 secs/batch = 0.2656s, grad.norm=16.32411957
 16989: 12 [ 1065/ 1327], train_loss/perplexity = 3.80958152/45.1315498 secs/batch = 0.2657s, grad.norm=16.05878448
 16994: 12 [ 1070/ 1327], train_loss/perplexity = 4.22952986/68.6849365 secs/batch = 0.2596s, grad.norm=16.46673012
 16999: 12 [ 1075/ 1327], train_loss/perplexity = 3.99133062/54.1268654 secs/batch = 0.2636s, grad.norm=15.99617195
 17004: 12 [ 1080/ 1327], train_loss/perplexity = 3.89317942/49.0666428 secs/batch = 0.2660s, grad.norm=15.85282993
 17009: 12 [ 1085/ 1327], train_loss/perplexity = 3.76806569/43.2962341 secs/batch = 0.2657s, grad.norm=15.92993355
 17014: 12 [ 1090/ 1327], train_loss/perplexity = 3.91068864/49.9333267 secs/batch = 0.2654s, grad.norm=17.03162956
 17019: 12 [ 1095/ 1327], train_loss/perplexity = 4.14328861/63.0096970 secs/batch = 0.2659s, grad.norm=16.67534065
 17024: 12 [ 1100/ 1327], train_loss/perplexity = 3.80696583/45.0136528 secs/batch = 0.2650s, grad.norm=17.86002731
 17029: 12 [ 1105/ 1327], train_loss/perplexity = 3.81144428/45.2156944 secs/batch = 0.2656s, grad.norm=16.34429741
 17034: 12 [ 1110/ 1327], train_loss/perplexity = 4.16812468/64.5942001 secs/batch = 0.2665s, grad.norm=16.98099709
 17039: 12 [ 1115/ 1327], train_loss/perplexity = 3.90239930/49.5211220 secs/batch = 0.2658s, grad.norm=15.36566830
 17044: 12 [ 1120/ 1327], train_loss/perplexity = 4.18509912/65.7000122 secs/batch = 0.2607s, grad.norm=15.94575024
 17049: 12 [ 1125/ 1327], train_loss/perplexity = 4.31686831/74.9535294 secs/batch = 0.2658s, grad.norm=16.71216393
 17054: 12 [ 1130/ 1327], train_loss/perplexity = 4.05869675/57.8988037 secs/batch = 0.2652s, grad.norm=16.47284126
 17059: 12 [ 1135/ 1327], train_loss/perplexity = 4.09105730/59.8030891 secs/batch = 0.2666s, grad.norm=16.04667473
 17064: 12 [ 1140/ 1327], train_loss/perplexity = 4.31948566/75.1499634 secs/batch = 0.2662s, grad.norm=16.78658295
 17069: 12 [ 1145/ 1327], train_loss/perplexity = 4.08825970/59.6360168 secs/batch = 0.2668s, grad.norm=15.39157867
 17074: 12 [ 1150/ 1327], train_loss/perplexity = 4.08404922/59.3854485 secs/batch = 0.2658s, grad.norm=16.26713181
 17079: 12 [ 1155/ 1327], train_loss/perplexity = 4.11884928/61.4884453 secs/batch = 0.2653s, grad.norm=16.45908356
 17084: 12 [ 1160/ 1327], train_loss/perplexity = 4.10304070/60.5240440 secs/batch = 0.2664s, grad.norm=16.11675453
 17089: 12 [ 1165/ 1327], train_loss/perplexity = 4.05954885/57.9481621 secs/batch = 0.2648s, grad.norm=15.75791836
 17094: 12 [ 1170/ 1327], train_loss/perplexity = 3.96469760/52.7043304 secs/batch = 0.2672s, grad.norm=16.17898560
 17099: 12 [ 1175/ 1327], train_loss/perplexity = 3.86216187/47.5680771 secs/batch = 0.2662s, grad.norm=16.64560699
 17104: 12 [ 1180/ 1327], train_loss/perplexity = 3.82270384/45.7276802 secs/batch = 0.2671s, grad.norm=15.98196697
 17109: 12 [ 1185/ 1327], train_loss/perplexity = 3.99679756/54.4235840 secs/batch = 0.2603s, grad.norm=15.82346821
 17114: 12 [ 1190/ 1327], train_loss/perplexity = 4.08465958/59.4217072 secs/batch = 0.2654s, grad.norm=16.28541565
 17119: 12 [ 1195/ 1327], train_loss/perplexity = 3.94146442/51.4939537 secs/batch = 0.2668s, grad.norm=15.63726330
 17124: 12 [ 1200/ 1327], train_loss/perplexity = 3.87340164/48.1057472 secs/batch = 0.2673s, grad.norm=16.07421684
 17129: 12 [ 1205/ 1327], train_loss/perplexity = 3.82964873/46.0463600 secs/batch = 0.2671s, grad.norm=16.25483513
 17134: 12 [ 1210/ 1327], train_loss/perplexity = 3.47384715/32.2606163 secs/batch = 0.2657s, grad.norm=16.27774620
 17139: 12 [ 1215/ 1327], train_loss/perplexity = 3.76716876/43.2574196 secs/batch = 0.2636s, grad.norm=15.35540676
 17144: 12 [ 1220/ 1327], train_loss/perplexity = 3.90357280/49.5792694 secs/batch = 0.2658s, grad.norm=16.35896873
 17149: 12 [ 1225/ 1327], train_loss/perplexity = 3.59226418/36.3162079 secs/batch = 0.2662s, grad.norm=16.84520340
 17154: 12 [ 1230/ 1327], train_loss/perplexity = 3.92283201/50.5433807 secs/batch = 0.2661s, grad.norm=15.83141232
 17159: 12 [ 1235/ 1327], train_loss/perplexity = 3.85654688/47.3017311 secs/batch = 0.2641s, grad.norm=15.62487316
 17164: 12 [ 1240/ 1327], train_loss/perplexity = 4.12321520/61.7574844 secs/batch = 0.2647s, grad.norm=16.83977890
 17169: 12 [ 1245/ 1327], train_loss/perplexity = 4.03813314/56.7203560 secs/batch = 0.2666s, grad.norm=15.78992462
 17174: 12 [ 1250/ 1327], train_loss/perplexity = 4.09548235/60.0683060 secs/batch = 0.2654s, grad.norm=15.50410080
 17179: 12 [ 1255/ 1327], train_loss/perplexity = 4.20859957/67.2622757 secs/batch = 0.2658s, grad.norm=15.78542328
 17184: 12 [ 1260/ 1327], train_loss/perplexity = 3.95716572/52.3088570 secs/batch = 0.2633s, grad.norm=16.87222862
 17189: 12 [ 1265/ 1327], train_loss/perplexity = 4.15132141/63.5178795 secs/batch = 0.2663s, grad.norm=16.20639038
 17194: 12 [ 1270/ 1327], train_loss/perplexity = 3.86266041/47.5917969 secs/batch = 0.2650s, grad.norm=16.64793396
 17199: 12 [ 1275/ 1327], train_loss/perplexity = 4.09261513/59.8963242 secs/batch = 0.2660s, grad.norm=16.37901497
 17204: 12 [ 1280/ 1327], train_loss/perplexity = 3.90153003/49.4780922 secs/batch = 0.2660s, grad.norm=16.55296326
 17209: 12 [ 1285/ 1327], train_loss/perplexity = 3.84855413/46.9251671 secs/batch = 0.2661s, grad.norm=16.16047859
 17214: 12 [ 1290/ 1327], train_loss/perplexity = 4.08151770/59.2353020 secs/batch = 0.2666s, grad.norm=15.98350620
 17219: 12 [ 1295/ 1327], train_loss/perplexity = 4.03298664/56.4291954 secs/batch = 0.2663s, grad.norm=16.13356781
 17224: 12 [ 1300/ 1327], train_loss/perplexity = 4.21985340/68.0235138 secs/batch = 0.2660s, grad.norm=15.21052742
 17229: 12 [ 1305/ 1327], train_loss/perplexity = 4.29712343/73.4880981 secs/batch = 0.2655s, grad.norm=16.89281845
 17234: 12 [ 1310/ 1327], train_loss/perplexity = 4.53916645/93.6127396 secs/batch = 0.2661s, grad.norm=16.47164154
 17239: 12 [ 1315/ 1327], train_loss/perplexity = 4.32309580/75.4217606 secs/batch = 0.2648s, grad.norm=16.18320084
 17244: 12 [ 1320/ 1327], train_loss/perplexity = 4.34904385/77.4044189 secs/batch = 0.2677s, grad.norm=16.36817169
 17249: 12 [ 1325/ 1327], train_loss/perplexity = 4.24520111/69.7697906 secs/batch = 0.2663s, grad.norm=16.34037399
Epoch training time: 352.3265302181244
	> validation loss = 4.64815187, perplexity = 104.39187622
	> validation loss = 4.60958290, perplexity = 100.44224548
	> validation loss = 4.56006670, perplexity = 95.58985901
	> validation loss = 4.58306599, perplexity = 97.81382751
	> validation loss = 4.73283863, perplexity = 113.61762238
	> validation loss = 4.66817141, perplexity = 106.50281525
	> validation loss = 4.59135342, perplexity = 98.62782288
	> validation loss = 4.42055655, perplexity = 83.14254761
	> validation loss = 4.21913624, perplexity = 67.97474670
	> validation loss = 4.34393644, perplexity = 77.01008606
	> validation loss = 4.52324343, perplexity = 92.13394165
	> validation loss = 4.53413677, perplexity = 93.14307404
	> validation loss = 4.52521753, perplexity = 92.31600189
	> validation loss = 4.28332520, perplexity = 72.48105621
	> validation loss = 4.20741987, perplexity = 67.18297577
	> validation loss = 4.22862053, perplexity = 68.62250519
	> validation loss = 4.64143753, perplexity = 103.69330597
	> validation loss = 4.19755125, perplexity = 66.52323151
	> validation loss = 4.66620731, perplexity = 106.29383850
	> validation loss = 4.54310322, perplexity = 93.98199463
	> validation loss = 4.33262062, perplexity = 76.14356995
at the end of epoch: 12
train loss = 4.20013185, perplexity = 66.69512405
validation loss = 4.48270358, perplexity = 88.47354505
Saved model cv/epoch012_4.4827.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.5
new learning rate is: 0.25
 17256: 13 [    5/ 1327], train_loss/perplexity = 4.31789303/75.0303726 secs/batch = 0.2661s, grad.norm=16.23492241
 17261: 13 [   10/ 1327], train_loss/perplexity = 3.99271655/54.2019310 secs/batch = 0.2648s, grad.norm=15.98544788
 17266: 13 [   15/ 1327], train_loss/perplexity = 4.26792908/71.3736725 secs/batch = 0.2658s, grad.norm=15.00277042
 17271: 13 [   20/ 1327], train_loss/perplexity = 4.40675449/82.0028915 secs/batch = 0.2643s, grad.norm=15.20744419
 17276: 13 [   25/ 1327], train_loss/perplexity = 4.24058962/69.4487915 secs/batch = 0.2661s, grad.norm=16.56930923
 17281: 13 [   30/ 1327], train_loss/perplexity = 4.30842781/74.3235474 secs/batch = 0.2667s, grad.norm=16.04585838
 17286: 13 [   35/ 1327], train_loss/perplexity = 4.10738754/60.7877045 secs/batch = 0.2656s, grad.norm=15.56009102
 17291: 13 [   40/ 1327], train_loss/perplexity = 4.09221077/59.8721085 secs/batch = 0.2665s, grad.norm=15.73273945
 17296: 13 [   45/ 1327], train_loss/perplexity = 3.91905832/50.3530045 secs/batch = 0.2665s, grad.norm=15.06194210
 17301: 13 [   50/ 1327], train_loss/perplexity = 4.10537100/60.6652489 secs/batch = 0.2663s, grad.norm=16.36063957
 17306: 13 [   55/ 1327], train_loss/perplexity = 4.01297569/55.3112144 secs/batch = 0.2648s, grad.norm=16.36082268
 17311: 13 [   60/ 1327], train_loss/perplexity = 4.31903028/75.1157532 secs/batch = 0.2663s, grad.norm=16.35698509
 17316: 13 [   65/ 1327], train_loss/perplexity = 3.89316249/49.0658112 secs/batch = 0.2662s, grad.norm=15.82212162
 17321: 13 [   70/ 1327], train_loss/perplexity = 3.76192760/43.0312920 secs/batch = 0.2659s, grad.norm=15.77090836
 17326: 13 [   75/ 1327], train_loss/perplexity = 3.57310390/35.6270027 secs/batch = 0.2657s, grad.norm=15.48258114
 17331: 13 [   80/ 1327], train_loss/perplexity = 3.94758463/51.8100739 secs/batch = 0.2641s, grad.norm=15.41116238
 17336: 13 [   85/ 1327], train_loss/perplexity = 4.03771782/56.6968040 secs/batch = 0.2659s, grad.norm=16.08205032
 17341: 13 [   90/ 1327], train_loss/perplexity = 4.13393021/62.4227753 secs/batch = 0.2660s, grad.norm=16.01587868
 17346: 13 [   95/ 1327], train_loss/perplexity = 3.97902036/53.4646339 secs/batch = 0.2668s, grad.norm=16.14816856
 17351: 13 [  100/ 1327], train_loss/perplexity = 4.18455315/65.6641541 secs/batch = 0.2667s, grad.norm=15.93179703
 17356: 13 [  105/ 1327], train_loss/perplexity = 4.03290176/56.4244041 secs/batch = 0.2662s, grad.norm=16.64185905
 17361: 13 [  110/ 1327], train_loss/perplexity = 3.91692114/50.2455063 secs/batch = 0.2632s, grad.norm=15.50833225
 17366: 13 [  115/ 1327], train_loss/perplexity = 3.93090582/50.9531097 secs/batch = 0.2657s, grad.norm=16.28504181
 17371: 13 [  120/ 1327], train_loss/perplexity = 4.02451086/55.9529343 secs/batch = 0.2654s, grad.norm=16.20991898
 17376: 13 [  125/ 1327], train_loss/perplexity = 3.98468637/53.7684250 secs/batch = 0.2661s, grad.norm=16.27326012
 17381: 13 [  130/ 1327], train_loss/perplexity = 3.94543409/51.6987762 secs/batch = 0.2658s, grad.norm=17.04237938
 17386: 13 [  135/ 1327], train_loss/perplexity = 3.92993617/50.9037285 secs/batch = 0.2668s, grad.norm=15.79158401
 17391: 13 [  140/ 1327], train_loss/perplexity = 4.29971981/73.6791458 secs/batch = 0.2663s, grad.norm=17.13979530
 17396: 13 [  145/ 1327], train_loss/perplexity = 4.16479301/64.3793564 secs/batch = 0.2661s, grad.norm=16.85136986
 17401: 13 [  150/ 1327], train_loss/perplexity = 4.18273497/65.5448685 secs/batch = 0.2664s, grad.norm=16.78709221
 17406: 13 [  155/ 1327], train_loss/perplexity = 4.40992308/82.2631378 secs/batch = 0.2667s, grad.norm=16.42311859
 17411: 13 [  160/ 1327], train_loss/perplexity = 4.14659548/63.2184067 secs/batch = 0.2682s, grad.norm=15.29644585
 17416: 13 [  165/ 1327], train_loss/perplexity = 4.25290966/70.3096924 secs/batch = 0.2667s, grad.norm=15.68373203
 17421: 13 [  170/ 1327], train_loss/perplexity = 4.04252911/56.9702454 secs/batch = 0.2652s, grad.norm=15.66898537
 17426: 13 [  175/ 1327], train_loss/perplexity = 4.35505533/77.8711319 secs/batch = 0.2657s, grad.norm=16.12241936
 17431: 13 [  180/ 1327], train_loss/perplexity = 4.11058187/60.9821930 secs/batch = 0.2666s, grad.norm=15.90753746
 17436: 13 [  185/ 1327], train_loss/perplexity = 4.46177101/86.6408157 secs/batch = 0.2608s, grad.norm=16.23777199
 17441: 13 [  190/ 1327], train_loss/perplexity = 3.99678326/54.4228058 secs/batch = 0.2684s, grad.norm=15.51607132
 17446: 13 [  195/ 1327], train_loss/perplexity = 4.33052731/75.9843445 secs/batch = 0.2657s, grad.norm=15.32360077
 17451: 13 [  200/ 1327], train_loss/perplexity = 4.12957287/62.1513710 secs/batch = 0.2669s, grad.norm=16.05921745
 17456: 13 [  205/ 1327], train_loss/perplexity = 4.36128998/78.3581467 secs/batch = 0.2658s, grad.norm=16.03586578
 17461: 13 [  210/ 1327], train_loss/perplexity = 4.23948383/69.3720322 secs/batch = 0.2652s, grad.norm=15.14102745
 17466: 13 [  215/ 1327], train_loss/perplexity = 4.36922550/78.9824371 secs/batch = 0.2654s, grad.norm=15.36065578
 17471: 13 [  220/ 1327], train_loss/perplexity = 4.26109314/70.8874283 secs/batch = 0.2657s, grad.norm=15.79249287
 17476: 13 [  225/ 1327], train_loss/perplexity = 4.47546768/87.8356705 secs/batch = 0.2658s, grad.norm=16.17968750
 17481: 13 [  230/ 1327], train_loss/perplexity = 4.26415825/71.1050415 secs/batch = 0.2659s, grad.norm=16.42840767
 17486: 13 [  235/ 1327], train_loss/perplexity = 4.12140083/61.6455383 secs/batch = 0.2659s, grad.norm=15.84718704
 17491: 13 [  240/ 1327], train_loss/perplexity = 3.90074158/49.4390984 secs/batch = 0.2657s, grad.norm=15.74341393
 17496: 13 [  245/ 1327], train_loss/perplexity = 4.20493650/67.0163422 secs/batch = 0.2660s, grad.norm=15.62223148
 17501: 13 [  250/ 1327], train_loss/perplexity = 4.04166937/56.9212875 secs/batch = 0.2661s, grad.norm=15.25271225
 17506: 13 [  255/ 1327], train_loss/perplexity = 4.06020927/57.9864464 secs/batch = 0.2663s, grad.norm=15.90582752
 17511: 13 [  260/ 1327], train_loss/perplexity = 4.25864410/70.7140350 secs/batch = 0.2648s, grad.norm=16.47736359
 17516: 13 [  265/ 1327], train_loss/perplexity = 4.42637825/83.6279907 secs/batch = 0.2630s, grad.norm=15.88500404
 17521: 13 [  270/ 1327], train_loss/perplexity = 4.48966074/89.0912170 secs/batch = 0.2657s, grad.norm=15.92486954
 17526: 13 [  275/ 1327], train_loss/perplexity = 4.47449398/87.7501831 secs/batch = 0.2673s, grad.norm=15.94264507
 17531: 13 [  280/ 1327], train_loss/perplexity = 4.27788973/72.0881500 secs/batch = 0.2672s, grad.norm=15.69410801
 17536: 13 [  285/ 1327], train_loss/perplexity = 4.54792786/94.4365158 secs/batch = 0.2664s, grad.norm=15.83751488
 17541: 13 [  290/ 1327], train_loss/perplexity = 4.29143238/73.0710602 secs/batch = 0.2669s, grad.norm=16.41721344
 17546: 13 [  295/ 1327], train_loss/perplexity = 4.02474165/55.9658470 secs/batch = 0.2663s, grad.norm=15.69013119
 17551: 13 [  300/ 1327], train_loss/perplexity = 3.54937649/34.7916183 secs/batch = 0.2672s, grad.norm=15.18672180
 17556: 13 [  305/ 1327], train_loss/perplexity = 4.08118582/59.2156487 secs/batch = 0.2663s, grad.norm=16.00325012
 17561: 13 [  310/ 1327], train_loss/perplexity = 4.09347820/59.9480400 secs/batch = 0.2667s, grad.norm=15.76013279
 17566: 13 [  315/ 1327], train_loss/perplexity = 3.63579273/37.9319115 secs/batch = 0.2663s, grad.norm=15.16177940
 17571: 13 [  320/ 1327], train_loss/perplexity = 3.58063841/35.8964500 secs/batch = 0.2649s, grad.norm=16.22953987
 17576: 13 [  325/ 1327], train_loss/perplexity = 3.56156564/35.2182922 secs/batch = 0.2657s, grad.norm=15.18436527
 17581: 13 [  330/ 1327], train_loss/perplexity = 4.14804792/63.3102913 secs/batch = 0.2665s, grad.norm=15.90764618
 17586: 13 [  335/ 1327], train_loss/perplexity = 3.62316370/37.4558792 secs/batch = 0.2647s, grad.norm=14.90653515
 17591: 13 [  340/ 1327], train_loss/perplexity = 4.40713596/82.0341797 secs/batch = 0.2652s, grad.norm=15.71445465
 17596: 13 [  345/ 1327], train_loss/perplexity = 4.15103483/63.4996796 secs/batch = 0.2670s, grad.norm=15.43285084
 17601: 13 [  350/ 1327], train_loss/perplexity = 4.04950428/57.3690109 secs/batch = 0.2669s, grad.norm=16.37953377
 17606: 13 [  355/ 1327], train_loss/perplexity = 4.08576107/59.4871941 secs/batch = 0.2662s, grad.norm=15.88159084
 17611: 13 [  360/ 1327], train_loss/perplexity = 4.24539471/69.7833023 secs/batch = 0.2661s, grad.norm=16.95276070
 17616: 13 [  365/ 1327], train_loss/perplexity = 4.21071720/67.4048691 secs/batch = 0.2663s, grad.norm=15.71513653
 17621: 13 [  370/ 1327], train_loss/perplexity = 4.31885052/75.1022491 secs/batch = 0.2667s, grad.norm=16.12018013
 17626: 13 [  375/ 1327], train_loss/perplexity = 3.69700027/40.3261566 secs/batch = 0.2663s, grad.norm=15.36927700
 17631: 13 [  380/ 1327], train_loss/perplexity = 3.81587505/45.4164810 secs/batch = 0.2657s, grad.norm=16.61250877
 17636: 13 [  385/ 1327], train_loss/perplexity = 3.95725656/52.3136101 secs/batch = 0.2675s, grad.norm=16.67485619
 17641: 13 [  390/ 1327], train_loss/perplexity = 4.12242222/61.7085342 secs/batch = 0.2660s, grad.norm=15.84133530
 17646: 13 [  395/ 1327], train_loss/perplexity = 4.13520193/62.5022125 secs/batch = 0.2601s, grad.norm=16.37376213
 17651: 13 [  400/ 1327], train_loss/perplexity = 4.12273741/61.7279854 secs/batch = 0.2646s, grad.norm=16.45391464
 17656: 13 [  405/ 1327], train_loss/perplexity = 4.36951160/79.0050354 secs/batch = 0.2653s, grad.norm=16.80051041
 17661: 13 [  410/ 1327], train_loss/perplexity = 3.99351573/54.2452660 secs/batch = 0.2658s, grad.norm=15.74864101
 17666: 13 [  415/ 1327], train_loss/perplexity = 3.97845030/53.4341621 secs/batch = 0.2656s, grad.norm=15.25099277
 17671: 13 [  420/ 1327], train_loss/perplexity = 3.58995962/36.2326126 secs/batch = 0.2661s, grad.norm=15.52765751
 17676: 13 [  425/ 1327], train_loss/perplexity = 3.97139120/53.0582924 secs/batch = 0.2651s, grad.norm=16.43119049
 17681: 13 [  430/ 1327], train_loss/perplexity = 4.12386227/61.7974586 secs/batch = 0.2666s, grad.norm=16.18899155
 17686: 13 [  435/ 1327], train_loss/perplexity = 4.18451452/65.6616135 secs/batch = 0.2666s, grad.norm=16.51852226
 17691: 13 [  440/ 1327], train_loss/perplexity = 3.72844005/41.6141396 secs/batch = 0.2665s, grad.norm=15.81000805
 17696: 13 [  445/ 1327], train_loss/perplexity = 4.11121082/61.0205574 secs/batch = 0.2651s, grad.norm=16.53600121
 17701: 13 [  450/ 1327], train_loss/perplexity = 4.07691860/58.9635010 secs/batch = 0.2672s, grad.norm=16.16961670
 17706: 13 [  455/ 1327], train_loss/perplexity = 3.98324966/53.6912308 secs/batch = 0.2667s, grad.norm=15.58300495
 17711: 13 [  460/ 1327], train_loss/perplexity = 3.92829537/50.8202744 secs/batch = 0.2660s, grad.norm=16.51880074
 17716: 13 [  465/ 1327], train_loss/perplexity = 3.67802620/39.5682182 secs/batch = 0.2659s, grad.norm=17.05933189
 17721: 13 [  470/ 1327], train_loss/perplexity = 4.39355803/80.9278488 secs/batch = 0.2676s, grad.norm=16.07811546
 17726: 13 [  475/ 1327], train_loss/perplexity = 3.88347006/48.5925407 secs/batch = 0.2660s, grad.norm=16.05696678
 17731: 13 [  480/ 1327], train_loss/perplexity = 4.01373672/55.3533249 secs/batch = 0.2657s, grad.norm=16.03860664
 17736: 13 [  485/ 1327], train_loss/perplexity = 4.00181532/54.6973534 secs/batch = 0.2661s, grad.norm=16.09356308
 17741: 13 [  490/ 1327], train_loss/perplexity = 3.87162876/48.0205345 secs/batch = 0.2652s, grad.norm=16.81463623
 17746: 13 [  495/ 1327], train_loss/perplexity = 3.95831871/52.3692055 secs/batch = 0.2655s, grad.norm=15.92584991
 17751: 13 [  500/ 1327], train_loss/perplexity = 4.13685560/62.6056557 secs/batch = 0.2663s, grad.norm=15.87425900
 17756: 13 [  505/ 1327], train_loss/perplexity = 4.23254824/68.8925629 secs/batch = 0.2658s, grad.norm=15.23678970
 17761: 13 [  510/ 1327], train_loss/perplexity = 4.55960846/95.5460587 secs/batch = 0.2662s, grad.norm=15.20865822
 17766: 13 [  515/ 1327], train_loss/perplexity = 4.17625761/65.1216888 secs/batch = 0.2664s, grad.norm=15.49674034
 17771: 13 [  520/ 1327], train_loss/perplexity = 4.37743521/79.6335297 secs/batch = 0.2684s, grad.norm=16.03666687
 17776: 13 [  525/ 1327], train_loss/perplexity = 3.92259550/50.5314293 secs/batch = 0.2658s, grad.norm=15.79512596
 17781: 13 [  530/ 1327], train_loss/perplexity = 3.99379945/54.2606583 secs/batch = 0.2663s, grad.norm=16.16570091
 17786: 13 [  535/ 1327], train_loss/perplexity = 4.11247063/61.0974808 secs/batch = 0.2672s, grad.norm=15.81709385
 17791: 13 [  540/ 1327], train_loss/perplexity = 4.18152523/65.4656296 secs/batch = 0.2661s, grad.norm=15.86080647
 17796: 13 [  545/ 1327], train_loss/perplexity = 4.15674114/63.8630638 secs/batch = 0.2663s, grad.norm=16.33589172
 17801: 13 [  550/ 1327], train_loss/perplexity = 4.09768486/60.2007523 secs/batch = 0.2652s, grad.norm=16.15552139
 17806: 13 [  555/ 1327], train_loss/perplexity = 3.90667057/49.7330933 secs/batch = 0.2674s, grad.norm=15.25746059
 17811: 13 [  560/ 1327], train_loss/perplexity = 4.06307220/58.1526947 secs/batch = 0.2596s, grad.norm=16.79847145
 17816: 13 [  565/ 1327], train_loss/perplexity = 3.94692922/51.7761307 secs/batch = 0.2649s, grad.norm=17.92091560
 17821: 13 [  570/ 1327], train_loss/perplexity = 3.99601364/54.3809357 secs/batch = 0.2657s, grad.norm=16.48892784
 17826: 13 [  575/ 1327], train_loss/perplexity = 3.78230238/43.9170380 secs/batch = 0.2663s, grad.norm=16.52101326
 17831: 13 [  580/ 1327], train_loss/perplexity = 4.25825357/70.6864243 secs/batch = 0.2667s, grad.norm=17.04539490
 17836: 13 [  585/ 1327], train_loss/perplexity = 3.76193476/43.0316010 secs/batch = 0.2667s, grad.norm=16.53697968
 17841: 13 [  590/ 1327], train_loss/perplexity = 4.14971352/63.4158287 secs/batch = 0.2664s, grad.norm=15.88739872
 17846: 13 [  595/ 1327], train_loss/perplexity = 4.03146219/56.3432350 secs/batch = 0.2670s, grad.norm=16.32152748
 17851: 13 [  600/ 1327], train_loss/perplexity = 4.31466770/74.7887650 secs/batch = 0.2644s, grad.norm=15.64782429
 17856: 13 [  605/ 1327], train_loss/perplexity = 4.18020582/65.3793106 secs/batch = 0.2660s, grad.norm=15.85656071
 17861: 13 [  610/ 1327], train_loss/perplexity = 4.38033915/79.8651123 secs/batch = 0.2657s, grad.norm=16.19013786
 17866: 13 [  615/ 1327], train_loss/perplexity = 3.94050360/51.4445038 secs/batch = 0.2669s, grad.norm=15.29917431
 17871: 13 [  620/ 1327], train_loss/perplexity = 4.27369261/71.7862244 secs/batch = 0.2675s, grad.norm=15.89507961
 17876: 13 [  625/ 1327], train_loss/perplexity = 4.17576265/65.0894623 secs/batch = 0.2660s, grad.norm=15.77052689
 17881: 13 [  630/ 1327], train_loss/perplexity = 4.40134001/81.5600891 secs/batch = 0.2658s, grad.norm=15.63283443
 17886: 13 [  635/ 1327], train_loss/perplexity = 4.05809736/57.8641129 secs/batch = 0.2671s, grad.norm=16.12355804
 17891: 13 [  640/ 1327], train_loss/perplexity = 4.00056839/54.6291924 secs/batch = 0.2625s, grad.norm=15.86368179
 17896: 13 [  645/ 1327], train_loss/perplexity = 4.25362062/70.3596954 secs/batch = 0.2626s, grad.norm=16.58433533
 17901: 13 [  650/ 1327], train_loss/perplexity = 3.75708270/42.8233147 secs/batch = 0.2657s, grad.norm=15.51262474
 17906: 13 [  655/ 1327], train_loss/perplexity = 4.04736519/57.2464256 secs/batch = 0.2666s, grad.norm=16.25751686
 17911: 13 [  660/ 1327], train_loss/perplexity = 3.93380642/51.1011200 secs/batch = 0.2680s, grad.norm=16.24260139
 17916: 13 [  665/ 1327], train_loss/perplexity = 4.06542492/58.2896729 secs/batch = 0.2673s, grad.norm=15.98995018
 17921: 13 [  670/ 1327], train_loss/perplexity = 4.08064175/59.1834373 secs/batch = 0.2657s, grad.norm=16.36725616
 17926: 13 [  675/ 1327], train_loss/perplexity = 3.80809975/45.0647240 secs/batch = 0.2660s, grad.norm=16.43593979
 17931: 13 [  680/ 1327], train_loss/perplexity = 4.05611086/57.7492790 secs/batch = 0.2660s, grad.norm=16.86345291
 17936: 13 [  685/ 1327], train_loss/perplexity = 3.76125407/43.0023193 secs/batch = 0.2685s, grad.norm=15.98070621
 17941: 13 [  690/ 1327], train_loss/perplexity = 4.20844460/67.2518539 secs/batch = 0.2662s, grad.norm=16.06745148
 17946: 13 [  695/ 1327], train_loss/perplexity = 4.12230015/61.7010002 secs/batch = 0.2669s, grad.norm=15.86507702
 17951: 13 [  700/ 1327], train_loss/perplexity = 4.33270359/76.1498871 secs/batch = 0.2670s, grad.norm=16.59221268
 17956: 13 [  705/ 1327], train_loss/perplexity = 4.02913237/56.2121201 secs/batch = 0.2667s, grad.norm=15.40698433
 17961: 13 [  710/ 1327], train_loss/perplexity = 3.93959045/51.3975487 secs/batch = 0.2665s, grad.norm=16.53074455
 17966: 13 [  715/ 1327], train_loss/perplexity = 3.83528352/46.3065529 secs/batch = 0.2586s, grad.norm=16.66872597
 17971: 13 [  720/ 1327], train_loss/perplexity = 3.85744190/47.3440857 secs/batch = 0.2665s, grad.norm=16.68768120
 17976: 13 [  725/ 1327], train_loss/perplexity = 3.94306517/51.5764503 secs/batch = 0.2652s, grad.norm=16.70870590
 17981: 13 [  730/ 1327], train_loss/perplexity = 4.04036140/56.8468857 secs/batch = 0.2657s, grad.norm=16.77532959
 17986: 13 [  735/ 1327], train_loss/perplexity = 4.09709167/60.1650543 secs/batch = 0.2599s, grad.norm=16.98868179
 17991: 13 [  740/ 1327], train_loss/perplexity = 3.65953469/38.8432655 secs/batch = 0.2659s, grad.norm=15.31830597
 17996: 13 [  745/ 1327], train_loss/perplexity = 4.06191635/58.0855179 secs/batch = 0.2635s, grad.norm=16.67918587
 18001: 13 [  750/ 1327], train_loss/perplexity = 3.92931914/50.8723297 secs/batch = 0.2660s, grad.norm=16.48301697
 18006: 13 [  755/ 1327], train_loss/perplexity = 3.83429575/46.2608376 secs/batch = 0.2655s, grad.norm=16.16768646
 18011: 13 [  760/ 1327], train_loss/perplexity = 3.74539042/42.3255272 secs/batch = 0.2657s, grad.norm=15.81066227
 18016: 13 [  765/ 1327], train_loss/perplexity = 3.78641224/44.0979042 secs/batch = 0.2647s, grad.norm=15.64346790
 18021: 13 [  770/ 1327], train_loss/perplexity = 3.79135609/44.3164558 secs/batch = 0.2668s, grad.norm=16.33905983
 18026: 13 [  775/ 1327], train_loss/perplexity = 3.81681013/45.4589691 secs/batch = 0.2654s, grad.norm=16.79798508
 18031: 13 [  780/ 1327], train_loss/perplexity = 4.18529129/65.7126389 secs/batch = 0.2682s, grad.norm=16.31882095
 18036: 13 [  785/ 1327], train_loss/perplexity = 4.06169939/58.0729141 secs/batch = 0.2668s, grad.norm=16.78449821
 18041: 13 [  790/ 1327], train_loss/perplexity = 3.81672239/45.4549789 secs/batch = 0.2627s, grad.norm=16.57069969
 18046: 13 [  795/ 1327], train_loss/perplexity = 4.17487907/65.0319748 secs/batch = 0.2653s, grad.norm=16.51879120
 18051: 13 [  800/ 1327], train_loss/perplexity = 4.01153278/55.2314644 secs/batch = 0.2663s, grad.norm=15.97267056
 18056: 13 [  805/ 1327], train_loss/perplexity = 4.41917706/83.0279312 secs/batch = 0.2616s, grad.norm=16.49255562
 18061: 13 [  810/ 1327], train_loss/perplexity = 4.02954721/56.2354431 secs/batch = 0.2662s, grad.norm=15.38992596
 18066: 13 [  815/ 1327], train_loss/perplexity = 3.93901014/51.3677292 secs/batch = 0.2671s, grad.norm=15.75878239
 18071: 13 [  820/ 1327], train_loss/perplexity = 3.80220389/44.7998085 secs/batch = 0.2642s, grad.norm=15.18028164
 18076: 13 [  825/ 1327], train_loss/perplexity = 3.98877931/53.9889450 secs/batch = 0.2670s, grad.norm=16.07010269
 18081: 13 [  830/ 1327], train_loss/perplexity = 3.64355230/38.2273903 secs/batch = 0.2665s, grad.norm=15.99715519
 18086: 13 [  835/ 1327], train_loss/perplexity = 4.00757790/55.0134621 secs/batch = 0.2662s, grad.norm=16.39003944
 18091: 13 [  840/ 1327], train_loss/perplexity = 4.06087160/58.0248642 secs/batch = 0.2666s, grad.norm=16.28152657
 18096: 13 [  845/ 1327], train_loss/perplexity = 3.91223693/50.0106964 secs/batch = 0.2639s, grad.norm=16.75209999
 18101: 13 [  850/ 1327], train_loss/perplexity = 3.93459654/51.1415138 secs/batch = 0.2664s, grad.norm=15.76154041
 18106: 13 [  855/ 1327], train_loss/perplexity = 3.94796634/51.8298569 secs/batch = 0.2659s, grad.norm=16.68898582
 18111: 13 [  860/ 1327], train_loss/perplexity = 3.73013854/41.6848831 secs/batch = 0.2653s, grad.norm=15.72944927
 18116: 13 [  865/ 1327], train_loss/perplexity = 4.18687630/65.8168793 secs/batch = 0.2655s, grad.norm=16.34987450
 18121: 13 [  870/ 1327], train_loss/perplexity = 4.04135609/56.9034576 secs/batch = 0.2664s, grad.norm=16.44521904
 18126: 13 [  875/ 1327], train_loss/perplexity = 3.66350412/38.9977570 secs/batch = 0.2670s, grad.norm=15.53898430
 18131: 13 [  880/ 1327], train_loss/perplexity = 3.81443119/45.3509521 secs/batch = 0.2665s, grad.norm=15.86858463
 18136: 13 [  885/ 1327], train_loss/perplexity = 4.07521057/58.8628731 secs/batch = 0.2661s, grad.norm=15.97253513
 18141: 13 [  890/ 1327], train_loss/perplexity = 4.19302368/66.2227249 secs/batch = 0.2659s, grad.norm=16.01916313
 18146: 13 [  895/ 1327], train_loss/perplexity = 4.04680920/57.2146034 secs/batch = 0.2677s, grad.norm=15.40970230
 18151: 13 [  900/ 1327], train_loss/perplexity = 4.02283812/55.8594170 secs/batch = 0.2658s, grad.norm=15.84778881
 18156: 13 [  905/ 1327], train_loss/perplexity = 3.89923835/49.3648376 secs/batch = 0.2664s, grad.norm=15.62865448
 18161: 13 [  910/ 1327], train_loss/perplexity = 3.91528559/50.1633949 secs/batch = 0.2663s, grad.norm=15.25221252
 18166: 13 [  915/ 1327], train_loss/perplexity = 4.14767075/63.2864189 secs/batch = 0.2685s, grad.norm=15.45422745
 18171: 13 [  920/ 1327], train_loss/perplexity = 4.31049156/74.4770889 secs/batch = 0.2650s, grad.norm=16.17671394
 18176: 13 [  925/ 1327], train_loss/perplexity = 4.02717876/56.1024094 secs/batch = 0.2671s, grad.norm=15.60917187
 18181: 13 [  930/ 1327], train_loss/perplexity = 4.10948896/60.9155807 secs/batch = 0.2661s, grad.norm=15.62291241
 18186: 13 [  935/ 1327], train_loss/perplexity = 4.15666771/63.8583717 secs/batch = 0.2657s, grad.norm=15.70037079
 18191: 13 [  940/ 1327], train_loss/perplexity = 4.17902517/65.3021622 secs/batch = 0.2651s, grad.norm=15.71921825
 18196: 13 [  945/ 1327], train_loss/perplexity = 4.34055853/76.7503967 secs/batch = 0.2668s, grad.norm=15.90187836
 18201: 13 [  950/ 1327], train_loss/perplexity = 4.05934811/57.9365311 secs/batch = 0.2647s, grad.norm=15.79798317
 18206: 13 [  955/ 1327], train_loss/perplexity = 4.04818964/57.2936401 secs/batch = 0.2667s, grad.norm=16.00568199
 18211: 13 [  960/ 1327], train_loss/perplexity = 4.36390972/78.5636978 secs/batch = 0.2665s, grad.norm=16.54403114
 18216: 13 [  965/ 1327], train_loss/perplexity = 4.07862568/59.0642395 secs/batch = 0.2671s, grad.norm=16.37458229
 18221: 13 [  970/ 1327], train_loss/perplexity = 4.34884071/77.3886948 secs/batch = 0.2665s, grad.norm=16.43854904
 18226: 13 [  975/ 1327], train_loss/perplexity = 4.06650066/58.3524094 secs/batch = 0.2667s, grad.norm=17.14539528
 18231: 13 [  980/ 1327], train_loss/perplexity = 3.88425446/48.6306725 secs/batch = 0.2663s, grad.norm=16.13614082
 18236: 13 [  985/ 1327], train_loss/perplexity = 4.00914621/55.0998077 secs/batch = 0.2661s, grad.norm=16.57135963
 18241: 13 [  990/ 1327], train_loss/perplexity = 4.21301556/67.5599670 secs/batch = 0.2647s, grad.norm=16.77456856
 18246: 13 [  995/ 1327], train_loss/perplexity = 4.27487564/71.8712006 secs/batch = 0.2650s, grad.norm=16.08208466
 18251: 13 [ 1000/ 1327], train_loss/perplexity = 3.69409633/40.2092209 secs/batch = 0.2654s, grad.norm=15.53597641
 18256: 13 [ 1005/ 1327], train_loss/perplexity = 4.19416857/66.2985840 secs/batch = 0.2661s, grad.norm=16.36697960
 18261: 13 [ 1010/ 1327], train_loss/perplexity = 3.81790018/45.5085487 secs/batch = 0.2643s, grad.norm=15.31188107
 18266: 13 [ 1015/ 1327], train_loss/perplexity = 4.25297451/70.3142548 secs/batch = 0.2613s, grad.norm=15.82335377
 18271: 13 [ 1020/ 1327], train_loss/perplexity = 4.29354858/73.2258530 secs/batch = 0.2643s, grad.norm=15.96239090
 18276: 13 [ 1025/ 1327], train_loss/perplexity = 4.26216412/70.9633942 secs/batch = 0.2626s, grad.norm=15.98960018
 18281: 13 [ 1030/ 1327], train_loss/perplexity = 4.02535248/56.0000458 secs/batch = 0.2658s, grad.norm=15.38053226
 18286: 13 [ 1035/ 1327], train_loss/perplexity = 3.97615409/53.3116074 secs/batch = 0.2660s, grad.norm=15.62700081
 18291: 13 [ 1040/ 1327], train_loss/perplexity = 4.10630798/60.7221146 secs/batch = 0.2659s, grad.norm=16.09308624
 18296: 13 [ 1045/ 1327], train_loss/perplexity = 3.73937702/42.0717735 secs/batch = 0.2652s, grad.norm=15.35725403
 18301: 13 [ 1050/ 1327], train_loss/perplexity = 3.77361608/43.5372162 secs/batch = 0.2651s, grad.norm=15.92426968
 18306: 13 [ 1055/ 1327], train_loss/perplexity = 3.94780779/51.8216400 secs/batch = 0.2650s, grad.norm=16.65097427
 18311: 13 [ 1060/ 1327], train_loss/perplexity = 3.50465918/33.2701035 secs/batch = 0.2665s, grad.norm=16.73235512
 18316: 13 [ 1065/ 1327], train_loss/perplexity = 3.67113495/39.2964783 secs/batch = 0.2666s, grad.norm=16.30892181
 18321: 13 [ 1070/ 1327], train_loss/perplexity = 3.99271822/54.2020226 secs/batch = 0.2665s, grad.norm=16.43397141
 18326: 13 [ 1075/ 1327], train_loss/perplexity = 3.81765556/45.4974174 secs/batch = 0.2661s, grad.norm=16.41550827
 18331: 13 [ 1080/ 1327], train_loss/perplexity = 3.73448229/41.8663445 secs/batch = 0.2658s, grad.norm=15.86956120
 18336: 13 [ 1085/ 1327], train_loss/perplexity = 3.66013646/38.8666458 secs/batch = 0.2634s, grad.norm=16.14060783
 18341: 13 [ 1090/ 1327], train_loss/perplexity = 3.81081963/45.1874619 secs/batch = 0.2667s, grad.norm=16.82294464
 18346: 13 [ 1095/ 1327], train_loss/perplexity = 4.02983618/56.2516937 secs/batch = 0.2668s, grad.norm=16.68879509
 18351: 13 [ 1100/ 1327], train_loss/perplexity = 3.64640045/38.3364220 secs/batch = 0.2637s, grad.norm=17.57219696
 18356: 13 [ 1105/ 1327], train_loss/perplexity = 3.69729185/40.3379135 secs/batch = 0.2675s, grad.norm=16.47161865
 18361: 13 [ 1110/ 1327], train_loss/perplexity = 3.99761200/54.4679260 secs/batch = 0.2641s, grad.norm=16.98017311
 18366: 13 [ 1115/ 1327], train_loss/perplexity = 3.80102396/44.7469788 secs/batch = 0.2664s, grad.norm=15.74190903
 18371: 13 [ 1120/ 1327], train_loss/perplexity = 4.03356028/56.4615707 secs/batch = 0.2664s, grad.norm=15.74490547
 18376: 13 [ 1125/ 1327], train_loss/perplexity = 4.23575687/69.1139679 secs/batch = 0.2657s, grad.norm=17.42833328
 18381: 13 [ 1130/ 1327], train_loss/perplexity = 3.88372564/48.6049614 secs/batch = 0.2658s, grad.norm=16.55183029
 18386: 13 [ 1135/ 1327], train_loss/perplexity = 3.86747742/47.8215981 secs/batch = 0.2653s, grad.norm=16.28619003
 18391: 13 [ 1140/ 1327], train_loss/perplexity = 4.19931841/66.6408920 secs/batch = 0.2655s, grad.norm=16.97427750
 18396: 13 [ 1145/ 1327], train_loss/perplexity = 3.90783048/49.7908134 secs/batch = 0.2647s, grad.norm=15.85922241
 18401: 13 [ 1150/ 1327], train_loss/perplexity = 3.89668512/49.2389565 secs/batch = 0.2686s, grad.norm=15.96479416
 18406: 13 [ 1155/ 1327], train_loss/perplexity = 4.00140095/54.6746941 secs/batch = 0.2664s, grad.norm=16.78638649
 18411: 13 [ 1160/ 1327], train_loss/perplexity = 3.90920877/49.8594856 secs/batch = 0.2657s, grad.norm=16.51774406
 18416: 13 [ 1165/ 1327], train_loss/perplexity = 3.92972088/50.8927689 secs/batch = 0.2638s, grad.norm=16.80567169
 18421: 13 [ 1170/ 1327], train_loss/perplexity = 3.80574155/44.9585762 secs/batch = 0.2669s, grad.norm=16.48937035
 18426: 13 [ 1175/ 1327], train_loss/perplexity = 3.70808530/40.7756577 secs/batch = 0.2670s, grad.norm=16.12435532
 18431: 13 [ 1180/ 1327], train_loss/perplexity = 3.67611027/39.4924812 secs/batch = 0.2615s, grad.norm=16.35921097
 18436: 13 [ 1185/ 1327], train_loss/perplexity = 3.86002326/47.4664536 secs/batch = 0.2672s, grad.norm=16.64387894
 18441: 13 [ 1190/ 1327], train_loss/perplexity = 3.93893003/51.3636131 secs/batch = 0.2661s, grad.norm=16.98094368
 18446: 13 [ 1195/ 1327], train_loss/perplexity = 3.81705379/45.4700470 secs/batch = 0.2657s, grad.norm=16.77172852
 18451: 13 [ 1200/ 1327], train_loss/perplexity = 3.79559827/44.5048561 secs/batch = 0.2627s, grad.norm=16.51019096
 18456: 13 [ 1205/ 1327], train_loss/perplexity = 3.78570056/44.0665321 secs/batch = 0.2660s, grad.norm=16.78411484
 18461: 13 [ 1210/ 1327], train_loss/perplexity = 3.34213448/28.2794247 secs/batch = 0.2650s, grad.norm=16.69216919
 18466: 13 [ 1215/ 1327], train_loss/perplexity = 3.63462400/37.8876038 secs/batch = 0.2687s, grad.norm=15.76566219
 18471: 13 [ 1220/ 1327], train_loss/perplexity = 3.80011797/44.7064590 secs/batch = 0.2675s, grad.norm=16.81754684
 18476: 13 [ 1225/ 1327], train_loss/perplexity = 3.41329479/30.3651257 secs/batch = 0.2669s, grad.norm=16.86960411
 18481: 13 [ 1230/ 1327], train_loss/perplexity = 3.78929472/44.2251968 secs/batch = 0.2665s, grad.norm=15.92037201
 18486: 13 [ 1235/ 1327], train_loss/perplexity = 3.71922874/41.2325821 secs/batch = 0.2655s, grad.norm=16.02258873
 18491: 13 [ 1240/ 1327], train_loss/perplexity = 3.92364264/50.5843697 secs/batch = 0.2621s, grad.norm=17.04582214
 18496: 13 [ 1245/ 1327], train_loss/perplexity = 3.87722445/48.2899971 secs/batch = 0.2657s, grad.norm=16.11272812
 18501: 13 [ 1250/ 1327], train_loss/perplexity = 3.95582771/52.2389145 secs/batch = 0.2658s, grad.norm=16.32963943
 18506: 13 [ 1255/ 1327], train_loss/perplexity = 4.07461166/58.8276329 secs/batch = 0.2677s, grad.norm=15.97374630
 18511: 13 [ 1260/ 1327], train_loss/perplexity = 3.78073454/43.8482399 secs/batch = 0.2682s, grad.norm=17.02735519
 18516: 13 [ 1265/ 1327], train_loss/perplexity = 3.98571539/53.8237801 secs/batch = 0.2666s, grad.norm=16.25423241
 18521: 13 [ 1270/ 1327], train_loss/perplexity = 3.78660464/44.1063881 secs/batch = 0.2670s, grad.norm=16.92664528
 18526: 13 [ 1275/ 1327], train_loss/perplexity = 3.94373560/51.6110382 secs/batch = 0.2661s, grad.norm=16.38960457
 18531: 13 [ 1280/ 1327], train_loss/perplexity = 3.79489827/44.4737129 secs/batch = 0.2668s, grad.norm=16.80684662
 18536: 13 [ 1285/ 1327], train_loss/perplexity = 3.66783428/39.1669884 secs/batch = 0.2661s, grad.norm=16.39664841
 18541: 13 [ 1290/ 1327], train_loss/perplexity = 3.94139671/51.4904671 secs/batch = 0.2649s, grad.norm=16.43120384
 18546: 13 [ 1295/ 1327], train_loss/perplexity = 3.92239308/50.5212021 secs/batch = 0.2659s, grad.norm=16.22669601
 18551: 13 [ 1300/ 1327], train_loss/perplexity = 4.10034132/60.3608856 secs/batch = 0.2653s, grad.norm=16.33548164
 18556: 13 [ 1305/ 1327], train_loss/perplexity = 4.08758593/59.5958481 secs/batch = 0.2682s, grad.norm=16.42339897
 18561: 13 [ 1310/ 1327], train_loss/perplexity = 4.38001156/79.8389587 secs/batch = 0.2662s, grad.norm=16.99862099
 18566: 13 [ 1315/ 1327], train_loss/perplexity = 4.17942905/65.3285446 secs/batch = 0.2656s, grad.norm=16.23269844
 18571: 13 [ 1320/ 1327], train_loss/perplexity = 4.17968988/65.3455887 secs/batch = 0.2679s, grad.norm=16.64382744
 18576: 13 [ 1325/ 1327], train_loss/perplexity = 4.10428095/60.5991554 secs/batch = 0.2662s, grad.norm=16.51217270
Epoch training time: 352.97727370262146
	> validation loss = 4.61799622, perplexity = 101.29086304
	> validation loss = 4.57163382, perplexity = 96.70197296
	> validation loss = 4.51082611, perplexity = 90.99696350
	> validation loss = 4.56018305, perplexity = 95.60097504
	> validation loss = 4.68702316, perplexity = 108.52962494
	> validation loss = 4.63551235, perplexity = 103.08071899
	> validation loss = 4.55898952, perplexity = 95.48694611
	> validation loss = 4.43425083, perplexity = 84.28895569
	> validation loss = 4.20389271, perplexity = 66.94642639
	> validation loss = 4.31022739, perplexity = 74.45742035
	> validation loss = 4.49602604, perplexity = 89.66011810
	> validation loss = 4.50745440, perplexity = 90.69065857
	> validation loss = 4.47870731, perplexity = 88.12068939
	> validation loss = 4.20677567, perplexity = 67.13970947
	> validation loss = 4.16939116, perplexity = 64.67606354
	> validation loss = 4.22927952, perplexity = 68.66773987
	> validation loss = 4.63054752, perplexity = 102.57020569
	> validation loss = 4.13883018, perplexity = 62.72939682
	> validation loss = 4.63347101, perplexity = 102.87050629
	> validation loss = 4.50374889, perplexity = 90.35523224
	> validation loss = 4.28786039, perplexity = 72.81051636
at the end of epoch: 13
train loss = 4.05461093, perplexity = 57.66272359
validation loss = 4.45463237, perplexity = 86.02452010
Saved model cv/epoch013_4.4546.model
 18583: 14 [    5/ 1327], train_loss/perplexity = 4.22812271/68.5883484 secs/batch = 0.2649s, grad.norm=16.47527122
 18588: 14 [   10/ 1327], train_loss/perplexity = 3.84337831/46.6829185 secs/batch = 0.2655s, grad.norm=15.84750366
 18593: 14 [   15/ 1327], train_loss/perplexity = 4.12802601/62.0553055 secs/batch = 0.2658s, grad.norm=15.60121822
 18598: 14 [   20/ 1327], train_loss/perplexity = 4.34142685/76.8170700 secs/batch = 0.2666s, grad.norm=15.89251232
 18603: 14 [   25/ 1327], train_loss/perplexity = 4.20290804/66.8805389 secs/batch = 0.2669s, grad.norm=16.89690018
 18608: 14 [   30/ 1327], train_loss/perplexity = 4.22777987/68.5648422 secs/batch = 0.2615s, grad.norm=16.62154961
 18613: 14 [   35/ 1327], train_loss/perplexity = 4.03404427/56.4889069 secs/batch = 0.2666s, grad.norm=16.30744743
 18618: 14 [   40/ 1327], train_loss/perplexity = 4.02202845/55.8142090 secs/batch = 0.2674s, grad.norm=16.18503189
 18623: 14 [   45/ 1327], train_loss/perplexity = 3.84070897/46.5584717 secs/batch = 0.2665s, grad.norm=15.82642078
 18628: 14 [   50/ 1327], train_loss/perplexity = 4.06618452/58.3339653 secs/batch = 0.2654s, grad.norm=16.46451569
 18633: 14 [   55/ 1327], train_loss/perplexity = 3.91030931/49.9143906 secs/batch = 0.2656s, grad.norm=16.33227921
 18638: 14 [   60/ 1327], train_loss/perplexity = 4.23536444/69.0868530 secs/batch = 0.2628s, grad.norm=16.47092438
 18643: 14 [   65/ 1327], train_loss/perplexity = 3.84651041/46.8293610 secs/batch = 0.2659s, grad.norm=15.92452431
 18648: 14 [   70/ 1327], train_loss/perplexity = 3.69838786/40.3821487 secs/batch = 0.2622s, grad.norm=16.15707779
 18653: 14 [   75/ 1327], train_loss/perplexity = 3.48890328/32.7500114 secs/batch = 0.2668s, grad.norm=15.13202381
 18658: 14 [   80/ 1327], train_loss/perplexity = 3.92122912/50.4624329 secs/batch = 0.2669s, grad.norm=16.26417351
 18663: 14 [   85/ 1327], train_loss/perplexity = 3.93415546/51.1189613 secs/batch = 0.2662s, grad.norm=16.24714661
 18668: 14 [   90/ 1327], train_loss/perplexity = 4.03028822/56.2771301 secs/batch = 0.2667s, grad.norm=16.51755333
 18673: 14 [   95/ 1327], train_loss/perplexity = 3.90913773/49.8559456 secs/batch = 0.2685s, grad.norm=16.38616753
 18678: 14 [  100/ 1327], train_loss/perplexity = 4.08271122/59.3060455 secs/batch = 0.2666s, grad.norm=16.53184128
 18683: 14 [  105/ 1327], train_loss/perplexity = 4.00163078/54.6872597 secs/batch = 0.2653s, grad.norm=16.91621590
 18688: 14 [  110/ 1327], train_loss/perplexity = 3.84562135/46.7877464 secs/batch = 0.2667s, grad.norm=16.15487289
 18693: 14 [  115/ 1327], train_loss/perplexity = 3.82251239/45.7189293 secs/batch = 0.2657s, grad.norm=16.65187454
 18698: 14 [  120/ 1327], train_loss/perplexity = 3.87624574/48.2427597 secs/batch = 0.2664s, grad.norm=17.05014992
 18703: 14 [  125/ 1327], train_loss/perplexity = 3.93890405/51.3622780 secs/batch = 0.2657s, grad.norm=16.65496826
 18708: 14 [  130/ 1327], train_loss/perplexity = 3.90561652/49.6806984 secs/batch = 0.2664s, grad.norm=17.14591980
 18713: 14 [  135/ 1327], train_loss/perplexity = 3.93062878/50.9389954 secs/batch = 0.2647s, grad.norm=16.15083885
 18718: 14 [  140/ 1327], train_loss/perplexity = 4.24386597/69.6766968 secs/batch = 0.2667s, grad.norm=16.86418533
 18723: 14 [  145/ 1327], train_loss/perplexity = 4.07637215/58.9312859 secs/batch = 0.2684s, grad.norm=17.31655884
 18728: 14 [  150/ 1327], train_loss/perplexity = 4.12688065/61.9842720 secs/batch = 0.2647s, grad.norm=16.86710167
 18733: 14 [  155/ 1327], train_loss/perplexity = 4.39952755/81.4123993 secs/batch = 0.2663s, grad.norm=16.52398491
 18738: 14 [  160/ 1327], train_loss/perplexity = 4.05265903/57.5502815 secs/batch = 0.2673s, grad.norm=15.49759579
 18743: 14 [  165/ 1327], train_loss/perplexity = 4.27617884/71.9649277 secs/batch = 0.2665s, grad.norm=16.49465942
 18748: 14 [  170/ 1327], train_loss/perplexity = 3.96827507/52.8932152 secs/batch = 0.2648s, grad.norm=15.98137283
 18753: 14 [  175/ 1327], train_loss/perplexity = 4.25574636/70.5094223 secs/batch = 0.2667s, grad.norm=15.95660782
 18758: 14 [  180/ 1327], train_loss/perplexity = 4.09426832/59.9954262 secs/batch = 0.2652s, grad.norm=16.51369095
 18763: 14 [  185/ 1327], train_loss/perplexity = 4.39114237/80.7325897 secs/batch = 0.2666s, grad.norm=16.52838898
 18768: 14 [  190/ 1327], train_loss/perplexity = 3.95256948/52.0689850 secs/batch = 0.2684s, grad.norm=15.77897549
 18773: 14 [  195/ 1327], train_loss/perplexity = 4.26510715/71.1725464 secs/batch = 0.2664s, grad.norm=15.79968739
 18778: 14 [  200/ 1327], train_loss/perplexity = 4.06126451/58.0476685 secs/batch = 0.2667s, grad.norm=16.93235970
 18783: 14 [  205/ 1327], train_loss/perplexity = 4.31786489/75.0282669 secs/batch = 0.2674s, grad.norm=16.51963806
 18788: 14 [  210/ 1327], train_loss/perplexity = 4.17304802/64.9130096 secs/batch = 0.2648s, grad.norm=15.51275826
 18793: 14 [  215/ 1327], train_loss/perplexity = 4.29881144/73.6122513 secs/batch = 0.2671s, grad.norm=15.85548306
 18798: 14 [  220/ 1327], train_loss/perplexity = 4.19087839/66.0808105 secs/batch = 0.2621s, grad.norm=16.10926247
 18803: 14 [  225/ 1327], train_loss/perplexity = 4.32658052/75.6850433 secs/batch = 0.2673s, grad.norm=16.34815216
 18808: 14 [  230/ 1327], train_loss/perplexity = 4.27022457/71.5376968 secs/batch = 0.2660s, grad.norm=16.89424706
 18813: 14 [  235/ 1327], train_loss/perplexity = 4.05393314/57.6236534 secs/batch = 0.2669s, grad.norm=16.18800545
 18818: 14 [  240/ 1327], train_loss/perplexity = 3.78921461/44.2216568 secs/batch = 0.2618s, grad.norm=16.27263832
 18823: 14 [  245/ 1327], train_loss/perplexity = 4.11046743/60.9752121 secs/batch = 0.2662s, grad.norm=16.40785599
 18828: 14 [  250/ 1327], train_loss/perplexity = 3.96290159/52.6097565 secs/batch = 0.2665s, grad.norm=15.70156288
 18833: 14 [  255/ 1327], train_loss/perplexity = 3.96676064/52.8131714 secs/batch = 0.2672s, grad.norm=16.39700508
 18838: 14 [  260/ 1327], train_loss/perplexity = 4.16272736/64.2465057 secs/batch = 0.2655s, grad.norm=17.03012848
 18843: 14 [  265/ 1327], train_loss/perplexity = 4.37848711/79.7173386 secs/batch = 0.2661s, grad.norm=16.42478561
 18848: 14 [  270/ 1327], train_loss/perplexity = 4.39062452/80.6907959 secs/batch = 0.2645s, grad.norm=16.55596161
 18853: 14 [  275/ 1327], train_loss/perplexity = 4.39433289/80.9905853 secs/batch = 0.2661s, grad.norm=16.10711670
 18858: 14 [  280/ 1327], train_loss/perplexity = 4.21293926/67.5548096 secs/batch = 0.2625s, grad.norm=15.76836872
 18863: 14 [  285/ 1327], train_loss/perplexity = 4.50006676/90.0231400 secs/batch = 0.2661s, grad.norm=16.16276550
 18868: 14 [  290/ 1327], train_loss/perplexity = 4.19596195/66.4175949 secs/batch = 0.2662s, grad.norm=16.69870377
 18873: 14 [  295/ 1327], train_loss/perplexity = 3.99739385/54.4560432 secs/batch = 0.2660s, grad.norm=16.32377434
 18878: 14 [  300/ 1327], train_loss/perplexity = 3.45997000/31.8160229 secs/batch = 0.2676s, grad.norm=15.55158329
 18883: 14 [  305/ 1327], train_loss/perplexity = 3.96095991/52.5077057 secs/batch = 0.2670s, grad.norm=16.18800354
 18888: 14 [  310/ 1327], train_loss/perplexity = 4.04406071/57.0575676 secs/batch = 0.2676s, grad.norm=16.37344742
 18893: 14 [  315/ 1327], train_loss/perplexity = 3.52591133/33.9847298 secs/batch = 0.2645s, grad.norm=15.56036663
 18898: 14 [  320/ 1327], train_loss/perplexity = 3.48148894/32.5080872 secs/batch = 0.2666s, grad.norm=16.72595978
 18903: 14 [  325/ 1327], train_loss/perplexity = 3.51043344/33.4627686 secs/batch = 0.2663s, grad.norm=15.71773148
 18908: 14 [  330/ 1327], train_loss/perplexity = 4.14705992/63.2477722 secs/batch = 0.2643s, grad.norm=16.47968864
 18913: 14 [  335/ 1327], train_loss/perplexity = 3.59109521/36.2737808 secs/batch = 0.2642s, grad.norm=15.40906715
 18918: 14 [  340/ 1327], train_loss/perplexity = 4.31793880/75.0338058 secs/batch = 0.2645s, grad.norm=16.19787216
 18923: 14 [  345/ 1327], train_loss/perplexity = 4.06611204/58.3297386 secs/batch = 0.2663s, grad.norm=16.08966446
 18928: 14 [  350/ 1327], train_loss/perplexity = 4.06490183/58.2591896 secs/batch = 0.2663s, grad.norm=16.83093643
 18933: 14 [  355/ 1327], train_loss/perplexity = 4.00290108/54.7567749 secs/batch = 0.2631s, grad.norm=16.31859207
 18938: 14 [  360/ 1327], train_loss/perplexity = 4.20486689/67.0116806 secs/batch = 0.2665s, grad.norm=17.97790527
 18943: 14 [  365/ 1327], train_loss/perplexity = 4.17205381/64.8485031 secs/batch = 0.2667s, grad.norm=16.89221382
 18948: 14 [  370/ 1327], train_loss/perplexity = 4.24538565/69.7826691 secs/batch = 0.2658s, grad.norm=16.64723587
 18953: 14 [  375/ 1327], train_loss/perplexity = 3.68060541/39.6704025 secs/batch = 0.2664s, grad.norm=16.62888908
 18958: 14 [  380/ 1327], train_loss/perplexity = 3.75022054/42.5304604 secs/batch = 0.2669s, grad.norm=16.98300552
 18963: 14 [  385/ 1327], train_loss/perplexity = 3.90921497/49.8597946 secs/batch = 0.2670s, grad.norm=16.95428848
 18968: 14 [  390/ 1327], train_loss/perplexity = 4.03089666/56.3113823 secs/batch = 0.2664s, grad.norm=16.46053886
 18973: 14 [  395/ 1327], train_loss/perplexity = 4.08684540/59.5517349 secs/batch = 0.2655s, grad.norm=16.46968651
 18978: 14 [  400/ 1327], train_loss/perplexity = 4.08656168/59.5348396 secs/batch = 0.2654s, grad.norm=16.29717445
 18983: 14 [  405/ 1327], train_loss/perplexity = 4.27882338/72.1554871 secs/batch = 0.2596s, grad.norm=16.43011093
 18988: 14 [  410/ 1327], train_loss/perplexity = 3.95127988/52.0018806 secs/batch = 0.2662s, grad.norm=16.50431824
 18993: 14 [  415/ 1327], train_loss/perplexity = 3.93614936/51.2209892 secs/batch = 0.2647s, grad.norm=16.13528633
 18998: 14 [  420/ 1327], train_loss/perplexity = 3.56235170/35.2459869 secs/batch = 0.2664s, grad.norm=16.28258133
 19003: 14 [  425/ 1327], train_loss/perplexity = 3.91203046/50.0003738 secs/batch = 0.2650s, grad.norm=17.39730453
 19008: 14 [  430/ 1327], train_loss/perplexity = 4.07931328/59.1048660 secs/batch = 0.2656s, grad.norm=16.90114212
 19013: 14 [  435/ 1327], train_loss/perplexity = 4.19236803/66.1793213 secs/batch = 0.2649s, grad.norm=16.93563461
 19018: 14 [  440/ 1327], train_loss/perplexity = 3.66049075/38.8804207 secs/batch = 0.2658s, grad.norm=16.29460526
 19023: 14 [  445/ 1327], train_loss/perplexity = 4.04223967/56.9537582 secs/batch = 0.2677s, grad.norm=17.29332352
 19028: 14 [  450/ 1327], train_loss/perplexity = 4.03416204/56.4955597 secs/batch = 0.2667s, grad.norm=16.62341118
 19033: 14 [  455/ 1327], train_loss/perplexity = 3.97752929/53.3849716 secs/batch = 0.2658s, grad.norm=16.03542519
 19038: 14 [  460/ 1327], train_loss/perplexity = 3.92141342/50.4717331 secs/batch = 0.2657s, grad.norm=16.68534470
 19043: 14 [  465/ 1327], train_loss/perplexity = 3.68748212/39.9441452 secs/batch = 0.2657s, grad.norm=17.76317978
 19048: 14 [  470/ 1327], train_loss/perplexity = 4.34466982/77.0665894 secs/batch = 0.2663s, grad.norm=16.19851685
 19053: 14 [  475/ 1327], train_loss/perplexity = 3.79447269/44.4547882 secs/batch = 0.2664s, grad.norm=16.57917213
 19058: 14 [  480/ 1327], train_loss/perplexity = 3.94696760/51.7781181 secs/batch = 0.2668s, grad.norm=16.39107132
 19063: 14 [  485/ 1327], train_loss/perplexity = 3.94401360/51.6253891 secs/batch = 0.2669s, grad.norm=16.82988548
 19068: 14 [  490/ 1327], train_loss/perplexity = 3.74644995/42.3703957 secs/batch = 0.2650s, grad.norm=17.21869659
 19073: 14 [  495/ 1327], train_loss/perplexity = 3.88674641/48.7520103 secs/batch = 0.2615s, grad.norm=16.31708717
 19078: 14 [  500/ 1327], train_loss/perplexity = 4.01740456/55.5567245 secs/batch = 0.2661s, grad.norm=16.64593697
 19083: 14 [  505/ 1327], train_loss/perplexity = 4.20108986/66.7590485 secs/batch = 0.2666s, grad.norm=15.75903416
 19088: 14 [  510/ 1327], train_loss/perplexity = 4.53379440/93.1111908 secs/batch = 0.2653s, grad.norm=15.95322990
 19093: 14 [  515/ 1327], train_loss/perplexity = 4.10181713/60.4500351 secs/batch = 0.2661s, grad.norm=15.77876568
 19098: 14 [  520/ 1327], train_loss/perplexity = 4.32034492/75.2145691 secs/batch = 0.2670s, grad.norm=16.75831795
 19103: 14 [  525/ 1327], train_loss/perplexity = 3.91392827/50.0953522 secs/batch = 0.2666s, grad.norm=16.22012329
 19108: 14 [  530/ 1327], train_loss/perplexity = 3.87944651/48.3974190 secs/batch = 0.2666s, grad.norm=16.55316925
 19113: 14 [  535/ 1327], train_loss/perplexity = 4.10724926/60.7793007 secs/batch = 0.2651s, grad.norm=16.53273201
 19118: 14 [  540/ 1327], train_loss/perplexity = 4.14585638/63.1716995 secs/batch = 0.2660s, grad.norm=16.49206924
 19123: 14 [  545/ 1327], train_loss/perplexity = 4.08113289/59.2125130 secs/batch = 0.2646s, grad.norm=16.34437752
 19128: 14 [  550/ 1327], train_loss/perplexity = 4.06097269/58.0307312 secs/batch = 0.2659s, grad.norm=16.33120728
 19133: 14 [  555/ 1327], train_loss/perplexity = 3.96218419/52.5720291 secs/batch = 0.2663s, grad.norm=16.54266548
 19138: 14 [  560/ 1327], train_loss/perplexity = 4.08250427/59.2937737 secs/batch = 0.2657s, grad.norm=17.37479973
 19143: 14 [  565/ 1327], train_loss/perplexity = 3.93681645/51.2551689 secs/batch = 0.2663s, grad.norm=17.40594673
 19148: 14 [  570/ 1327], train_loss/perplexity = 3.93913031/51.3739014 secs/batch = 0.2663s, grad.norm=17.23877335
 19153: 14 [  575/ 1327], train_loss/perplexity = 3.76144218/43.0104103 secs/batch = 0.2664s, grad.norm=16.69025230
 19158: 14 [  580/ 1327], train_loss/perplexity = 4.16586590/64.4484634 secs/batch = 0.2656s, grad.norm=17.56044769
 19163: 14 [  585/ 1327], train_loss/perplexity = 3.71981788/41.2568779 secs/batch = 0.2670s, grad.norm=16.60617065
 19168: 14 [  590/ 1327], train_loss/perplexity = 4.08401775/59.3835793 secs/batch = 0.2650s, grad.norm=16.28001976
 19173: 14 [  595/ 1327], train_loss/perplexity = 4.08021069/59.1579323 secs/batch = 0.2642s, grad.norm=17.03552437
 19178: 14 [  600/ 1327], train_loss/perplexity = 4.27379274/71.7934113 secs/batch = 0.2658s, grad.norm=16.01765442
 19183: 14 [  605/ 1327], train_loss/perplexity = 4.10588694/60.6965561 secs/batch = 0.2661s, grad.norm=16.08708572
 19188: 14 [  610/ 1327], train_loss/perplexity = 4.29722404/73.4954910 secs/batch = 0.2646s, grad.norm=16.24256897
 19193: 14 [  615/ 1327], train_loss/perplexity = 3.84940290/46.9650116 secs/batch = 0.2669s, grad.norm=15.76795483
 19198: 14 [  620/ 1327], train_loss/perplexity = 4.28278637/72.4420090 secs/batch = 0.2661s, grad.norm=16.32610130
 19203: 14 [  625/ 1327], train_loss/perplexity = 4.22252035/68.2051697 secs/batch = 0.2660s, grad.norm=16.58001137
 19208: 14 [  630/ 1327], train_loss/perplexity = 4.28886652/72.8838120 secs/batch = 0.2619s, grad.norm=16.37093163
 19213: 14 [  635/ 1327], train_loss/perplexity = 4.03183556/56.3642769 secs/batch = 0.2655s, grad.norm=16.33730316
 19218: 14 [  640/ 1327], train_loss/perplexity = 4.05943632/57.9416428 secs/batch = 0.2655s, grad.norm=16.45576859
 19223: 14 [  645/ 1327], train_loss/perplexity = 4.28488159/72.5939484 secs/batch = 0.2670s, grad.norm=17.24052620
 19228: 14 [  650/ 1327], train_loss/perplexity = 3.81386995/45.3255081 secs/batch = 0.2664s, grad.norm=16.85534286
 19233: 14 [  655/ 1327], train_loss/perplexity = 3.98704052/53.8951530 secs/batch = 0.2662s, grad.norm=16.65911293
 19238: 14 [  660/ 1327], train_loss/perplexity = 3.80682087/45.0071259 secs/batch = 0.2641s, grad.norm=16.62629700
 19243: 14 [  665/ 1327], train_loss/perplexity = 4.02022362/55.7135620 secs/batch = 0.2656s, grad.norm=16.55644035
 19248: 14 [  670/ 1327], train_loss/perplexity = 3.96728992/52.8411331 secs/batch = 0.2640s, grad.norm=16.55514145
 19253: 14 [  675/ 1327], train_loss/perplexity = 3.76995802/43.3782425 secs/batch = 0.2589s, grad.norm=16.89151192
 19258: 14 [  680/ 1327], train_loss/perplexity = 3.97875929/53.4506760 secs/batch = 0.2665s, grad.norm=17.32392502
 19263: 14 [  685/ 1327], train_loss/perplexity = 3.70356798/40.5918770 secs/batch = 0.2638s, grad.norm=16.51297379
 19268: 14 [  690/ 1327], train_loss/perplexity = 4.17786264/65.2262955 secs/batch = 0.2653s, grad.norm=16.17414856
 19273: 14 [  695/ 1327], train_loss/perplexity = 3.94924641/51.8962440 secs/batch = 0.2657s, grad.norm=16.70377922
 19278: 14 [  700/ 1327], train_loss/perplexity = 4.21859550/67.9379959 secs/batch = 0.2628s, grad.norm=16.81710815
 19283: 14 [  705/ 1327], train_loss/perplexity = 3.94356346/51.6021576 secs/batch = 0.2664s, grad.norm=15.82047367
 19288: 14 [  710/ 1327], train_loss/perplexity = 3.91597843/50.1981621 secs/batch = 0.2665s, grad.norm=17.06493950
 19293: 14 [  715/ 1327], train_loss/perplexity = 3.82752252/45.9485588 secs/batch = 0.2651s, grad.norm=16.53018951
 19298: 14 [  720/ 1327], train_loss/perplexity = 3.80041099/44.7195587 secs/batch = 0.2668s, grad.norm=17.27444458
 19303: 14 [  725/ 1327], train_loss/perplexity = 3.81252146/45.2644272 secs/batch = 0.2664s, grad.norm=17.03903770
 19308: 14 [  730/ 1327], train_loss/perplexity = 3.97160983/53.0698967 secs/batch = 0.2661s, grad.norm=16.93717575
 19313: 14 [  735/ 1327], train_loss/perplexity = 4.02828217/56.1643486 secs/batch = 0.2657s, grad.norm=17.19295883
 19318: 14 [  740/ 1327], train_loss/perplexity = 3.57990503/35.8701324 secs/batch = 0.2628s, grad.norm=16.12003136
 19323: 14 [  745/ 1327], train_loss/perplexity = 3.99227047/54.1777573 secs/batch = 0.2656s, grad.norm=17.09113884
 19328: 14 [  750/ 1327], train_loss/perplexity = 3.95940781/52.4262695 secs/batch = 0.2661s, grad.norm=16.68487549
 19333: 14 [  755/ 1327], train_loss/perplexity = 3.77146006/43.4434471 secs/batch = 0.2669s, grad.norm=16.09078789
 19338: 14 [  760/ 1327], train_loss/perplexity = 3.57465243/35.6822166 secs/batch = 0.2658s, grad.norm=15.53121567
 19343: 14 [  765/ 1327], train_loss/perplexity = 3.73290777/41.8004761 secs/batch = 0.2644s, grad.norm=16.27929306
 19348: 14 [  770/ 1327], train_loss/perplexity = 3.64595723/38.3194351 secs/batch = 0.2658s, grad.norm=16.05091286
 19353: 14 [  775/ 1327], train_loss/perplexity = 3.78993773/44.2536430 secs/batch = 0.2662s, grad.norm=16.92298698
 19358: 14 [  780/ 1327], train_loss/perplexity = 4.13643694/62.5794487 secs/batch = 0.2609s, grad.norm=17.25364685
 19363: 14 [  785/ 1327], train_loss/perplexity = 3.95884752/52.3969040 secs/batch = 0.2658s, grad.norm=16.94986916
 19368: 14 [  790/ 1327], train_loss/perplexity = 3.77785492/43.7221527 secs/batch = 0.2667s, grad.norm=16.86717796
 19373: 14 [  795/ 1327], train_loss/perplexity = 4.17422819/64.9896622 secs/batch = 0.2660s, grad.norm=16.69386864
 19378: 14 [  800/ 1327], train_loss/perplexity = 4.03554058/56.5734940 secs/batch = 0.2638s, grad.norm=16.91360092
 19383: 14 [  805/ 1327], train_loss/perplexity = 4.38218546/80.0127106 secs/batch = 0.2667s, grad.norm=16.91691971
 19388: 14 [  810/ 1327], train_loss/perplexity = 3.91452837/50.1254234 secs/batch = 0.2636s, grad.norm=15.61026192
 19393: 14 [  815/ 1327], train_loss/perplexity = 3.86073494/47.5002480 secs/batch = 0.2660s, grad.norm=16.04781151
 19398: 14 [  820/ 1327], train_loss/perplexity = 3.72054887/41.2870483 secs/batch = 0.2660s, grad.norm=15.40405560
 19403: 14 [  825/ 1327], train_loss/perplexity = 3.97362757/53.1770859 secs/batch = 0.2665s, grad.norm=16.53986931
 19408: 14 [  830/ 1327], train_loss/perplexity = 3.63533020/37.9143715 secs/batch = 0.2656s, grad.norm=16.74848366
 19413: 14 [  835/ 1327], train_loss/perplexity = 3.95748353/52.3254852 secs/batch = 0.2668s, grad.norm=16.72341347
 19418: 14 [  840/ 1327], train_loss/perplexity = 4.00872231/55.0764542 secs/batch = 0.2657s, grad.norm=16.35210419
 19423: 14 [  845/ 1327], train_loss/perplexity = 3.84863949/46.9291725 secs/batch = 0.2663s, grad.norm=16.67006493
 19428: 14 [  850/ 1327], train_loss/perplexity = 3.86446571/47.6777916 secs/batch = 0.2659s, grad.norm=15.95059967
 19433: 14 [  855/ 1327], train_loss/perplexity = 3.95135570/52.0058250 secs/batch = 0.2666s, grad.norm=16.94188499
 19438: 14 [  860/ 1327], train_loss/perplexity = 3.64829779/38.4092293 secs/batch = 0.2662s, grad.norm=15.92730522
 19443: 14 [  865/ 1327], train_loss/perplexity = 4.13825607/62.6933937 secs/batch = 0.2630s, grad.norm=16.78131866
 19448: 14 [  870/ 1327], train_loss/perplexity = 3.99963903/54.5784454 secs/batch = 0.2663s, grad.norm=16.99284935
 19453: 14 [  875/ 1327], train_loss/perplexity = 3.58086681/35.9046516 secs/batch = 0.2640s, grad.norm=16.14855385
 19458: 14 [  880/ 1327], train_loss/perplexity = 3.81183290/45.2332726 secs/batch = 0.2662s, grad.norm=16.73557091
 19463: 14 [  885/ 1327], train_loss/perplexity = 3.95891261/52.4003143 secs/batch = 0.2662s, grad.norm=15.89652729
 19468: 14 [  890/ 1327], train_loss/perplexity = 4.09055138/59.7728386 secs/batch = 0.2653s, grad.norm=16.39426231
 19473: 14 [  895/ 1327], train_loss/perplexity = 4.06816816/58.4497948 secs/batch = 0.2656s, grad.norm=15.95861626
 19478: 14 [  900/ 1327], train_loss/perplexity = 3.92427254/50.6162415 secs/batch = 0.2653s, grad.norm=15.67025185
 19483: 14 [  905/ 1327], train_loss/perplexity = 3.85532522/47.2439804 secs/batch = 0.2655s, grad.norm=15.93847847
 19488: 14 [  910/ 1327], train_loss/perplexity = 3.87446237/48.1567993 secs/batch = 0.2673s, grad.norm=15.34499550
 19493: 14 [  915/ 1327], train_loss/perplexity = 4.16159105/64.1735458 secs/batch = 0.2669s, grad.norm=16.42334747
 19498: 14 [  920/ 1327], train_loss/perplexity = 4.25483608/70.4452667 secs/batch = 0.2661s, grad.norm=16.36842728
 19503: 14 [  925/ 1327], train_loss/perplexity = 3.99664831/54.4154587 secs/batch = 0.2673s, grad.norm=16.21494293
 19508: 14 [  930/ 1327], train_loss/perplexity = 4.10876656/60.8715897 secs/batch = 0.2665s, grad.norm=16.11255074
 19513: 14 [  935/ 1327], train_loss/perplexity = 4.15262079/63.6004639 secs/batch = 0.2658s, grad.norm=16.02536774
 19518: 14 [  940/ 1327], train_loss/perplexity = 4.06967068/58.5376816 secs/batch = 0.2649s, grad.norm=15.77691174
 19523: 14 [  945/ 1327], train_loss/perplexity = 4.29808140/73.5585251 secs/batch = 0.2583s, grad.norm=16.05990219
 19528: 14 [  950/ 1327], train_loss/perplexity = 4.04312611/57.0042648 secs/batch = 0.2668s, grad.norm=16.22693825
 19533: 14 [  955/ 1327], train_loss/perplexity = 4.07443190/58.8170586 secs/batch = 0.2623s, grad.norm=16.66747284
 19538: 14 [  960/ 1327], train_loss/perplexity = 4.34598255/77.1678238 secs/batch = 0.2657s, grad.norm=16.91990280
 19543: 14 [  965/ 1327], train_loss/perplexity = 4.04102468/56.8846016 secs/batch = 0.2644s, grad.norm=16.28020096
 19548: 14 [  970/ 1327], train_loss/perplexity = 4.27985144/72.2297058 secs/batch = 0.2653s, grad.norm=16.80713844
 19553: 14 [  975/ 1327], train_loss/perplexity = 4.00278282/54.7502975 secs/batch = 0.2639s, grad.norm=17.64064789
 19558: 14 [  980/ 1327], train_loss/perplexity = 3.83136129/46.1252861 secs/batch = 0.2671s, grad.norm=16.33779526
 19563: 14 [  985/ 1327], train_loss/perplexity = 3.98061585/53.5500031 secs/batch = 0.2668s, grad.norm=17.07264137
 19568: 14 [  990/ 1327], train_loss/perplexity = 4.22323513/68.2539368 secs/batch = 0.2670s, grad.norm=17.41650963
 19573: 14 [  995/ 1327], train_loss/perplexity = 4.21781158/67.8847580 secs/batch = 0.2656s, grad.norm=16.16597748
 19578: 14 [ 1000/ 1327], train_loss/perplexity = 3.72493291/41.4684486 secs/batch = 0.2616s, grad.norm=15.93964481
 19583: 14 [ 1005/ 1327], train_loss/perplexity = 4.12992239/62.1730957 secs/batch = 0.2665s, grad.norm=16.03465080
 19588: 14 [ 1010/ 1327], train_loss/perplexity = 3.78738594/44.1408615 secs/batch = 0.2651s, grad.norm=15.74717712
 19593: 14 [ 1015/ 1327], train_loss/perplexity = 4.23184633/68.8442230 secs/batch = 0.2672s, grad.norm=16.07068443
 19598: 14 [ 1020/ 1327], train_loss/perplexity = 4.35890245/78.1712875 secs/batch = 0.2668s, grad.norm=16.87771225
 19603: 14 [ 1025/ 1327], train_loss/perplexity = 4.20592451/67.0825882 secs/batch = 0.2656s, grad.norm=16.04743195
 19608: 14 [ 1030/ 1327], train_loss/perplexity = 3.96937585/52.9514694 secs/batch = 0.2659s, grad.norm=16.15796280
 19613: 14 [ 1035/ 1327], train_loss/perplexity = 3.91309094/50.0534248 secs/batch = 0.2665s, grad.norm=16.10021782
 19618: 14 [ 1040/ 1327], train_loss/perplexity = 4.14111328/62.8727760 secs/batch = 0.2656s, grad.norm=16.53203011
 19623: 14 [ 1045/ 1327], train_loss/perplexity = 3.73186445/41.7568893 secs/batch = 0.2673s, grad.norm=16.14985466
 19628: 14 [ 1050/ 1327], train_loss/perplexity = 3.72237301/41.3624306 secs/batch = 0.2681s, grad.norm=16.57933235
 19633: 14 [ 1055/ 1327], train_loss/perplexity = 3.88536787/48.6848488 secs/batch = 0.2628s, grad.norm=17.21460915
 19638: 14 [ 1060/ 1327], train_loss/perplexity = 3.53927565/34.4419632 secs/batch = 0.2663s, grad.norm=17.61582756
 19643: 14 [ 1065/ 1327], train_loss/perplexity = 3.70263171/40.5538902 secs/batch = 0.2660s, grad.norm=16.94537735
 19648: 14 [ 1070/ 1327], train_loss/perplexity = 4.01202059/55.2584114 secs/batch = 0.2672s, grad.norm=16.65053177
 19653: 14 [ 1075/ 1327], train_loss/perplexity = 3.79601955/44.5236092 secs/batch = 0.2654s, grad.norm=16.71142578
 19658: 14 [ 1080/ 1327], train_loss/perplexity = 3.73613095/41.9354248 secs/batch = 0.2671s, grad.norm=16.23131943
 19663: 14 [ 1085/ 1327], train_loss/perplexity = 3.55664301/35.0453529 secs/batch = 0.2659s, grad.norm=16.33530807
 19668: 14 [ 1090/ 1327], train_loss/perplexity = 3.84218574/46.6272774 secs/batch = 0.2658s, grad.norm=17.28455544
 19673: 14 [ 1095/ 1327], train_loss/perplexity = 3.93880105/51.3569908 secs/batch = 0.2634s, grad.norm=17.08811378
 19678: 14 [ 1100/ 1327], train_loss/perplexity = 3.62501931/37.5254478 secs/batch = 0.2652s, grad.norm=17.91548157
 19683: 14 [ 1105/ 1327], train_loss/perplexity = 3.62911963/37.6796303 secs/batch = 0.2657s, grad.norm=17.27407837
 19688: 14 [ 1110/ 1327], train_loss/perplexity = 3.95527577/52.2100906 secs/batch = 0.2652s, grad.norm=17.65035248
 19693: 14 [ 1115/ 1327], train_loss/perplexity = 3.75496840/42.7328682 secs/batch = 0.2672s, grad.norm=16.41830254
 19698: 14 [ 1120/ 1327], train_loss/perplexity = 4.05193758/57.5087776 secs/batch = 0.2666s, grad.norm=16.55151367
 19703: 14 [ 1125/ 1327], train_loss/perplexity = 4.18410110/65.6344757 secs/batch = 0.2660s, grad.norm=17.41934586
 19708: 14 [ 1130/ 1327], train_loss/perplexity = 3.87114263/47.9971962 secs/batch = 0.2656s, grad.norm=17.44050026
 19713: 14 [ 1135/ 1327], train_loss/perplexity = 3.83852363/46.4568367 secs/batch = 0.2656s, grad.norm=16.78365135
 19718: 14 [ 1140/ 1327], train_loss/perplexity = 4.07128716/58.6323814 secs/batch = 0.2662s, grad.norm=17.37519455
 19723: 14 [ 1145/ 1327], train_loss/perplexity = 3.96752620/52.8536186 secs/batch = 0.2661s, grad.norm=16.61754799
 19728: 14 [ 1150/ 1327], train_loss/perplexity = 3.92264318/50.5338402 secs/batch = 0.2661s, grad.norm=16.86375618
 19733: 14 [ 1155/ 1327], train_loss/perplexity = 4.01273680/55.2980042 secs/batch = 0.2663s, grad.norm=17.32666779
 19738: 14 [ 1160/ 1327], train_loss/perplexity = 3.94701624/51.7806358 secs/batch = 0.2652s, grad.norm=16.90562439
 19743: 14 [ 1165/ 1327], train_loss/perplexity = 3.91859412/50.3296394 secs/batch = 0.2656s, grad.norm=16.97028542
 19748: 14 [ 1170/ 1327], train_loss/perplexity = 3.78462005/44.0189438 secs/batch = 0.2661s, grad.norm=16.62215805
 19753: 14 [ 1175/ 1327], train_loss/perplexity = 3.75190854/42.6023140 secs/batch = 0.2670s, grad.norm=16.94763565
 19758: 14 [ 1180/ 1327], train_loss/perplexity = 3.62396574/37.4859314 secs/batch = 0.2653s, grad.norm=17.11983109
 19763: 14 [ 1185/ 1327], train_loss/perplexity = 3.78943634/44.2314606 secs/batch = 0.2654s, grad.norm=17.10888481
 19768: 14 [ 1190/ 1327], train_loss/perplexity = 3.93513846/51.1692352 secs/batch = 0.2659s, grad.norm=17.29404259
 19773: 14 [ 1195/ 1327], train_loss/perplexity = 3.81741619/45.4865265 secs/batch = 0.2663s, grad.norm=17.15770149
 19778: 14 [ 1200/ 1327], train_loss/perplexity = 3.63681197/37.9705925 secs/batch = 0.2663s, grad.norm=16.64089775
 19783: 14 [ 1205/ 1327], train_loss/perplexity = 3.75129652/42.5762482 secs/batch = 0.2669s, grad.norm=17.35263824
 19788: 14 [ 1210/ 1327], train_loss/perplexity = 3.26878858/26.2794838 secs/batch = 0.2666s, grad.norm=16.63965225
 19793: 14 [ 1215/ 1327], train_loss/perplexity = 3.62894082/37.6728935 secs/batch = 0.2633s, grad.norm=16.32711601
 19798: 14 [ 1220/ 1327], train_loss/perplexity = 3.79645181/44.5428581 secs/batch = 0.2637s, grad.norm=17.54247093
 19803: 14 [ 1225/ 1327], train_loss/perplexity = 3.42324996/30.6689262 secs/batch = 0.2656s, grad.norm=17.76076889
 19808: 14 [ 1230/ 1327], train_loss/perplexity = 3.71088171/40.8898430 secs/batch = 0.2663s, grad.norm=16.60546875
 19813: 14 [ 1235/ 1327], train_loss/perplexity = 3.74324036/42.2346230 secs/batch = 0.2664s, grad.norm=16.24507904
 19818: 14 [ 1240/ 1327], train_loss/perplexity = 3.90319681/49.5606308 secs/batch = 0.2671s, grad.norm=17.17319870
 19823: 14 [ 1245/ 1327], train_loss/perplexity = 3.86080575/47.5036125 secs/batch = 0.2664s, grad.norm=16.41633034
 19828: 14 [ 1250/ 1327], train_loss/perplexity = 4.01858997/55.6226196 secs/batch = 0.2653s, grad.norm=16.51375008
 19833: 14 [ 1255/ 1327], train_loss/perplexity = 4.02266216/55.8495903 secs/batch = 0.2656s, grad.norm=16.50754738
 19838: 14 [ 1260/ 1327], train_loss/perplexity = 3.80855012/45.0850220 secs/batch = 0.2655s, grad.norm=17.77034187
 19843: 14 [ 1265/ 1327], train_loss/perplexity = 3.99844837/54.5135002 secs/batch = 0.2662s, grad.norm=16.70716095
 19848: 14 [ 1270/ 1327], train_loss/perplexity = 3.67908478/39.6101265 secs/batch = 0.2647s, grad.norm=17.25445747
 19853: 14 [ 1275/ 1327], train_loss/perplexity = 3.91479850/50.1389656 secs/batch = 0.2664s, grad.norm=17.02916527
 19858: 14 [ 1280/ 1327], train_loss/perplexity = 3.73647547/41.9498749 secs/batch = 0.2657s, grad.norm=16.96449661
 19863: 14 [ 1285/ 1327], train_loss/perplexity = 3.67782927/39.5604248 secs/batch = 0.2660s, grad.norm=16.77248192
 19868: 14 [ 1290/ 1327], train_loss/perplexity = 3.92806959/50.8087997 secs/batch = 0.2664s, grad.norm=17.03369331
 19873: 14 [ 1295/ 1327], train_loss/perplexity = 3.90789080/49.7938156 secs/batch = 0.2641s, grad.norm=16.81062889
 19878: 14 [ 1300/ 1327], train_loss/perplexity = 4.07215023/58.6830101 secs/batch = 0.2604s, grad.norm=16.22845459
 19883: 14 [ 1305/ 1327], train_loss/perplexity = 4.17153263/64.8147125 secs/batch = 0.2617s, grad.norm=16.98276329
 19888: 14 [ 1310/ 1327], train_loss/perplexity = 4.33832550/76.5792007 secs/batch = 0.2641s, grad.norm=17.32296371
 19893: 14 [ 1315/ 1327], train_loss/perplexity = 4.18782377/65.8792648 secs/batch = 0.2667s, grad.norm=16.93645096
 19898: 14 [ 1320/ 1327], train_loss/perplexity = 4.16187334/64.1916656 secs/batch = 0.2658s, grad.norm=17.14688110
 19903: 14 [ 1325/ 1327], train_loss/perplexity = 4.08807898/59.6252403 secs/batch = 0.2657s, grad.norm=17.09770393
Epoch training time: 352.7164647579193
	> validation loss = 4.59447765, perplexity = 98.93643951
	> validation loss = 4.56571531, perplexity = 96.13133240
	> validation loss = 4.52179956, perplexity = 92.00100708
	> validation loss = 4.57791376, perplexity = 97.31116486
	> validation loss = 4.69862413, perplexity = 109.79600525
	> validation loss = 4.66400480, perplexity = 106.05998230
	> validation loss = 4.56402159, perplexity = 95.96865082
	> validation loss = 4.39562464, perplexity = 81.09526825
	> validation loss = 4.18943167, perplexity = 65.98527527
	> validation loss = 4.29719448, perplexity = 73.49331665
	> validation loss = 4.48986769, perplexity = 89.10965729
	> validation loss = 4.49780989, perplexity = 89.82019806
	> validation loss = 4.48440218, perplexity = 88.62395477
	> validation loss = 4.20496130, perplexity = 67.01800537
	> validation loss = 4.17600250, perplexity = 65.10507202
	> validation loss = 4.21861410, perplexity = 67.93926239
	> validation loss = 4.62578869, perplexity = 102.08325195
	> validation loss = 4.13547325, perplexity = 62.51917267
	> validation loss = 4.64921761, perplexity = 104.50318909
	> validation loss = 4.49979067, perplexity = 89.99829102
	> validation loss = 4.28147602, perplexity = 72.34714508
at the end of epoch: 14
train loss = 4.03344475, perplexity = 56.45505038
validation loss = 4.44994651, perplexity = 85.62236357
Saved model cv/epoch014_4.4499.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.25
new learning rate is: 0.125
 19910: 15 [    5/ 1327], train_loss/perplexity = 4.20361137/66.9275970 secs/batch = 0.2664s, grad.norm=17.28270340
 19915: 15 [   10/ 1327], train_loss/perplexity = 3.76888657/43.3317909 secs/batch = 0.2661s, grad.norm=16.31798744
 19920: 15 [   15/ 1327], train_loss/perplexity = 4.14944410/63.3987465 secs/batch = 0.2659s, grad.norm=16.27335167
 19925: 15 [   20/ 1327], train_loss/perplexity = 4.28517723/72.6154175 secs/batch = 0.2612s, grad.norm=15.83554363
 19930: 15 [   25/ 1327], train_loss/perplexity = 4.16835022/64.6087723 secs/batch = 0.2662s, grad.norm=17.89058113
 19935: 15 [   30/ 1327], train_loss/perplexity = 4.12443113/61.8326263 secs/batch = 0.2634s, grad.norm=17.06419182
 19940: 15 [   35/ 1327], train_loss/perplexity = 3.96964478/52.9657135 secs/batch = 0.2671s, grad.norm=16.43720627
 19945: 15 [   40/ 1327], train_loss/perplexity = 3.92885876/50.8489151 secs/batch = 0.2653s, grad.norm=16.56162453
 19950: 15 [   45/ 1327], train_loss/perplexity = 3.76394343/43.1181259 secs/batch = 0.2605s, grad.norm=15.60176086
 19955: 15 [   50/ 1327], train_loss/perplexity = 4.02904510/56.2072144 secs/batch = 0.2652s, grad.norm=16.84354019
 19960: 15 [   55/ 1327], train_loss/perplexity = 3.91547751/50.1730232 secs/batch = 0.2661s, grad.norm=17.13585281
 19965: 15 [   60/ 1327], train_loss/perplexity = 4.17849493/65.2675476 secs/batch = 0.2656s, grad.norm=16.68437767
 19970: 15 [   65/ 1327], train_loss/perplexity = 3.81125259/45.2070312 secs/batch = 0.2663s, grad.norm=16.37482452
 19975: 15 [   70/ 1327], train_loss/perplexity = 3.59012127/36.2384720 secs/batch = 0.2664s, grad.norm=16.88252258
 19980: 15 [   75/ 1327], train_loss/perplexity = 3.45562816/31.6781807 secs/batch = 0.2650s, grad.norm=15.82552242
 19985: 15 [   80/ 1327], train_loss/perplexity = 3.93829775/51.3311501 secs/batch = 0.2666s, grad.norm=16.90589523
 19990: 15 [   85/ 1327], train_loss/perplexity = 3.91192818/49.9952583 secs/batch = 0.2643s, grad.norm=16.72441483
 19995: 15 [   90/ 1327], train_loss/perplexity = 4.00186586/54.7001190 secs/batch = 0.2670s, grad.norm=16.90331841
 20000: 15 [   95/ 1327], train_loss/perplexity = 3.85692930/47.3198242 secs/batch = 0.2600s, grad.norm=16.64182854
 20005: 15 [  100/ 1327], train_loss/perplexity = 4.11649227/61.3436890 secs/batch = 0.2666s, grad.norm=16.81947136
 20010: 15 [  105/ 1327], train_loss/perplexity = 3.91917300/50.3587799 secs/batch = 0.2673s, grad.norm=17.29602814
 20015: 15 [  110/ 1327], train_loss/perplexity = 3.76490545/43.1596260 secs/batch = 0.2656s, grad.norm=16.65379333
 20020: 15 [  115/ 1327], train_loss/perplexity = 3.79888201/44.6512375 secs/batch = 0.2650s, grad.norm=17.44222450
 20025: 15 [  120/ 1327], train_loss/perplexity = 3.87447810/48.1575584 secs/batch = 0.2663s, grad.norm=17.08174133
 20030: 15 [  125/ 1327], train_loss/perplexity = 3.88593435/48.7124367 secs/batch = 0.2656s, grad.norm=17.20865250
 20035: 15 [  130/ 1327], train_loss/perplexity = 3.83879304/46.4693527 secs/batch = 0.2665s, grad.norm=17.46678162
 20040: 15 [  135/ 1327], train_loss/perplexity = 3.86702132/47.7997932 secs/batch = 0.2660s, grad.norm=16.75637817
 20045: 15 [  140/ 1327], train_loss/perplexity = 4.17014694/64.7249603 secs/batch = 0.2664s, grad.norm=17.35445976
 20050: 15 [  145/ 1327], train_loss/perplexity = 4.05944300/57.9420280 secs/batch = 0.2659s, grad.norm=18.09730530
 20055: 15 [  150/ 1327], train_loss/perplexity = 4.11132193/61.0273361 secs/batch = 0.2657s, grad.norm=17.45086861
 20060: 15 [  155/ 1327], train_loss/perplexity = 4.37894058/79.7534943 secs/batch = 0.2657s, grad.norm=16.90952110
 20065: 15 [  160/ 1327], train_loss/perplexity = 3.98310041/53.6832161 secs/batch = 0.2664s, grad.norm=15.73961353
 20070: 15 [  165/ 1327], train_loss/perplexity = 4.13482141/62.4784317 secs/batch = 0.2657s, grad.norm=16.45202637
 20075: 15 [  170/ 1327], train_loss/perplexity = 3.99652004/54.4084816 secs/batch = 0.2654s, grad.norm=16.38338280
 20080: 15 [  175/ 1327], train_loss/perplexity = 4.20737410/67.1799011 secs/batch = 0.2659s, grad.norm=16.60060692
 20085: 15 [  180/ 1327], train_loss/perplexity = 4.04495907/57.1088486 secs/batch = 0.2663s, grad.norm=17.19344711
 20090: 15 [  185/ 1327], train_loss/perplexity = 4.39281178/80.8674774 secs/batch = 0.2661s, grad.norm=17.19040108
 20095: 15 [  190/ 1327], train_loss/perplexity = 3.92771769/50.7909241 secs/batch = 0.2640s, grad.norm=15.95863914
 20100: 15 [  195/ 1327], train_loss/perplexity = 4.24305010/69.6198730 secs/batch = 0.2664s, grad.norm=16.05377960
 20105: 15 [  200/ 1327], train_loss/perplexity = 4.06030798/57.9921684 secs/batch = 0.2670s, grad.norm=17.25287437
 20110: 15 [  205/ 1327], train_loss/perplexity = 4.22315168/68.2482452 secs/batch = 0.2659s, grad.norm=17.20249367
 20115: 15 [  210/ 1327], train_loss/perplexity = 4.11418867/61.2025375 secs/batch = 0.2668s, grad.norm=15.83539009
 20120: 15 [  215/ 1327], train_loss/perplexity = 4.25514269/70.4668732 secs/batch = 0.2645s, grad.norm=16.01353073
 20125: 15 [  220/ 1327], train_loss/perplexity = 4.14501381/63.1184921 secs/batch = 0.2664s, grad.norm=16.22161293
 20130: 15 [  225/ 1327], train_loss/perplexity = 4.34067535/76.7593613 secs/batch = 0.2631s, grad.norm=17.05025864
 20135: 15 [  230/ 1327], train_loss/perplexity = 4.12862062/62.0922165 secs/batch = 0.2662s, grad.norm=17.24365616
 20140: 15 [  235/ 1327], train_loss/perplexity = 4.03742027/56.6799355 secs/batch = 0.2660s, grad.norm=16.71854401
 20145: 15 [  240/ 1327], train_loss/perplexity = 3.79489088/44.4733849 secs/batch = 0.2641s, grad.norm=16.98784447
 20150: 15 [  245/ 1327], train_loss/perplexity = 4.09156990/59.8337517 secs/batch = 0.2660s, grad.norm=16.40080452
 20155: 15 [  250/ 1327], train_loss/perplexity = 3.93616581/51.2218285 secs/batch = 0.2661s, grad.norm=16.13857079
 20160: 15 [  255/ 1327], train_loss/perplexity = 3.96650124/52.7994728 secs/batch = 0.2655s, grad.norm=16.67753410
 20165: 15 [  260/ 1327], train_loss/perplexity = 4.10794830/60.8218002 secs/batch = 0.2654s, grad.norm=17.52689934
 20170: 15 [  265/ 1327], train_loss/perplexity = 4.34843683/77.3574448 secs/batch = 0.2639s, grad.norm=16.10181236
 20175: 15 [  270/ 1327], train_loss/perplexity = 4.38030863/79.8626785 secs/batch = 0.2667s, grad.norm=17.05811691
 20180: 15 [  275/ 1327], train_loss/perplexity = 4.39185953/80.7905121 secs/batch = 0.2663s, grad.norm=16.59562874
 20185: 15 [  280/ 1327], train_loss/perplexity = 4.07854176/59.0592842 secs/batch = 0.2649s, grad.norm=15.88371181
 20190: 15 [  285/ 1327], train_loss/perplexity = 4.48794842/88.9387970 secs/batch = 0.2659s, grad.norm=16.63167381
 20195: 15 [  290/ 1327], train_loss/perplexity = 4.06177616/58.0773735 secs/batch = 0.2667s, grad.norm=16.69434929
 20200: 15 [  295/ 1327], train_loss/perplexity = 3.91718435/50.2587357 secs/batch = 0.2655s, grad.norm=16.39211655
 20205: 15 [  300/ 1327], train_loss/perplexity = 3.46439958/31.9572659 secs/batch = 0.2660s, grad.norm=15.86510849
 20210: 15 [  305/ 1327], train_loss/perplexity = 3.90844822/49.8215790 secs/batch = 0.2657s, grad.norm=16.23243523
 20215: 15 [  310/ 1327], train_loss/perplexity = 3.89776468/49.2921410 secs/batch = 0.2659s, grad.norm=16.35914230
 20220: 15 [  315/ 1327], train_loss/perplexity = 3.48757768/32.7066269 secs/batch = 0.2659s, grad.norm=15.57272243
 20225: 15 [  320/ 1327], train_loss/perplexity = 3.44808197/31.4400311 secs/batch = 0.2663s, grad.norm=17.38037109
 20230: 15 [  325/ 1327], train_loss/perplexity = 3.44731116/31.4158058 secs/batch = 0.2658s, grad.norm=15.77045727
 20235: 15 [  330/ 1327], train_loss/perplexity = 4.07895565/59.0837326 secs/batch = 0.2666s, grad.norm=17.28079987
 20240: 15 [  335/ 1327], train_loss/perplexity = 3.52843738/34.0706863 secs/batch = 0.2653s, grad.norm=16.20052147
 20245: 15 [  340/ 1327], train_loss/perplexity = 4.21888351/67.9575653 secs/batch = 0.2660s, grad.norm=16.27594757
 20250: 15 [  345/ 1327], train_loss/perplexity = 4.00096560/54.6508942 secs/batch = 0.2661s, grad.norm=15.59089851
 20255: 15 [  350/ 1327], train_loss/perplexity = 4.03900671/56.7699242 secs/batch = 0.2613s, grad.norm=17.33401108
 20260: 15 [  355/ 1327], train_loss/perplexity = 4.03680038/56.6448097 secs/batch = 0.2667s, grad.norm=16.62258148
 20265: 15 [  360/ 1327], train_loss/perplexity = 4.12658548/61.9659767 secs/batch = 0.2659s, grad.norm=17.88010788
 20270: 15 [  365/ 1327], train_loss/perplexity = 4.12122965/61.6349869 secs/batch = 0.2659s, grad.norm=16.66861153
 20275: 15 [  370/ 1327], train_loss/perplexity = 4.15745640/63.9087563 secs/batch = 0.2668s, grad.norm=16.49391937
 20280: 15 [  375/ 1327], train_loss/perplexity = 3.64835525/38.4114380 secs/batch = 0.2658s, grad.norm=16.80255890
 20285: 15 [  380/ 1327], train_loss/perplexity = 3.70734239/40.7453766 secs/batch = 0.2661s, grad.norm=17.13726234
 20290: 15 [  385/ 1327], train_loss/perplexity = 3.82994246/46.0598869 secs/batch = 0.2633s, grad.norm=17.06948280
 20295: 15 [  390/ 1327], train_loss/perplexity = 3.98961782/54.0342331 secs/batch = 0.2657s, grad.norm=16.57199287
 20300: 15 [  395/ 1327], train_loss/perplexity = 3.96774936/52.8654175 secs/batch = 0.2666s, grad.norm=16.39353752
 20305: 15 [  400/ 1327], train_loss/perplexity = 4.00419998/54.8279419 secs/batch = 0.2666s, grad.norm=16.42051506
 20310: 15 [  405/ 1327], train_loss/perplexity = 4.26035833/70.8353653 secs/batch = 0.2651s, grad.norm=16.65729713
 20315: 15 [  410/ 1327], train_loss/perplexity = 3.88875246/48.8499069 secs/batch = 0.2675s, grad.norm=16.59323692
 20320: 15 [  415/ 1327], train_loss/perplexity = 3.91331792/50.0647888 secs/batch = 0.2667s, grad.norm=16.74004364
 20325: 15 [  420/ 1327], train_loss/perplexity = 3.53983712/34.4613075 secs/batch = 0.2656s, grad.norm=16.31133461
 20330: 15 [  425/ 1327], train_loss/perplexity = 3.83381605/46.2386513 secs/batch = 0.2672s, grad.norm=16.99141312
 20335: 15 [  430/ 1327], train_loss/perplexity = 4.00139046/54.6741180 secs/batch = 0.2662s, grad.norm=17.04889297
 20340: 15 [  435/ 1327], train_loss/perplexity = 4.11149979/61.0381927 secs/batch = 0.2665s, grad.norm=17.53489304
 20345: 15 [  440/ 1327], train_loss/perplexity = 3.64036918/38.1059036 secs/batch = 0.2616s, grad.norm=16.53070259
 20350: 15 [  445/ 1327], train_loss/perplexity = 3.99573708/54.3658981 secs/batch = 0.2677s, grad.norm=17.06695175
 20355: 15 [  450/ 1327], train_loss/perplexity = 3.99994659/54.5952339 secs/batch = 0.2671s, grad.norm=16.43564224
 20360: 15 [  455/ 1327], train_loss/perplexity = 3.97491264/53.2454643 secs/batch = 0.2665s, grad.norm=16.17851639
 20365: 15 [  460/ 1327], train_loss/perplexity = 3.87076926/47.9792824 secs/batch = 0.2669s, grad.norm=17.37356949
 20370: 15 [  465/ 1327], train_loss/perplexity = 3.61808634/37.2661858 secs/batch = 0.2669s, grad.norm=17.34704399
 20375: 15 [  470/ 1327], train_loss/perplexity = 4.35368013/77.7641220 secs/batch = 0.2623s, grad.norm=16.72754478
 20380: 15 [  475/ 1327], train_loss/perplexity = 3.80261207/44.8181000 secs/batch = 0.2661s, grad.norm=16.90452766
 20385: 15 [  480/ 1327], train_loss/perplexity = 3.93375111/51.0982933 secs/batch = 0.2638s, grad.norm=16.90476799
 20390: 15 [  485/ 1327], train_loss/perplexity = 3.80822134/45.0702019 secs/batch = 0.2659s, grad.norm=16.49466133
 20395: 15 [  490/ 1327], train_loss/perplexity = 3.77033615/43.3946495 secs/batch = 0.2671s, grad.norm=18.22426796
 20400: 15 [  495/ 1327], train_loss/perplexity = 3.92614937/50.7113304 secs/batch = 0.2670s, grad.norm=17.15276146
 20405: 15 [  500/ 1327], train_loss/perplexity = 3.92865276/50.8384399 secs/batch = 0.2660s, grad.norm=16.32096481
 20410: 15 [  505/ 1327], train_loss/perplexity = 4.10278177/60.5083733 secs/batch = 0.2661s, grad.norm=15.67484665
 20415: 15 [  510/ 1327], train_loss/perplexity = 4.37437773/79.3904190 secs/batch = 0.2665s, grad.norm=15.69351292
 20420: 15 [  515/ 1327], train_loss/perplexity = 4.05129385/57.4717674 secs/batch = 0.2659s, grad.norm=16.05917931
 20425: 15 [  520/ 1327], train_loss/perplexity = 4.23862362/69.3123856 secs/batch = 0.2665s, grad.norm=16.98950577
 20430: 15 [  525/ 1327], train_loss/perplexity = 3.84705448/46.8548470 secs/batch = 0.2661s, grad.norm=16.49886894
 20435: 15 [  530/ 1327], train_loss/perplexity = 3.81087542/45.1899834 secs/batch = 0.2652s, grad.norm=16.54986763
 20440: 15 [  535/ 1327], train_loss/perplexity = 3.97721648/53.3682747 secs/batch = 0.2664s, grad.norm=16.92206764
 20445: 15 [  540/ 1327], train_loss/perplexity = 3.99745202/54.4592133 secs/batch = 0.2664s, grad.norm=16.54799271
 20450: 15 [  545/ 1327], train_loss/perplexity = 4.03431129/56.5039940 secs/batch = 0.2649s, grad.norm=16.76824570
 20455: 15 [  550/ 1327], train_loss/perplexity = 4.00131464/54.6699753 secs/batch = 0.2671s, grad.norm=16.75586128
 20460: 15 [  555/ 1327], train_loss/perplexity = 3.86275148/47.5961304 secs/batch = 0.2658s, grad.norm=16.41004372
 20465: 15 [  560/ 1327], train_loss/perplexity = 3.99987626/54.5913963 secs/batch = 0.2661s, grad.norm=17.37988281
 20470: 15 [  565/ 1327], train_loss/perplexity = 3.79054546/44.2805481 secs/batch = 0.2645s, grad.norm=17.24907494
 20475: 15 [  570/ 1327], train_loss/perplexity = 3.82809210/45.9747391 secs/batch = 0.2649s, grad.norm=17.17749405
 20480: 15 [  575/ 1327], train_loss/perplexity = 3.68592358/39.8819389 secs/batch = 0.2671s, grad.norm=16.84957504
 20485: 15 [  580/ 1327], train_loss/perplexity = 4.08119011/59.2159004 secs/batch = 0.2666s, grad.norm=17.32265663
 20490: 15 [  585/ 1327], train_loss/perplexity = 3.67320728/39.3779984 secs/batch = 0.2654s, grad.norm=16.71359444
 20495: 15 [  590/ 1327], train_loss/perplexity = 4.03286982/56.4225998 secs/batch = 0.2638s, grad.norm=16.43840981
 20500: 15 [  595/ 1327], train_loss/perplexity = 3.97033644/53.0023613 secs/batch = 0.2676s, grad.norm=17.21869850
 20505: 15 [  600/ 1327], train_loss/perplexity = 4.24939203/70.0628052 secs/batch = 0.2656s, grad.norm=16.49865150
 20510: 15 [  605/ 1327], train_loss/perplexity = 4.07286978/58.7252502 secs/batch = 0.2657s, grad.norm=16.02230453
 20515: 15 [  610/ 1327], train_loss/perplexity = 4.25556898/70.4969177 secs/batch = 0.2655s, grad.norm=16.36011314
 20520: 15 [  615/ 1327], train_loss/perplexity = 3.82177496/45.6852264 secs/batch = 0.2655s, grad.norm=16.13128853
 20525: 15 [  620/ 1327], train_loss/perplexity = 4.20809937/67.2286377 secs/batch = 0.2664s, grad.norm=16.54464912
 20530: 15 [  625/ 1327], train_loss/perplexity = 4.14982653/63.4229965 secs/batch = 0.2657s, grad.norm=16.41663170
 20535: 15 [  630/ 1327], train_loss/perplexity = 4.28360939/72.5016556 secs/batch = 0.2649s, grad.norm=16.60143661
 20540: 15 [  635/ 1327], train_loss/perplexity = 3.95550966/52.2223015 secs/batch = 0.2664s, grad.norm=16.30154037
 20545: 15 [  640/ 1327], train_loss/perplexity = 3.93412900/51.1176071 secs/batch = 0.2662s, grad.norm=16.36227989
 20550: 15 [  645/ 1327], train_loss/perplexity = 4.19191265/66.1491928 secs/batch = 0.2648s, grad.norm=17.29000854
 20555: 15 [  650/ 1327], train_loss/perplexity = 3.66128445/38.9112892 secs/batch = 0.2664s, grad.norm=16.48145866
 20560: 15 [  655/ 1327], train_loss/perplexity = 3.94235086/51.5396233 secs/batch = 0.2653s, grad.norm=17.15425682
 20565: 15 [  660/ 1327], train_loss/perplexity = 3.83525753/46.3053513 secs/batch = 0.2650s, grad.norm=17.30642891
 20570: 15 [  665/ 1327], train_loss/perplexity = 3.94866943/51.8663101 secs/batch = 0.2660s, grad.norm=16.64634323
 20575: 15 [  670/ 1327], train_loss/perplexity = 3.93354058/51.0875359 secs/batch = 0.2672s, grad.norm=17.25664139
 20580: 15 [  675/ 1327], train_loss/perplexity = 3.72842169/41.6133766 secs/batch = 0.2658s, grad.norm=17.67486763
 20585: 15 [  680/ 1327], train_loss/perplexity = 3.96178913/52.5512619 secs/batch = 0.2651s, grad.norm=17.54668617
 20590: 15 [  685/ 1327], train_loss/perplexity = 3.75341702/42.6666260 secs/batch = 0.2660s, grad.norm=16.62663078
 20595: 15 [  690/ 1327], train_loss/perplexity = 4.19043398/66.0514526 secs/batch = 0.2653s, grad.norm=16.68276215
 20600: 15 [  695/ 1327], train_loss/perplexity = 3.97168875/53.0740852 secs/batch = 0.2611s, grad.norm=16.56837082
 20605: 15 [  700/ 1327], train_loss/perplexity = 4.17676592/65.1547928 secs/batch = 0.2663s, grad.norm=17.23597527
 20610: 15 [  705/ 1327], train_loss/perplexity = 3.93987846/51.4123535 secs/batch = 0.2663s, grad.norm=16.23468399
 20615: 15 [  710/ 1327], train_loss/perplexity = 3.79280043/44.3805122 secs/batch = 0.2667s, grad.norm=17.13752174
 20620: 15 [  715/ 1327], train_loss/perplexity = 3.72300148/41.3884354 secs/batch = 0.2672s, grad.norm=17.39295769
 20625: 15 [  720/ 1327], train_loss/perplexity = 3.74055767/42.1214752 secs/batch = 0.2595s, grad.norm=17.45096779
 20630: 15 [  725/ 1327], train_loss/perplexity = 3.81518912/45.3853378 secs/batch = 0.2668s, grad.norm=17.24373436
 20635: 15 [  730/ 1327], train_loss/perplexity = 3.90261579/49.5318451 secs/batch = 0.2656s, grad.norm=17.51631927
 20640: 15 [  735/ 1327], train_loss/perplexity = 3.96697497/52.8244934 secs/batch = 0.2664s, grad.norm=18.17538643
 20645: 15 [  740/ 1327], train_loss/perplexity = 3.55824232/35.1014442 secs/batch = 0.2663s, grad.norm=16.23353004
 20650: 15 [  745/ 1327], train_loss/perplexity = 3.98352575/53.7060547 secs/batch = 0.2652s, grad.norm=17.47305489
 20655: 15 [  750/ 1327], train_loss/perplexity = 3.81229329/45.2541008 secs/batch = 0.2659s, grad.norm=16.70359230
 20660: 15 [  755/ 1327], train_loss/perplexity = 3.71543598/41.0764923 secs/batch = 0.2663s, grad.norm=16.36955643
 20665: 15 [  760/ 1327], train_loss/perplexity = 3.58930993/36.2090797 secs/batch = 0.2662s, grad.norm=16.28613281
 20670: 15 [  765/ 1327], train_loss/perplexity = 3.74634027/42.3657494 secs/batch = 0.2672s, grad.norm=16.60950089
 20675: 15 [  770/ 1327], train_loss/perplexity = 3.69960308/40.4312515 secs/batch = 0.2653s, grad.norm=17.01119804
 20680: 15 [  775/ 1327], train_loss/perplexity = 3.75523376/42.7442093 secs/batch = 0.2653s, grad.norm=17.33068657
 20685: 15 [  780/ 1327], train_loss/perplexity = 4.14502621/63.1192741 secs/batch = 0.2659s, grad.norm=17.35519409
 20690: 15 [  785/ 1327], train_loss/perplexity = 3.90657806/49.7284927 secs/batch = 0.2663s, grad.norm=17.20767403
 20695: 15 [  790/ 1327], train_loss/perplexity = 3.74829268/42.4485474 secs/batch = 0.2679s, grad.norm=17.47051430
 20700: 15 [  795/ 1327], train_loss/perplexity = 4.07067966/58.5967751 secs/batch = 0.2665s, grad.norm=16.64714432
 20705: 15 [  800/ 1327], train_loss/perplexity = 3.96627569/52.7875671 secs/batch = 0.2670s, grad.norm=17.24048615
 20710: 15 [  805/ 1327], train_loss/perplexity = 4.30203962/73.8502655 secs/batch = 0.2658s, grad.norm=17.18662453
 20715: 15 [  810/ 1327], train_loss/perplexity = 3.83630133/46.3537102 secs/batch = 0.2665s, grad.norm=15.67842197
 20720: 15 [  815/ 1327], train_loss/perplexity = 3.80707312/45.0184822 secs/batch = 0.2665s, grad.norm=16.30548286
 20725: 15 [  820/ 1327], train_loss/perplexity = 3.73581290/41.9220886 secs/batch = 0.2658s, grad.norm=15.87438679
 20730: 15 [  825/ 1327], train_loss/perplexity = 3.88055229/48.4509659 secs/batch = 0.2663s, grad.norm=16.58384323
 20735: 15 [  830/ 1327], train_loss/perplexity = 3.57066846/35.5403442 secs/batch = 0.2666s, grad.norm=16.82089615
 20740: 15 [  835/ 1327], train_loss/perplexity = 3.82263589/45.7245750 secs/batch = 0.2645s, grad.norm=17.02464676
 20745: 15 [  840/ 1327], train_loss/perplexity = 3.98960066/54.0333061 secs/batch = 0.2659s, grad.norm=17.38574600
 20750: 15 [  845/ 1327], train_loss/perplexity = 3.72601581/41.5133820 secs/batch = 0.2666s, grad.norm=16.88278961
 20755: 15 [  850/ 1327], train_loss/perplexity = 3.85685062/47.3161011 secs/batch = 0.2663s, grad.norm=16.32239532
 20760: 15 [  855/ 1327], train_loss/perplexity = 3.85700655/47.3234787 secs/batch = 0.2671s, grad.norm=17.17195702
 20765: 15 [  860/ 1327], train_loss/perplexity = 3.64057875/38.1138878 secs/batch = 0.2666s, grad.norm=16.49692535
 20770: 15 [  865/ 1327], train_loss/perplexity = 4.08105373/59.2078247 secs/batch = 0.2674s, grad.norm=16.98144722
 20775: 15 [  870/ 1327], train_loss/perplexity = 3.89368200/49.0913086 secs/batch = 0.2663s, grad.norm=16.95301437
 20780: 15 [  875/ 1327], train_loss/perplexity = 3.57401538/35.6594925 secs/batch = 0.2659s, grad.norm=16.18659019
 20785: 15 [  880/ 1327], train_loss/perplexity = 3.76337504/43.0936241 secs/batch = 0.2612s, grad.norm=16.48184967
 20790: 15 [  885/ 1327], train_loss/perplexity = 3.97244978/53.1144905 secs/batch = 0.2601s, grad.norm=16.59270096
 20795: 15 [  890/ 1327], train_loss/perplexity = 4.06817913/58.4504356 secs/batch = 0.2686s, grad.norm=17.31423378
 20800: 15 [  895/ 1327], train_loss/perplexity = 4.03574467/56.5850410 secs/batch = 0.2660s, grad.norm=16.13035583
 20805: 15 [  900/ 1327], train_loss/perplexity = 3.83766437/46.4169350 secs/batch = 0.2658s, grad.norm=16.18637657
 20810: 15 [  905/ 1327], train_loss/perplexity = 3.76970720/43.3673668 secs/batch = 0.2665s, grad.norm=16.08548927
 20815: 15 [  910/ 1327], train_loss/perplexity = 3.82461357/45.8150940 secs/batch = 0.2617s, grad.norm=15.76744175
 20820: 15 [  915/ 1327], train_loss/perplexity = 4.11226654/61.0850143 secs/batch = 0.2654s, grad.norm=16.23609543
 20825: 15 [  920/ 1327], train_loss/perplexity = 4.15174627/63.5448685 secs/batch = 0.2648s, grad.norm=16.97658348
 20830: 15 [  925/ 1327], train_loss/perplexity = 3.89044333/48.9325752 secs/batch = 0.2612s, grad.norm=16.50876999
 20835: 15 [  930/ 1327], train_loss/perplexity = 3.97356582/53.1738014 secs/batch = 0.2658s, grad.norm=16.50293350
 20840: 15 [  935/ 1327], train_loss/perplexity = 4.06281090/58.1375008 secs/batch = 0.2658s, grad.norm=16.21946907
 20845: 15 [  940/ 1327], train_loss/perplexity = 4.01924801/55.6592331 secs/batch = 0.2647s, grad.norm=16.24962234
 20850: 15 [  945/ 1327], train_loss/perplexity = 4.20704603/67.1578674 secs/batch = 0.2669s, grad.norm=16.44502068
 20855: 15 [  950/ 1327], train_loss/perplexity = 3.94669199/51.7638474 secs/batch = 0.2671s, grad.norm=16.36652946
 20860: 15 [  955/ 1327], train_loss/perplexity = 4.00503922/54.8739777 secs/batch = 0.2672s, grad.norm=16.81743431
 20865: 15 [  960/ 1327], train_loss/perplexity = 4.26283455/71.0109863 secs/batch = 0.2661s, grad.norm=16.69867134
 20870: 15 [  965/ 1327], train_loss/perplexity = 3.95356297/52.1207428 secs/batch = 0.2644s, grad.norm=16.52193451
 20875: 15 [  970/ 1327], train_loss/perplexity = 4.15620708/63.8289642 secs/batch = 0.2673s, grad.norm=16.58834076
 20880: 15 [  975/ 1327], train_loss/perplexity = 3.88591671/48.7115746 secs/batch = 0.2663s, grad.norm=17.67728233
 20885: 15 [  980/ 1327], train_loss/perplexity = 3.72640014/41.5293388 secs/batch = 0.2659s, grad.norm=16.67296410
 20890: 15 [  985/ 1327], train_loss/perplexity = 3.92339945/50.5720711 secs/batch = 0.2620s, grad.norm=17.20338249
 20895: 15 [  990/ 1327], train_loss/perplexity = 4.09360409/59.9555893 secs/batch = 0.2655s, grad.norm=17.30385780
 20900: 15 [  995/ 1327], train_loss/perplexity = 4.18763447/65.8667984 secs/batch = 0.2636s, grad.norm=16.86607361
 20905: 15 [ 1000/ 1327], train_loss/perplexity = 3.63966298/38.0790024 secs/batch = 0.2658s, grad.norm=16.17590523
 20910: 15 [ 1005/ 1327], train_loss/perplexity = 4.07572412/58.8931122 secs/batch = 0.2668s, grad.norm=16.83214188
 20915: 15 [ 1010/ 1327], train_loss/perplexity = 3.74456835/42.2907486 secs/batch = 0.2653s, grad.norm=15.91456223
 20920: 15 [ 1015/ 1327], train_loss/perplexity = 4.21539688/67.7210388 secs/batch = 0.2637s, grad.norm=16.57379532
 20925: 15 [ 1020/ 1327], train_loss/perplexity = 4.20242596/66.8483047 secs/batch = 0.2652s, grad.norm=17.08852196
 20930: 15 [ 1025/ 1327], train_loss/perplexity = 4.16472197/64.3747787 secs/batch = 0.2665s, grad.norm=16.58344078
 20935: 15 [ 1030/ 1327], train_loss/perplexity = 3.89877558/49.3419952 secs/batch = 0.2665s, grad.norm=16.16310120
 20940: 15 [ 1035/ 1327], train_loss/perplexity = 3.78239536/43.9211235 secs/batch = 0.2659s, grad.norm=16.17041206
 20945: 15 [ 1040/ 1327], train_loss/perplexity = 4.09168625/59.8407135 secs/batch = 0.2654s, grad.norm=17.00010872
 20950: 15 [ 1045/ 1327], train_loss/perplexity = 3.64381862/38.2375717 secs/batch = 0.2664s, grad.norm=15.79074764
 20955: 15 [ 1050/ 1327], train_loss/perplexity = 3.71704078/41.1424637 secs/batch = 0.2656s, grad.norm=16.66164398
 20960: 15 [ 1055/ 1327], train_loss/perplexity = 3.74619985/42.3598022 secs/batch = 0.2673s, grad.norm=17.17426491
 20965: 15 [ 1060/ 1327], train_loss/perplexity = 3.47153878/32.1862335 secs/batch = 0.2660s, grad.norm=17.86604118
 20970: 15 [ 1065/ 1327], train_loss/perplexity = 3.59256840/36.3272591 secs/batch = 0.2651s, grad.norm=17.11472893
 20975: 15 [ 1070/ 1327], train_loss/perplexity = 3.95827007/52.3666573 secs/batch = 0.2669s, grad.norm=17.36558151
 20980: 15 [ 1075/ 1327], train_loss/perplexity = 3.69943929/40.4246330 secs/batch = 0.2664s, grad.norm=16.53166962
 20985: 15 [ 1080/ 1327], train_loss/perplexity = 3.66101980/38.9009933 secs/batch = 0.2661s, grad.norm=16.49168015
 20990: 15 [ 1085/ 1327], train_loss/perplexity = 3.52734900/34.0336227 secs/batch = 0.2659s, grad.norm=16.97981453
 20995: 15 [ 1090/ 1327], train_loss/perplexity = 3.69686031/40.3205109 secs/batch = 0.2648s, grad.norm=17.75466537
 21000: 15 [ 1095/ 1327], train_loss/perplexity = 3.86789703/47.8416710 secs/batch = 0.2665s, grad.norm=17.37237549
 21005: 15 [ 1100/ 1327], train_loss/perplexity = 3.58352494/36.0002174 secs/batch = 0.2663s, grad.norm=18.41944122
 21010: 15 [ 1105/ 1327], train_loss/perplexity = 3.58198237/35.9447250 secs/batch = 0.2660s, grad.norm=17.37875938
 21015: 15 [ 1110/ 1327], train_loss/perplexity = 3.87914181/48.3826752 secs/batch = 0.2667s, grad.norm=17.95217514
 21020: 15 [ 1115/ 1327], train_loss/perplexity = 3.71090364/40.8907394 secs/batch = 0.2653s, grad.norm=16.42198181
 21025: 15 [ 1120/ 1327], train_loss/perplexity = 3.90378714/49.5898972 secs/batch = 0.2642s, grad.norm=16.49078751
 21030: 15 [ 1125/ 1327], train_loss/perplexity = 4.08234501/59.2843285 secs/batch = 0.2659s, grad.norm=17.97419357
 21035: 15 [ 1130/ 1327], train_loss/perplexity = 3.76200080/43.0344429 secs/batch = 0.2678s, grad.norm=17.04225731
 21040: 15 [ 1135/ 1327], train_loss/perplexity = 3.77318621/43.5185013 secs/batch = 0.2645s, grad.norm=16.79049683
 21045: 15 [ 1140/ 1327], train_loss/perplexity = 4.09674835/60.1444016 secs/batch = 0.2660s, grad.norm=18.12902451
 21050: 15 [ 1145/ 1327], train_loss/perplexity = 3.83781672/46.4240074 secs/batch = 0.2664s, grad.norm=16.41491508
 21055: 15 [ 1150/ 1327], train_loss/perplexity = 3.82952738/46.0407715 secs/batch = 0.2672s, grad.norm=17.05326653
 21060: 15 [ 1155/ 1327], train_loss/perplexity = 3.93819737/51.3259964 secs/batch = 0.2654s, grad.norm=17.37772751
 21065: 15 [ 1160/ 1327], train_loss/perplexity = 3.80611110/44.9751930 secs/batch = 0.2656s, grad.norm=17.12744713
 21070: 15 [ 1165/ 1327], train_loss/perplexity = 3.82892990/46.0132713 secs/batch = 0.2642s, grad.norm=16.95667839
 21075: 15 [ 1170/ 1327], train_loss/perplexity = 3.69470930/40.2338753 secs/batch = 0.2675s, grad.norm=16.66844749
 21080: 15 [ 1175/ 1327], train_loss/perplexity = 3.63722324/37.9862099 secs/batch = 0.2643s, grad.norm=17.12352753
 21085: 15 [ 1180/ 1327], train_loss/perplexity = 3.65427017/38.6393089 secs/batch = 0.2667s, grad.norm=17.62694359
 21090: 15 [ 1185/ 1327], train_loss/perplexity = 3.74080133/42.1317368 secs/batch = 0.2656s, grad.norm=17.38647270
 21095: 15 [ 1190/ 1327], train_loss/perplexity = 3.84394789/46.7095146 secs/batch = 0.2662s, grad.norm=17.62784576
 21100: 15 [ 1195/ 1327], train_loss/perplexity = 3.66881990/39.2056122 secs/batch = 0.2660s, grad.norm=16.81923866
 21105: 15 [ 1200/ 1327], train_loss/perplexity = 3.63588476/37.9354019 secs/batch = 0.2661s, grad.norm=17.17263603
 21110: 15 [ 1205/ 1327], train_loss/perplexity = 3.69565701/40.2720222 secs/batch = 0.2659s, grad.norm=17.59287071
 21115: 15 [ 1210/ 1327], train_loss/perplexity = 3.24891138/25.7622795 secs/batch = 0.2663s, grad.norm=16.86705780
 21120: 15 [ 1215/ 1327], train_loss/perplexity = 3.51941776/33.7647629 secs/batch = 0.2666s, grad.norm=16.52450371
 21125: 15 [ 1220/ 1327], train_loss/perplexity = 3.66402555/39.0180969 secs/batch = 0.2665s, grad.norm=17.43330002
 21130: 15 [ 1225/ 1327], train_loss/perplexity = 3.33347273/28.0355320 secs/batch = 0.2664s, grad.norm=18.36976624
 21135: 15 [ 1230/ 1327], train_loss/perplexity = 3.67690587/39.5239143 secs/batch = 0.2669s, grad.norm=17.37197304
 21140: 15 [ 1235/ 1327], train_loss/perplexity = 3.64437580/38.2588844 secs/batch = 0.2653s, grad.norm=16.64188004
 21145: 15 [ 1240/ 1327], train_loss/perplexity = 3.75820470/42.8713913 secs/batch = 0.2659s, grad.norm=17.49518776
 21150: 15 [ 1245/ 1327], train_loss/perplexity = 3.77058840/43.4055977 secs/batch = 0.2660s, grad.norm=16.63859749
 21155: 15 [ 1250/ 1327], train_loss/perplexity = 3.94412804/51.6312981 secs/batch = 0.2654s, grad.norm=16.54058075
 21160: 15 [ 1255/ 1327], train_loss/perplexity = 3.92366791/50.5856476 secs/batch = 0.2660s, grad.norm=16.45151329
 21165: 15 [ 1260/ 1327], train_loss/perplexity = 3.69387221/40.2002106 secs/batch = 0.2659s, grad.norm=17.91911125
 21170: 15 [ 1265/ 1327], train_loss/perplexity = 3.91603637/50.2010727 secs/batch = 0.2665s, grad.norm=17.01552010
 21175: 15 [ 1270/ 1327], train_loss/perplexity = 3.63968205/38.0797272 secs/batch = 0.2660s, grad.norm=17.44288254
 21180: 15 [ 1275/ 1327], train_loss/perplexity = 3.85274911/47.1224289 secs/batch = 0.2657s, grad.norm=17.55048370
 21185: 15 [ 1280/ 1327], train_loss/perplexity = 3.68186665/39.7204704 secs/batch = 0.2668s, grad.norm=17.65962601
 21190: 15 [ 1285/ 1327], train_loss/perplexity = 3.59253073/36.3258896 secs/batch = 0.2652s, grad.norm=17.38417053
 21195: 15 [ 1290/ 1327], train_loss/perplexity = 3.77649355/43.6626701 secs/batch = 0.2642s, grad.norm=16.60282898
 21200: 15 [ 1295/ 1327], train_loss/perplexity = 3.85396719/47.1798630 secs/batch = 0.2642s, grad.norm=17.18269730
 21205: 15 [ 1300/ 1327], train_loss/perplexity = 3.99297857/54.2161369 secs/batch = 0.2679s, grad.norm=16.59824944
 21210: 15 [ 1305/ 1327], train_loss/perplexity = 4.05332136/57.5884132 secs/batch = 0.2651s, grad.norm=17.44989586
 21215: 15 [ 1310/ 1327], train_loss/perplexity = 4.28301144/72.4583130 secs/batch = 0.2673s, grad.norm=17.44520760
 21220: 15 [ 1315/ 1327], train_loss/perplexity = 4.04689407/57.2194595 secs/batch = 0.2659s, grad.norm=16.97691917
 21225: 15 [ 1320/ 1327], train_loss/perplexity = 4.08689070/59.5544319 secs/batch = 0.2662s, grad.norm=17.01538277
 21230: 15 [ 1325/ 1327], train_loss/perplexity = 4.00990677/55.1417313 secs/batch = 0.2660s, grad.norm=17.78673553
Epoch training time: 352.7679488658905
	> validation loss = 4.58831501, perplexity = 98.32860565
	> validation loss = 4.54073000, perplexity = 93.75921631
	> validation loss = 4.50870562, perplexity = 90.80420685
	> validation loss = 4.57567453, perplexity = 97.09351349
	> validation loss = 4.68656492, perplexity = 108.47990417
	> validation loss = 4.63885593, perplexity = 103.42595673
	> validation loss = 4.56051731, perplexity = 95.63294220
	> validation loss = 4.38656950, perplexity = 80.36425781
	> validation loss = 4.18295002, perplexity = 65.55896759
	> validation loss = 4.29500914, perplexity = 73.33288574
	> validation loss = 4.48712778, perplexity = 88.86583710
	> validation loss = 4.48558807, perplexity = 88.72911835
	> validation loss = 4.47622871, perplexity = 87.90254211
	> validation loss = 4.19816685, perplexity = 66.56419373
	> validation loss = 4.15879297, perplexity = 63.99423218
	> validation loss = 4.21390772, perplexity = 67.62026215
	> validation loss = 4.62198067, perplexity = 101.69525909
	> validation loss = 4.12981319, perplexity = 62.16630936
	> validation loss = 4.64107513, perplexity = 103.65573120
	> validation loss = 4.48987484, perplexity = 89.11029053
	> validation loss = 4.27211189, perplexity = 71.67284393
at the end of epoch: 15
train loss = 3.94880722, perplexity = 51.87345635
validation loss = 4.44157006, perplexity = 84.90814811
Saved model cv/epoch015_4.4416.model
 21237: 16 [    5/ 1327], train_loss/perplexity = 4.17144108/64.8087769 secs/batch = 0.2634s, grad.norm=17.27635956
 21242: 16 [   10/ 1327], train_loss/perplexity = 3.73081708/41.7131767 secs/batch = 0.2662s, grad.norm=16.95899582
 21247: 16 [   15/ 1327], train_loss/perplexity = 4.06531239/58.2831116 secs/batch = 0.2648s, grad.norm=16.00669289
 21252: 16 [   20/ 1327], train_loss/perplexity = 4.24983358/70.0937500 secs/batch = 0.2668s, grad.norm=16.64951515
 21257: 16 [   25/ 1327], train_loss/perplexity = 4.13920879/62.7531509 secs/batch = 0.2673s, grad.norm=17.87821198
 21262: 16 [   30/ 1327], train_loss/perplexity = 4.14539719/63.1426964 secs/batch = 0.2646s, grad.norm=17.48734665
 21267: 16 [   35/ 1327], train_loss/perplexity = 3.91677284/50.2380562 secs/batch = 0.2605s, grad.norm=16.77714729
 21272: 16 [   40/ 1327], train_loss/perplexity = 3.97239852/53.1117668 secs/batch = 0.2660s, grad.norm=17.05788994
 21277: 16 [   45/ 1327], train_loss/perplexity = 3.68112206/39.6909065 secs/batch = 0.2662s, grad.norm=16.03195381
 21282: 16 [   50/ 1327], train_loss/perplexity = 3.94775462/51.8188820 secs/batch = 0.2655s, grad.norm=16.97332764
 21287: 16 [   55/ 1327], train_loss/perplexity = 3.83105135/46.1109924 secs/batch = 0.2628s, grad.norm=17.14147949
 21292: 16 [   60/ 1327], train_loss/perplexity = 4.12575150/61.9143219 secs/batch = 0.2667s, grad.norm=17.32861710
 21297: 16 [   65/ 1327], train_loss/perplexity = 3.79664660/44.5515327 secs/batch = 0.2666s, grad.norm=16.88137436
 21302: 16 [   70/ 1327], train_loss/perplexity = 3.52390194/33.9165115 secs/batch = 0.2671s, grad.norm=17.00064659
 21307: 16 [   75/ 1327], train_loss/perplexity = 3.44266891/31.2703037 secs/batch = 0.2661s, grad.norm=16.13793182
 21312: 16 [   80/ 1327], train_loss/perplexity = 3.81026125/45.1622353 secs/batch = 0.2659s, grad.norm=16.91183281
 21317: 16 [   85/ 1327], train_loss/perplexity = 3.87301946/48.0873642 secs/batch = 0.2670s, grad.norm=17.22159195
 21322: 16 [   90/ 1327], train_loss/perplexity = 3.95692968/52.2965126 secs/batch = 0.2667s, grad.norm=17.30844116
 21327: 16 [   95/ 1327], train_loss/perplexity = 3.82180834/45.6867523 secs/batch = 0.2659s, grad.norm=17.21948814
 21332: 16 [  100/ 1327], train_loss/perplexity = 4.02238560/55.8341446 secs/batch = 0.2653s, grad.norm=16.70010567
 21337: 16 [  105/ 1327], train_loss/perplexity = 3.88672948/48.7511826 secs/batch = 0.2660s, grad.norm=17.71500969
 21342: 16 [  110/ 1327], train_loss/perplexity = 3.77168465/43.4532051 secs/batch = 0.2656s, grad.norm=17.00743294
 21347: 16 [  115/ 1327], train_loss/perplexity = 3.77759886/43.7109604 secs/batch = 0.2659s, grad.norm=17.54708481
 21352: 16 [  120/ 1327], train_loss/perplexity = 3.89598036/49.2042694 secs/batch = 0.2665s, grad.norm=17.38980103
 21357: 16 [  125/ 1327], train_loss/perplexity = 3.84608102/46.8092575 secs/batch = 0.2670s, grad.norm=17.40609169
 21362: 16 [  130/ 1327], train_loss/perplexity = 3.80183506/44.7832909 secs/batch = 0.2668s, grad.norm=17.64293480
 21367: 16 [  135/ 1327], train_loss/perplexity = 3.79853678/44.6358261 secs/batch = 0.2649s, grad.norm=17.02974319
 21372: 16 [  140/ 1327], train_loss/perplexity = 4.10226297/60.4769897 secs/batch = 0.2669s, grad.norm=17.74036217
 21377: 16 [  145/ 1327], train_loss/perplexity = 4.05152893/57.4852791 secs/batch = 0.2651s, grad.norm=18.22342873
 21382: 16 [  150/ 1327], train_loss/perplexity = 4.05836964/57.8798676 secs/batch = 0.2656s, grad.norm=18.01561356
 21387: 16 [  155/ 1327], train_loss/perplexity = 4.33967495/76.6826096 secs/batch = 0.2669s, grad.norm=17.54258919
 21392: 16 [  160/ 1327], train_loss/perplexity = 3.88477135/48.6558151 secs/batch = 0.2645s, grad.norm=16.19386292
 21397: 16 [  165/ 1327], train_loss/perplexity = 4.09779930/60.2076416 secs/batch = 0.2660s, grad.norm=17.20474434
 21402: 16 [  170/ 1327], train_loss/perplexity = 3.92534781/50.6707001 secs/batch = 0.2661s, grad.norm=16.57489586
 21407: 16 [  175/ 1327], train_loss/perplexity = 4.15474892/63.7359581 secs/batch = 0.2661s, grad.norm=16.84084892
 21412: 16 [  180/ 1327], train_loss/perplexity = 3.99375701/54.2583542 secs/batch = 0.2662s, grad.norm=16.84179306
 21417: 16 [  185/ 1327], train_loss/perplexity = 4.34275532/76.9191818 secs/batch = 0.2659s, grad.norm=17.15338326
 21422: 16 [  190/ 1327], train_loss/perplexity = 3.90882611/49.8404121 secs/batch = 0.2665s, grad.norm=16.01965141
 21427: 16 [  195/ 1327], train_loss/perplexity = 4.17290115/64.9034729 secs/batch = 0.2669s, grad.norm=16.03194809
 21432: 16 [  200/ 1327], train_loss/perplexity = 3.99203110/54.1647911 secs/batch = 0.2587s, grad.norm=17.17941284
 21437: 16 [  205/ 1327], train_loss/perplexity = 4.23140621/68.8139343 secs/batch = 0.2653s, grad.norm=17.61198997
 21442: 16 [  210/ 1327], train_loss/perplexity = 4.05979490/57.9624214 secs/batch = 0.2662s, grad.norm=16.01177025
 21447: 16 [  215/ 1327], train_loss/perplexity = 4.21335411/67.5828400 secs/batch = 0.2682s, grad.norm=16.16489029
 21452: 16 [  220/ 1327], train_loss/perplexity = 4.12466860/61.8473091 secs/batch = 0.2666s, grad.norm=16.71156883
 21457: 16 [  225/ 1327], train_loss/perplexity = 4.29397821/73.2573242 secs/batch = 0.2672s, grad.norm=17.20452499
 21462: 16 [  230/ 1327], train_loss/perplexity = 4.13090515/62.2342300 secs/batch = 0.2655s, grad.norm=17.83142281
 21467: 16 [  235/ 1327], train_loss/perplexity = 3.98664856/53.8740311 secs/batch = 0.2674s, grad.norm=16.97938538
 21472: 16 [  240/ 1327], train_loss/perplexity = 3.77855420/43.7527390 secs/batch = 0.2661s, grad.norm=17.00556183
 21477: 16 [  245/ 1327], train_loss/perplexity = 4.07588673/58.9026871 secs/batch = 0.2674s, grad.norm=16.98908806
 21482: 16 [  250/ 1327], train_loss/perplexity = 3.96740294/52.8471069 secs/batch = 0.2658s, grad.norm=16.25774574
 21487: 16 [  255/ 1327], train_loss/perplexity = 3.90785122/49.7918434 secs/batch = 0.2651s, grad.norm=16.84353256
 21492: 16 [  260/ 1327], train_loss/perplexity = 4.04591990/57.1637459 secs/batch = 0.2656s, grad.norm=18.04951859
 21497: 16 [  265/ 1327], train_loss/perplexity = 4.30645132/74.1767883 secs/batch = 0.2665s, grad.norm=16.51641464
 21502: 16 [  270/ 1327], train_loss/perplexity = 4.33912230/76.6402435 secs/batch = 0.2665s, grad.norm=17.47006989
 21507: 16 [  275/ 1327], train_loss/perplexity = 4.28830194/72.8426743 secs/batch = 0.2654s, grad.norm=16.75767708
 21512: 16 [  280/ 1327], train_loss/perplexity = 4.12001657/61.5602608 secs/batch = 0.2626s, grad.norm=16.36304665
 21517: 16 [  285/ 1327], train_loss/perplexity = 4.36933279/78.9909134 secs/batch = 0.2664s, grad.norm=16.74106026
 21522: 16 [  290/ 1327], train_loss/perplexity = 4.03618526/56.6099777 secs/batch = 0.2656s, grad.norm=17.67722321
 21527: 16 [  295/ 1327], train_loss/perplexity = 3.89595556/49.2030487 secs/batch = 0.2640s, grad.norm=17.00543785
 21532: 16 [  300/ 1327], train_loss/perplexity = 3.36552811/28.9487820 secs/batch = 0.2667s, grad.norm=16.22079659
 21537: 16 [  305/ 1327], train_loss/perplexity = 3.88215852/48.5288544 secs/batch = 0.2673s, grad.norm=16.41422081
 21542: 16 [  310/ 1327], train_loss/perplexity = 3.93622684/51.2249565 secs/batch = 0.2650s, grad.norm=16.94448853
 21547: 16 [  315/ 1327], train_loss/perplexity = 3.48903441/32.7543068 secs/batch = 0.2642s, grad.norm=15.79739571
 21552: 16 [  320/ 1327], train_loss/perplexity = 3.38758826/29.5944920 secs/batch = 0.2661s, grad.norm=16.92717743
 21557: 16 [  325/ 1327], train_loss/perplexity = 3.39947677/29.9484253 secs/batch = 0.2671s, grad.norm=16.03130913
 21562: 16 [  330/ 1327], train_loss/perplexity = 4.10605145/60.7065392 secs/batch = 0.2595s, grad.norm=17.29582024
 21567: 16 [  335/ 1327], train_loss/perplexity = 3.52348566/33.9023933 secs/batch = 0.2664s, grad.norm=15.89393997
 21572: 16 [  340/ 1327], train_loss/perplexity = 4.21824741/67.9143524 secs/batch = 0.2672s, grad.norm=17.09127235
 21577: 16 [  345/ 1327], train_loss/perplexity = 3.96367884/52.6506615 secs/batch = 0.2652s, grad.norm=16.22499466
 21582: 16 [  350/ 1327], train_loss/perplexity = 3.92291570/50.5476112 secs/batch = 0.2643s, grad.norm=17.22060585
 21587: 16 [  355/ 1327], train_loss/perplexity = 3.97910023/53.4689026 secs/batch = 0.2666s, grad.norm=16.64160156
 21592: 16 [  360/ 1327], train_loss/perplexity = 4.10581493/60.6921844 secs/batch = 0.2662s, grad.norm=17.67071342
 21597: 16 [  365/ 1327], train_loss/perplexity = 4.11564541/61.2917595 secs/batch = 0.2668s, grad.norm=17.02482033
 21602: 16 [  370/ 1327], train_loss/perplexity = 4.20252657/66.8550339 secs/batch = 0.2627s, grad.norm=17.00786972
 21607: 16 [  375/ 1327], train_loss/perplexity = 3.57681513/35.7594681 secs/batch = 0.2672s, grad.norm=16.98278618
 21612: 16 [  380/ 1327], train_loss/perplexity = 3.64443684/38.2612190 secs/batch = 0.2645s, grad.norm=17.33156204
 21617: 16 [  385/ 1327], train_loss/perplexity = 3.78938413/44.2291527 secs/batch = 0.2665s, grad.norm=17.19098091
 21622: 16 [  390/ 1327], train_loss/perplexity = 3.96582484/52.7637749 secs/batch = 0.2667s, grad.norm=17.01131439
 21627: 16 [  395/ 1327], train_loss/perplexity = 3.96810341/52.8841362 secs/batch = 0.2652s, grad.norm=16.92324448
 21632: 16 [  400/ 1327], train_loss/perplexity = 3.98088789/53.5645714 secs/batch = 0.2662s, grad.norm=16.55583572
 21637: 16 [  405/ 1327], train_loss/perplexity = 4.26056004/70.8496475 secs/batch = 0.2661s, grad.norm=17.33896065
 21642: 16 [  410/ 1327], train_loss/perplexity = 3.90380883/49.5909729 secs/batch = 0.2658s, grad.norm=17.11575890
 21647: 16 [  415/ 1327], train_loss/perplexity = 3.89672470/49.2409058 secs/batch = 0.2656s, grad.norm=16.79049683
 21652: 16 [  420/ 1327], train_loss/perplexity = 3.50568366/33.3042030 secs/batch = 0.2663s, grad.norm=16.62888527
 21657: 16 [  425/ 1327], train_loss/perplexity = 3.79559422/44.5046730 secs/batch = 0.2672s, grad.norm=17.36453247
 21662: 16 [  430/ 1327], train_loss/perplexity = 3.97627592/53.3181038 secs/batch = 0.2673s, grad.norm=18.29581642
 21667: 16 [  435/ 1327], train_loss/perplexity = 4.08645821/59.5286789 secs/batch = 0.2669s, grad.norm=17.92765808
 21672: 16 [  440/ 1327], train_loss/perplexity = 3.58359385/36.0026970 secs/batch = 0.2668s, grad.norm=16.60903931
 21677: 16 [  445/ 1327], train_loss/perplexity = 4.01265526/55.2934952 secs/batch = 0.2641s, grad.norm=17.83665466
 21682: 16 [  450/ 1327], train_loss/perplexity = 3.98697329/53.8915291 secs/batch = 0.2631s, grad.norm=17.03430939
 21687: 16 [  455/ 1327], train_loss/perplexity = 3.92813778/50.8122673 secs/batch = 0.2666s, grad.norm=17.08443069
 21692: 16 [  460/ 1327], train_loss/perplexity = 3.81674600/45.4560547 secs/batch = 0.2646s, grad.norm=17.60832405
 21697: 16 [  465/ 1327], train_loss/perplexity = 3.49118567/32.8248444 secs/batch = 0.2662s, grad.norm=17.56357193
 21702: 16 [  470/ 1327], train_loss/perplexity = 4.28322792/72.4740067 secs/batch = 0.2661s, grad.norm=17.05420685
 21707: 16 [  475/ 1327], train_loss/perplexity = 3.72446370/41.4489975 secs/batch = 0.2660s, grad.norm=17.09454346
 21712: 16 [  480/ 1327], train_loss/perplexity = 3.86128759/47.5265083 secs/batch = 0.2659s, grad.norm=16.64480782
 21717: 16 [  485/ 1327], train_loss/perplexity = 3.81632328/45.4368439 secs/batch = 0.2661s, grad.norm=17.37990379
 21722: 16 [  490/ 1327], train_loss/perplexity = 3.74947596/42.4988060 secs/batch = 0.2661s, grad.norm=17.92209435
 21727: 16 [  495/ 1327], train_loss/perplexity = 3.74874258/42.4676476 secs/batch = 0.2629s, grad.norm=16.50065613
 21732: 16 [  500/ 1327], train_loss/perplexity = 3.91810203/50.3048782 secs/batch = 0.2656s, grad.norm=16.65515900
 21737: 16 [  505/ 1327], train_loss/perplexity = 4.09626627/60.1154137 secs/batch = 0.2654s, grad.norm=16.56403351
 21742: 16 [  510/ 1327], train_loss/perplexity = 4.39400673/80.9641724 secs/batch = 0.2645s, grad.norm=16.64746094
 21747: 16 [  515/ 1327], train_loss/perplexity = 4.01045847/55.1721611 secs/batch = 0.2652s, grad.norm=16.11370087
 21752: 16 [  520/ 1327], train_loss/perplexity = 4.15091991/63.4923820 secs/batch = 0.2667s, grad.norm=17.01172638
 21757: 16 [  525/ 1327], train_loss/perplexity = 3.74870396/42.4660072 secs/batch = 0.2658s, grad.norm=16.73112869
 21762: 16 [  530/ 1327], train_loss/perplexity = 3.82384586/45.7799339 secs/batch = 0.2652s, grad.norm=16.98354912
 21767: 16 [  535/ 1327], train_loss/perplexity = 3.96544313/52.7436371 secs/batch = 0.2649s, grad.norm=16.75184441
 21772: 16 [  540/ 1327], train_loss/perplexity = 4.05034494/57.4172592 secs/batch = 0.2655s, grad.norm=16.79431915
 21777: 16 [  545/ 1327], train_loss/perplexity = 4.03721142/56.6680984 secs/batch = 0.2643s, grad.norm=16.95790482
 21782: 16 [  550/ 1327], train_loss/perplexity = 3.94485164/51.6686707 secs/batch = 0.2670s, grad.norm=16.83766365
 21787: 16 [  555/ 1327], train_loss/perplexity = 3.79887509/44.6509285 secs/batch = 0.2659s, grad.norm=16.24688339
 21792: 16 [  560/ 1327], train_loss/perplexity = 3.91166353/49.9820290 secs/batch = 0.2671s, grad.norm=17.66380882
 21797: 16 [  565/ 1327], train_loss/perplexity = 3.78504944/44.0378494 secs/batch = 0.2665s, grad.norm=18.27873993
 21802: 16 [  570/ 1327], train_loss/perplexity = 3.75470209/42.7214890 secs/batch = 0.2675s, grad.norm=17.65900230
 21807: 16 [  575/ 1327], train_loss/perplexity = 3.68810201/39.9689140 secs/batch = 0.2669s, grad.norm=17.53525734
 21812: 16 [  580/ 1327], train_loss/perplexity = 4.01819181/55.6004791 secs/batch = 0.2641s, grad.norm=17.63480568
 21817: 16 [  585/ 1327], train_loss/perplexity = 3.64598870/38.3206406 secs/batch = 0.2664s, grad.norm=16.67953110
 21822: 16 [  590/ 1327], train_loss/perplexity = 4.00422430/54.8292770 secs/batch = 0.2667s, grad.norm=16.92399979
 21827: 16 [  595/ 1327], train_loss/perplexity = 3.94735956/51.7984161 secs/batch = 0.2647s, grad.norm=17.35656357
 21832: 16 [  600/ 1327], train_loss/perplexity = 4.12742519/62.0180321 secs/batch = 0.2654s, grad.norm=16.28583145
 21837: 16 [  605/ 1327], train_loss/perplexity = 4.04668093/57.2072678 secs/batch = 0.2654s, grad.norm=16.42824936
 21842: 16 [  610/ 1327], train_loss/perplexity = 4.24104929/69.4807205 secs/batch = 0.2664s, grad.norm=17.24394226
 21847: 16 [  615/ 1327], train_loss/perplexity = 3.77000713/43.3803749 secs/batch = 0.2648s, grad.norm=16.24030495
 21852: 16 [  620/ 1327], train_loss/perplexity = 4.16814232/64.5953445 secs/batch = 0.2653s, grad.norm=16.72658348
 21857: 16 [  625/ 1327], train_loss/perplexity = 4.08498335/59.4409485 secs/batch = 0.2655s, grad.norm=16.85705948
 21862: 16 [  630/ 1327], train_loss/perplexity = 4.22193480/68.1652451 secs/batch = 0.2654s, grad.norm=16.84849167
 21867: 16 [  635/ 1327], train_loss/perplexity = 3.89146709/48.9826965 secs/batch = 0.2638s, grad.norm=16.88140869
 21872: 16 [  640/ 1327], train_loss/perplexity = 3.96608067/52.7772713 secs/batch = 0.2667s, grad.norm=16.69770432
 21877: 16 [  645/ 1327], train_loss/perplexity = 4.17305851/64.9136887 secs/batch = 0.2658s, grad.norm=17.85863495
 21882: 16 [  650/ 1327], train_loss/perplexity = 3.74932957/42.4925842 secs/batch = 0.2669s, grad.norm=17.14764023
 21887: 16 [  655/ 1327], train_loss/perplexity = 3.87286568/48.0799713 secs/batch = 0.2639s, grad.norm=17.20601273
 21892: 16 [  660/ 1327], train_loss/perplexity = 3.73455954/41.8695793 secs/batch = 0.2645s, grad.norm=17.39989662
 21897: 16 [  665/ 1327], train_loss/perplexity = 3.88505292/48.6695175 secs/batch = 0.2659s, grad.norm=17.06465912
 21902: 16 [  670/ 1327], train_loss/perplexity = 3.89878798/49.3426094 secs/batch = 0.2667s, grad.norm=16.96070862
 21907: 16 [  675/ 1327], train_loss/perplexity = 3.63283372/37.8198357 secs/batch = 0.2664s, grad.norm=17.16178131
 21912: 16 [  680/ 1327], train_loss/perplexity = 3.91774416/50.2868767 secs/batch = 0.2664s, grad.norm=17.77620506
 21917: 16 [  685/ 1327], train_loss/perplexity = 3.60590482/36.8149796 secs/batch = 0.2686s, grad.norm=16.59494781
 21922: 16 [  690/ 1327], train_loss/perplexity = 4.12921238/62.1289711 secs/batch = 0.2632s, grad.norm=16.73458481
 21927: 16 [  695/ 1327], train_loss/perplexity = 3.98531008/53.8019714 secs/batch = 0.2659s, grad.norm=17.00180817
 21932: 16 [  700/ 1327], train_loss/perplexity = 4.19626999/66.4380569 secs/batch = 0.2664s, grad.norm=17.27795982
 21937: 16 [  705/ 1327], train_loss/perplexity = 3.94678545/51.7686844 secs/batch = 0.2651s, grad.norm=16.84456825
 21942: 16 [  710/ 1327], train_loss/perplexity = 3.84859514/46.9270897 secs/batch = 0.2657s, grad.norm=17.91643906
 21947: 16 [  715/ 1327], train_loss/perplexity = 3.65543580/38.6843758 secs/batch = 0.2670s, grad.norm=16.93633652
 21952: 16 [  720/ 1327], train_loss/perplexity = 3.70255661/40.5508461 secs/batch = 0.2668s, grad.norm=17.56090736
 21957: 16 [  725/ 1327], train_loss/perplexity = 3.71204162/40.9372978 secs/batch = 0.2660s, grad.norm=16.89000130
 21962: 16 [  730/ 1327], train_loss/perplexity = 3.90192056/49.4974213 secs/batch = 0.2644s, grad.norm=17.38263512
 21967: 16 [  735/ 1327], train_loss/perplexity = 3.93561721/51.1937370 secs/batch = 0.2666s, grad.norm=18.17895126
 21972: 16 [  740/ 1327], train_loss/perplexity = 3.49098492/32.8182564 secs/batch = 0.2669s, grad.norm=16.47827339
 21977: 16 [  745/ 1327], train_loss/perplexity = 3.89432621/49.1229439 secs/batch = 0.2652s, grad.norm=17.86526871
 21982: 16 [  750/ 1327], train_loss/perplexity = 3.83507633/46.2969627 secs/batch = 0.2650s, grad.norm=16.94927216
 21987: 16 [  755/ 1327], train_loss/perplexity = 3.68508244/39.8484077 secs/batch = 0.2662s, grad.norm=16.57907295
 21992: 16 [  760/ 1327], train_loss/perplexity = 3.49796462/33.0481186 secs/batch = 0.2662s, grad.norm=15.74995041
 21997: 16 [  765/ 1327], train_loss/perplexity = 3.69164538/40.1107903 secs/batch = 0.2667s, grad.norm=17.03901672
 22002: 16 [  770/ 1327], train_loss/perplexity = 3.65353823/38.6110382 secs/batch = 0.2670s, grad.norm=16.77435493
 22007: 16 [  775/ 1327], train_loss/perplexity = 3.66383338/39.0105972 secs/batch = 0.2637s, grad.norm=17.68461990
 22012: 16 [  780/ 1327], train_loss/perplexity = 4.04591560/57.1635017 secs/batch = 0.2651s, grad.norm=17.49594688
 22017: 16 [  785/ 1327], train_loss/perplexity = 3.90947294/49.8726578 secs/batch = 0.2607s, grad.norm=17.57903290
 22022: 16 [  790/ 1327], train_loss/perplexity = 3.67839122/39.5826645 secs/batch = 0.2662s, grad.norm=16.73547554
 22027: 16 [  795/ 1327], train_loss/perplexity = 4.08606052/59.5050125 secs/batch = 0.2638s, grad.norm=17.48404884
 22032: 16 [  800/ 1327], train_loss/perplexity = 3.89332795/49.0739288 secs/batch = 0.2662s, grad.norm=17.43890381
 22037: 16 [  805/ 1327], train_loss/perplexity = 4.20683479/67.1436768 secs/batch = 0.2663s, grad.norm=17.04626465
 22042: 16 [  810/ 1327], train_loss/perplexity = 3.90817881/49.8081589 secs/batch = 0.2659s, grad.norm=16.17566872
 22047: 16 [  815/ 1327], train_loss/perplexity = 3.77402425/43.5549889 secs/batch = 0.2600s, grad.norm=16.85626030
 22052: 16 [  820/ 1327], train_loss/perplexity = 3.66404700/39.0189323 secs/batch = 0.2613s, grad.norm=16.20431137
 22057: 16 [  825/ 1327], train_loss/perplexity = 3.87088513/47.9848404 secs/batch = 0.2679s, grad.norm=17.34436226
 22062: 16 [  830/ 1327], train_loss/perplexity = 3.53898287/34.4318810 secs/batch = 0.2607s, grad.norm=17.38448906
 22067: 16 [  835/ 1327], train_loss/perplexity = 3.86908484/47.8985291 secs/batch = 0.2660s, grad.norm=17.41956902
 22072: 16 [  840/ 1327], train_loss/perplexity = 3.99786711/54.4818230 secs/batch = 0.2642s, grad.norm=17.55362320
 22077: 16 [  845/ 1327], train_loss/perplexity = 3.74855638/42.4597435 secs/batch = 0.2616s, grad.norm=17.48608780
 22082: 16 [  850/ 1327], train_loss/perplexity = 3.82681799/45.9161987 secs/batch = 0.2670s, grad.norm=16.72212410
 22087: 16 [  855/ 1327], train_loss/perplexity = 3.83491898/46.2896767 secs/batch = 0.2641s, grad.norm=17.69805717
 22092: 16 [  860/ 1327], train_loss/perplexity = 3.62428474/37.4978943 secs/batch = 0.2659s, grad.norm=16.34360886
 22097: 16 [  865/ 1327], train_loss/perplexity = 3.97662783/53.3368683 secs/batch = 0.2657s, grad.norm=17.31972694
 22102: 16 [  870/ 1327], train_loss/perplexity = 3.89311790/49.0636253 secs/batch = 0.2661s, grad.norm=17.50465584
 22107: 16 [  875/ 1327], train_loss/perplexity = 3.52420020/33.9266281 secs/batch = 0.2661s, grad.norm=16.82369614
 22112: 16 [  880/ 1327], train_loss/perplexity = 3.73714209/41.9778481 secs/batch = 0.2655s, grad.norm=16.96153259
 22117: 16 [  885/ 1327], train_loss/perplexity = 3.92419434/50.6122856 secs/batch = 0.2661s, grad.norm=17.07555008
 22122: 16 [  890/ 1327], train_loss/perplexity = 4.11993694/61.5553589 secs/batch = 0.2657s, grad.norm=17.15415382
 22127: 16 [  895/ 1327], train_loss/perplexity = 3.97483516/53.2413406 secs/batch = 0.2614s, grad.norm=16.66940308
 22132: 16 [  900/ 1327], train_loss/perplexity = 3.87554359/48.2088966 secs/batch = 0.2624s, grad.norm=16.66040802
 22137: 16 [  905/ 1327], train_loss/perplexity = 3.82043409/45.6240082 secs/batch = 0.2667s, grad.norm=16.27752686
 22142: 16 [  910/ 1327], train_loss/perplexity = 3.81309938/45.2905960 secs/batch = 0.2669s, grad.norm=16.15033150
 22147: 16 [  915/ 1327], train_loss/perplexity = 4.00393820/54.8135910 secs/batch = 0.2666s, grad.norm=16.42461205
 22152: 16 [  920/ 1327], train_loss/perplexity = 4.16470432/64.3736496 secs/batch = 0.2654s, grad.norm=17.11713028
 22157: 16 [  925/ 1327], train_loss/perplexity = 3.99846935/54.5146446 secs/batch = 0.2656s, grad.norm=17.00739098
 22162: 16 [  930/ 1327], train_loss/perplexity = 4.00628185/54.9422073 secs/batch = 0.2639s, grad.norm=16.67793655
 22167: 16 [  935/ 1327], train_loss/perplexity = 4.00663519/54.9616241 secs/batch = 0.2663s, grad.norm=16.60313988
 22172: 16 [  940/ 1327], train_loss/perplexity = 4.02925587/56.2190628 secs/batch = 0.2665s, grad.norm=16.72797394
 22177: 16 [  945/ 1327], train_loss/perplexity = 4.18566275/65.7370529 secs/batch = 0.2669s, grad.norm=16.58248138
 22182: 16 [  950/ 1327], train_loss/perplexity = 4.02818394/56.1588287 secs/batch = 0.2665s, grad.norm=16.57774734
 22187: 16 [  955/ 1327], train_loss/perplexity = 3.92308283/50.5560608 secs/batch = 0.2628s, grad.norm=17.00132370
 22192: 16 [  960/ 1327], train_loss/perplexity = 4.21452904/67.6622925 secs/batch = 0.2607s, grad.norm=17.01942253
 22197: 16 [  965/ 1327], train_loss/perplexity = 3.97516823/53.2590752 secs/batch = 0.2620s, grad.norm=16.95025063
 22202: 16 [  970/ 1327], train_loss/perplexity = 4.17605972/65.1088028 secs/batch = 0.2640s, grad.norm=17.21718788
 22207: 16 [  975/ 1327], train_loss/perplexity = 3.86602139/47.7520218 secs/batch = 0.2655s, grad.norm=17.83852577
 22212: 16 [  980/ 1327], train_loss/perplexity = 3.69990921/40.4436340 secs/batch = 0.2670s, grad.norm=17.02026367
 22217: 16 [  985/ 1327], train_loss/perplexity = 3.86247611/47.5830269 secs/batch = 0.2661s, grad.norm=17.23273849
 22222: 16 [  990/ 1327], train_loss/perplexity = 4.09464979/60.0183182 secs/batch = 0.2659s, grad.norm=17.54290962
 22227: 16 [  995/ 1327], train_loss/perplexity = 4.04457521/57.0869293 secs/batch = 0.2659s, grad.norm=16.81373596
 22232: 16 [ 1000/ 1327], train_loss/perplexity = 3.63243055/37.8045921 secs/batch = 0.2675s, grad.norm=16.77472115
 22237: 16 [ 1005/ 1327], train_loss/perplexity = 4.02275562/55.8548088 secs/batch = 0.2663s, grad.norm=17.20045471
 22242: 16 [ 1010/ 1327], train_loss/perplexity = 3.67529249/39.4601974 secs/batch = 0.2605s, grad.norm=16.11325073
 22247: 16 [ 1015/ 1327], train_loss/perplexity = 4.16120768/64.1489487 secs/batch = 0.2657s, grad.norm=16.65583420
 22252: 16 [ 1020/ 1327], train_loss/perplexity = 4.26898527/71.4490967 secs/batch = 0.2658s, grad.norm=16.85323143
 22257: 16 [ 1025/ 1327], train_loss/perplexity = 4.09094954/59.7966423 secs/batch = 0.2664s, grad.norm=16.90603065
 22262: 16 [ 1030/ 1327], train_loss/perplexity = 3.93986177/51.4114952 secs/batch = 0.2663s, grad.norm=16.29740524
 22267: 16 [ 1035/ 1327], train_loss/perplexity = 3.83093357/46.1055603 secs/batch = 0.2688s, grad.norm=16.80264473
 22272: 16 [ 1040/ 1327], train_loss/perplexity = 4.06473875/58.2496872 secs/batch = 0.2668s, grad.norm=17.67320061
 22277: 16 [ 1045/ 1327], train_loss/perplexity = 3.58423591/36.0258217 secs/batch = 0.2673s, grad.norm=16.36775398
 22282: 16 [ 1050/ 1327], train_loss/perplexity = 3.67869282/39.5946045 secs/batch = 0.2662s, grad.norm=16.83567810
 22287: 16 [ 1055/ 1327], train_loss/perplexity = 3.73265409/41.7898750 secs/batch = 0.2670s, grad.norm=17.86686707
 22292: 16 [ 1060/ 1327], train_loss/perplexity = 3.38219762/29.4353886 secs/batch = 0.2660s, grad.norm=17.20756149
 22297: 16 [ 1065/ 1327], train_loss/perplexity = 3.61503363/37.1525955 secs/batch = 0.2678s, grad.norm=17.21804047
 22302: 16 [ 1070/ 1327], train_loss/perplexity = 3.87859941/48.3564415 secs/batch = 0.2659s, grad.norm=17.52605438
 22307: 16 [ 1075/ 1327], train_loss/perplexity = 3.72159290/41.3301773 secs/batch = 0.2665s, grad.norm=16.59783936
 22312: 16 [ 1080/ 1327], train_loss/perplexity = 3.56051636/35.1813583 secs/batch = 0.2653s, grad.norm=16.86479187
 22317: 16 [ 1085/ 1327], train_loss/perplexity = 3.55216479/34.8887634 secs/batch = 0.2583s, grad.norm=17.23989105
 22322: 16 [ 1090/ 1327], train_loss/perplexity = 3.66691327/39.1309319 secs/batch = 0.2677s, grad.norm=17.97180176
 22327: 16 [ 1095/ 1327], train_loss/perplexity = 3.86284828/47.6007385 secs/batch = 0.2657s, grad.norm=17.82196045
 22332: 16 [ 1100/ 1327], train_loss/perplexity = 3.49877787/33.0750046 secs/batch = 0.2662s, grad.norm=18.04809380
 22337: 16 [ 1105/ 1327], train_loss/perplexity = 3.57436037/35.6717949 secs/batch = 0.2630s, grad.norm=17.64517593
 22342: 16 [ 1110/ 1327], train_loss/perplexity = 3.90730906/49.7648582 secs/batch = 0.2667s, grad.norm=17.73410034
 22347: 16 [ 1115/ 1327], train_loss/perplexity = 3.62741518/37.6154633 secs/batch = 0.2648s, grad.norm=16.47867203
 22352: 16 [ 1120/ 1327], train_loss/perplexity = 3.97402096/53.1980095 secs/batch = 0.2641s, grad.norm=16.98064041
 22357: 16 [ 1125/ 1327], train_loss/perplexity = 4.03249979/56.4017258 secs/batch = 0.2658s, grad.norm=18.16954041
 22362: 16 [ 1130/ 1327], train_loss/perplexity = 3.73275137/41.7939415 secs/batch = 0.2674s, grad.norm=17.39178848
 22367: 16 [ 1135/ 1327], train_loss/perplexity = 3.72820044/41.6041718 secs/batch = 0.2654s, grad.norm=17.26416397
 22372: 16 [ 1140/ 1327], train_loss/perplexity = 4.05126619/57.4701805 secs/batch = 0.2659s, grad.norm=18.11058426
 22377: 16 [ 1145/ 1327], train_loss/perplexity = 3.89192176/49.0049706 secs/batch = 0.2672s, grad.norm=16.71884918
 22382: 16 [ 1150/ 1327], train_loss/perplexity = 3.81906986/45.5618095 secs/batch = 0.2666s, grad.norm=16.77966690
 22387: 16 [ 1155/ 1327], train_loss/perplexity = 3.84021711/46.5355759 secs/batch = 0.2709s, grad.norm=17.55012894
 22392: 16 [ 1160/ 1327], train_loss/perplexity = 3.79887724/44.6510239 secs/batch = 0.2678s, grad.norm=17.06129646
 22397: 16 [ 1165/ 1327], train_loss/perplexity = 3.86501026/47.7037621 secs/batch = 0.2666s, grad.norm=17.31062508
 22402: 16 [ 1170/ 1327], train_loss/perplexity = 3.74155664/42.1635742 secs/batch = 0.2666s, grad.norm=17.36465645
 22407: 16 [ 1175/ 1327], train_loss/perplexity = 3.54052448/34.4850006 secs/batch = 0.2658s, grad.norm=16.77447510
 22412: 16 [ 1180/ 1327], train_loss/perplexity = 3.60120511/36.6423645 secs/batch = 0.2645s, grad.norm=17.97012901
 22417: 16 [ 1185/ 1327], train_loss/perplexity = 3.73383236/41.8391457 secs/batch = 0.2653s, grad.norm=17.43633461
 22422: 16 [ 1190/ 1327], train_loss/perplexity = 3.81304693/45.2882195 secs/batch = 0.2671s, grad.norm=18.02001190
 22427: 16 [ 1195/ 1327], train_loss/perplexity = 3.64456916/38.2662811 secs/batch = 0.2659s, grad.norm=17.27977371
 22432: 16 [ 1200/ 1327], train_loss/perplexity = 3.61543822/37.1676292 secs/batch = 0.2654s, grad.norm=17.35950470
 22437: 16 [ 1205/ 1327], train_loss/perplexity = 3.70749784/40.7517128 secs/batch = 0.2649s, grad.norm=17.57842636
 22442: 16 [ 1210/ 1327], train_loss/perplexity = 3.21950912/25.0158367 secs/batch = 0.2661s, grad.norm=17.28266716
 22447: 16 [ 1215/ 1327], train_loss/perplexity = 3.50905347/33.4166222 secs/batch = 0.2649s, grad.norm=17.13198280
 22452: 16 [ 1220/ 1327], train_loss/perplexity = 3.64880610/38.4287567 secs/batch = 0.2646s, grad.norm=17.75904846
 22457: 16 [ 1225/ 1327], train_loss/perplexity = 3.37919021/29.3469963 secs/batch = 0.2682s, grad.norm=18.25996780
 22462: 16 [ 1230/ 1327], train_loss/perplexity = 3.62374997/37.4778442 secs/batch = 0.2648s, grad.norm=17.24281883
 22467: 16 [ 1235/ 1327], train_loss/perplexity = 3.56733179/35.4219551 secs/batch = 0.2656s, grad.norm=16.59007835
 22472: 16 [ 1240/ 1327], train_loss/perplexity = 3.81160760/45.2230797 secs/batch = 0.2651s, grad.norm=17.76340485
 22477: 16 [ 1245/ 1327], train_loss/perplexity = 3.72126913/41.3167992 secs/batch = 0.2662s, grad.norm=16.77059174
 22482: 16 [ 1250/ 1327], train_loss/perplexity = 3.90531301/49.6656227 secs/batch = 0.2653s, grad.norm=16.89929771
 22487: 16 [ 1255/ 1327], train_loss/perplexity = 3.98205090/53.6269035 secs/batch = 0.2661s, grad.norm=16.98204803
 22492: 16 [ 1260/ 1327], train_loss/perplexity = 3.68670988/39.9133110 secs/batch = 0.2658s, grad.norm=18.27743340
 22497: 16 [ 1265/ 1327], train_loss/perplexity = 3.88676643/48.7529869 secs/batch = 0.2679s, grad.norm=17.38591957
 22502: 16 [ 1270/ 1327], train_loss/perplexity = 3.63941526/38.0695686 secs/batch = 0.2653s, grad.norm=17.49516869
 22507: 16 [ 1275/ 1327], train_loss/perplexity = 3.79574013/44.5111694 secs/batch = 0.2655s, grad.norm=17.78673553
 22512: 16 [ 1280/ 1327], train_loss/perplexity = 3.69378471/40.1966934 secs/batch = 0.2657s, grad.norm=17.64895058
 22517: 16 [ 1285/ 1327], train_loss/perplexity = 3.58745098/36.1418304 secs/batch = 0.2660s, grad.norm=17.39360237
 22522: 16 [ 1290/ 1327], train_loss/perplexity = 3.80328608/44.8483162 secs/batch = 0.2669s, grad.norm=17.02906799
 22527: 16 [ 1295/ 1327], train_loss/perplexity = 3.80974126/45.1387596 secs/batch = 0.2662s, grad.norm=17.07020950
 22532: 16 [ 1300/ 1327], train_loss/perplexity = 3.99598646/54.3794556 secs/batch = 0.2640s, grad.norm=16.64801979
 22537: 16 [ 1305/ 1327], train_loss/perplexity = 4.04334450/57.0167160 secs/batch = 0.2671s, grad.norm=17.81565094
 22542: 16 [ 1310/ 1327], train_loss/perplexity = 4.27483273/71.8681183 secs/batch = 0.2660s, grad.norm=17.91100693
 22547: 16 [ 1315/ 1327], train_loss/perplexity = 4.09286451/59.9112625 secs/batch = 0.2665s, grad.norm=17.81880951
 22552: 16 [ 1320/ 1327], train_loss/perplexity = 4.04419041/57.0649681 secs/batch = 0.2654s, grad.norm=17.41312408
 22557: 16 [ 1325/ 1327], train_loss/perplexity = 3.99686098/54.4270325 secs/batch = 0.2654s, grad.norm=17.61554337
Epoch training time: 352.7171025276184
	> validation loss = 4.58400202, perplexity = 97.90543365
	> validation loss = 4.54489565, perplexity = 94.15060425
	> validation loss = 4.48981857, perplexity = 89.10527802
	> validation loss = 4.57467270, perplexity = 96.99628448
	> validation loss = 4.68653440, perplexity = 108.47659302
	> validation loss = 4.62063456, perplexity = 101.55845642
	> validation loss = 4.55466557, perplexity = 95.07495117
	> validation loss = 4.39110708, perplexity = 80.72974396
	> validation loss = 4.17420435, perplexity = 64.98811340
	> validation loss = 4.28587627, perplexity = 72.66619110
	> validation loss = 4.48875093, perplexity = 89.01020050
	> validation loss = 4.47801256, perplexity = 88.05948639
	> validation loss = 4.45440674, perplexity = 86.00511169
	> validation loss = 4.19025230, perplexity = 66.03945160
	> validation loss = 4.15176296, perplexity = 63.54592896
	> validation loss = 4.21051693, perplexity = 67.39136505
	> validation loss = 4.61386776, perplexity = 100.87355042
	> validation loss = 4.12733412, perplexity = 62.01238632
	> validation loss = 4.63117981, perplexity = 102.63508606
	> validation loss = 4.49468851, perplexity = 89.54027557
	> validation loss = 4.26315260, perplexity = 71.03356934
at the end of epoch: 16
train loss = 3.94006687, perplexity = 51.42204002
validation loss = 4.43733855, perplexity = 84.54961755
Saved model cv/epoch016_4.4373.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.125
new learning rate is: 0.0625
 22564: 17 [    5/ 1327], train_loss/perplexity = 4.10579491/60.6909676 secs/batch = 0.2663s, grad.norm=17.16806602
 22569: 17 [   10/ 1327], train_loss/perplexity = 3.75855827/42.8865509 secs/batch = 0.2612s, grad.norm=17.07509613
 22574: 17 [   15/ 1327], train_loss/perplexity = 4.09583139/60.0892754 secs/batch = 0.2648s, grad.norm=16.89052010
 22579: 17 [   20/ 1327], train_loss/perplexity = 4.22114420/68.1113739 secs/batch = 0.2666s, grad.norm=16.70125961
 22584: 17 [   25/ 1327], train_loss/perplexity = 4.09749699/60.1894455 secs/batch = 0.2662s, grad.norm=18.09317398
 22589: 17 [   30/ 1327], train_loss/perplexity = 4.09730339/60.1777916 secs/batch = 0.2655s, grad.norm=17.86529922
 22594: 17 [   35/ 1327], train_loss/perplexity = 3.87214398/48.0452843 secs/batch = 0.2639s, grad.norm=17.07292747
 22599: 17 [   40/ 1327], train_loss/perplexity = 3.88782692/48.8047142 secs/batch = 0.2658s, grad.norm=17.08994484
 22604: 17 [   45/ 1327], train_loss/perplexity = 3.70100117/40.4878197 secs/batch = 0.2625s, grad.norm=16.69098091
 22609: 17 [   50/ 1327], train_loss/perplexity = 3.89772224/49.2900505 secs/batch = 0.2657s, grad.norm=17.35164261
 22614: 17 [   55/ 1327], train_loss/perplexity = 3.81157780/45.2217331 secs/batch = 0.2657s, grad.norm=17.50802994
 22619: 17 [   60/ 1327], train_loss/perplexity = 4.11230659/61.0874596 secs/batch = 0.2649s, grad.norm=17.34402466
 22624: 17 [   65/ 1327], train_loss/perplexity = 3.73344636/41.8229980 secs/batch = 0.2645s, grad.norm=16.78759575
 22629: 17 [   70/ 1327], train_loss/perplexity = 3.53827143/34.4073906 secs/batch = 0.2661s, grad.norm=17.25817108
 22634: 17 [   75/ 1327], train_loss/perplexity = 3.38327980/29.4672604 secs/batch = 0.2664s, grad.norm=16.44212723
 22639: 17 [   80/ 1327], train_loss/perplexity = 3.83614135/46.3462944 secs/batch = 0.2641s, grad.norm=17.55040359
 22644: 17 [   85/ 1327], train_loss/perplexity = 3.85081935/47.0315819 secs/batch = 0.2669s, grad.norm=17.71801949
 22649: 17 [   90/ 1327], train_loss/perplexity = 3.95101404/51.9880600 secs/batch = 0.2666s, grad.norm=17.89699173
 22654: 17 [   95/ 1327], train_loss/perplexity = 3.80807424/45.0635719 secs/batch = 0.2665s, grad.norm=17.53775406
 22659: 17 [  100/ 1327], train_loss/perplexity = 4.12449741/61.8367233 secs/batch = 0.2663s, grad.norm=17.63451195
 22664: 17 [  105/ 1327], train_loss/perplexity = 3.85107422/47.0435715 secs/batch = 0.2666s, grad.norm=18.01957893
 22669: 17 [  110/ 1327], train_loss/perplexity = 3.78632164/44.0939102 secs/batch = 0.2656s, grad.norm=17.59464836
 22674: 17 [  115/ 1327], train_loss/perplexity = 3.69045401/40.0630302 secs/batch = 0.2662s, grad.norm=18.04402924
 22679: 17 [  120/ 1327], train_loss/perplexity = 3.86172104/47.5471115 secs/batch = 0.2654s, grad.norm=17.82856369
 22684: 17 [  125/ 1327], train_loss/perplexity = 3.87997389/48.4229507 secs/batch = 0.2662s, grad.norm=17.98222733
 22689: 17 [  130/ 1327], train_loss/perplexity = 3.83904457/46.4810448 secs/batch = 0.2656s, grad.norm=18.20829391
 22694: 17 [  135/ 1327], train_loss/perplexity = 3.83364344/46.2306709 secs/batch = 0.2646s, grad.norm=17.10349083
 22699: 17 [  140/ 1327], train_loss/perplexity = 4.11985254/61.5501671 secs/batch = 0.2624s, grad.norm=18.05024529
 22704: 17 [  145/ 1327], train_loss/perplexity = 3.99974823/54.5844040 secs/batch = 0.2650s, grad.norm=18.28772163
 22709: 17 [  150/ 1327], train_loss/perplexity = 3.97891235/53.4588585 secs/batch = 0.2638s, grad.norm=18.53477097
 22714: 17 [  155/ 1327], train_loss/perplexity = 4.31802177/75.0400314 secs/batch = 0.2641s, grad.norm=18.26318169
 22719: 17 [  160/ 1327], train_loss/perplexity = 3.92474270/50.6400452 secs/batch = 0.2656s, grad.norm=16.77089691
 22724: 17 [  165/ 1327], train_loss/perplexity = 4.08421898/59.3955307 secs/batch = 0.2659s, grad.norm=17.28283310
 22729: 17 [  170/ 1327], train_loss/perplexity = 3.94840622/51.8526573 secs/batch = 0.2659s, grad.norm=17.01358604
 22734: 17 [  175/ 1327], train_loss/perplexity = 4.11858273/61.4720573 secs/batch = 0.2664s, grad.norm=17.19308472
 22739: 17 [  180/ 1327], train_loss/perplexity = 4.03636742/56.6202927 secs/batch = 0.2655s, grad.norm=17.76994514
 22744: 17 [  185/ 1327], train_loss/perplexity = 4.34793568/77.3186874 secs/batch = 0.2668s, grad.norm=18.05870056
 22749: 17 [  190/ 1327], train_loss/perplexity = 3.91782403/50.2908936 secs/batch = 0.2663s, grad.norm=16.62226677
 22754: 17 [  195/ 1327], train_loss/perplexity = 4.13726902/62.6315422 secs/batch = 0.2658s, grad.norm=16.65987015
 22759: 17 [  200/ 1327], train_loss/perplexity = 3.96517992/52.7297554 secs/batch = 0.2649s, grad.norm=17.97902489
 22764: 17 [  205/ 1327], train_loss/perplexity = 4.13118505/62.2516518 secs/batch = 0.2665s, grad.norm=17.50361633
 22769: 17 [  210/ 1327], train_loss/perplexity = 4.02651596/56.0652390 secs/batch = 0.2661s, grad.norm=16.44006920
 22774: 17 [  215/ 1327], train_loss/perplexity = 4.20719147/67.1676331 secs/batch = 0.2662s, grad.norm=16.71936226
 22779: 17 [  220/ 1327], train_loss/perplexity = 4.08429050/59.3997803 secs/batch = 0.2669s, grad.norm=16.83199501
 22784: 17 [  225/ 1327], train_loss/perplexity = 4.29455614/73.2996750 secs/batch = 0.2659s, grad.norm=17.34842873
 22789: 17 [  230/ 1327], train_loss/perplexity = 4.11231279/61.0878372 secs/batch = 0.2623s, grad.norm=18.18432045
 22794: 17 [  235/ 1327], train_loss/perplexity = 4.00201368/54.7082024 secs/batch = 0.2669s, grad.norm=17.62395287
 22799: 17 [  240/ 1327], train_loss/perplexity = 3.71822953/41.1914024 secs/batch = 0.2662s, grad.norm=17.07207298
 22804: 17 [  245/ 1327], train_loss/perplexity = 4.02259588/55.8458862 secs/batch = 0.2665s, grad.norm=17.26165009
 22809: 17 [  250/ 1327], train_loss/perplexity = 3.87886477/48.3692741 secs/batch = 0.2659s, grad.norm=16.73829651
 22814: 17 [  255/ 1327], train_loss/perplexity = 3.88654566/48.7422218 secs/batch = 0.2671s, grad.norm=17.18291473
 22819: 17 [  260/ 1327], train_loss/perplexity = 4.03029490/56.2775040 secs/batch = 0.2661s, grad.norm=17.97978973
 22824: 17 [  265/ 1327], train_loss/perplexity = 4.23992157/69.4024048 secs/batch = 0.2660s, grad.norm=17.00268936
 22829: 17 [  270/ 1327], train_loss/perplexity = 4.30047083/73.7345047 secs/batch = 0.2657s, grad.norm=17.32811546
 22834: 17 [  275/ 1327], train_loss/perplexity = 4.27683687/72.0122986 secs/batch = 0.2664s, grad.norm=17.31569099
 22839: 17 [  280/ 1327], train_loss/perplexity = 3.99524546/54.3391762 secs/batch = 0.2655s, grad.norm=16.70672607
 22844: 17 [  285/ 1327], train_loss/perplexity = 4.43026161/83.9533768 secs/batch = 0.2661s, grad.norm=17.18547821
 22849: 17 [  290/ 1327], train_loss/perplexity = 4.06483412/58.2552452 secs/batch = 0.2647s, grad.norm=18.05893898
 22854: 17 [  295/ 1327], train_loss/perplexity = 3.87602043/48.2318916 secs/batch = 0.2669s, grad.norm=17.20641327
 22859: 17 [  300/ 1327], train_loss/perplexity = 3.36777735/29.0139675 secs/batch = 0.2655s, grad.norm=16.50101852
 22864: 17 [  305/ 1327], train_loss/perplexity = 3.86234951/47.5770035 secs/batch = 0.2664s, grad.norm=16.77722359
 22869: 17 [  310/ 1327], train_loss/perplexity = 3.85011244/46.9983482 secs/batch = 0.2662s, grad.norm=17.10477638
 22874: 17 [  315/ 1327], train_loss/perplexity = 3.42669773/30.7748470 secs/batch = 0.2675s, grad.norm=15.78652763
 22879: 17 [  320/ 1327], train_loss/perplexity = 3.37111163/29.1108704 secs/batch = 0.2667s, grad.norm=17.17007256
 22884: 17 [  325/ 1327], train_loss/perplexity = 3.41422439/30.3933659 secs/batch = 0.2667s, grad.norm=16.29479408
 22889: 17 [  330/ 1327], train_loss/perplexity = 3.98403025/53.7331581 secs/batch = 0.2664s, grad.norm=17.22873878
 22894: 17 [  335/ 1327], train_loss/perplexity = 3.47613001/32.3343468 secs/batch = 0.2679s, grad.norm=16.10579491
 22899: 17 [  340/ 1327], train_loss/perplexity = 4.16617918/64.4686584 secs/batch = 0.2654s, grad.norm=17.02064896
 22904: 17 [  345/ 1327], train_loss/perplexity = 4.01233244/55.2756462 secs/batch = 0.2615s, grad.norm=16.49597168
 22909: 17 [  350/ 1327], train_loss/perplexity = 3.90990210/49.8940659 secs/batch = 0.2660s, grad.norm=17.42117691
 22914: 17 [  355/ 1327], train_loss/perplexity = 3.88243866/48.5424500 secs/batch = 0.2660s, grad.norm=17.14387131
 22919: 17 [  360/ 1327], train_loss/perplexity = 4.00688744/54.9754906 secs/batch = 0.2670s, grad.norm=17.87289047
 22924: 17 [  365/ 1327], train_loss/perplexity = 4.05310059/57.5756989 secs/batch = 0.2650s, grad.norm=17.49483871
 22929: 17 [  370/ 1327], train_loss/perplexity = 4.15277576/63.6103210 secs/batch = 0.2659s, grad.norm=17.63158035
 22934: 17 [  375/ 1327], train_loss/perplexity = 3.54541612/34.6541023 secs/batch = 0.2662s, grad.norm=17.28852844
 22939: 17 [  380/ 1327], train_loss/perplexity = 3.57985616/35.8683815 secs/batch = 0.2659s, grad.norm=17.10936165
 22944: 17 [  385/ 1327], train_loss/perplexity = 3.79484153/44.4711876 secs/batch = 0.2659s, grad.norm=17.52801704
 22949: 17 [  390/ 1327], train_loss/perplexity = 3.89300489/49.0580788 secs/batch = 0.2663s, grad.norm=17.16119003
 22954: 17 [  395/ 1327], train_loss/perplexity = 3.92017794/50.4094124 secs/batch = 0.2658s, grad.norm=17.12157059
 22959: 17 [  400/ 1327], train_loss/perplexity = 3.99015522/54.0632820 secs/batch = 0.2660s, grad.norm=16.72101974
 22964: 17 [  405/ 1327], train_loss/perplexity = 4.16267776/64.2433243 secs/batch = 0.2643s, grad.norm=17.09968185
 22969: 17 [  410/ 1327], train_loss/perplexity = 3.84848690/46.9220123 secs/batch = 0.2640s, grad.norm=16.99226189
 22974: 17 [  415/ 1327], train_loss/perplexity = 3.82432508/45.8018761 secs/batch = 0.2653s, grad.norm=16.69014168
 22979: 17 [  420/ 1327], train_loss/perplexity = 3.41100264/30.2956047 secs/batch = 0.2647s, grad.norm=16.66001129
 22984: 17 [  425/ 1327], train_loss/perplexity = 3.77944255/43.7916222 secs/batch = 0.2662s, grad.norm=17.68585587
 22989: 17 [  430/ 1327], train_loss/perplexity = 3.96364045/52.6486435 secs/batch = 0.2658s, grad.norm=17.71386337
 22994: 17 [  435/ 1327], train_loss/perplexity = 4.10373116/60.5658455 secs/batch = 0.2653s, grad.norm=18.22651100
 22999: 17 [  440/ 1327], train_loss/perplexity = 3.48989749/32.7825851 secs/batch = 0.2656s, grad.norm=16.91559410
 23004: 17 [  445/ 1327], train_loss/perplexity = 3.99538040/54.3465118 secs/batch = 0.2629s, grad.norm=17.77789116
 23009: 17 [  450/ 1327], train_loss/perplexity = 3.92081547/50.4415627 secs/batch = 0.2673s, grad.norm=17.19339371
 23014: 17 [  455/ 1327], train_loss/perplexity = 3.83302021/46.2018661 secs/batch = 0.2666s, grad.norm=16.47519875
 23019: 17 [  460/ 1327], train_loss/perplexity = 3.85804749/47.3727646 secs/batch = 0.2670s, grad.norm=17.59954834
 23024: 17 [  465/ 1327], train_loss/perplexity = 3.54142165/34.5159531 secs/batch = 0.2632s, grad.norm=18.26768684
 23029: 17 [  470/ 1327], train_loss/perplexity = 4.26608658/71.2422867 secs/batch = 0.2669s, grad.norm=17.44447899
 23034: 17 [  475/ 1327], train_loss/perplexity = 3.71344304/40.9947090 secs/batch = 0.2644s, grad.norm=17.02205086
 23039: 17 [  480/ 1327], train_loss/perplexity = 3.81456685/45.3571053 secs/batch = 0.2665s, grad.norm=17.25437737
 23044: 17 [  485/ 1327], train_loss/perplexity = 3.82042313/45.6235085 secs/batch = 0.2667s, grad.norm=16.89114952
 23049: 17 [  490/ 1327], train_loss/perplexity = 3.64940500/38.4517822 secs/batch = 0.2645s, grad.norm=18.34056854
 23054: 17 [  495/ 1327], train_loss/perplexity = 3.72726059/41.5650864 secs/batch = 0.2627s, grad.norm=16.99279213
 23059: 17 [  500/ 1327], train_loss/perplexity = 3.86128497/47.5263824 secs/batch = 0.2670s, grad.norm=17.27534103
 23064: 17 [  505/ 1327], train_loss/perplexity = 4.03990746/56.8210831 secs/batch = 0.2664s, grad.norm=16.38896370
 23069: 17 [  510/ 1327], train_loss/perplexity = 4.34423637/77.0331879 secs/batch = 0.2652s, grad.norm=16.43581772
 23074: 17 [  515/ 1327], train_loss/perplexity = 4.02377462/55.9117546 secs/batch = 0.2656s, grad.norm=16.42256546
 23079: 17 [  520/ 1327], train_loss/perplexity = 4.21608734/67.7678146 secs/batch = 0.2654s, grad.norm=17.61476326
 23084: 17 [  525/ 1327], train_loss/perplexity = 3.76722240/43.2597389 secs/batch = 0.2657s, grad.norm=16.57476044
 23089: 17 [  530/ 1327], train_loss/perplexity = 3.74801397/42.4367180 secs/batch = 0.2662s, grad.norm=17.44790840
 23094: 17 [  535/ 1327], train_loss/perplexity = 3.99408579/54.2761993 secs/batch = 0.2667s, grad.norm=17.39923286
 23099: 17 [  540/ 1327], train_loss/perplexity = 3.97350693/53.1706696 secs/batch = 0.2658s, grad.norm=17.19764328
 23104: 17 [  545/ 1327], train_loss/perplexity = 4.01767731/55.5718803 secs/batch = 0.2648s, grad.norm=16.99798393
 23109: 17 [  550/ 1327], train_loss/perplexity = 3.91554880/50.1766014 secs/batch = 0.2649s, grad.norm=17.03865623
 23114: 17 [  555/ 1327], train_loss/perplexity = 3.83812618/46.4383774 secs/batch = 0.2647s, grad.norm=16.47477913
 23119: 17 [  560/ 1327], train_loss/perplexity = 3.94905925/51.8865318 secs/batch = 0.2642s, grad.norm=17.77057838
 23124: 17 [  565/ 1327], train_loss/perplexity = 3.74560857/42.3347626 secs/batch = 0.2663s, grad.norm=17.70435905
 23129: 17 [  570/ 1327], train_loss/perplexity = 3.80497408/44.9240875 secs/batch = 0.2656s, grad.norm=17.55981827
 23134: 17 [  575/ 1327], train_loss/perplexity = 3.63671947/37.9670792 secs/batch = 0.2662s, grad.norm=17.84095955
 23139: 17 [  580/ 1327], train_loss/perplexity = 4.01297235/55.3110313 secs/batch = 0.2656s, grad.norm=17.86511993
 23144: 17 [  585/ 1327], train_loss/perplexity = 3.60556340/36.8024139 secs/batch = 0.2646s, grad.norm=16.87512207
 23149: 17 [  590/ 1327], train_loss/perplexity = 4.01992893/55.6971474 secs/batch = 0.2674s, grad.norm=16.89909554
 23154: 17 [  595/ 1327], train_loss/perplexity = 3.96068978/52.4935226 secs/batch = 0.2659s, grad.norm=17.55179977
 23159: 17 [  600/ 1327], train_loss/perplexity = 4.11589241/61.3069000 secs/batch = 0.2668s, grad.norm=16.80060196
 23164: 17 [  605/ 1327], train_loss/perplexity = 4.00945377/55.1167564 secs/batch = 0.2646s, grad.norm=16.84837341
 23169: 17 [  610/ 1327], train_loss/perplexity = 4.17178297/64.8309402 secs/batch = 0.2653s, grad.norm=17.22655106
 23174: 17 [  615/ 1327], train_loss/perplexity = 3.77804995/43.7306824 secs/batch = 0.2665s, grad.norm=16.39372063
 23179: 17 [  620/ 1327], train_loss/perplexity = 4.08673477/59.5451469 secs/batch = 0.2651s, grad.norm=16.97256088
 23184: 17 [  625/ 1327], train_loss/perplexity = 4.14363718/63.0316620 secs/batch = 0.2675s, grad.norm=16.88997650
 23189: 17 [  630/ 1327], train_loss/perplexity = 4.22079754/68.0877686 secs/batch = 0.2655s, grad.norm=17.40894127
 23194: 17 [  635/ 1327], train_loss/perplexity = 3.98266697/53.6599541 secs/batch = 0.2659s, grad.norm=17.33182526
 23199: 17 [  640/ 1327], train_loss/perplexity = 3.89063716/48.9420624 secs/batch = 0.2656s, grad.norm=17.23303604
 23204: 17 [  645/ 1327], train_loss/perplexity = 4.12754679/62.0255737 secs/batch = 0.2635s, grad.norm=17.75385857
 23209: 17 [  650/ 1327], train_loss/perplexity = 3.64247346/38.1861725 secs/batch = 0.2665s, grad.norm=17.10209274
 23214: 17 [  655/ 1327], train_loss/perplexity = 3.77762890/43.7122726 secs/batch = 0.2666s, grad.norm=16.93665314
 23219: 17 [  660/ 1327], train_loss/perplexity = 3.76019192/42.9566689 secs/batch = 0.2667s, grad.norm=17.55343246
 23224: 17 [  665/ 1327], train_loss/perplexity = 3.90439415/49.6200104 secs/batch = 0.2657s, grad.norm=17.23360634
 23229: 17 [  670/ 1327], train_loss/perplexity = 3.90739965/49.7693672 secs/batch = 0.2665s, grad.norm=17.22395706
 23234: 17 [  675/ 1327], train_loss/perplexity = 3.71934295/41.2372894 secs/batch = 0.2653s, grad.norm=17.58597946
 23239: 17 [  680/ 1327], train_loss/perplexity = 3.87604976/48.2333031 secs/batch = 0.2659s, grad.norm=18.21792412
 23244: 17 [  685/ 1327], train_loss/perplexity = 3.61594653/37.1865273 secs/batch = 0.2654s, grad.norm=16.77717972
 23249: 17 [  690/ 1327], train_loss/perplexity = 4.09035301/59.7609825 secs/batch = 0.2578s, grad.norm=17.03940010
 23254: 17 [  695/ 1327], train_loss/perplexity = 3.94933200/51.9006844 secs/batch = 0.2642s, grad.norm=17.38512993
 23259: 17 [  700/ 1327], train_loss/perplexity = 4.14177656/62.9144936 secs/batch = 0.2668s, grad.norm=17.76994324
 23264: 17 [  705/ 1327], train_loss/perplexity = 3.88616180/48.7235184 secs/batch = 0.2656s, grad.norm=16.68541908
 23269: 17 [  710/ 1327], train_loss/perplexity = 3.75255680/42.6299400 secs/batch = 0.2656s, grad.norm=17.84125519
 23274: 17 [  715/ 1327], train_loss/perplexity = 3.63448286/37.8822556 secs/batch = 0.2672s, grad.norm=17.81446838
 23279: 17 [  720/ 1327], train_loss/perplexity = 3.70867252/40.7996101 secs/batch = 0.2600s, grad.norm=17.67168617
 23284: 17 [  725/ 1327], train_loss/perplexity = 3.76588297/43.2018356 secs/batch = 0.2669s, grad.norm=17.31799126
 23289: 17 [  730/ 1327], train_loss/perplexity = 3.83391070/46.2430267 secs/batch = 0.2665s, grad.norm=17.76766396
 23294: 17 [  735/ 1327], train_loss/perplexity = 3.88623142/48.7269096 secs/batch = 0.2664s, grad.norm=18.29743385
 23299: 17 [  740/ 1327], train_loss/perplexity = 3.45250154/31.5792904 secs/batch = 0.2664s, grad.norm=16.44524384
 23304: 17 [  745/ 1327], train_loss/perplexity = 3.95000505/51.9356308 secs/batch = 0.2638s, grad.norm=17.89344406
 23309: 17 [  750/ 1327], train_loss/perplexity = 3.78208399/43.9074478 secs/batch = 0.2652s, grad.norm=17.12050819
 23314: 17 [  755/ 1327], train_loss/perplexity = 3.71867990/41.2099571 secs/batch = 0.2665s, grad.norm=17.05733299
 23319: 17 [  760/ 1327], train_loss/perplexity = 3.53753519/34.3820686 secs/batch = 0.2661s, grad.norm=15.75660896
 23324: 17 [  765/ 1327], train_loss/perplexity = 3.60656929/36.8394508 secs/batch = 0.2663s, grad.norm=16.36264992
 23329: 17 [  770/ 1327], train_loss/perplexity = 3.65255547/38.5731125 secs/batch = 0.2612s, grad.norm=16.87839699
 23334: 17 [  775/ 1327], train_loss/perplexity = 3.71473932/41.0478859 secs/batch = 0.2659s, grad.norm=17.92219353
 23339: 17 [  780/ 1327], train_loss/perplexity = 3.99182463/54.1536102 secs/batch = 0.2668s, grad.norm=17.62621307
 23344: 17 [  785/ 1327], train_loss/perplexity = 3.91397715/50.0978012 secs/batch = 0.2595s, grad.norm=18.15348053
 23349: 17 [  790/ 1327], train_loss/perplexity = 3.61870551/37.2892647 secs/batch = 0.2657s, grad.norm=17.37632751
 23354: 17 [  795/ 1327], train_loss/perplexity = 4.03059053/56.2941437 secs/batch = 0.2659s, grad.norm=17.39760399
 23359: 17 [  800/ 1327], train_loss/perplexity = 3.87751961/48.3042526 secs/batch = 0.2640s, grad.norm=17.44359398
 23364: 17 [  805/ 1327], train_loss/perplexity = 4.20415258/66.9638290 secs/batch = 0.2646s, grad.norm=17.85084724
 23369: 17 [  810/ 1327], train_loss/perplexity = 3.79806924/44.6149597 secs/batch = 0.2654s, grad.norm=16.06190491
 23374: 17 [  815/ 1327], train_loss/perplexity = 3.70733166/40.7449417 secs/batch = 0.2666s, grad.norm=16.98762894
 23379: 17 [  820/ 1327], train_loss/perplexity = 3.67941761/39.6233101 secs/batch = 0.2645s, grad.norm=16.13532448
 23384: 17 [  825/ 1327], train_loss/perplexity = 3.79915857/44.6635857 secs/batch = 0.2665s, grad.norm=17.11510468
 23389: 17 [  830/ 1327], train_loss/perplexity = 3.49982476/33.1096497 secs/batch = 0.2644s, grad.norm=17.52015114
 23394: 17 [  835/ 1327], train_loss/perplexity = 3.86142659/47.5331116 secs/batch = 0.2632s, grad.norm=17.53270721
 23399: 17 [  840/ 1327], train_loss/perplexity = 3.90248299/49.5252686 secs/batch = 0.2637s, grad.norm=17.45345688
 23404: 17 [  845/ 1327], train_loss/perplexity = 3.72759223/41.5788765 secs/batch = 0.2663s, grad.norm=17.57409668
 23409: 17 [  850/ 1327], train_loss/perplexity = 3.82807255/45.9738388 secs/batch = 0.2651s, grad.norm=17.01851654
 23414: 17 [  855/ 1327], train_loss/perplexity = 3.75272608/42.6371574 secs/batch = 0.2666s, grad.norm=17.99395943
 23419: 17 [  860/ 1327], train_loss/perplexity = 3.56180739/35.2268066 secs/batch = 0.2660s, grad.norm=16.68083572
 23424: 17 [  865/ 1327], train_loss/perplexity = 3.99278355/54.2055626 secs/batch = 0.2651s, grad.norm=17.36746025
 23429: 17 [  870/ 1327], train_loss/perplexity = 3.85284209/47.1268120 secs/batch = 0.2673s, grad.norm=17.75232887
 23434: 17 [  875/ 1327], train_loss/perplexity = 3.53280258/34.2197380 secs/batch = 0.2671s, grad.norm=16.69713783
 23439: 17 [  880/ 1327], train_loss/perplexity = 3.66896057/39.2111282 secs/batch = 0.2672s, grad.norm=16.77828789
 23444: 17 [  885/ 1327], train_loss/perplexity = 3.87509823/48.1874313 secs/batch = 0.2670s, grad.norm=17.11061859
 23449: 17 [  890/ 1327], train_loss/perplexity = 3.99846292/54.5142937 secs/batch = 0.2666s, grad.norm=17.18745422
 23454: 17 [  895/ 1327], train_loss/perplexity = 3.94519138/51.6862297 secs/batch = 0.2664s, grad.norm=17.01373863
 23459: 17 [  900/ 1327], train_loss/perplexity = 3.80409765/44.8847313 secs/batch = 0.2663s, grad.norm=16.78411865
 23464: 17 [  905/ 1327], train_loss/perplexity = 3.75692272/42.8164635 secs/batch = 0.2658s, grad.norm=16.42514420
 23469: 17 [  910/ 1327], train_loss/perplexity = 3.76114178/42.9974899 secs/batch = 0.2656s, grad.norm=16.05251312
 23474: 17 [  915/ 1327], train_loss/perplexity = 3.97393084/53.1932144 secs/batch = 0.2658s, grad.norm=16.37793922
 23479: 17 [  920/ 1327], train_loss/perplexity = 4.11199713/61.0685577 secs/batch = 0.2669s, grad.norm=17.39019775
 23484: 17 [  925/ 1327], train_loss/perplexity = 3.95596600/52.2461395 secs/batch = 0.2651s, grad.norm=17.27761841
 23489: 17 [  930/ 1327], train_loss/perplexity = 3.99228168/54.1783676 secs/batch = 0.2654s, grad.norm=17.19999886
 23494: 17 [  935/ 1327], train_loss/perplexity = 4.01443005/55.3917160 secs/batch = 0.2675s, grad.norm=16.96060944
 23499: 17 [  940/ 1327], train_loss/perplexity = 3.95340919/52.1127281 secs/batch = 0.2655s, grad.norm=16.47406578
 23504: 17 [  945/ 1327], train_loss/perplexity = 4.16595507/64.4542084 secs/batch = 0.2668s, grad.norm=16.76532936
 23509: 17 [  950/ 1327], train_loss/perplexity = 3.92932415/50.8725853 secs/batch = 0.2664s, grad.norm=17.30131721
 23514: 17 [  955/ 1327], train_loss/perplexity = 3.89924955/49.3653908 secs/batch = 0.2662s, grad.norm=16.96097183
 23519: 17 [  960/ 1327], train_loss/perplexity = 4.15558243/63.7891045 secs/batch = 0.2609s, grad.norm=17.51705742
 23524: 17 [  965/ 1327], train_loss/perplexity = 3.92372203/50.5883865 secs/batch = 0.2662s, grad.norm=16.92425537
 23529: 17 [  970/ 1327], train_loss/perplexity = 4.13971424/62.7848778 secs/batch = 0.2637s, grad.norm=17.25827217
 23534: 17 [  975/ 1327], train_loss/perplexity = 3.78286529/43.9417686 secs/batch = 0.2597s, grad.norm=18.20666122
 23539: 17 [  980/ 1327], train_loss/perplexity = 3.70739579/40.7475548 secs/batch = 0.2646s, grad.norm=17.20501328
 23544: 17 [  985/ 1327], train_loss/perplexity = 3.81957150/45.5846710 secs/batch = 0.2681s, grad.norm=17.75409889
 23549: 17 [  990/ 1327], train_loss/perplexity = 4.06872368/58.4822731 secs/batch = 0.2662s, grad.norm=17.83739853
 23554: 17 [  995/ 1327], train_loss/perplexity = 4.00248766/54.7341423 secs/batch = 0.2656s, grad.norm=16.89135361
 23559: 17 [ 1000/ 1327], train_loss/perplexity = 3.58441782/36.0323753 secs/batch = 0.2651s, grad.norm=16.47231483
 23564: 17 [ 1005/ 1327], train_loss/perplexity = 4.01632357/55.4967003 secs/batch = 0.2667s, grad.norm=16.84513283
 23569: 17 [ 1010/ 1327], train_loss/perplexity = 3.64051819/38.1115799 secs/batch = 0.2661s, grad.norm=16.48521233
 23574: 17 [ 1015/ 1327], train_loss/perplexity = 4.11040974/60.9716949 secs/batch = 0.2663s, grad.norm=16.79551697
 23579: 17 [ 1020/ 1327], train_loss/perplexity = 4.15117168/63.5083694 secs/batch = 0.2663s, grad.norm=16.88824081
 23584: 17 [ 1025/ 1327], train_loss/perplexity = 4.15676451/63.8645554 secs/batch = 0.2655s, grad.norm=16.99261093
 23589: 17 [ 1030/ 1327], train_loss/perplexity = 3.81345034/45.3064919 secs/batch = 0.2669s, grad.norm=16.44354439
 23594: 17 [ 1035/ 1327], train_loss/perplexity = 3.82878542/46.0066261 secs/batch = 0.2662s, grad.norm=16.54284477
 23599: 17 [ 1040/ 1327], train_loss/perplexity = 3.95915556/52.4130478 secs/batch = 0.2670s, grad.norm=17.18832397
 23604: 17 [ 1045/ 1327], train_loss/perplexity = 3.54365659/34.5931816 secs/batch = 0.2659s, grad.norm=15.98985004
 23609: 17 [ 1050/ 1327], train_loss/perplexity = 3.59111404/36.2744637 secs/batch = 0.2653s, grad.norm=16.42439079
 23614: 17 [ 1055/ 1327], train_loss/perplexity = 3.71507764/41.0617752 secs/batch = 0.2671s, grad.norm=17.76634979
 23619: 17 [ 1060/ 1327], train_loss/perplexity = 3.32955503/27.9259129 secs/batch = 0.2667s, grad.norm=17.44528961
 23624: 17 [ 1065/ 1327], train_loss/perplexity = 3.50812793/33.3857079 secs/batch = 0.2654s, grad.norm=16.99102211
 23629: 17 [ 1070/ 1327], train_loss/perplexity = 3.91054249/49.9260292 secs/batch = 0.2607s, grad.norm=18.14358902
 23634: 17 [ 1075/ 1327], train_loss/perplexity = 3.67543793/39.4659348 secs/batch = 0.2655s, grad.norm=17.36962700
 23639: 17 [ 1080/ 1327], train_loss/perplexity = 3.60025883/36.6077080 secs/batch = 0.2643s, grad.norm=16.98438072
 23644: 17 [ 1085/ 1327], train_loss/perplexity = 3.48325658/32.5656013 secs/batch = 0.2665s, grad.norm=17.16712952
 23649: 17 [ 1090/ 1327], train_loss/perplexity = 3.67845893/39.5853424 secs/batch = 0.2661s, grad.norm=17.78379059
 23654: 17 [ 1095/ 1327], train_loss/perplexity = 3.80295849/44.8336296 secs/batch = 0.2655s, grad.norm=17.90932846
 23659: 17 [ 1100/ 1327], train_loss/perplexity = 3.49660254/33.0031357 secs/batch = 0.2661s, grad.norm=18.45869446
 23664: 17 [ 1105/ 1327], train_loss/perplexity = 3.49062538/32.8064575 secs/batch = 0.2671s, grad.norm=17.68172455
 23669: 17 [ 1110/ 1327], train_loss/perplexity = 3.83759332/46.4136353 secs/batch = 0.2661s, grad.norm=18.23585701
 23674: 17 [ 1115/ 1327], train_loss/perplexity = 3.57759595/35.7874031 secs/batch = 0.2647s, grad.norm=16.57361603
 23679: 17 [ 1120/ 1327], train_loss/perplexity = 3.88649845/48.7399216 secs/batch = 0.2661s, grad.norm=17.29985428
 23684: 17 [ 1125/ 1327], train_loss/perplexity = 4.04038334/56.8481293 secs/batch = 0.2650s, grad.norm=18.31302452
 23689: 17 [ 1130/ 1327], train_loss/perplexity = 3.72607398/41.5157967 secs/batch = 0.2664s, grad.norm=17.48598671
 23694: 17 [ 1135/ 1327], train_loss/perplexity = 3.78182030/43.8958740 secs/batch = 0.2649s, grad.norm=17.54394913
 23699: 17 [ 1140/ 1327], train_loss/perplexity = 3.93009949/50.9120407 secs/batch = 0.2652s, grad.norm=18.26520538
 23704: 17 [ 1145/ 1327], train_loss/perplexity = 3.77439785/43.5712624 secs/batch = 0.2668s, grad.norm=16.83098412
 23709: 17 [ 1150/ 1327], train_loss/perplexity = 3.72524118/41.4812355 secs/batch = 0.2663s, grad.norm=16.86827660
 23714: 17 [ 1155/ 1327], train_loss/perplexity = 3.84676123/46.8411102 secs/batch = 0.2664s, grad.norm=17.52131844
 23719: 17 [ 1160/ 1327], train_loss/perplexity = 3.70539713/40.6661949 secs/batch = 0.2661s, grad.norm=17.07898331
 23724: 17 [ 1165/ 1327], train_loss/perplexity = 3.75258374/42.6310883 secs/batch = 0.2653s, grad.norm=17.24363708
 23729: 17 [ 1170/ 1327], train_loss/perplexity = 3.70767689/40.7590103 secs/batch = 0.2612s, grad.norm=17.63223267
 23734: 17 [ 1175/ 1327], train_loss/perplexity = 3.59082723/36.2640610 secs/batch = 0.2676s, grad.norm=16.93496513
 23739: 17 [ 1180/ 1327], train_loss/perplexity = 3.56718922/35.4169044 secs/batch = 0.2640s, grad.norm=17.87276459
 23744: 17 [ 1185/ 1327], train_loss/perplexity = 3.66886687/39.2074547 secs/batch = 0.2660s, grad.norm=17.63840866
 23749: 17 [ 1190/ 1327], train_loss/perplexity = 3.77602935/43.6424103 secs/batch = 0.2642s, grad.norm=17.65104103
 23754: 17 [ 1195/ 1327], train_loss/perplexity = 3.58140373/35.9239349 secs/batch = 0.2619s, grad.norm=17.21001816
 23759: 17 [ 1200/ 1327], train_loss/perplexity = 3.57930660/35.8486748 secs/batch = 0.2654s, grad.norm=17.51418495
 23764: 17 [ 1205/ 1327], train_loss/perplexity = 3.58336782/35.9945602 secs/batch = 0.2670s, grad.norm=17.78152275
 23769: 17 [ 1210/ 1327], train_loss/perplexity = 3.19521523/24.4154282 secs/batch = 0.2650s, grad.norm=16.76610756
 23774: 17 [ 1215/ 1327], train_loss/perplexity = 3.44649553/31.3901939 secs/batch = 0.2660s, grad.norm=16.72383690
 23779: 17 [ 1220/ 1327], train_loss/perplexity = 3.60525155/36.7909393 secs/batch = 0.2657s, grad.norm=17.76763153
 23784: 17 [ 1225/ 1327], train_loss/perplexity = 3.34017277/28.2240028 secs/batch = 0.2649s, grad.norm=18.40084267
 23789: 17 [ 1230/ 1327], train_loss/perplexity = 3.62191010/37.4089546 secs/batch = 0.2655s, grad.norm=16.95157051
 23794: 17 [ 1235/ 1327], train_loss/perplexity = 3.57070565/35.5416641 secs/batch = 0.2675s, grad.norm=17.50395584
 23799: 17 [ 1240/ 1327], train_loss/perplexity = 3.76364875/43.1054192 secs/batch = 0.2675s, grad.norm=17.97446823
 23804: 17 [ 1245/ 1327], train_loss/perplexity = 3.67011094/39.2562599 secs/batch = 0.2668s, grad.norm=16.97245979
 23809: 17 [ 1250/ 1327], train_loss/perplexity = 3.86037183/47.4830017 secs/batch = 0.2653s, grad.norm=17.00169563
 23814: 17 [ 1255/ 1327], train_loss/perplexity = 3.86604333/47.7530670 secs/batch = 0.2657s, grad.norm=17.17531776
 23819: 17 [ 1260/ 1327], train_loss/perplexity = 3.64464521/38.2691917 secs/batch = 0.2667s, grad.norm=18.14121628
 23824: 17 [ 1265/ 1327], train_loss/perplexity = 3.81069636/45.1818924 secs/batch = 0.2654s, grad.norm=17.11867523
 23829: 17 [ 1270/ 1327], train_loss/perplexity = 3.55493474/34.9855385 secs/batch = 0.2663s, grad.norm=17.52796173
 23834: 17 [ 1275/ 1327], train_loss/perplexity = 3.76802874/43.2946358 secs/batch = 0.2662s, grad.norm=17.41240692
 23839: 17 [ 1280/ 1327], train_loss/perplexity = 3.65646124/38.7240639 secs/batch = 0.2638s, grad.norm=18.01709175
 23844: 17 [ 1285/ 1327], train_loss/perplexity = 3.54752445/34.7272415 secs/batch = 0.2639s, grad.norm=17.39573860
 23849: 17 [ 1290/ 1327], train_loss/perplexity = 3.78517008/44.0431595 secs/batch = 0.2671s, grad.norm=17.20990562
 23854: 17 [ 1295/ 1327], train_loss/perplexity = 3.75275421/42.6383553 secs/batch = 0.2655s, grad.norm=17.34065628
 23859: 17 [ 1300/ 1327], train_loss/perplexity = 3.85452938/47.2063942 secs/batch = 0.2660s, grad.norm=16.69255829
 23864: 17 [ 1305/ 1327], train_loss/perplexity = 3.97521019/53.2613106 secs/batch = 0.2654s, grad.norm=17.34141922
 23869: 17 [ 1310/ 1327], train_loss/perplexity = 4.20081711/66.7408447 secs/batch = 0.2658s, grad.norm=18.02228355
 23874: 17 [ 1315/ 1327], train_loss/perplexity = 4.02772045/56.1328087 secs/batch = 0.2608s, grad.norm=17.82905388
 23879: 17 [ 1320/ 1327], train_loss/perplexity = 4.02403259/55.9261780 secs/batch = 0.2661s, grad.norm=17.58409691
 23884: 17 [ 1325/ 1327], train_loss/perplexity = 3.99427652/54.2865524 secs/batch = 0.2663s, grad.norm=17.53152275
Epoch training time: 352.65192914009094
	> validation loss = 4.57764435, perplexity = 97.28495789
	> validation loss = 4.53534555, perplexity = 93.25573730
	> validation loss = 4.49197102, perplexity = 89.29727936
	> validation loss = 4.57275534, perplexity = 96.81048584
	> validation loss = 4.67366314, perplexity = 107.08930969
	> validation loss = 4.62321615, perplexity = 101.82097626
	> validation loss = 4.55347109, perplexity = 94.96145630
	> validation loss = 4.38808107, perplexity = 80.48582458
	> validation loss = 4.17799759, perplexity = 65.23509216
	> validation loss = 4.28258419, perplexity = 72.42736816
	> validation loss = 4.47917795, perplexity = 88.16217041
	> validation loss = 4.46947718, perplexity = 87.31106567
	> validation loss = 4.45245314, perplexity = 85.83725739
	> validation loss = 4.19048882, perplexity = 66.05506897
	> validation loss = 4.14298677, perplexity = 62.99068069
	> validation loss = 4.19652557, perplexity = 66.45503998
	> validation loss = 4.61409187, perplexity = 100.89616394
	> validation loss = 4.12345600, perplexity = 61.77235794
	> validation loss = 4.63012743, perplexity = 102.52713013
	> validation loss = 4.48516130, perplexity = 88.69125366
	> validation loss = 4.27496290, perplexity = 71.87747192
at the end of epoch: 17
train loss = 3.89051875, perplexity = 48.93626569
validation loss = 4.43274479, perplexity = 84.16210740
Saved model cv/epoch017_4.4327.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.0625
new learning rate is: 0.03125
 23891: 18 [    5/ 1327], train_loss/perplexity = 4.07973957/59.1300697 secs/batch = 0.2669s, grad.norm=17.41461182
 23896: 18 [   10/ 1327], train_loss/perplexity = 3.62338400/37.4641304 secs/batch = 0.2660s, grad.norm=16.54968452
 23901: 18 [   15/ 1327], train_loss/perplexity = 4.02270365/55.8519058 secs/batch = 0.2655s, grad.norm=16.69969559
 23906: 18 [   20/ 1327], train_loss/perplexity = 4.21876478/67.9495010 secs/batch = 0.2666s, grad.norm=16.98886871
 23911: 18 [   25/ 1327], train_loss/perplexity = 4.07463646/58.8290901 secs/batch = 0.2669s, grad.norm=18.45874214
 23916: 18 [   30/ 1327], train_loss/perplexity = 4.07781315/59.0162697 secs/batch = 0.2655s, grad.norm=17.82804298
 23921: 18 [   35/ 1327], train_loss/perplexity = 3.88719130/48.7737045 secs/batch = 0.2643s, grad.norm=17.13842201
 23926: 18 [   40/ 1327], train_loss/perplexity = 3.86996269/47.9405975 secs/batch = 0.2671s, grad.norm=17.22010994
 23931: 18 [   45/ 1327], train_loss/perplexity = 3.66288710/38.9737015 secs/batch = 0.2667s, grad.norm=16.74509239
 23936: 18 [   50/ 1327], train_loss/perplexity = 3.90707207/49.7530632 secs/batch = 0.2653s, grad.norm=17.61232758
 23941: 18 [   55/ 1327], train_loss/perplexity = 3.77431941/43.5678482 secs/batch = 0.2646s, grad.norm=17.88456726
 23946: 18 [   60/ 1327], train_loss/perplexity = 4.08089066/59.1981697 secs/batch = 0.2645s, grad.norm=18.16834450
 23951: 18 [   65/ 1327], train_loss/perplexity = 3.74210215/42.1865807 secs/batch = 0.2645s, grad.norm=17.59619904
 23956: 18 [   70/ 1327], train_loss/perplexity = 3.50917244/33.4205971 secs/batch = 0.2643s, grad.norm=17.26074409
 23961: 18 [   75/ 1327], train_loss/perplexity = 3.35912323/28.7639599 secs/batch = 0.2661s, grad.norm=16.61403084
 23966: 18 [   80/ 1327], train_loss/perplexity = 3.82576299/45.8677826 secs/batch = 0.2670s, grad.norm=17.70359612
 23971: 18 [   85/ 1327], train_loss/perplexity = 3.82588506/45.8733826 secs/batch = 0.2664s, grad.norm=18.20387459
 23976: 18 [   90/ 1327], train_loss/perplexity = 3.89678407/49.2438278 secs/batch = 0.2659s, grad.norm=17.88273048
 23981: 18 [   95/ 1327], train_loss/perplexity = 3.79681015/44.5588226 secs/batch = 0.2634s, grad.norm=17.81996727
 23986: 18 [  100/ 1327], train_loss/perplexity = 4.01881790/55.6352997 secs/batch = 0.2658s, grad.norm=17.82203102
 23991: 18 [  105/ 1327], train_loss/perplexity = 3.77009916/43.3843651 secs/batch = 0.2660s, grad.norm=17.95886993
 23996: 18 [  110/ 1327], train_loss/perplexity = 3.69136739/40.0996399 secs/batch = 0.2666s, grad.norm=17.53650093
 24001: 18 [  115/ 1327], train_loss/perplexity = 3.69760752/40.3506508 secs/batch = 0.2646s, grad.norm=18.02167511
 24006: 18 [  120/ 1327], train_loss/perplexity = 3.80501175/44.9257774 secs/batch = 0.2644s, grad.norm=17.61351776
 24011: 18 [  125/ 1327], train_loss/perplexity = 3.79108429/44.3044128 secs/batch = 0.2671s, grad.norm=18.32798195
 24016: 18 [  130/ 1327], train_loss/perplexity = 3.81499696/45.3766174 secs/batch = 0.2657s, grad.norm=18.87274170
 24021: 18 [  135/ 1327], train_loss/perplexity = 3.82422805/45.7974319 secs/batch = 0.2664s, grad.norm=17.16832542
 24026: 18 [  140/ 1327], train_loss/perplexity = 4.02254963/55.8433037 secs/batch = 0.2654s, grad.norm=17.84773445
 24031: 18 [  145/ 1327], train_loss/perplexity = 3.87669897/48.2646294 secs/batch = 0.2632s, grad.norm=18.41523170
 24036: 18 [  150/ 1327], train_loss/perplexity = 3.98627639/53.8539848 secs/batch = 0.2664s, grad.norm=18.12933731
 24041: 18 [  155/ 1327], train_loss/perplexity = 4.30015135/73.7109451 secs/batch = 0.2658s, grad.norm=18.48836327
 24046: 18 [  160/ 1327], train_loss/perplexity = 3.93379021/51.1002922 secs/batch = 0.2644s, grad.norm=17.18574715
 24051: 18 [  165/ 1327], train_loss/perplexity = 4.06010342/57.9803085 secs/batch = 0.2587s, grad.norm=17.43922615
 24056: 18 [  170/ 1327], train_loss/perplexity = 3.90284109/49.5430069 secs/batch = 0.2661s, grad.norm=17.51699448
 24061: 18 [  175/ 1327], train_loss/perplexity = 4.17443848/65.0033264 secs/batch = 0.2663s, grad.norm=17.66626167
 24066: 18 [  180/ 1327], train_loss/perplexity = 4.01911163/55.6516457 secs/batch = 0.2657s, grad.norm=17.85834503
 24071: 18 [  185/ 1327], train_loss/perplexity = 4.34635496/77.1965637 secs/batch = 0.2679s, grad.norm=18.35176659
 24076: 18 [  190/ 1327], train_loss/perplexity = 3.88121510/48.4830894 secs/batch = 0.2661s, grad.norm=17.02158165
 24081: 18 [  195/ 1327], train_loss/perplexity = 4.16479921/64.3797531 secs/batch = 0.2656s, grad.norm=17.29039764
 24086: 18 [  200/ 1327], train_loss/perplexity = 3.95553732/52.2237473 secs/batch = 0.2655s, grad.norm=18.18304443
 24091: 18 [  205/ 1327], train_loss/perplexity = 4.21073055/67.4057617 secs/batch = 0.2666s, grad.norm=18.03690338
 24096: 18 [  210/ 1327], train_loss/perplexity = 4.04880571/57.3289490 secs/batch = 0.2658s, grad.norm=16.71536255
 24101: 18 [  215/ 1327], train_loss/perplexity = 4.23191547/68.8489838 secs/batch = 0.2669s, grad.norm=16.79789162
 24106: 18 [  220/ 1327], train_loss/perplexity = 4.06091595/58.0274353 secs/batch = 0.2638s, grad.norm=17.24477959
 24111: 18 [  225/ 1327], train_loss/perplexity = 4.27410555/71.8158722 secs/batch = 0.2660s, grad.norm=17.30832100
 24116: 18 [  230/ 1327], train_loss/perplexity = 4.02038193/55.7223854 secs/batch = 0.2677s, grad.norm=18.78561783
 24121: 18 [  235/ 1327], train_loss/perplexity = 3.98808718/53.9515915 secs/batch = 0.2646s, grad.norm=17.65394402
 24126: 18 [  240/ 1327], train_loss/perplexity = 3.74272108/42.2126999 secs/batch = 0.2661s, grad.norm=17.83777618
 24131: 18 [  245/ 1327], train_loss/perplexity = 3.97608709/53.3080368 secs/batch = 0.2653s, grad.norm=17.66419411
 24136: 18 [  250/ 1327], train_loss/perplexity = 3.91087437/49.9426003 secs/batch = 0.2648s, grad.norm=17.09246635
 24141: 18 [  255/ 1327], train_loss/perplexity = 3.81319571/45.2949562 secs/batch = 0.2671s, grad.norm=17.27246666
 24146: 18 [  260/ 1327], train_loss/perplexity = 3.98973012/54.0403023 secs/batch = 0.2669s, grad.norm=17.63154984
 24151: 18 [  265/ 1327], train_loss/perplexity = 4.23381662/68.9800034 secs/batch = 0.2682s, grad.norm=17.24315834
 24156: 18 [  270/ 1327], train_loss/perplexity = 4.28903866/72.8963547 secs/batch = 0.2638s, grad.norm=17.69768524
 24161: 18 [  275/ 1327], train_loss/perplexity = 4.23807716/69.2745209 secs/batch = 0.2605s, grad.norm=17.69923019
 24166: 18 [  280/ 1327], train_loss/perplexity = 4.02266073/55.8495102 secs/batch = 0.2664s, grad.norm=16.92738533
 24171: 18 [  285/ 1327], train_loss/perplexity = 4.36447334/78.6079865 secs/batch = 0.2661s, grad.norm=17.37053108
 24176: 18 [  290/ 1327], train_loss/perplexity = 3.94050193/51.4444160 secs/batch = 0.2654s, grad.norm=17.97563171
 24181: 18 [  295/ 1327], train_loss/perplexity = 3.85072327/47.0270653 secs/batch = 0.2681s, grad.norm=17.74167252
 24186: 18 [  300/ 1327], train_loss/perplexity = 3.35243154/28.5721226 secs/batch = 0.2653s, grad.norm=16.38368797
 24191: 18 [  305/ 1327], train_loss/perplexity = 3.88361168/48.5994225 secs/batch = 0.2660s, grad.norm=16.94250679
 24196: 18 [  310/ 1327], train_loss/perplexity = 3.83101940/46.1095200 secs/batch = 0.2613s, grad.norm=17.08605194
 24201: 18 [  315/ 1327], train_loss/perplexity = 3.45798397/31.7528973 secs/batch = 0.2659s, grad.norm=16.32628632
 24206: 18 [  320/ 1327], train_loss/perplexity = 3.28223276/26.6351757 secs/batch = 0.2658s, grad.norm=17.42005348
 24211: 18 [  325/ 1327], train_loss/perplexity = 3.44244123/31.2631855 secs/batch = 0.2652s, grad.norm=16.48190880
 24216: 18 [  330/ 1327], train_loss/perplexity = 4.01149750/55.2295151 secs/batch = 0.2661s, grad.norm=17.46790123
 24221: 18 [  335/ 1327], train_loss/perplexity = 3.44460678/31.3309612 secs/batch = 0.2635s, grad.norm=16.11118698
 24226: 18 [  340/ 1327], train_loss/perplexity = 4.12968206/62.1581573 secs/batch = 0.2665s, grad.norm=17.15790367
 24231: 18 [  345/ 1327], train_loss/perplexity = 3.91664124/50.2314453 secs/batch = 0.2666s, grad.norm=16.75214386
 24236: 18 [  350/ 1327], train_loss/perplexity = 3.84708834/46.8564339 secs/batch = 0.2664s, grad.norm=17.39612961
 24241: 18 [  355/ 1327], train_loss/perplexity = 3.93203449/51.0106544 secs/batch = 0.2663s, grad.norm=16.88327789
 24246: 18 [  360/ 1327], train_loss/perplexity = 4.04746103/57.2519112 secs/batch = 0.2659s, grad.norm=19.08395576
 24251: 18 [  365/ 1327], train_loss/perplexity = 4.00706768/54.9853973 secs/batch = 0.2663s, grad.norm=17.04124451
 24256: 18 [  370/ 1327], train_loss/perplexity = 4.09306145/59.9230614 secs/batch = 0.2667s, grad.norm=17.66691208
 24261: 18 [  375/ 1327], train_loss/perplexity = 3.56688571/35.4061546 secs/batch = 0.2633s, grad.norm=17.32538605
 24266: 18 [  380/ 1327], train_loss/perplexity = 3.62995529/37.7111320 secs/batch = 0.2609s, grad.norm=17.73881721
 24271: 18 [  385/ 1327], train_loss/perplexity = 3.74004316/42.0998077 secs/batch = 0.2665s, grad.norm=18.03086662
 24276: 18 [  390/ 1327], train_loss/perplexity = 3.91479754/50.1389198 secs/batch = 0.2666s, grad.norm=17.31279755
 24281: 18 [  395/ 1327], train_loss/perplexity = 3.89878631/49.3425255 secs/batch = 0.2672s, grad.norm=16.96567535
 24286: 18 [  400/ 1327], train_loss/perplexity = 3.94359350/51.6037064 secs/batch = 0.2661s, grad.norm=17.01829338
 24291: 18 [  405/ 1327], train_loss/perplexity = 4.15408564/63.6936989 secs/batch = 0.2644s, grad.norm=17.53825188
 24296: 18 [  410/ 1327], train_loss/perplexity = 3.89263439/49.0399055 secs/batch = 0.2668s, grad.norm=17.29331779
 24301: 18 [  415/ 1327], train_loss/perplexity = 3.75665903/42.8051758 secs/batch = 0.2667s, grad.norm=17.14673805
 24306: 18 [  420/ 1327], train_loss/perplexity = 3.47477746/32.2906418 secs/batch = 0.2669s, grad.norm=17.27388763
 24311: 18 [  425/ 1327], train_loss/perplexity = 3.75414920/42.6978760 secs/batch = 0.2655s, grad.norm=17.85474205
 24316: 18 [  430/ 1327], train_loss/perplexity = 3.95204973/52.0419312 secs/batch = 0.2660s, grad.norm=17.86299515
 24321: 18 [  435/ 1327], train_loss/perplexity = 4.00446224/54.8423233 secs/batch = 0.2651s, grad.norm=17.78544807
 24326: 18 [  440/ 1327], train_loss/perplexity = 3.52961373/34.1107903 secs/batch = 0.2665s, grad.norm=16.89945793
 24331: 18 [  445/ 1327], train_loss/perplexity = 3.88735580/48.7817268 secs/batch = 0.2660s, grad.norm=17.46305275
 24336: 18 [  450/ 1327], train_loss/perplexity = 3.82510090/45.8374252 secs/batch = 0.2658s, grad.norm=17.11934853
 24341: 18 [  455/ 1327], train_loss/perplexity = 3.87145448/48.0121689 secs/batch = 0.2647s, grad.norm=16.75715637
 24346: 18 [  460/ 1327], train_loss/perplexity = 3.83857989/46.4594498 secs/batch = 0.2653s, grad.norm=17.73660088
 24351: 18 [  465/ 1327], train_loss/perplexity = 3.54397631/34.6042442 secs/batch = 0.2670s, grad.norm=18.02681160
 24356: 18 [  470/ 1327], train_loss/perplexity = 4.24864912/70.0107727 secs/batch = 0.2659s, grad.norm=17.33655357
 24361: 18 [  475/ 1327], train_loss/perplexity = 3.70497990/40.6492310 secs/batch = 0.2663s, grad.norm=17.30010605
 24366: 18 [  480/ 1327], train_loss/perplexity = 3.82995224/46.0603371 secs/batch = 0.2658s, grad.norm=17.35858154
 24371: 18 [  485/ 1327], train_loss/perplexity = 3.83179855/46.1454582 secs/batch = 0.2656s, grad.norm=17.62605476
 24376: 18 [  490/ 1327], train_loss/perplexity = 3.63876152/38.0446892 secs/batch = 0.2608s, grad.norm=18.37400627
 24381: 18 [  495/ 1327], train_loss/perplexity = 3.75789118/42.8579521 secs/batch = 0.2675s, grad.norm=17.11106110
 24386: 18 [  500/ 1327], train_loss/perplexity = 3.88505363/48.6695518 secs/batch = 0.2668s, grad.norm=17.06867790
 24391: 18 [  505/ 1327], train_loss/perplexity = 4.05619144/57.7539330 secs/batch = 0.2668s, grad.norm=16.75901985
 24396: 18 [  510/ 1327], train_loss/perplexity = 4.30804682/74.2952347 secs/batch = 0.2662s, grad.norm=16.90680885
 24401: 18 [  515/ 1327], train_loss/perplexity = 4.03154039/56.3476410 secs/batch = 0.2664s, grad.norm=16.95455742
 24406: 18 [  520/ 1327], train_loss/perplexity = 4.23129082/68.8059921 secs/batch = 0.2667s, grad.norm=17.42701340
 24411: 18 [  525/ 1327], train_loss/perplexity = 3.77152944/43.4464645 secs/batch = 0.2662s, grad.norm=17.19645500
 24416: 18 [  530/ 1327], train_loss/perplexity = 3.74219465/42.1904831 secs/batch = 0.2674s, grad.norm=17.14410973
 24421: 18 [  535/ 1327], train_loss/perplexity = 3.87892962/48.3724098 secs/batch = 0.2662s, grad.norm=17.07511902
 24426: 18 [  540/ 1327], train_loss/perplexity = 3.99085855/54.1013184 secs/batch = 0.2658s, grad.norm=17.12494850
 24431: 18 [  545/ 1327], train_loss/perplexity = 3.97570038/53.2874260 secs/batch = 0.2599s, grad.norm=17.22679710
 24436: 18 [  550/ 1327], train_loss/perplexity = 3.93680882/51.2547760 secs/batch = 0.2660s, grad.norm=17.32838440
 24441: 18 [  555/ 1327], train_loss/perplexity = 3.77226162/43.4782867 secs/batch = 0.2666s, grad.norm=16.46807289
 24446: 18 [  560/ 1327], train_loss/perplexity = 3.89584398/49.1975594 secs/batch = 0.2658s, grad.norm=17.77481270
 24451: 18 [  565/ 1327], train_loss/perplexity = 3.76250434/43.0561180 secs/batch = 0.2659s, grad.norm=18.36215210
 24456: 18 [  570/ 1327], train_loss/perplexity = 3.71185589/40.9296951 secs/batch = 0.2675s, grad.norm=17.86099052
 24461: 18 [  575/ 1327], train_loss/perplexity = 3.65018106/38.4816322 secs/batch = 0.2642s, grad.norm=17.50090027
 24466: 18 [  580/ 1327], train_loss/perplexity = 3.99096704/54.1071892 secs/batch = 0.2659s, grad.norm=18.12804794
 24471: 18 [  585/ 1327], train_loss/perplexity = 3.59464073/36.4026184 secs/batch = 0.2658s, grad.norm=16.98088264
 24476: 18 [  590/ 1327], train_loss/perplexity = 3.99494648/54.3229332 secs/batch = 0.2662s, grad.norm=17.47671700
 24481: 18 [  595/ 1327], train_loss/perplexity = 3.87196636/48.0367508 secs/batch = 0.2664s, grad.norm=17.74972343
 24486: 18 [  600/ 1327], train_loss/perplexity = 4.12982035/62.1667519 secs/batch = 0.2672s, grad.norm=16.71393394
 24491: 18 [  605/ 1327], train_loss/perplexity = 4.02825260/56.1626854 secs/batch = 0.2655s, grad.norm=17.20021439
 24496: 18 [  610/ 1327], train_loss/perplexity = 4.13879871/62.7274208 secs/batch = 0.2648s, grad.norm=17.29428291
 24501: 18 [  615/ 1327], train_loss/perplexity = 3.70986652/40.8483543 secs/batch = 0.2662s, grad.norm=16.67489624
 24506: 18 [  620/ 1327], train_loss/perplexity = 4.12275314/61.7289581 secs/batch = 0.2663s, grad.norm=17.39470100
 24511: 18 [  625/ 1327], train_loss/perplexity = 4.03791428/56.7079430 secs/batch = 0.2660s, grad.norm=17.19390678
 24516: 18 [  630/ 1327], train_loss/perplexity = 4.23921490/69.3533783 secs/batch = 0.2654s, grad.norm=17.32882690
 24521: 18 [  635/ 1327], train_loss/perplexity = 3.90199876/49.5012932 secs/batch = 0.2659s, grad.norm=17.30186462
 24526: 18 [  640/ 1327], train_loss/perplexity = 3.90075231/49.4396286 secs/batch = 0.2599s, grad.norm=16.90090942
 24531: 18 [  645/ 1327], train_loss/perplexity = 4.13866901/62.7192879 secs/batch = 0.2671s, grad.norm=17.92106819
 24536: 18 [  650/ 1327], train_loss/perplexity = 3.67781997/39.5600586 secs/batch = 0.2662s, grad.norm=17.14714241
 24541: 18 [  655/ 1327], train_loss/perplexity = 3.81856132/45.5386467 secs/batch = 0.2660s, grad.norm=17.15932465
 24546: 18 [  660/ 1327], train_loss/perplexity = 3.72598124/41.5119476 secs/batch = 0.2653s, grad.norm=17.64470673
 24551: 18 [  665/ 1327], train_loss/perplexity = 3.89354730/49.0846977 secs/batch = 0.2665s, grad.norm=17.43081856
 24556: 18 [  670/ 1327], train_loss/perplexity = 3.82938337/46.0341454 secs/batch = 0.2656s, grad.norm=17.15979385
 24561: 18 [  675/ 1327], train_loss/perplexity = 3.63192916/37.7856407 secs/batch = 0.2659s, grad.norm=17.35384750
 24566: 18 [  680/ 1327], train_loss/perplexity = 3.83741879/46.4055367 secs/batch = 0.2659s, grad.norm=18.27980042
 24571: 18 [  685/ 1327], train_loss/perplexity = 3.60366631/36.7326622 secs/batch = 0.2656s, grad.norm=17.06135559
 24576: 18 [  690/ 1327], train_loss/perplexity = 4.01405430/55.3709068 secs/batch = 0.2658s, grad.norm=16.45928383
 24581: 18 [  695/ 1327], train_loss/perplexity = 3.86854315/47.8725929 secs/batch = 0.2662s, grad.norm=17.25190353
 24586: 18 [  700/ 1327], train_loss/perplexity = 4.17447662/65.0058060 secs/batch = 0.2653s, grad.norm=17.90081024
 24591: 18 [  705/ 1327], train_loss/perplexity = 3.82651353/45.9022217 secs/batch = 0.2641s, grad.norm=16.64423180
 24596: 18 [  710/ 1327], train_loss/perplexity = 3.77732086/43.6988106 secs/batch = 0.2656s, grad.norm=17.86647415
 24601: 18 [  715/ 1327], train_loss/perplexity = 3.65268064/38.5779419 secs/batch = 0.2665s, grad.norm=17.33433151
 24606: 18 [  720/ 1327], train_loss/perplexity = 3.65909696/38.8262634 secs/batch = 0.2663s, grad.norm=18.02775955
 24611: 18 [  725/ 1327], train_loss/perplexity = 3.72036290/41.2793732 secs/batch = 0.2657s, grad.norm=17.19637108
 24616: 18 [  730/ 1327], train_loss/perplexity = 3.88584876/48.7082672 secs/batch = 0.2657s, grad.norm=17.50998688
 24621: 18 [  735/ 1327], train_loss/perplexity = 3.90817285/49.8078613 secs/batch = 0.2669s, grad.norm=18.33955574
 24626: 18 [  740/ 1327], train_loss/perplexity = 3.44320631/31.2871132 secs/batch = 0.2624s, grad.norm=16.39273262
 24631: 18 [  745/ 1327], train_loss/perplexity = 3.87228537/48.0520782 secs/batch = 0.2664s, grad.norm=17.66708565
 24636: 18 [  750/ 1327], train_loss/perplexity = 3.79070473/44.2876015 secs/batch = 0.2665s, grad.norm=17.32617760
 24641: 18 [  755/ 1327], train_loss/perplexity = 3.67046952/39.2703400 secs/batch = 0.2624s, grad.norm=17.15789223
 24646: 18 [  760/ 1327], train_loss/perplexity = 3.52239037/33.8652840 secs/batch = 0.2666s, grad.norm=16.07588387
 24651: 18 [  765/ 1327], train_loss/perplexity = 3.58129764/35.9201202 secs/batch = 0.2657s, grad.norm=16.89516258
 24656: 18 [  770/ 1327], train_loss/perplexity = 3.56663132/35.3971519 secs/batch = 0.2666s, grad.norm=16.81711006
 24661: 18 [  775/ 1327], train_loss/perplexity = 3.62171912/37.4018097 secs/batch = 0.2665s, grad.norm=17.50844193
 24666: 18 [  780/ 1327], train_loss/perplexity = 4.03420067/56.4977417 secs/batch = 0.2674s, grad.norm=17.99291801
 24671: 18 [  785/ 1327], train_loss/perplexity = 3.84774661/46.8872871 secs/batch = 0.2657s, grad.norm=17.96557236
 24676: 18 [  790/ 1327], train_loss/perplexity = 3.63835096/38.0290718 secs/batch = 0.2664s, grad.norm=17.79812431
 24681: 18 [  795/ 1327], train_loss/perplexity = 4.06856155/58.4727936 secs/batch = 0.2660s, grad.norm=17.71231079
 24686: 18 [  800/ 1327], train_loss/perplexity = 3.87630987/48.2458534 secs/batch = 0.2664s, grad.norm=17.79754257
 24691: 18 [  805/ 1327], train_loss/perplexity = 4.21647358/67.7939911 secs/batch = 0.2664s, grad.norm=17.89278984
 24696: 18 [  810/ 1327], train_loss/perplexity = 3.76289368/43.0728836 secs/batch = 0.2667s, grad.norm=16.44823647
 24701: 18 [  815/ 1327], train_loss/perplexity = 3.75834179/42.8772659 secs/batch = 0.2668s, grad.norm=17.01420593
 24706: 18 [  820/ 1327], train_loss/perplexity = 3.60517359/36.7880707 secs/batch = 0.2667s, grad.norm=16.17512894
 24711: 18 [  825/ 1327], train_loss/perplexity = 3.81222296/45.2509193 secs/batch = 0.2636s, grad.norm=17.80098534
 24716: 18 [  830/ 1327], train_loss/perplexity = 3.48998523/32.7854652 secs/batch = 0.2662s, grad.norm=17.55202293
 24721: 18 [  835/ 1327], train_loss/perplexity = 3.81303692/45.2877655 secs/batch = 0.2663s, grad.norm=17.71714020
 24726: 18 [  840/ 1327], train_loss/perplexity = 3.80655694/44.9952507 secs/batch = 0.2593s, grad.norm=17.22621155
 24731: 18 [  845/ 1327], train_loss/perplexity = 3.65813541/38.7889519 secs/batch = 0.2671s, grad.norm=17.61587906
 24736: 18 [  850/ 1327], train_loss/perplexity = 3.78162432/43.8872719 secs/batch = 0.2661s, grad.norm=16.79600716
 24741: 18 [  855/ 1327], train_loss/perplexity = 3.75556302/42.7582855 secs/batch = 0.2664s, grad.norm=17.74150658
 24746: 18 [  860/ 1327], train_loss/perplexity = 3.56938887/35.4948959 secs/batch = 0.2659s, grad.norm=16.81030083
 24751: 18 [  865/ 1327], train_loss/perplexity = 3.92718863/50.7640610 secs/batch = 0.2660s, grad.norm=17.09076691
 24756: 18 [  870/ 1327], train_loss/perplexity = 3.85503078/47.2300720 secs/batch = 0.2643s, grad.norm=17.72035027
 24761: 18 [  875/ 1327], train_loss/perplexity = 3.46677494/32.0332680 secs/batch = 0.2660s, grad.norm=16.79378700
 24766: 18 [  880/ 1327], train_loss/perplexity = 3.71933961/41.2371521 secs/batch = 0.2660s, grad.norm=17.27933502
 24771: 18 [  885/ 1327], train_loss/perplexity = 3.84557557/46.7856064 secs/batch = 0.2672s, grad.norm=17.14806557
 24776: 18 [  890/ 1327], train_loss/perplexity = 3.98030567/53.5333939 secs/batch = 0.2642s, grad.norm=17.33751106
 24781: 18 [  895/ 1327], train_loss/perplexity = 3.99713135/54.4417496 secs/batch = 0.2661s, grad.norm=17.08034134
 24786: 18 [  900/ 1327], train_loss/perplexity = 3.76143360/43.0100403 secs/batch = 0.2666s, grad.norm=16.34917831
 24791: 18 [  905/ 1327], train_loss/perplexity = 3.71074581/40.8842888 secs/batch = 0.2653s, grad.norm=16.40197182
 24796: 18 [  910/ 1327], train_loss/perplexity = 3.76186109/43.0284309 secs/batch = 0.2657s, grad.norm=15.70116711
 24801: 18 [  915/ 1327], train_loss/perplexity = 3.97212934/53.0974731 secs/batch = 0.2669s, grad.norm=16.97414589
 24806: 18 [  920/ 1327], train_loss/perplexity = 4.10773420/60.8087807 secs/batch = 0.2655s, grad.norm=17.29659271
 24811: 18 [  925/ 1327], train_loss/perplexity = 3.84651375/46.8295174 secs/batch = 0.2660s, grad.norm=16.85040474
 24816: 18 [  930/ 1327], train_loss/perplexity = 3.90955806/49.8769035 secs/batch = 0.2644s, grad.norm=17.10871887
 24821: 18 [  935/ 1327], train_loss/perplexity = 3.95315456/52.0994606 secs/batch = 0.2665s, grad.norm=16.88584900
 24826: 18 [  940/ 1327], train_loss/perplexity = 3.91844201/50.3219833 secs/batch = 0.2658s, grad.norm=16.76104546
 24831: 18 [  945/ 1327], train_loss/perplexity = 4.12729073/62.0096931 secs/batch = 0.2663s, grad.norm=16.96552658
 24836: 18 [  950/ 1327], train_loss/perplexity = 3.90870070/49.8341599 secs/batch = 0.2646s, grad.norm=17.08823776
 24841: 18 [  955/ 1327], train_loss/perplexity = 3.86552501/47.7283249 secs/batch = 0.2661s, grad.norm=17.04008675
 24846: 18 [  960/ 1327], train_loss/perplexity = 4.20403814/66.9561615 secs/batch = 0.2590s, grad.norm=17.48816681
 24851: 18 [  965/ 1327], train_loss/perplexity = 3.94609785/51.7331009 secs/batch = 0.2659s, grad.norm=17.72979355
 24856: 18 [  970/ 1327], train_loss/perplexity = 4.12605906/61.9333649 secs/batch = 0.2657s, grad.norm=17.23238564
 24861: 18 [  975/ 1327], train_loss/perplexity = 3.79293776/44.3866043 secs/batch = 0.2663s, grad.norm=18.41722298
 24866: 18 [  980/ 1327], train_loss/perplexity = 3.69251633/40.1457405 secs/batch = 0.2658s, grad.norm=16.97057724
 24871: 18 [  985/ 1327], train_loss/perplexity = 3.78003120/43.8174095 secs/batch = 0.2662s, grad.norm=17.12309647
 24876: 18 [  990/ 1327], train_loss/perplexity = 4.04784679/57.2740021 secs/batch = 0.2664s, grad.norm=17.97350121
 24881: 18 [  995/ 1327], train_loss/perplexity = 4.03405523/56.4895248 secs/batch = 0.2658s, grad.norm=17.18731689
 24886: 18 [ 1000/ 1327], train_loss/perplexity = 3.52885032/34.0847588 secs/batch = 0.2663s, grad.norm=16.38625145
 24891: 18 [ 1005/ 1327], train_loss/perplexity = 3.99721169/54.4461250 secs/batch = 0.2659s, grad.norm=16.98748589
 24896: 18 [ 1010/ 1327], train_loss/perplexity = 3.62520146/37.5322838 secs/batch = 0.2667s, grad.norm=16.26850128
 24901: 18 [ 1015/ 1327], train_loss/perplexity = 4.06448317/58.2348022 secs/batch = 0.2656s, grad.norm=16.82865524
 24906: 18 [ 1020/ 1327], train_loss/perplexity = 4.15513754/63.7607346 secs/batch = 0.2662s, grad.norm=17.16642380
 24911: 18 [ 1025/ 1327], train_loss/perplexity = 4.14787579/63.2993965 secs/batch = 0.2654s, grad.norm=17.07039833
 24916: 18 [ 1030/ 1327], train_loss/perplexity = 3.81752825/45.4916267 secs/batch = 0.2661s, grad.norm=16.51123619
 24921: 18 [ 1035/ 1327], train_loss/perplexity = 3.78600764/44.0800667 secs/batch = 0.2662s, grad.norm=16.95677376
 24926: 18 [ 1040/ 1327], train_loss/perplexity = 3.98794246/53.9437828 secs/batch = 0.2658s, grad.norm=17.65664864
 24931: 18 [ 1045/ 1327], train_loss/perplexity = 3.57156110/35.5720825 secs/batch = 0.2660s, grad.norm=16.28729248
 24936: 18 [ 1050/ 1327], train_loss/perplexity = 3.59730911/36.4998856 secs/batch = 0.2604s, grad.norm=16.90990257
 24941: 18 [ 1055/ 1327], train_loss/perplexity = 3.69660878/40.3103714 secs/batch = 0.2667s, grad.norm=18.32131004
 24946: 18 [ 1060/ 1327], train_loss/perplexity = 3.28905392/26.8174801 secs/batch = 0.2654s, grad.norm=17.34386635
 24951: 18 [ 1065/ 1327], train_loss/perplexity = 3.48219514/32.5310555 secs/batch = 0.2653s, grad.norm=16.93808365
 24956: 18 [ 1070/ 1327], train_loss/perplexity = 3.87014771/47.9494667 secs/batch = 0.2653s, grad.norm=18.03447914
 24961: 18 [ 1075/ 1327], train_loss/perplexity = 3.58508229/36.0563240 secs/batch = 0.2659s, grad.norm=16.77759171
 24966: 18 [ 1080/ 1327], train_loss/perplexity = 3.60434127/36.7574615 secs/batch = 0.2678s, grad.norm=17.41843605
 24971: 18 [ 1085/ 1327], train_loss/perplexity = 3.45948625/31.8006344 secs/batch = 0.2671s, grad.norm=17.02178574
 24976: 18 [ 1090/ 1327], train_loss/perplexity = 3.60697484/36.8543930 secs/batch = 0.2663s, grad.norm=17.42816544
 24981: 18 [ 1095/ 1327], train_loss/perplexity = 3.82786727/45.9644051 secs/batch = 0.2647s, grad.norm=18.33701515
 24986: 18 [ 1100/ 1327], train_loss/perplexity = 3.48442745/32.6037560 secs/batch = 0.2660s, grad.norm=19.24272728
 24991: 18 [ 1105/ 1327], train_loss/perplexity = 3.50341225/33.2286415 secs/batch = 0.2665s, grad.norm=17.67067719
 24996: 18 [ 1110/ 1327], train_loss/perplexity = 3.75281572/42.6409798 secs/batch = 0.2665s, grad.norm=17.94822311
 25001: 18 [ 1115/ 1327], train_loss/perplexity = 3.58352661/36.0002747 secs/batch = 0.2653s, grad.norm=16.85103035
 25006: 18 [ 1120/ 1327], train_loss/perplexity = 3.86004472/47.4674721 secs/batch = 0.2664s, grad.norm=16.91531754
 25011: 18 [ 1125/ 1327], train_loss/perplexity = 3.99635959/54.3997498 secs/batch = 0.2608s, grad.norm=18.27655983
 25016: 18 [ 1130/ 1327], train_loss/perplexity = 3.71641445/41.1167030 secs/batch = 0.2652s, grad.norm=16.98934937
 25021: 18 [ 1135/ 1327], train_loss/perplexity = 3.68262196/39.7504807 secs/batch = 0.2645s, grad.norm=17.11166954
 25026: 18 [ 1140/ 1327], train_loss/perplexity = 3.94964838/51.9171104 secs/batch = 0.2642s, grad.norm=17.57742310
 25031: 18 [ 1145/ 1327], train_loss/perplexity = 3.82794857/45.9681396 secs/batch = 0.2666s, grad.norm=16.87066269
 25036: 18 [ 1150/ 1327], train_loss/perplexity = 3.74228525/42.1943054 secs/batch = 0.2664s, grad.norm=17.21867180
 25041: 18 [ 1155/ 1327], train_loss/perplexity = 3.72818685/41.6036072 secs/batch = 0.2667s, grad.norm=17.46011162
 25046: 18 [ 1160/ 1327], train_loss/perplexity = 3.78702831/44.1250801 secs/batch = 0.2669s, grad.norm=17.45643806
 25051: 18 [ 1165/ 1327], train_loss/perplexity = 3.81514978/45.3835526 secs/batch = 0.2661s, grad.norm=17.84657097
 25056: 18 [ 1170/ 1327], train_loss/perplexity = 3.64407873/38.2475204 secs/batch = 0.2658s, grad.norm=17.21884918
 25061: 18 [ 1175/ 1327], train_loss/perplexity = 3.47945428/32.4420128 secs/batch = 0.2665s, grad.norm=16.82923889
 25066: 18 [ 1180/ 1327], train_loss/perplexity = 3.51616740/33.6551933 secs/batch = 0.2643s, grad.norm=17.66310501
 25071: 18 [ 1185/ 1327], train_loss/perplexity = 3.75126076/42.5747261 secs/batch = 0.2664s, grad.norm=17.61557007
 25076: 18 [ 1190/ 1327], train_loss/perplexity = 3.78040934/43.8339806 secs/batch = 0.2647s, grad.norm=18.01085854
 25081: 18 [ 1195/ 1327], train_loss/perplexity = 3.58027506/35.8834114 secs/batch = 0.2660s, grad.norm=16.91896820
 25086: 18 [ 1200/ 1327], train_loss/perplexity = 3.54727221/34.7184830 secs/batch = 0.2665s, grad.norm=17.21914482
 25091: 18 [ 1205/ 1327], train_loss/perplexity = 3.59982586/36.5918617 secs/batch = 0.2659s, grad.norm=17.46662331
 25096: 18 [ 1210/ 1327], train_loss/perplexity = 3.19200706/24.3372250 secs/batch = 0.2664s, grad.norm=17.12026596
 25101: 18 [ 1215/ 1327], train_loss/perplexity = 3.47583485/32.3248024 secs/batch = 0.2662s, grad.norm=17.00057220
 25106: 18 [ 1220/ 1327], train_loss/perplexity = 3.60138321/36.6488914 secs/batch = 0.2656s, grad.norm=17.85560036
 25111: 18 [ 1225/ 1327], train_loss/perplexity = 3.32888222/27.9071293 secs/batch = 0.2666s, grad.norm=18.12687111
 25116: 18 [ 1230/ 1327], train_loss/perplexity = 3.62083602/37.3687973 secs/batch = 0.2665s, grad.norm=17.15665627
 25121: 18 [ 1235/ 1327], train_loss/perplexity = 3.53500175/34.2950745 secs/batch = 0.2666s, grad.norm=17.53611755
 25126: 18 [ 1240/ 1327], train_loss/perplexity = 3.80476952/44.9148979 secs/batch = 0.2662s, grad.norm=17.75012398
 25131: 18 [ 1245/ 1327], train_loss/perplexity = 3.63411832/37.8684502 secs/batch = 0.2660s, grad.norm=17.37260628
 25136: 18 [ 1250/ 1327], train_loss/perplexity = 3.83748770/46.4087334 secs/batch = 0.2661s, grad.norm=17.03487015
 25141: 18 [ 1255/ 1327], train_loss/perplexity = 3.85426497/47.1939163 secs/batch = 0.2663s, grad.norm=17.27032852
 25146: 18 [ 1260/ 1327], train_loss/perplexity = 3.59966445/36.5859566 secs/batch = 0.2668s, grad.norm=17.59228134
 25151: 18 [ 1265/ 1327], train_loss/perplexity = 3.87316895/48.0945549 secs/batch = 0.2708s, grad.norm=17.69203186
 25156: 18 [ 1270/ 1327], train_loss/perplexity = 3.53133750/34.1696396 secs/batch = 0.2658s, grad.norm=17.89291763
 25161: 18 [ 1275/ 1327], train_loss/perplexity = 3.77602649/43.6422844 secs/batch = 0.2660s, grad.norm=17.46989059
 25166: 18 [ 1280/ 1327], train_loss/perplexity = 3.71395612/41.0157509 secs/batch = 0.2666s, grad.norm=18.35692024
 25171: 18 [ 1285/ 1327], train_loss/perplexity = 3.50493097/33.2791481 secs/batch = 0.2659s, grad.norm=17.41451645
 25176: 18 [ 1290/ 1327], train_loss/perplexity = 3.80037165/44.7178001 secs/batch = 0.2624s, grad.norm=17.40614319
 25181: 18 [ 1295/ 1327], train_loss/perplexity = 3.68804240/39.9665337 secs/batch = 0.2669s, grad.norm=17.27348137
 25186: 18 [ 1300/ 1327], train_loss/perplexity = 3.94233131/51.5386124 secs/batch = 0.2654s, grad.norm=16.37808037
 25191: 18 [ 1305/ 1327], train_loss/perplexity = 3.97715569/53.3650322 secs/batch = 0.2658s, grad.norm=18.05140114
 25196: 18 [ 1310/ 1327], train_loss/perplexity = 4.15488815/63.7448349 secs/batch = 0.2672s, grad.norm=17.98699188
 25201: 18 [ 1315/ 1327], train_loss/perplexity = 3.97173929/53.0767670 secs/batch = 0.2657s, grad.norm=17.55992317
 25206: 18 [ 1320/ 1327], train_loss/perplexity = 3.95751166/52.3269577 secs/batch = 0.2663s, grad.norm=17.28030586
 25211: 18 [ 1325/ 1327], train_loss/perplexity = 3.94300556/51.5733757 secs/batch = 0.2666s, grad.norm=17.98255730
Epoch training time: 352.8631842136383
	> validation loss = 4.57977390, perplexity = 97.49234772
	> validation loss = 4.52462673, perplexity = 92.26148224
	> validation loss = 4.48750591, perplexity = 88.89944458
	> validation loss = 4.56747723, perplexity = 96.30085754
	> validation loss = 4.66988564, perplexity = 106.68553925
	> validation loss = 4.62084866, perplexity = 101.58020020
	> validation loss = 4.54659748, perplexity = 94.31096649
	> validation loss = 4.38073587, perplexity = 79.89680481
	> validation loss = 4.17917585, perplexity = 65.31200409
	> validation loss = 4.28206873, perplexity = 72.39003754
	> validation loss = 4.47333574, perplexity = 87.64860535
	> validation loss = 4.46788645, perplexity = 87.17228699
	> validation loss = 4.45044041, perplexity = 85.66466522
	> validation loss = 4.18918562, perplexity = 65.96904755
	> validation loss = 4.14296627, perplexity = 62.98938751
	> validation loss = 4.19207811, perplexity = 66.16013336
	> validation loss = 4.60845232, perplexity = 100.32875061
	> validation loss = 4.11516762, perplexity = 61.26248169
	> validation loss = 4.62618732, perplexity = 102.12395477
	> validation loss = 4.48794079, perplexity = 88.93811798
	> validation loss = 4.26786661, perplexity = 71.36921692
at the end of epoch: 18
train loss = 3.86806514, perplexity = 47.84971398
validation loss = 4.42786129, perplexity = 83.75210384
Saved model cv/epoch018_4.4279.model
 25218: 19 [    5/ 1327], train_loss/perplexity = 4.03728151/56.6720695 secs/batch = 0.2693s, grad.norm=17.33484268
 25223: 19 [   10/ 1327], train_loss/perplexity = 3.67663693/39.5132828 secs/batch = 0.2663s, grad.norm=16.99657822
 25228: 19 [   15/ 1327], train_loss/perplexity = 4.03354788/56.4608727 secs/batch = 0.2658s, grad.norm=16.91041756
 25233: 19 [   20/ 1327], train_loss/perplexity = 4.18654633/65.7951660 secs/batch = 0.2664s, grad.norm=16.98075104
 25238: 19 [   25/ 1327], train_loss/perplexity = 4.05913353/57.9240990 secs/batch = 0.2627s, grad.norm=17.92772293
 25243: 19 [   30/ 1327], train_loss/perplexity = 3.97720194/53.3675003 secs/batch = 0.2663s, grad.norm=17.73382759
 25248: 19 [   35/ 1327], train_loss/perplexity = 3.94184041/51.5133209 secs/batch = 0.2638s, grad.norm=17.47127151
 25253: 19 [   40/ 1327], train_loss/perplexity = 3.85808229/47.3744125 secs/batch = 0.2659s, grad.norm=17.65798950
 25258: 19 [   45/ 1327], train_loss/perplexity = 3.70505118/40.6521263 secs/batch = 0.2672s, grad.norm=16.98731995
 25263: 19 [   50/ 1327], train_loss/perplexity = 3.84375811/46.7006493 secs/batch = 0.2664s, grad.norm=17.28745461
 25268: 19 [   55/ 1327], train_loss/perplexity = 3.72216892/41.3539886 secs/batch = 0.2664s, grad.norm=17.88126755
 25273: 19 [   60/ 1327], train_loss/perplexity = 4.06478310/58.2522736 secs/batch = 0.2666s, grad.norm=17.89702797
 25278: 19 [   65/ 1327], train_loss/perplexity = 3.66891909/39.2094994 secs/batch = 0.2630s, grad.norm=17.46120071
 25283: 19 [   70/ 1327], train_loss/perplexity = 3.48346186/32.5722885 secs/batch = 0.2658s, grad.norm=16.67167664
 25288: 19 [   75/ 1327], train_loss/perplexity = 3.31672525/27.5699177 secs/batch = 0.2663s, grad.norm=16.61480331
 25293: 19 [   80/ 1327], train_loss/perplexity = 3.83267856/46.1860847 secs/batch = 0.2661s, grad.norm=17.50610161
 25298: 19 [   85/ 1327], train_loss/perplexity = 3.83074760/46.0969887 secs/batch = 0.2643s, grad.norm=17.76590919
 25303: 19 [   90/ 1327], train_loss/perplexity = 3.85969162/47.4507179 secs/batch = 0.2663s, grad.norm=18.05201340
 25308: 19 [   95/ 1327], train_loss/perplexity = 3.77093458/43.4206276 secs/batch = 0.2654s, grad.norm=17.69748306
 25313: 19 [  100/ 1327], train_loss/perplexity = 3.90508175/49.6541405 secs/batch = 0.2650s, grad.norm=17.74164200
 25318: 19 [  105/ 1327], train_loss/perplexity = 3.78729486/44.1368408 secs/batch = 0.2654s, grad.norm=18.16871834
 25323: 19 [  110/ 1327], train_loss/perplexity = 3.69600534/40.2860527 secs/batch = 0.2677s, grad.norm=17.53531075
 25328: 19 [  115/ 1327], train_loss/perplexity = 3.64660668/38.3443298 secs/batch = 0.2665s, grad.norm=17.84855270
 25333: 19 [  120/ 1327], train_loss/perplexity = 3.74654627/42.3744774 secs/batch = 0.2661s, grad.norm=17.37800217
 25338: 19 [  125/ 1327], train_loss/perplexity = 3.84254265/46.6439247 secs/batch = 0.2659s, grad.norm=18.34268570
 25343: 19 [  130/ 1327], train_loss/perplexity = 3.71975660/41.2543526 secs/batch = 0.2647s, grad.norm=18.35990524
 25348: 19 [  135/ 1327], train_loss/perplexity = 3.70216608/40.5350113 secs/batch = 0.2652s, grad.norm=17.54190254
 25353: 19 [  140/ 1327], train_loss/perplexity = 4.03083706/56.3080254 secs/batch = 0.2656s, grad.norm=18.46041489
 25358: 19 [  145/ 1327], train_loss/perplexity = 3.91588235/50.1933403 secs/batch = 0.2659s, grad.norm=18.47887039
 25363: 19 [  150/ 1327], train_loss/perplexity = 4.03576517/56.5862007 secs/batch = 0.2698s, grad.norm=18.52107811
 25368: 19 [  155/ 1327], train_loss/perplexity = 4.25113583/70.1850815 secs/batch = 0.2673s, grad.norm=18.85936356
 25373: 19 [  160/ 1327], train_loss/perplexity = 3.83239198/46.1728516 secs/batch = 0.2667s, grad.norm=16.93076134
 25378: 19 [  165/ 1327], train_loss/perplexity = 4.06830263/58.4576530 secs/batch = 0.2677s, grad.norm=17.67156410
 25383: 19 [  170/ 1327], train_loss/perplexity = 3.87759447/48.3078690 secs/batch = 0.2671s, grad.norm=17.56520844
 25388: 19 [  175/ 1327], train_loss/perplexity = 4.15876961/63.9927368 secs/batch = 0.2656s, grad.norm=17.61844635
 25393: 19 [  180/ 1327], train_loss/perplexity = 3.96321106/52.6260414 secs/batch = 0.2658s, grad.norm=17.87121010
 25398: 19 [  185/ 1327], train_loss/perplexity = 4.30500746/74.0697708 secs/batch = 0.2664s, grad.norm=17.92677689
 25403: 19 [  190/ 1327], train_loss/perplexity = 3.86166620/47.5445061 secs/batch = 0.2663s, grad.norm=17.07674789
 25408: 19 [  195/ 1327], train_loss/perplexity = 4.16513109/64.4011230 secs/batch = 0.2635s, grad.norm=17.20937538
 25413: 19 [  200/ 1327], train_loss/perplexity = 3.96077371/52.4979286 secs/batch = 0.2654s, grad.norm=17.87694550
 25418: 19 [  205/ 1327], train_loss/perplexity = 4.17610550/65.1117783 secs/batch = 0.2663s, grad.norm=17.93414688
 25423: 19 [  210/ 1327], train_loss/perplexity = 4.03711987/56.6629105 secs/batch = 0.2663s, grad.norm=16.91037560
 25428: 19 [  215/ 1327], train_loss/perplexity = 4.14262676/62.9680061 secs/batch = 0.2665s, grad.norm=17.33750725
 25433: 19 [  220/ 1327], train_loss/perplexity = 4.05788565/57.8518639 secs/batch = 0.2648s, grad.norm=17.05605888
 25438: 19 [  225/ 1327], train_loss/perplexity = 4.23639393/69.1580124 secs/batch = 0.2667s, grad.norm=17.69347382
 25443: 19 [  230/ 1327], train_loss/perplexity = 4.06996250/58.5547676 secs/batch = 0.2686s, grad.norm=18.34311867
 25448: 19 [  235/ 1327], train_loss/perplexity = 4.01290560/55.3073387 secs/batch = 0.2682s, grad.norm=17.85870743
 25453: 19 [  240/ 1327], train_loss/perplexity = 3.67231917/39.3430443 secs/batch = 0.2646s, grad.norm=17.61425018
 25458: 19 [  245/ 1327], train_loss/perplexity = 4.02341747/55.8917885 secs/batch = 0.2663s, grad.norm=17.36244774
 25463: 19 [  250/ 1327], train_loss/perplexity = 3.91675019/50.2369194 secs/batch = 0.2660s, grad.norm=17.04504395
 25468: 19 [  255/ 1327], train_loss/perplexity = 3.80335617/44.8514595 secs/batch = 0.2669s, grad.norm=17.64960480
 25473: 19 [  260/ 1327], train_loss/perplexity = 4.00668335/54.9642715 secs/batch = 0.2660s, grad.norm=18.22849274
 25478: 19 [  265/ 1327], train_loss/perplexity = 4.22127962/68.1205978 secs/batch = 0.2592s, grad.norm=17.49055099
 25483: 19 [  270/ 1327], train_loss/perplexity = 4.28269863/72.4356537 secs/batch = 0.2666s, grad.norm=17.88036919
 25488: 19 [  275/ 1327], train_loss/perplexity = 4.25180054/70.2317505 secs/batch = 0.2599s, grad.norm=17.34835052
 25493: 19 [  280/ 1327], train_loss/perplexity = 4.06560898/58.3003998 secs/batch = 0.2671s, grad.norm=17.28307915
 25498: 19 [  285/ 1327], train_loss/perplexity = 4.36754513/78.8498306 secs/batch = 0.2665s, grad.norm=17.25632095
 25503: 19 [  290/ 1327], train_loss/perplexity = 3.99853802/54.5183868 secs/batch = 0.2671s, grad.norm=17.71433449
 25508: 19 [  295/ 1327], train_loss/perplexity = 3.83942747/46.4988441 secs/batch = 0.2646s, grad.norm=17.51126671
 25513: 19 [  300/ 1327], train_loss/perplexity = 3.29397893/26.9498825 secs/batch = 0.2654s, grad.norm=16.20218086
 25518: 19 [  305/ 1327], train_loss/perplexity = 3.84822130/46.9095497 secs/batch = 0.2650s, grad.norm=16.98419762
 25523: 19 [  310/ 1327], train_loss/perplexity = 3.84280205/46.6560249 secs/batch = 0.2667s, grad.norm=16.91555023
 25528: 19 [  315/ 1327], train_loss/perplexity = 3.44153285/31.2348003 secs/batch = 0.2656s, grad.norm=16.74195480
 25533: 19 [  320/ 1327], train_loss/perplexity = 3.40551829/30.1299076 secs/batch = 0.2671s, grad.norm=17.88293457
 25538: 19 [  325/ 1327], train_loss/perplexity = 3.38189292/29.4264202 secs/batch = 0.2656s, grad.norm=15.90581608
 25543: 19 [  330/ 1327], train_loss/perplexity = 3.98179674/53.6132774 secs/batch = 0.2664s, grad.norm=17.23398209
 25548: 19 [  335/ 1327], train_loss/perplexity = 3.49654818/33.0013390 secs/batch = 0.2671s, grad.norm=16.11504364
 25553: 19 [  340/ 1327], train_loss/perplexity = 4.12609196/61.9354019 secs/batch = 0.2664s, grad.norm=17.20265388
 25558: 19 [  345/ 1327], train_loss/perplexity = 3.99204922/54.1657715 secs/batch = 0.2638s, grad.norm=16.87695122
 25563: 19 [  350/ 1327], train_loss/perplexity = 3.83898187/46.4781303 secs/batch = 0.2671s, grad.norm=17.60277939
 25568: 19 [  355/ 1327], train_loss/perplexity = 3.88205051/48.5236130 secs/batch = 0.2660s, grad.norm=17.29018402
 25573: 19 [  360/ 1327], train_loss/perplexity = 4.07110500/58.6217041 secs/batch = 0.2675s, grad.norm=18.88044167
 25578: 19 [  365/ 1327], train_loss/perplexity = 4.01881742/55.6352730 secs/batch = 0.2661s, grad.norm=17.39356041
 25583: 19 [  370/ 1327], train_loss/perplexity = 4.12092018/61.6159134 secs/batch = 0.2654s, grad.norm=17.46976471
 25588: 19 [  375/ 1327], train_loss/perplexity = 3.51099682/33.4816246 secs/batch = 0.2663s, grad.norm=17.27509880
 25593: 19 [  380/ 1327], train_loss/perplexity = 3.55344176/34.9333420 secs/batch = 0.2664s, grad.norm=17.58791542
 25598: 19 [  385/ 1327], train_loss/perplexity = 3.75471759/42.7221527 secs/batch = 0.2590s, grad.norm=18.11956215
 25603: 19 [  390/ 1327], train_loss/perplexity = 3.91055918/49.9268608 secs/batch = 0.2667s, grad.norm=17.70187950
 25608: 19 [  395/ 1327], train_loss/perplexity = 3.94829273/51.8467751 secs/batch = 0.2672s, grad.norm=17.56430817
 25613: 19 [  400/ 1327], train_loss/perplexity = 3.88642430/48.7363091 secs/batch = 0.2661s, grad.norm=17.09737587
 25618: 19 [  405/ 1327], train_loss/perplexity = 4.14273834/62.9750328 secs/batch = 0.2666s, grad.norm=17.54006386
 25623: 19 [  410/ 1327], train_loss/perplexity = 3.79479456/44.4691010 secs/batch = 0.2662s, grad.norm=17.24505615
 25628: 19 [  415/ 1327], train_loss/perplexity = 3.80356836/44.8609810 secs/batch = 0.2660s, grad.norm=17.31706619
 25633: 19 [  420/ 1327], train_loss/perplexity = 3.43400693/31.0006123 secs/batch = 0.2655s, grad.norm=16.94042397
 25638: 19 [  425/ 1327], train_loss/perplexity = 3.78103209/43.8612862 secs/batch = 0.2680s, grad.norm=18.36988831
 25643: 19 [  430/ 1327], train_loss/perplexity = 3.92153144/50.4776878 secs/batch = 0.2614s, grad.norm=17.94496536
 25648: 19 [  435/ 1327], train_loss/perplexity = 3.99921656/54.5553932 secs/batch = 0.2666s, grad.norm=18.07656670
 25653: 19 [  440/ 1327], train_loss/perplexity = 3.55613971/35.0277176 secs/batch = 0.2672s, grad.norm=17.84615898
 25658: 19 [  445/ 1327], train_loss/perplexity = 3.91110325/49.9540329 secs/batch = 0.2660s, grad.norm=17.41261292
 25663: 19 [  450/ 1327], train_loss/perplexity = 3.84856081/46.9254799 secs/batch = 0.2673s, grad.norm=17.12243462
 25668: 19 [  455/ 1327], train_loss/perplexity = 3.85617352/47.2840729 secs/batch = 0.2671s, grad.norm=16.93895721
 25673: 19 [  460/ 1327], train_loss/perplexity = 3.78262043/43.9310074 secs/batch = 0.2670s, grad.norm=17.54638672
 25678: 19 [  465/ 1327], train_loss/perplexity = 3.51578236/33.6422386 secs/batch = 0.2647s, grad.norm=17.81684875
 25683: 19 [  470/ 1327], train_loss/perplexity = 4.18040466/65.3923111 secs/batch = 0.2658s, grad.norm=17.39820290
 25688: 19 [  475/ 1327], train_loss/perplexity = 3.71693993/41.1383133 secs/batch = 0.2674s, grad.norm=17.46953964
 25693: 19 [  480/ 1327], train_loss/perplexity = 3.79226971/44.3569641 secs/batch = 0.2647s, grad.norm=17.19643593
 25698: 19 [  485/ 1327], train_loss/perplexity = 3.69929910/40.4189644 secs/batch = 0.2632s, grad.norm=17.73525047
 25703: 19 [  490/ 1327], train_loss/perplexity = 3.63989186/38.0877190 secs/batch = 0.2656s, grad.norm=18.61429405
 25708: 19 [  495/ 1327], train_loss/perplexity = 3.76461744/43.1471977 secs/batch = 0.2660s, grad.norm=17.26045418
 25713: 19 [  500/ 1327], train_loss/perplexity = 3.85975361/47.4536591 secs/batch = 0.2663s, grad.norm=17.11734581
 25718: 19 [  505/ 1327], train_loss/perplexity = 3.96021056/52.4683723 secs/batch = 0.2592s, grad.norm=16.13300133
 25723: 19 [  510/ 1327], train_loss/perplexity = 4.30385399/73.9843826 secs/batch = 0.2673s, grad.norm=16.82454300
 25728: 19 [  515/ 1327], train_loss/perplexity = 4.03701878/56.6571846 secs/batch = 0.2678s, grad.norm=16.76416779
 25733: 19 [  520/ 1327], train_loss/perplexity = 4.12746191/62.0203094 secs/batch = 0.2606s, grad.norm=17.73382378
 25738: 19 [  525/ 1327], train_loss/perplexity = 3.74829555/42.4486694 secs/batch = 0.2669s, grad.norm=17.30862808
 25743: 19 [  530/ 1327], train_loss/perplexity = 3.72240448/41.3637314 secs/batch = 0.2664s, grad.norm=17.40357208
 25748: 19 [  535/ 1327], train_loss/perplexity = 3.88719606/48.7739372 secs/batch = 0.2660s, grad.norm=17.71495438
 25753: 19 [  540/ 1327], train_loss/perplexity = 3.94850349/51.8577042 secs/batch = 0.2660s, grad.norm=17.32062531
 25758: 19 [  545/ 1327], train_loss/perplexity = 3.93314099/51.0671272 secs/batch = 0.2660s, grad.norm=17.14548492
 25763: 19 [  550/ 1327], train_loss/perplexity = 3.96044350/52.4805946 secs/batch = 0.2672s, grad.norm=17.35828018
 25768: 19 [  555/ 1327], train_loss/perplexity = 3.77355003/43.5343399 secs/batch = 0.2650s, grad.norm=16.63943863
 25773: 19 [  560/ 1327], train_loss/perplexity = 3.94410467/51.6300926 secs/batch = 0.2663s, grad.norm=18.45709991
 25778: 19 [  565/ 1327], train_loss/perplexity = 3.74575186/42.3408279 secs/batch = 0.2659s, grad.norm=18.14873123
 25783: 19 [  570/ 1327], train_loss/perplexity = 3.73669434/41.9590569 secs/batch = 0.2661s, grad.norm=17.90263176
 25788: 19 [  575/ 1327], train_loss/perplexity = 3.59848428/36.5428047 secs/batch = 0.2674s, grad.norm=17.52764130
 25793: 19 [  580/ 1327], train_loss/perplexity = 3.99119449/54.1194954 secs/batch = 0.2653s, grad.norm=18.29046249
 25798: 19 [  585/ 1327], train_loss/perplexity = 3.56198621/35.2331085 secs/batch = 0.2641s, grad.norm=17.34843826
 25803: 19 [  590/ 1327], train_loss/perplexity = 3.99200225/54.1632309 secs/batch = 0.2658s, grad.norm=17.41483498
 25808: 19 [  595/ 1327], train_loss/perplexity = 3.85637736/47.2937126 secs/batch = 0.2670s, grad.norm=17.73145485
 25813: 19 [  600/ 1327], train_loss/perplexity = 4.12646484/61.9585037 secs/batch = 0.2671s, grad.norm=16.67327499
 25818: 19 [  605/ 1327], train_loss/perplexity = 3.94765449/51.8136940 secs/batch = 0.2661s, grad.norm=16.80007553
 25823: 19 [  610/ 1327], train_loss/perplexity = 4.14135647/62.8880692 secs/batch = 0.2665s, grad.norm=17.37350273
 25828: 19 [  615/ 1327], train_loss/perplexity = 3.76467419/43.1496429 secs/batch = 0.2658s, grad.norm=16.63706589
 25833: 19 [  620/ 1327], train_loss/perplexity = 4.12830305/62.0724983 secs/batch = 0.2662s, grad.norm=17.24574471
 25838: 19 [  625/ 1327], train_loss/perplexity = 4.04993010/57.3934441 secs/batch = 0.2661s, grad.norm=17.31418991
 25843: 19 [  630/ 1327], train_loss/perplexity = 4.11864996/61.4761925 secs/batch = 0.2662s, grad.norm=17.46121979
 25848: 19 [  635/ 1327], train_loss/perplexity = 3.86478448/47.6929932 secs/batch = 0.2659s, grad.norm=17.09632111
 25853: 19 [  640/ 1327], train_loss/perplexity = 3.90363359/49.5822830 secs/batch = 0.2653s, grad.norm=17.37998962
 25858: 19 [  645/ 1327], train_loss/perplexity = 4.12704897/61.9947052 secs/batch = 0.2673s, grad.norm=18.06524277
 25863: 19 [  650/ 1327], train_loss/perplexity = 3.68535614/39.8593140 secs/batch = 0.2668s, grad.norm=17.15422058
 25868: 19 [  655/ 1327], train_loss/perplexity = 3.83738422/46.4039345 secs/batch = 0.2670s, grad.norm=17.48269653
 25873: 19 [  660/ 1327], train_loss/perplexity = 3.74893165/42.4756775 secs/batch = 0.2666s, grad.norm=17.97774696
 25878: 19 [  665/ 1327], train_loss/perplexity = 3.89729452/49.2689743 secs/batch = 0.2666s, grad.norm=17.69182777
 25883: 19 [  670/ 1327], train_loss/perplexity = 3.87609196/48.2353401 secs/batch = 0.2659s, grad.norm=17.32869911
 25888: 19 [  675/ 1327], train_loss/perplexity = 3.62588501/37.5579491 secs/batch = 0.2662s, grad.norm=17.26019669
 25893: 19 [  680/ 1327], train_loss/perplexity = 3.86700368/47.7989502 secs/batch = 0.2644s, grad.norm=18.31667900
 25898: 19 [  685/ 1327], train_loss/perplexity = 3.66070700/38.8888283 secs/batch = 0.2661s, grad.norm=17.30568314
 25903: 19 [  690/ 1327], train_loss/perplexity = 4.03155804/56.3486366 secs/batch = 0.2660s, grad.norm=17.20215988
 25908: 19 [  695/ 1327], train_loss/perplexity = 3.88512850/48.6731949 secs/batch = 0.2656s, grad.norm=18.34033394
 25913: 19 [  700/ 1327], train_loss/perplexity = 4.10384941/60.5730095 secs/batch = 0.2639s, grad.norm=17.84983444
 25918: 19 [  705/ 1327], train_loss/perplexity = 3.82389975/45.7824020 secs/batch = 0.2657s, grad.norm=16.81807327
 25923: 19 [  710/ 1327], train_loss/perplexity = 3.72943068/41.6553879 secs/batch = 0.2673s, grad.norm=17.45442390
 25928: 19 [  715/ 1327], train_loss/perplexity = 3.62543154/37.5409203 secs/batch = 0.2650s, grad.norm=17.36989021
 25933: 19 [  720/ 1327], train_loss/perplexity = 3.64027429/38.1022873 secs/batch = 0.2624s, grad.norm=17.73422623
 25938: 19 [  725/ 1327], train_loss/perplexity = 3.68912458/40.0098076 secs/batch = 0.2666s, grad.norm=17.73325539
 25943: 19 [  730/ 1327], train_loss/perplexity = 3.80397439/44.8791962 secs/batch = 0.2682s, grad.norm=17.47740936
 25948: 19 [  735/ 1327], train_loss/perplexity = 3.89748073/49.2781487 secs/batch = 0.2687s, grad.norm=18.67006493
 25953: 19 [  740/ 1327], train_loss/perplexity = 3.47870541/32.4177284 secs/batch = 0.2648s, grad.norm=16.77060127
 25958: 19 [  745/ 1327], train_loss/perplexity = 3.87577200/48.2199097 secs/batch = 0.2671s, grad.norm=17.65615845
 25963: 19 [  750/ 1327], train_loss/perplexity = 3.78886175/44.2060547 secs/batch = 0.2664s, grad.norm=17.10007477
 25968: 19 [  755/ 1327], train_loss/perplexity = 3.63942504/38.0699425 secs/batch = 0.2658s, grad.norm=17.01445389
 25973: 19 [  760/ 1327], train_loss/perplexity = 3.49440694/32.9307518 secs/batch = 0.2662s, grad.norm=16.13306236
 25978: 19 [  765/ 1327], train_loss/perplexity = 3.51390219/33.5790443 secs/batch = 0.2612s, grad.norm=16.80250549
 25983: 19 [  770/ 1327], train_loss/perplexity = 3.58775926/36.1529770 secs/batch = 0.2666s, grad.norm=17.08857918
 25988: 19 [  775/ 1327], train_loss/perplexity = 3.67460799/39.4331970 secs/batch = 0.2643s, grad.norm=18.07573700
 25993: 19 [  780/ 1327], train_loss/perplexity = 3.97353506/53.1721649 secs/batch = 0.2659s, grad.norm=17.93708992
 25998: 19 [  785/ 1327], train_loss/perplexity = 3.86159158/47.5409546 secs/batch = 0.2666s, grad.norm=17.77610016
 26003: 19 [  790/ 1327], train_loss/perplexity = 3.65364790/38.6152725 secs/batch = 0.2666s, grad.norm=17.96448517
 26008: 19 [  795/ 1327], train_loss/perplexity = 4.03147650/56.3440399 secs/batch = 0.2668s, grad.norm=17.59270287
 26013: 19 [  800/ 1327], train_loss/perplexity = 3.80871940/45.0926552 secs/batch = 0.2643s, grad.norm=17.64029503
 26018: 19 [  805/ 1327], train_loss/perplexity = 4.19530725/66.3741226 secs/batch = 0.2648s, grad.norm=18.12883568
 26023: 19 [  810/ 1327], train_loss/perplexity = 3.80862331/45.0883255 secs/batch = 0.2611s, grad.norm=16.67644310
 26028: 19 [  815/ 1327], train_loss/perplexity = 3.73609614/41.9339676 secs/batch = 0.2656s, grad.norm=16.84427452
 26033: 19 [  820/ 1327], train_loss/perplexity = 3.65469432/38.6557045 secs/batch = 0.2665s, grad.norm=16.75300217
 26038: 19 [  825/ 1327], train_loss/perplexity = 3.75350308/42.6702995 secs/batch = 0.2666s, grad.norm=17.57267570
 26043: 19 [  830/ 1327], train_loss/perplexity = 3.43883157/31.1505394 secs/batch = 0.2637s, grad.norm=17.51955414
 26048: 19 [  835/ 1327], train_loss/perplexity = 3.76808715/43.2971649 secs/batch = 0.2663s, grad.norm=17.70331573
 26053: 19 [  840/ 1327], train_loss/perplexity = 3.78892803/44.2089844 secs/batch = 0.2655s, grad.norm=17.22560501
 26058: 19 [  845/ 1327], train_loss/perplexity = 3.65729284/38.7562828 secs/batch = 0.2662s, grad.norm=17.66268158
 26063: 19 [  850/ 1327], train_loss/perplexity = 3.79703832/44.5689888 secs/batch = 0.2651s, grad.norm=16.77964211
 26068: 19 [  855/ 1327], train_loss/perplexity = 3.75160575/42.5894165 secs/batch = 0.2603s, grad.norm=17.47992706
 26073: 19 [  860/ 1327], train_loss/perplexity = 3.52332973/33.8971100 secs/batch = 0.2666s, grad.norm=16.56369591
 26078: 19 [  865/ 1327], train_loss/perplexity = 3.99924564/54.5569801 secs/batch = 0.2660s, grad.norm=17.45730972
 26083: 19 [  870/ 1327], train_loss/perplexity = 3.73222256/41.7718468 secs/batch = 0.2663s, grad.norm=17.96071625
 26088: 19 [  875/ 1327], train_loss/perplexity = 3.48696852/32.6867065 secs/batch = 0.2639s, grad.norm=16.84730911
 26093: 19 [  880/ 1327], train_loss/perplexity = 3.65024710/38.4841728 secs/batch = 0.2675s, grad.norm=16.92231369
 26098: 19 [  885/ 1327], train_loss/perplexity = 3.81287527/45.2804451 secs/batch = 0.2660s, grad.norm=16.99746132
 26103: 19 [  890/ 1327], train_loss/perplexity = 3.95273399/52.0775528 secs/batch = 0.2668s, grad.norm=17.36947441
 26108: 19 [  895/ 1327], train_loss/perplexity = 3.94223261/51.5335274 secs/batch = 0.2640s, grad.norm=16.94519043
 26113: 19 [  900/ 1327], train_loss/perplexity = 3.82867813/46.0016899 secs/batch = 0.2626s, grad.norm=16.70090103
 26118: 19 [  905/ 1327], train_loss/perplexity = 3.68705869/39.9272346 secs/batch = 0.2658s, grad.norm=16.53325844
 26123: 19 [  910/ 1327], train_loss/perplexity = 3.79568982/44.5089302 secs/batch = 0.2664s, grad.norm=16.18953896
 26128: 19 [  915/ 1327], train_loss/perplexity = 3.93630290/51.2288513 secs/batch = 0.2620s, grad.norm=16.70347977
 26133: 19 [  920/ 1327], train_loss/perplexity = 4.08140802/59.2288055 secs/batch = 0.2657s, grad.norm=17.43106079
 26138: 19 [  925/ 1327], train_loss/perplexity = 3.84551859/46.7829399 secs/batch = 0.2603s, grad.norm=16.69390869
 26143: 19 [  930/ 1327], train_loss/perplexity = 3.93575525/51.2008057 secs/batch = 0.2657s, grad.norm=17.04117966
 26148: 19 [  935/ 1327], train_loss/perplexity = 3.99392748/54.2676048 secs/batch = 0.2665s, grad.norm=16.77873802
 26153: 19 [  940/ 1327], train_loss/perplexity = 3.96919155/52.9417114 secs/batch = 0.2648s, grad.norm=16.94534111
 26158: 19 [  945/ 1327], train_loss/perplexity = 4.19335318/66.2445450 secs/batch = 0.2652s, grad.norm=16.94920158
 26163: 19 [  950/ 1327], train_loss/perplexity = 3.90424609/49.6126633 secs/batch = 0.2648s, grad.norm=16.95803261
 26168: 19 [  955/ 1327], train_loss/perplexity = 3.86994600/47.9397964 secs/batch = 0.2660s, grad.norm=17.04224396
 26173: 19 [  960/ 1327], train_loss/perplexity = 4.18967867/66.0015793 secs/batch = 0.2661s, grad.norm=17.95725060
 26178: 19 [  965/ 1327], train_loss/perplexity = 3.90795660/49.7970924 secs/batch = 0.2657s, grad.norm=17.15071106
 26183: 19 [  970/ 1327], train_loss/perplexity = 4.16265965/64.2421570 secs/batch = 0.2650s, grad.norm=17.62493706
 26188: 19 [  975/ 1327], train_loss/perplexity = 3.77278614/43.5010948 secs/batch = 0.2672s, grad.norm=18.31170082
 26193: 19 [  980/ 1327], train_loss/perplexity = 3.61970568/37.3265800 secs/batch = 0.2673s, grad.norm=16.79170418
 26198: 19 [  985/ 1327], train_loss/perplexity = 3.76617074/43.2142677 secs/batch = 0.2661s, grad.norm=17.88706779
 26203: 19 [  990/ 1327], train_loss/perplexity = 4.01600981/55.4792900 secs/batch = 0.2671s, grad.norm=18.31842995
 26208: 19 [  995/ 1327], train_loss/perplexity = 4.02225399/55.8267975 secs/batch = 0.2660s, grad.norm=17.41513443
 26213: 19 [ 1000/ 1327], train_loss/perplexity = 3.54589820/34.6708145 secs/batch = 0.2663s, grad.norm=16.53503418
 26218: 19 [ 1005/ 1327], train_loss/perplexity = 4.05153894/57.4858551 secs/batch = 0.2680s, grad.norm=17.44171906
 26223: 19 [ 1010/ 1327], train_loss/perplexity = 3.60454893/36.7650948 secs/batch = 0.2669s, grad.norm=16.23511314
 26228: 19 [ 1015/ 1327], train_loss/perplexity = 4.06922626/58.5116730 secs/batch = 0.2638s, grad.norm=16.68749619
 26233: 19 [ 1020/ 1327], train_loss/perplexity = 4.08087826/59.1974373 secs/batch = 0.2666s, grad.norm=17.08054161
 26238: 19 [ 1025/ 1327], train_loss/perplexity = 4.10564947/60.6821442 secs/batch = 0.2605s, grad.norm=17.04709244
 26243: 19 [ 1030/ 1327], train_loss/perplexity = 3.83406925/46.2503586 secs/batch = 0.2667s, grad.norm=16.57068443
 26248: 19 [ 1035/ 1327], train_loss/perplexity = 3.80033970/44.7163734 secs/batch = 0.2655s, grad.norm=16.55974197
 26253: 19 [ 1040/ 1327], train_loss/perplexity = 4.00370884/54.8010216 secs/batch = 0.2655s, grad.norm=17.72043800
 26258: 19 [ 1045/ 1327], train_loss/perplexity = 3.49952435/33.0997047 secs/batch = 0.2659s, grad.norm=16.00112534
 26263: 19 [ 1050/ 1327], train_loss/perplexity = 3.57089972/35.5485611 secs/batch = 0.2664s, grad.norm=17.22751236
 26268: 19 [ 1055/ 1327], train_loss/perplexity = 3.70489907/40.6459465 secs/batch = 0.2668s, grad.norm=17.82375717
 26273: 19 [ 1060/ 1327], train_loss/perplexity = 3.33463073/28.0680161 secs/batch = 0.2655s, grad.norm=17.48385048
 26278: 19 [ 1065/ 1327], train_loss/perplexity = 3.51861691/33.7377357 secs/batch = 0.2664s, grad.norm=17.49591064
 26283: 19 [ 1070/ 1327], train_loss/perplexity = 3.81779075/45.5035667 secs/batch = 0.2659s, grad.norm=17.56494141
 26288: 19 [ 1075/ 1327], train_loss/perplexity = 3.57626891/35.7399445 secs/batch = 0.2648s, grad.norm=17.06731606
 26293: 19 [ 1080/ 1327], train_loss/perplexity = 3.51592588/33.6470680 secs/batch = 0.2655s, grad.norm=17.04113960
 26298: 19 [ 1085/ 1327], train_loss/perplexity = 3.47638083/32.3424568 secs/batch = 0.2642s, grad.norm=17.29342651
 26303: 19 [ 1090/ 1327], train_loss/perplexity = 3.64722323/38.3679810 secs/batch = 0.2663s, grad.norm=18.54120827
 26308: 19 [ 1095/ 1327], train_loss/perplexity = 3.76255226/43.0581818 secs/batch = 0.2655s, grad.norm=17.53430748
 26313: 19 [ 1100/ 1327], train_loss/perplexity = 3.46389318/31.9410877 secs/batch = 0.2674s, grad.norm=18.85556793
 26318: 19 [ 1105/ 1327], train_loss/perplexity = 3.46110678/31.8522110 secs/batch = 0.2658s, grad.norm=17.22072601
 26323: 19 [ 1110/ 1327], train_loss/perplexity = 3.68947959/40.0240135 secs/batch = 0.2606s, grad.norm=17.70794487
 26328: 19 [ 1115/ 1327], train_loss/perplexity = 3.61388397/37.1099052 secs/batch = 0.2663s, grad.norm=16.70327950
 26333: 19 [ 1120/ 1327], train_loss/perplexity = 3.91760111/50.2796860 secs/batch = 0.2660s, grad.norm=17.19964790
 26338: 19 [ 1125/ 1327], train_loss/perplexity = 4.00805330/55.0396194 secs/batch = 0.2650s, grad.norm=18.42816734
 26343: 19 [ 1130/ 1327], train_loss/perplexity = 3.68217134/39.7325745 secs/batch = 0.2633s, grad.norm=17.71974945
 26348: 19 [ 1135/ 1327], train_loss/perplexity = 3.66172791/38.9285507 secs/batch = 0.2665s, grad.norm=17.39086723
 26353: 19 [ 1140/ 1327], train_loss/perplexity = 3.89064407/48.9423981 secs/batch = 0.2658s, grad.norm=17.93868256
 26358: 19 [ 1145/ 1327], train_loss/perplexity = 3.70275688/40.5589676 secs/batch = 0.2665s, grad.norm=16.99793434
 26363: 19 [ 1150/ 1327], train_loss/perplexity = 3.68782616/39.9578896 secs/batch = 0.2676s, grad.norm=17.15203094
 26368: 19 [ 1155/ 1327], train_loss/perplexity = 3.82695055/45.9222870 secs/batch = 0.2675s, grad.norm=18.06156158
 26373: 19 [ 1160/ 1327], train_loss/perplexity = 3.76086020/42.9853859 secs/batch = 0.2665s, grad.norm=17.39797783
 26378: 19 [ 1165/ 1327], train_loss/perplexity = 3.76495457/43.1617432 secs/batch = 0.2667s, grad.norm=17.44464302
 26383: 19 [ 1170/ 1327], train_loss/perplexity = 3.62484598/37.5189438 secs/batch = 0.2648s, grad.norm=17.01320076
 26388: 19 [ 1175/ 1327], train_loss/perplexity = 3.53882051/34.4262886 secs/batch = 0.2648s, grad.norm=17.36650658
 26393: 19 [ 1180/ 1327], train_loss/perplexity = 3.53646207/34.3451920 secs/batch = 0.2649s, grad.norm=17.49620438
 26398: 19 [ 1185/ 1327], train_loss/perplexity = 3.69392180/40.2022018 secs/batch = 0.2672s, grad.norm=17.89558411
 26403: 19 [ 1190/ 1327], train_loss/perplexity = 3.75495529/42.7323074 secs/batch = 0.2670s, grad.norm=17.65406799
 26408: 19 [ 1195/ 1327], train_loss/perplexity = 3.57122779/35.5602264 secs/batch = 0.2661s, grad.norm=17.02784348
 26413: 19 [ 1200/ 1327], train_loss/perplexity = 3.45062542/31.5200996 secs/batch = 0.2663s, grad.norm=17.31978989
 26418: 19 [ 1205/ 1327], train_loss/perplexity = 3.59849834/36.5433159 secs/batch = 0.2662s, grad.norm=18.85702324
 26423: 19 [ 1210/ 1327], train_loss/perplexity = 3.14345169/23.1837521 secs/batch = 0.2673s, grad.norm=17.13401985
 26428: 19 [ 1215/ 1327], train_loss/perplexity = 3.44706869/31.4081898 secs/batch = 0.2675s, grad.norm=16.61166763
 26433: 19 [ 1220/ 1327], train_loss/perplexity = 3.57922196/35.8456421 secs/batch = 0.2660s, grad.norm=17.67328644
 26438: 19 [ 1225/ 1327], train_loss/perplexity = 3.29162073/26.8864040 secs/batch = 0.2671s, grad.norm=18.19280815
 26443: 19 [ 1230/ 1327], train_loss/perplexity = 3.63223267/37.7971115 secs/batch = 0.2639s, grad.norm=17.06480217
 26448: 19 [ 1235/ 1327], train_loss/perplexity = 3.54258704/34.5562019 secs/batch = 0.2675s, grad.norm=17.19672012
 26453: 19 [ 1240/ 1327], train_loss/perplexity = 3.76040673/42.9658966 secs/batch = 0.2665s, grad.norm=18.01637459
 26458: 19 [ 1245/ 1327], train_loss/perplexity = 3.68423009/39.8144569 secs/batch = 0.2661s, grad.norm=17.12658691
 26463: 19 [ 1250/ 1327], train_loss/perplexity = 3.77485132/43.5910263 secs/batch = 0.2672s, grad.norm=16.90088272
 26468: 19 [ 1255/ 1327], train_loss/perplexity = 3.88321066/48.5799370 secs/batch = 0.2653s, grad.norm=17.33561325
 26473: 19 [ 1260/ 1327], train_loss/perplexity = 3.59554458/36.4355354 secs/batch = 0.2687s, grad.norm=17.86134338
 26478: 19 [ 1265/ 1327], train_loss/perplexity = 3.76368189/43.1068497 secs/batch = 0.2659s, grad.norm=17.38102531
 26483: 19 [ 1270/ 1327], train_loss/perplexity = 3.53074551/34.1494179 secs/batch = 0.2646s, grad.norm=17.97068214
 26488: 19 [ 1275/ 1327], train_loss/perplexity = 3.71432018/41.0306854 secs/batch = 0.2675s, grad.norm=17.50060081
 26493: 19 [ 1280/ 1327], train_loss/perplexity = 3.63671303/37.9668350 secs/batch = 0.2662s, grad.norm=18.84222031
 26498: 19 [ 1285/ 1327], train_loss/perplexity = 3.48118448/32.4981918 secs/batch = 0.2662s, grad.norm=17.38195992
 26503: 19 [ 1290/ 1327], train_loss/perplexity = 3.77421665/43.5633698 secs/batch = 0.2652s, grad.norm=17.47543907
 26508: 19 [ 1295/ 1327], train_loss/perplexity = 3.71484709/41.0523109 secs/batch = 0.2664s, grad.norm=17.31624413
 26513: 19 [ 1300/ 1327], train_loss/perplexity = 3.92199206/50.5009460 secs/batch = 0.2664s, grad.norm=16.87042236
 26518: 19 [ 1305/ 1327], train_loss/perplexity = 3.94824886/51.8445015 secs/batch = 0.2656s, grad.norm=17.95525932
 26523: 19 [ 1310/ 1327], train_loss/perplexity = 4.17569113/65.0848083 secs/batch = 0.2658s, grad.norm=18.14034271
 26528: 19 [ 1315/ 1327], train_loss/perplexity = 3.95256639/52.0688248 secs/batch = 0.2655s, grad.norm=17.64465332
 26533: 19 [ 1320/ 1327], train_loss/perplexity = 3.99772072/54.4738464 secs/batch = 0.2662s, grad.norm=17.82223320
 26538: 19 [ 1325/ 1327], train_loss/perplexity = 3.86419010/47.6646538 secs/batch = 0.2663s, grad.norm=17.93656349
Epoch training time: 352.95426082611084
	> validation loss = 4.57678032, perplexity = 97.20093536
	> validation loss = 4.52762032, perplexity = 92.53808594
	> validation loss = 4.48871851, perplexity = 89.00730896
	> validation loss = 4.56302977, perplexity = 95.87351227
	> validation loss = 4.66995001, perplexity = 106.69240570
	> validation loss = 4.62360477, perplexity = 101.86055756
	> validation loss = 4.55034685, perplexity = 94.66523743
	> validation loss = 4.37986422, perplexity = 79.82719421
	> validation loss = 4.18101215, perplexity = 65.43204498
	> validation loss = 4.27917814, perplexity = 72.18109131
	> validation loss = 4.47591496, perplexity = 87.87496185
	> validation loss = 4.46765566, perplexity = 87.15216827
	> validation loss = 4.45490074, perplexity = 86.04760742
	> validation loss = 4.19183397, perplexity = 66.14398956
	> validation loss = 4.14410210, perplexity = 63.06097412
	> validation loss = 4.19249249, perplexity = 66.18755341
	> validation loss = 4.60986376, perplexity = 100.47045898
	> validation loss = 4.11885643, perplexity = 61.48888397
	> validation loss = 4.62683868, perplexity = 102.19049835
	> validation loss = 4.48464966, perplexity = 88.64588928
	> validation loss = 4.26921511, perplexity = 71.46552277
at the end of epoch: 19
train loss = 3.86305832, perplexity = 47.61073762
validation loss = 4.42773814, perplexity = 83.74179069
Saved model cv/epoch019_4.4277.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.03125
new learning rate is: 0.015625
 26545: 20 [    5/ 1327], train_loss/perplexity = 4.03752756/56.6860161 secs/batch = 0.2692s, grad.norm=17.95591164
 26550: 20 [   10/ 1327], train_loss/perplexity = 3.65120769/38.5211601 secs/batch = 0.2620s, grad.norm=17.01884460
 26555: 20 [   15/ 1327], train_loss/perplexity = 4.00771284/55.0208855 secs/batch = 0.2667s, grad.norm=16.90345383
 26560: 20 [   20/ 1327], train_loss/perplexity = 4.21275473/67.5423431 secs/batch = 0.2658s, grad.norm=17.16452026
 26565: 20 [   25/ 1327], train_loss/perplexity = 4.00908232/55.0962868 secs/batch = 0.2662s, grad.norm=17.95348167
 26570: 20 [   30/ 1327], train_loss/perplexity = 4.01196337/55.2552490 secs/batch = 0.2668s, grad.norm=18.22388649
 26575: 20 [   35/ 1327], train_loss/perplexity = 3.87328148/48.0999680 secs/batch = 0.2657s, grad.norm=17.58873558
 26580: 20 [   40/ 1327], train_loss/perplexity = 3.89294696/49.0552368 secs/batch = 0.2655s, grad.norm=17.43395615
 26585: 20 [   45/ 1327], train_loss/perplexity = 3.72558093/41.4953308 secs/batch = 0.2669s, grad.norm=16.82131577
 26590: 20 [   50/ 1327], train_loss/perplexity = 3.89534569/49.1730499 secs/batch = 0.2665s, grad.norm=17.87201500
 26595: 20 [   55/ 1327], train_loss/perplexity = 3.75362825/42.6756401 secs/batch = 0.2667s, grad.norm=17.67737198
 26600: 20 [   60/ 1327], train_loss/perplexity = 4.04463577/57.0903893 secs/batch = 0.2655s, grad.norm=18.16024017
 26605: 20 [   65/ 1327], train_loss/perplexity = 3.64946365/38.4540367 secs/batch = 0.2663s, grad.norm=17.38323021
 26610: 20 [   70/ 1327], train_loss/perplexity = 3.56531239/35.3504944 secs/batch = 0.2663s, grad.norm=17.83708382
 26615: 20 [   75/ 1327], train_loss/perplexity = 3.35676718/28.6962700 secs/batch = 0.2666s, grad.norm=16.80280876
 26620: 20 [   80/ 1327], train_loss/perplexity = 3.77236509/43.4827843 secs/batch = 0.2634s, grad.norm=17.53707886
 26625: 20 [   85/ 1327], train_loss/perplexity = 3.81329727/45.2995567 secs/batch = 0.2656s, grad.norm=18.09743500
 26630: 20 [   90/ 1327], train_loss/perplexity = 3.83799028/46.4320641 secs/batch = 0.2662s, grad.norm=17.84420586
 26635: 20 [   95/ 1327], train_loss/perplexity = 3.72698402/41.5535927 secs/batch = 0.2659s, grad.norm=17.49571991
 26640: 20 [  100/ 1327], train_loss/perplexity = 3.99296832/54.2155800 secs/batch = 0.2656s, grad.norm=18.12821770
 26645: 20 [  105/ 1327], train_loss/perplexity = 3.80784583/45.0532837 secs/batch = 0.2674s, grad.norm=18.53866005
 26650: 20 [  110/ 1327], train_loss/perplexity = 3.74826646/42.4474335 secs/batch = 0.2673s, grad.norm=17.98156738
 26655: 20 [  115/ 1327], train_loss/perplexity = 3.67647719/39.5069733 secs/batch = 0.2661s, grad.norm=18.28683472
 26660: 20 [  120/ 1327], train_loss/perplexity = 3.75224161/42.6165047 secs/batch = 0.2660s, grad.norm=17.59304428
 26665: 20 [  125/ 1327], train_loss/perplexity = 3.80194664/44.7882881 secs/batch = 0.2664s, grad.norm=18.25449371
 26670: 20 [  130/ 1327], train_loss/perplexity = 3.80803013/45.0615845 secs/batch = 0.2660s, grad.norm=18.68587494
 26675: 20 [  135/ 1327], train_loss/perplexity = 3.76501989/43.1645660 secs/batch = 0.2661s, grad.norm=17.82742882
 26680: 20 [  140/ 1327], train_loss/perplexity = 3.98601222/53.8397598 secs/batch = 0.2663s, grad.norm=18.06936455
 26685: 20 [  145/ 1327], train_loss/perplexity = 3.96631527/52.7896576 secs/batch = 0.2655s, grad.norm=18.76130295
 26690: 20 [  150/ 1327], train_loss/perplexity = 3.95920467/52.4156227 secs/batch = 0.2647s, grad.norm=18.67240715
 26695: 20 [  155/ 1327], train_loss/perplexity = 4.26383495/71.0820541 secs/batch = 0.2654s, grad.norm=18.32289696
 26700: 20 [  160/ 1327], train_loss/perplexity = 3.92198849/50.5007668 secs/batch = 0.2680s, grad.norm=17.32927513
 26705: 20 [  165/ 1327], train_loss/perplexity = 4.06225109/58.1049652 secs/batch = 0.2657s, grad.norm=17.61634254
 26710: 20 [  170/ 1327], train_loss/perplexity = 3.87415218/48.1418648 secs/batch = 0.2649s, grad.norm=17.50318146
 26715: 20 [  175/ 1327], train_loss/perplexity = 4.09933472/60.3001595 secs/batch = 0.2658s, grad.norm=17.88765717
 26720: 20 [  180/ 1327], train_loss/perplexity = 3.90920496/49.8592949 secs/batch = 0.2657s, grad.norm=17.75281334
 26725: 20 [  185/ 1327], train_loss/perplexity = 4.25654078/70.5654602 secs/batch = 0.2664s, grad.norm=17.86313438
 26730: 20 [  190/ 1327], train_loss/perplexity = 3.83145761/46.1297302 secs/batch = 0.2668s, grad.norm=17.16193771
 26735: 20 [  195/ 1327], train_loss/perplexity = 4.14114237/62.8746071 secs/batch = 0.2677s, grad.norm=17.09878922
 26740: 20 [  200/ 1327], train_loss/perplexity = 3.91066623/49.9322052 secs/batch = 0.2668s, grad.norm=18.23168182
 26745: 20 [  205/ 1327], train_loss/perplexity = 4.14206648/62.9327354 secs/batch = 0.2660s, grad.norm=17.72904396
 26750: 20 [  210/ 1327], train_loss/perplexity = 3.99794221/54.4859123 secs/batch = 0.2663s, grad.norm=16.97331429
 26755: 20 [  215/ 1327], train_loss/perplexity = 4.12895918/62.1132393 secs/batch = 0.2661s, grad.norm=17.10095596
 26760: 20 [  220/ 1327], train_loss/perplexity = 4.03828096/56.7287407 secs/batch = 0.2659s, grad.norm=17.10296059
 26765: 20 [  225/ 1327], train_loss/perplexity = 4.25254536/70.2840805 secs/batch = 0.2635s, grad.norm=17.86185265
 26770: 20 [  230/ 1327], train_loss/perplexity = 3.99676013/54.4215469 secs/batch = 0.2675s, grad.norm=18.57138634
 26775: 20 [  235/ 1327], train_loss/perplexity = 3.96537113/52.7398376 secs/batch = 0.2664s, grad.norm=18.12003708
 26780: 20 [  240/ 1327], train_loss/perplexity = 3.72298479/41.3877449 secs/batch = 0.2660s, grad.norm=17.86738968
 26785: 20 [  245/ 1327], train_loss/perplexity = 3.94587970/51.7218170 secs/batch = 0.2662s, grad.norm=17.82465935
 26790: 20 [  250/ 1327], train_loss/perplexity = 3.89118791/48.9690208 secs/batch = 0.2670s, grad.norm=16.90199852
 26795: 20 [  255/ 1327], train_loss/perplexity = 3.80444932/44.9005165 secs/batch = 0.2662s, grad.norm=17.55035400
 26800: 20 [  260/ 1327], train_loss/perplexity = 4.03968811/56.8086205 secs/batch = 0.2664s, grad.norm=19.04087639
 26805: 20 [  265/ 1327], train_loss/perplexity = 4.16354465/64.2990341 secs/batch = 0.2668s, grad.norm=17.37039185
 26810: 20 [  270/ 1327], train_loss/perplexity = 4.21419048/67.6393890 secs/batch = 0.2670s, grad.norm=17.60874939
 26815: 20 [  275/ 1327], train_loss/perplexity = 4.19495344/66.3506393 secs/batch = 0.2659s, grad.norm=17.59011459
 26820: 20 [  280/ 1327], train_loss/perplexity = 4.03044128/56.2857437 secs/batch = 0.2644s, grad.norm=17.09329605
 26825: 20 [  285/ 1327], train_loss/perplexity = 4.39866638/81.3423157 secs/batch = 0.2661s, grad.norm=17.89743233
 26830: 20 [  290/ 1327], train_loss/perplexity = 3.99611068/54.3862114 secs/batch = 0.2659s, grad.norm=18.29554367
 26835: 20 [  295/ 1327], train_loss/perplexity = 3.76348972/43.0985641 secs/batch = 0.2662s, grad.norm=17.68574905
 26840: 20 [  300/ 1327], train_loss/perplexity = 3.37971163/29.3623028 secs/batch = 0.2657s, grad.norm=17.23085594
 26845: 20 [  305/ 1327], train_loss/perplexity = 3.89045739/48.9332619 secs/batch = 0.2642s, grad.norm=17.18890953
 26850: 20 [  310/ 1327], train_loss/perplexity = 3.81555510/45.4019508 secs/batch = 0.2662s, grad.norm=17.62270737
 26855: 20 [  315/ 1327], train_loss/perplexity = 3.38327837/29.4672165 secs/batch = 0.2657s, grad.norm=16.32357788
 26860: 20 [  320/ 1327], train_loss/perplexity = 3.33944082/28.2033520 secs/batch = 0.2632s, grad.norm=18.07003975
 26865: 20 [  325/ 1327], train_loss/perplexity = 3.36012030/28.7926540 secs/batch = 0.2670s, grad.norm=16.46541023
 26870: 20 [  330/ 1327], train_loss/perplexity = 4.00119829/54.6636124 secs/batch = 0.2668s, grad.norm=17.92843819
 26875: 20 [  335/ 1327], train_loss/perplexity = 3.41319680/30.3621502 secs/batch = 0.2660s, grad.norm=16.37916756
 26880: 20 [  340/ 1327], train_loss/perplexity = 4.13430452/62.4461479 secs/batch = 0.2666s, grad.norm=17.67852402
 26885: 20 [  345/ 1327], train_loss/perplexity = 3.91291785/50.0447617 secs/batch = 0.2661s, grad.norm=16.73748589
 26890: 20 [  350/ 1327], train_loss/perplexity = 3.86723256/47.8098907 secs/batch = 0.2657s, grad.norm=17.64262009
 26895: 20 [  355/ 1327], train_loss/perplexity = 3.90344739/49.5730515 secs/batch = 0.2657s, grad.norm=17.20043373
 26900: 20 [  360/ 1327], train_loss/perplexity = 3.94121933/51.4813347 secs/batch = 0.2651s, grad.norm=18.39582062
 26905: 20 [  365/ 1327], train_loss/perplexity = 4.03776264/56.6993446 secs/batch = 0.2662s, grad.norm=17.65907860
 26910: 20 [  370/ 1327], train_loss/perplexity = 4.11516476/61.2623062 secs/batch = 0.2669s, grad.norm=17.98947334
 26915: 20 [  375/ 1327], train_loss/perplexity = 3.49563265/32.9711418 secs/batch = 0.2672s, grad.norm=17.31759071
 26920: 20 [  380/ 1327], train_loss/perplexity = 3.52907658/34.0924721 secs/batch = 0.2655s, grad.norm=17.39440727
 26925: 20 [  385/ 1327], train_loss/perplexity = 3.71665931/41.1267738 secs/batch = 0.2667s, grad.norm=18.13462448
 26930: 20 [  390/ 1327], train_loss/perplexity = 3.94763327/51.8125954 secs/batch = 0.2666s, grad.norm=17.69782066
 26935: 20 [  395/ 1327], train_loss/perplexity = 3.91275740/50.0367317 secs/batch = 0.2647s, grad.norm=17.47826385
 26940: 20 [  400/ 1327], train_loss/perplexity = 3.91402578/50.1002388 secs/batch = 0.2666s, grad.norm=17.03907585
 26945: 20 [  405/ 1327], train_loss/perplexity = 4.08760500/59.5969849 secs/batch = 0.2589s, grad.norm=17.45402718
 26950: 20 [  410/ 1327], train_loss/perplexity = 3.78499460/44.0354347 secs/batch = 0.2661s, grad.norm=17.49969673
 26955: 20 [  415/ 1327], train_loss/perplexity = 3.76754975/43.2739029 secs/batch = 0.2650s, grad.norm=17.74239731
 26960: 20 [  420/ 1327], train_loss/perplexity = 3.46707845/32.0429916 secs/batch = 0.2680s, grad.norm=17.37459755
 26965: 20 [  425/ 1327], train_loss/perplexity = 3.69784498/40.3602333 secs/batch = 0.2611s, grad.norm=17.98076630
 26970: 20 [  430/ 1327], train_loss/perplexity = 3.91457844/50.1279335 secs/batch = 0.2655s, grad.norm=18.29553604
 26975: 20 [  435/ 1327], train_loss/perplexity = 3.97770810/53.3945198 secs/batch = 0.2654s, grad.norm=18.41020203
 26980: 20 [  440/ 1327], train_loss/perplexity = 3.50425935/33.2568016 secs/batch = 0.2632s, grad.norm=17.67145538
 26985: 20 [  445/ 1327], train_loss/perplexity = 3.89971495/49.3883705 secs/batch = 0.2639s, grad.norm=17.70352936
 26990: 20 [  450/ 1327], train_loss/perplexity = 3.92711687/50.7604179 secs/batch = 0.2666s, grad.norm=17.70461845
 26995: 20 [  455/ 1327], train_loss/perplexity = 3.80710459/45.0198975 secs/batch = 0.2660s, grad.norm=17.20864487
 27000: 20 [  460/ 1327], train_loss/perplexity = 3.80226827/44.8026924 secs/batch = 0.2661s, grad.norm=17.93171883
 27005: 20 [  465/ 1327], train_loss/perplexity = 3.49584866/32.9782639 secs/batch = 0.2633s, grad.norm=18.38980103
 27010: 20 [  470/ 1327], train_loss/perplexity = 4.24950123/70.0704575 secs/batch = 0.2642s, grad.norm=17.51066208
 27015: 20 [  475/ 1327], train_loss/perplexity = 3.68055725/39.6684914 secs/batch = 0.2668s, grad.norm=17.54916382
 27020: 20 [  480/ 1327], train_loss/perplexity = 3.81540775/45.3952637 secs/batch = 0.2663s, grad.norm=17.92585564
 27025: 20 [  485/ 1327], train_loss/perplexity = 3.75490403/42.7301178 secs/batch = 0.2661s, grad.norm=17.92021370
 27030: 20 [  490/ 1327], train_loss/perplexity = 3.67151642/39.3114738 secs/batch = 0.2660s, grad.norm=18.91996002
 27035: 20 [  495/ 1327], train_loss/perplexity = 3.74937654/42.4945793 secs/batch = 0.2643s, grad.norm=17.34008026
 27040: 20 [  500/ 1327], train_loss/perplexity = 3.91981912/50.3913307 secs/batch = 0.2665s, grad.norm=17.55865860
 27045: 20 [  505/ 1327], train_loss/perplexity = 4.01342201/55.3359070 secs/batch = 0.2656s, grad.norm=16.75016594
 27050: 20 [  510/ 1327], train_loss/perplexity = 4.36606169/78.7329483 secs/batch = 0.2654s, grad.norm=16.90919876
 27055: 20 [  515/ 1327], train_loss/perplexity = 4.00931168/55.1089249 secs/batch = 0.2663s, grad.norm=17.00653458
 27060: 20 [  520/ 1327], train_loss/perplexity = 4.18586922/65.7506256 secs/batch = 0.2679s, grad.norm=17.71937943
 27065: 20 [  525/ 1327], train_loss/perplexity = 3.77205634/43.4693604 secs/batch = 0.2658s, grad.norm=17.45348930
 27070: 20 [  530/ 1327], train_loss/perplexity = 3.78458953/44.0175972 secs/batch = 0.2670s, grad.norm=17.42766762
 27075: 20 [  535/ 1327], train_loss/perplexity = 3.90235782/49.5190697 secs/batch = 0.2658s, grad.norm=17.37380981
 27080: 20 [  540/ 1327], train_loss/perplexity = 3.92552519/50.6796875 secs/batch = 0.2648s, grad.norm=16.97082710
 27085: 20 [  545/ 1327], train_loss/perplexity = 3.97410727/53.2025986 secs/batch = 0.2658s, grad.norm=17.53495407
 27090: 20 [  550/ 1327], train_loss/perplexity = 3.88978219/48.9002342 secs/batch = 0.2668s, grad.norm=17.44532394
 27095: 20 [  555/ 1327], train_loss/perplexity = 3.72195125/41.3449898 secs/batch = 0.2654s, grad.norm=16.37867165
 27100: 20 [  560/ 1327], train_loss/perplexity = 3.88982987/48.9025650 secs/batch = 0.2668s, grad.norm=17.80820274
 27105: 20 [  565/ 1327], train_loss/perplexity = 3.71820545/41.1904106 secs/batch = 0.2665s, grad.norm=18.48914337
 27110: 20 [  570/ 1327], train_loss/perplexity = 3.70830655/40.7846794 secs/batch = 0.2665s, grad.norm=18.08816338
 27115: 20 [  575/ 1327], train_loss/perplexity = 3.63858294/38.0378952 secs/batch = 0.2674s, grad.norm=17.73423195
 27120: 20 [  580/ 1327], train_loss/perplexity = 3.95376420/52.1312294 secs/batch = 0.2654s, grad.norm=18.12936020
 27125: 20 [  585/ 1327], train_loss/perplexity = 3.57985234/35.8682442 secs/batch = 0.2672s, grad.norm=17.12112999
 27130: 20 [  590/ 1327], train_loss/perplexity = 3.97532654/53.2675095 secs/batch = 0.2662s, grad.norm=17.44202232
 27135: 20 [  595/ 1327], train_loss/perplexity = 3.84972072/46.9799423 secs/batch = 0.2668s, grad.norm=17.88668060
 27140: 20 [  600/ 1327], train_loss/perplexity = 4.11050177/60.9773064 secs/batch = 0.2678s, grad.norm=17.15841103
 27145: 20 [  605/ 1327], train_loss/perplexity = 4.00819492/55.0474167 secs/batch = 0.2668s, grad.norm=17.17594147
 27150: 20 [  610/ 1327], train_loss/perplexity = 4.17424250/64.9905930 secs/batch = 0.2631s, grad.norm=17.27039337
 27155: 20 [  615/ 1327], train_loss/perplexity = 3.71705723/41.1431427 secs/batch = 0.2640s, grad.norm=17.09365845
 27160: 20 [  620/ 1327], train_loss/perplexity = 4.11392784/61.1865768 secs/batch = 0.2659s, grad.norm=17.46999168
 27165: 20 [  625/ 1327], train_loss/perplexity = 4.01031113/55.1640320 secs/batch = 0.2665s, grad.norm=17.42324448
 27170: 20 [  630/ 1327], train_loss/perplexity = 4.14769745/63.2881088 secs/batch = 0.2670s, grad.norm=17.32888222
 27175: 20 [  635/ 1327], train_loss/perplexity = 3.81227279/45.2531738 secs/batch = 0.2645s, grad.norm=17.53181839
 27180: 20 [  640/ 1327], train_loss/perplexity = 3.87215734/48.0459251 secs/batch = 0.2666s, grad.norm=17.30685425
 27185: 20 [  645/ 1327], train_loss/perplexity = 4.16652441/64.4909210 secs/batch = 0.2656s, grad.norm=18.68973923
 27190: 20 [  650/ 1327], train_loss/perplexity = 3.62152243/37.3944550 secs/batch = 0.2661s, grad.norm=17.36074448
 27195: 20 [  655/ 1327], train_loss/perplexity = 3.86550283/47.7272644 secs/batch = 0.2597s, grad.norm=17.89785957
 27200: 20 [  660/ 1327], train_loss/perplexity = 3.73301911/41.8051338 secs/batch = 0.2665s, grad.norm=17.65180016
 27205: 20 [  665/ 1327], train_loss/perplexity = 3.83428478/46.2603302 secs/batch = 0.2640s, grad.norm=17.47477341
 27210: 20 [  670/ 1327], train_loss/perplexity = 3.81106496/45.1985474 secs/batch = 0.2659s, grad.norm=17.58575249
 27215: 20 [  675/ 1327], train_loss/perplexity = 3.56782579/35.4394569 secs/batch = 0.2656s, grad.norm=17.46241379
 27220: 20 [  680/ 1327], train_loss/perplexity = 3.82251978/45.7192650 secs/batch = 0.2626s, grad.norm=18.10872841
 27225: 20 [  685/ 1327], train_loss/perplexity = 3.61677504/37.2173500 secs/batch = 0.2659s, grad.norm=17.24553871
 27230: 20 [  690/ 1327], train_loss/perplexity = 4.03518200/56.5532112 secs/batch = 0.2660s, grad.norm=16.89921761
 27235: 20 [  695/ 1327], train_loss/perplexity = 3.85127234/47.0528908 secs/batch = 0.2667s, grad.norm=17.53823090
 27240: 20 [  700/ 1327], train_loss/perplexity = 4.13954544/62.7742805 secs/batch = 0.2668s, grad.norm=17.92604446
 27245: 20 [  705/ 1327], train_loss/perplexity = 3.82106352/45.6527367 secs/batch = 0.2662s, grad.norm=16.91061592
 27250: 20 [  710/ 1327], train_loss/perplexity = 3.76747489/43.2706642 secs/batch = 0.2633s, grad.norm=18.01930428
 27255: 20 [  715/ 1327], train_loss/perplexity = 3.57996583/35.8723145 secs/batch = 0.2660s, grad.norm=17.15998840
 27260: 20 [  720/ 1327], train_loss/perplexity = 3.58569455/36.0784073 secs/batch = 0.2669s, grad.norm=18.13709259
 27265: 20 [  725/ 1327], train_loss/perplexity = 3.67090988/39.2876358 secs/batch = 0.2643s, grad.norm=17.54152870
 27270: 20 [  730/ 1327], train_loss/perplexity = 3.81755304/45.4927521 secs/batch = 0.2668s, grad.norm=17.82127190
 27275: 20 [  735/ 1327], train_loss/perplexity = 3.88097763/48.4715805 secs/batch = 0.2665s, grad.norm=18.23759651
 27280: 20 [  740/ 1327], train_loss/perplexity = 3.44338989/31.2928581 secs/batch = 0.2671s, grad.norm=16.84775162
 27285: 20 [  745/ 1327], train_loss/perplexity = 3.84981155/46.9842072 secs/batch = 0.2593s, grad.norm=17.80212021
 27290: 20 [  750/ 1327], train_loss/perplexity = 3.74578071/42.3420525 secs/batch = 0.2671s, grad.norm=17.73992729
 27295: 20 [  755/ 1327], train_loss/perplexity = 3.58488584/36.0492401 secs/batch = 0.2661s, grad.norm=16.79853821
 27300: 20 [  760/ 1327], train_loss/perplexity = 3.45512724/31.6623173 secs/batch = 0.2649s, grad.norm=16.30310249
 27305: 20 [  765/ 1327], train_loss/perplexity = 3.58477354/36.0451927 secs/batch = 0.2664s, grad.norm=16.55829811
 27310: 20 [  770/ 1327], train_loss/perplexity = 3.56904197/35.4825821 secs/batch = 0.2651s, grad.norm=16.95835114
 27315: 20 [  775/ 1327], train_loss/perplexity = 3.64124513/38.1392937 secs/batch = 0.2664s, grad.norm=17.70635986
 27320: 20 [  780/ 1327], train_loss/perplexity = 4.00331974/54.7797012 secs/batch = 0.2637s, grad.norm=18.17980194
 27325: 20 [  785/ 1327], train_loss/perplexity = 3.89096546/48.9581299 secs/batch = 0.2654s, grad.norm=17.93515205
 27330: 20 [  790/ 1327], train_loss/perplexity = 3.65157962/38.5354881 secs/batch = 0.2668s, grad.norm=17.76695442
 27335: 20 [  795/ 1327], train_loss/perplexity = 3.98630285/53.8554077 secs/batch = 0.2661s, grad.norm=17.60225868
 27340: 20 [  800/ 1327], train_loss/perplexity = 3.86123919/47.5242081 secs/batch = 0.2653s, grad.norm=17.60075569
 27345: 20 [  805/ 1327], train_loss/perplexity = 4.24050903/69.4431915 secs/batch = 0.2656s, grad.norm=17.89785957
 27350: 20 [  810/ 1327], train_loss/perplexity = 3.77450514/43.5759392 secs/batch = 0.2659s, grad.norm=16.74921799
 27355: 20 [  815/ 1327], train_loss/perplexity = 3.72285438/41.3823471 secs/batch = 0.2659s, grad.norm=17.08022118
 27360: 20 [  820/ 1327], train_loss/perplexity = 3.62906337/37.6775093 secs/batch = 0.2654s, grad.norm=16.76570892
 27365: 20 [  825/ 1327], train_loss/perplexity = 3.80521035/44.9347000 secs/batch = 0.2667s, grad.norm=17.67150116
 27370: 20 [  830/ 1327], train_loss/perplexity = 3.49245834/32.8666458 secs/batch = 0.2655s, grad.norm=17.43173981
 27375: 20 [  835/ 1327], train_loss/perplexity = 3.72134757/41.3200378 secs/batch = 0.2657s, grad.norm=17.59127808
 27380: 20 [  840/ 1327], train_loss/perplexity = 3.93290901/51.0552826 secs/batch = 0.2646s, grad.norm=17.71054840
 27385: 20 [  845/ 1327], train_loss/perplexity = 3.66712618/39.1392632 secs/batch = 0.2660s, grad.norm=17.79573822
 27390: 20 [  850/ 1327], train_loss/perplexity = 3.78516912/44.0431175 secs/batch = 0.2653s, grad.norm=17.23946571
 27395: 20 [  855/ 1327], train_loss/perplexity = 3.74370480/42.2542458 secs/batch = 0.2670s, grad.norm=17.83378601
 27400: 20 [  860/ 1327], train_loss/perplexity = 3.49632645/32.9940224 secs/batch = 0.2658s, grad.norm=16.44478798
 27405: 20 [  865/ 1327], train_loss/perplexity = 4.00008535/54.6028099 secs/batch = 0.2642s, grad.norm=17.60937309
 27410: 20 [  870/ 1327], train_loss/perplexity = 3.84114575/46.5788116 secs/batch = 0.2608s, grad.norm=18.28456497
 27415: 20 [  875/ 1327], train_loss/perplexity = 3.46481895/31.9706707 secs/batch = 0.2652s, grad.norm=17.00078964
 27420: 20 [  880/ 1327], train_loss/perplexity = 3.65678477/38.7365952 secs/batch = 0.2618s, grad.norm=17.12639046
 27425: 20 [  885/ 1327], train_loss/perplexity = 3.78377080/43.9815750 secs/batch = 0.2671s, grad.norm=17.26645660
 27430: 20 [  890/ 1327], train_loss/perplexity = 3.96462917/52.7007217 secs/batch = 0.2623s, grad.norm=17.25498199
 27435: 20 [  895/ 1327], train_loss/perplexity = 3.90167546/49.4852905 secs/batch = 0.2641s, grad.norm=17.07225800
 27440: 20 [  900/ 1327], train_loss/perplexity = 3.81481075/45.3681679 secs/batch = 0.2651s, grad.norm=17.16549683
 27445: 20 [  905/ 1327], train_loss/perplexity = 3.68220258/39.7338142 secs/batch = 0.2652s, grad.norm=16.68358994
 27450: 20 [  910/ 1327], train_loss/perplexity = 3.78529263/44.0485573 secs/batch = 0.2601s, grad.norm=16.09963989
 27455: 20 [  915/ 1327], train_loss/perplexity = 3.94342422/51.5949707 secs/batch = 0.2639s, grad.norm=16.78976250
 27460: 20 [  920/ 1327], train_loss/perplexity = 4.11857653/61.4716759 secs/batch = 0.2657s, grad.norm=17.82442093
 27465: 20 [  925/ 1327], train_loss/perplexity = 3.86549592/47.7269363 secs/batch = 0.2667s, grad.norm=17.21898842
 27470: 20 [  930/ 1327], train_loss/perplexity = 3.93143177/50.9799156 secs/batch = 0.2640s, grad.norm=17.02882957
 27475: 20 [  935/ 1327], train_loss/perplexity = 3.95218992/52.0492249 secs/batch = 0.2655s, grad.norm=17.20700645
 27480: 20 [  940/ 1327], train_loss/perplexity = 3.91995502/50.3981781 secs/batch = 0.2663s, grad.norm=16.49888420
 27485: 20 [  945/ 1327], train_loss/perplexity = 4.16279840/64.2510681 secs/batch = 0.2599s, grad.norm=17.21739769
 27490: 20 [  950/ 1327], train_loss/perplexity = 3.95071507/51.9725189 secs/batch = 0.2647s, grad.norm=17.07901382
 27495: 20 [  955/ 1327], train_loss/perplexity = 3.81137586/45.2126007 secs/batch = 0.2661s, grad.norm=17.20869446
 27500: 20 [  960/ 1327], train_loss/perplexity = 4.21013689/67.3657608 secs/batch = 0.2661s, grad.norm=18.03695488
 27505: 20 [  965/ 1327], train_loss/perplexity = 3.90204334/49.5034981 secs/batch = 0.2660s, grad.norm=17.62256622
 27510: 20 [  970/ 1327], train_loss/perplexity = 4.15376759/63.6734428 secs/batch = 0.2643s, grad.norm=17.86908340
 27515: 20 [  975/ 1327], train_loss/perplexity = 3.80816197/45.0675278 secs/batch = 0.2640s, grad.norm=18.33252335
 27520: 20 [  980/ 1327], train_loss/perplexity = 3.63305330/37.8281403 secs/batch = 0.2659s, grad.norm=17.17121696
 27525: 20 [  985/ 1327], train_loss/perplexity = 3.81763840/45.4966354 secs/batch = 0.2649s, grad.norm=17.66108322
 27530: 20 [  990/ 1327], train_loss/perplexity = 3.99159241/54.1410370 secs/batch = 0.2659s, grad.norm=18.18823433
 27535: 20 [  995/ 1327], train_loss/perplexity = 4.06043530/57.9995537 secs/batch = 0.2634s, grad.norm=17.74853134
 27540: 20 [ 1000/ 1327], train_loss/perplexity = 3.53946924/34.4486313 secs/batch = 0.2662s, grad.norm=16.85933304
 27545: 20 [ 1005/ 1327], train_loss/perplexity = 4.02013636/55.7087021 secs/batch = 0.2663s, grad.norm=17.13211250
 27550: 20 [ 1010/ 1327], train_loss/perplexity = 3.54518342/34.6460381 secs/batch = 0.2663s, grad.norm=16.39471245
 27555: 20 [ 1015/ 1327], train_loss/perplexity = 4.03002739/56.2624512 secs/batch = 0.2662s, grad.norm=16.82095528
 27560: 20 [ 1020/ 1327], train_loss/perplexity = 4.11170197/61.0505333 secs/batch = 0.2672s, grad.norm=17.07686806
 27565: 20 [ 1025/ 1327], train_loss/perplexity = 4.09326744/59.9354057 secs/batch = 0.2660s, grad.norm=17.14208794
 27570: 20 [ 1030/ 1327], train_loss/perplexity = 3.78716493/44.1311073 secs/batch = 0.2661s, grad.norm=16.56850433
 27575: 20 [ 1035/ 1327], train_loss/perplexity = 3.78193259/43.9008026 secs/batch = 0.2659s, grad.norm=16.69086075
 27580: 20 [ 1040/ 1327], train_loss/perplexity = 3.97801614/53.4109688 secs/batch = 0.2650s, grad.norm=17.69743347
 27585: 20 [ 1045/ 1327], train_loss/perplexity = 3.51952863/33.7685089 secs/batch = 0.2657s, grad.norm=16.37064171
 27590: 20 [ 1050/ 1327], train_loss/perplexity = 3.60576653/36.8098907 secs/batch = 0.2666s, grad.norm=16.77010918
 27595: 20 [ 1055/ 1327], train_loss/perplexity = 3.70855284/40.7947273 secs/batch = 0.2662s, grad.norm=18.08217239
 27600: 20 [ 1060/ 1327], train_loss/perplexity = 3.36459708/28.9218426 secs/batch = 0.2668s, grad.norm=17.75564003
 27605: 20 [ 1065/ 1327], train_loss/perplexity = 3.51111794/33.4856834 secs/batch = 0.2661s, grad.norm=17.50926399
 27610: 20 [ 1070/ 1327], train_loss/perplexity = 3.81598806/45.4216118 secs/batch = 0.2655s, grad.norm=17.87095451
 27615: 20 [ 1075/ 1327], train_loss/perplexity = 3.55165148/34.8708572 secs/batch = 0.2648s, grad.norm=16.83669662
 27620: 20 [ 1080/ 1327], train_loss/perplexity = 3.56928301/35.4911385 secs/batch = 0.2651s, grad.norm=17.06535530
 27625: 20 [ 1085/ 1327], train_loss/perplexity = 3.44333243/31.2910595 secs/batch = 0.2668s, grad.norm=17.34330559
 27630: 20 [ 1090/ 1327], train_loss/perplexity = 3.63427901/37.8745346 secs/batch = 0.2665s, grad.norm=17.75475502
 27635: 20 [ 1095/ 1327], train_loss/perplexity = 3.78054333/43.8398552 secs/batch = 0.2664s, grad.norm=18.41969490
 27640: 20 [ 1100/ 1327], train_loss/perplexity = 3.44190812/31.2465229 secs/batch = 0.2599s, grad.norm=18.48666573
 27645: 20 [ 1105/ 1327], train_loss/perplexity = 3.47398758/32.2651443 secs/batch = 0.2652s, grad.norm=17.46596718
 27650: 20 [ 1110/ 1327], train_loss/perplexity = 3.78102922/43.8611603 secs/batch = 0.2667s, grad.norm=17.99687958
 27655: 20 [ 1115/ 1327], train_loss/perplexity = 3.54356861/34.5901375 secs/batch = 0.2662s, grad.norm=16.94704056
 27660: 20 [ 1120/ 1327], train_loss/perplexity = 3.88116002/48.4804192 secs/batch = 0.2671s, grad.norm=17.19305229
 27665: 20 [ 1125/ 1327], train_loss/perplexity = 3.99635601/54.3995552 secs/batch = 0.2661s, grad.norm=18.68063927
 27670: 20 [ 1130/ 1327], train_loss/perplexity = 3.66251445/38.9591789 secs/batch = 0.2636s, grad.norm=17.59207153
 27675: 20 [ 1135/ 1327], train_loss/perplexity = 3.72560835/41.4964676 secs/batch = 0.2666s, grad.norm=17.39669037
 27680: 20 [ 1140/ 1327], train_loss/perplexity = 3.92485404/50.6456871 secs/batch = 0.2662s, grad.norm=18.51505280
 27685: 20 [ 1145/ 1327], train_loss/perplexity = 3.79298234/44.3885841 secs/batch = 0.2660s, grad.norm=16.96560478
 27690: 20 [ 1150/ 1327], train_loss/perplexity = 3.76428294/43.1327667 secs/batch = 0.2649s, grad.norm=17.29936981
 27695: 20 [ 1155/ 1327], train_loss/perplexity = 3.75639629/42.7939301 secs/batch = 0.2681s, grad.norm=17.52544022
 27700: 20 [ 1160/ 1327], train_loss/perplexity = 3.77207637/43.4702301 secs/batch = 0.2655s, grad.norm=17.27627182
 27705: 20 [ 1165/ 1327], train_loss/perplexity = 3.78186536/43.8978500 secs/batch = 0.2663s, grad.norm=17.76072121
 27710: 20 [ 1170/ 1327], train_loss/perplexity = 3.61751246/37.2448044 secs/batch = 0.2668s, grad.norm=17.48665047
 27715: 20 [ 1175/ 1327], train_loss/perplexity = 3.49823427/33.0570297 secs/batch = 0.2665s, grad.norm=16.80068779
 27720: 20 [ 1180/ 1327], train_loss/perplexity = 3.47675705/32.3546257 secs/batch = 0.2649s, grad.norm=17.57995415
 27725: 20 [ 1185/ 1327], train_loss/perplexity = 3.64542866/38.2991867 secs/batch = 0.2655s, grad.norm=17.73587799
 27730: 20 [ 1190/ 1327], train_loss/perplexity = 3.72886658/41.6318932 secs/batch = 0.2629s, grad.norm=17.72043228
 27735: 20 [ 1195/ 1327], train_loss/perplexity = 3.57786775/35.7971306 secs/batch = 0.2667s, grad.norm=17.14141273
 27740: 20 [ 1200/ 1327], train_loss/perplexity = 3.50328255/33.2243347 secs/batch = 0.2657s, grad.norm=17.09693718
 27745: 20 [ 1205/ 1327], train_loss/perplexity = 3.57351804/35.6417618 secs/batch = 0.2674s, grad.norm=17.22214127
 27750: 20 [ 1210/ 1327], train_loss/perplexity = 3.11976147/22.6409779 secs/batch = 0.2656s, grad.norm=16.96568298
 27755: 20 [ 1215/ 1327], train_loss/perplexity = 3.39198780/29.7249813 secs/batch = 0.2662s, grad.norm=16.48074722
 27760: 20 [ 1220/ 1327], train_loss/perplexity = 3.54812670/34.7481613 secs/batch = 0.2663s, grad.norm=17.85467720
 27765: 20 [ 1225/ 1327], train_loss/perplexity = 3.27057266/26.3264103 secs/batch = 0.2668s, grad.norm=18.22700310
 27770: 20 [ 1230/ 1327], train_loss/perplexity = 3.61530614/37.1627197 secs/batch = 0.2662s, grad.norm=16.94967461
 27775: 20 [ 1235/ 1327], train_loss/perplexity = 3.49793935/33.0472832 secs/batch = 0.2654s, grad.norm=16.93300629
 27780: 20 [ 1240/ 1327], train_loss/perplexity = 3.74184036/42.1755371 secs/batch = 0.2668s, grad.norm=17.71423340
 27785: 20 [ 1245/ 1327], train_loss/perplexity = 3.67618752/39.4955292 secs/batch = 0.2662s, grad.norm=16.98584175
 27790: 20 [ 1250/ 1327], train_loss/perplexity = 3.76666307/43.2355499 secs/batch = 0.2662s, grad.norm=16.81469917
 27795: 20 [ 1255/ 1327], train_loss/perplexity = 3.84836030/46.9160728 secs/batch = 0.2642s, grad.norm=17.07601166
 27800: 20 [ 1260/ 1327], train_loss/perplexity = 3.61308050/37.0801010 secs/batch = 0.2651s, grad.norm=18.41871643
 27805: 20 [ 1265/ 1327], train_loss/perplexity = 3.73878312/42.0467949 secs/batch = 0.2645s, grad.norm=17.03220558
 27810: 20 [ 1270/ 1327], train_loss/perplexity = 3.55184436/34.8775864 secs/batch = 0.2639s, grad.norm=17.53825188
 27815: 20 [ 1275/ 1327], train_loss/perplexity = 3.66638899/39.1104240 secs/batch = 0.2669s, grad.norm=18.14570045
 27820: 20 [ 1280/ 1327], train_loss/perplexity = 3.65663099/38.7306404 secs/batch = 0.2652s, grad.norm=18.28626633
 27825: 20 [ 1285/ 1327], train_loss/perplexity = 3.46810699/32.0759659 secs/batch = 0.2656s, grad.norm=17.26921082
 27830: 20 [ 1290/ 1327], train_loss/perplexity = 3.73927021/42.0672798 secs/batch = 0.2657s, grad.norm=17.28786087
 27835: 20 [ 1295/ 1327], train_loss/perplexity = 3.70983505/40.8470688 secs/batch = 0.2664s, grad.norm=17.10187912
 27840: 20 [ 1300/ 1327], train_loss/perplexity = 3.96810913/52.8844376 secs/batch = 0.2663s, grad.norm=17.26858139
 27845: 20 [ 1305/ 1327], train_loss/perplexity = 3.95511198/52.2015381 secs/batch = 0.2646s, grad.norm=17.74958992
 27850: 20 [ 1310/ 1327], train_loss/perplexity = 4.16986847/64.7069397 secs/batch = 0.2664s, grad.norm=18.13037491
 27855: 20 [ 1315/ 1327], train_loss/perplexity = 4.03765678/56.6933441 secs/batch = 0.2650s, grad.norm=17.82275581
 27860: 20 [ 1320/ 1327], train_loss/perplexity = 3.90579677/49.6896553 secs/batch = 0.2661s, grad.norm=17.31402016
 27865: 20 [ 1325/ 1327], train_loss/perplexity = 3.85482693/47.2204437 secs/batch = 0.2660s, grad.norm=18.10188293
Epoch training time: 352.62103843688965
	> validation loss = 4.57526398, perplexity = 97.05365753
	> validation loss = 4.52209759, perplexity = 92.02843475
	> validation loss = 4.48266506, perplexity = 88.47013855
	> validation loss = 4.56206226, perplexity = 95.78079987
	> validation loss = 4.66774607, perplexity = 106.45752716
	> validation loss = 4.62038612, perplexity = 101.53322601
	> validation loss = 4.54494286, perplexity = 94.15504456
	> validation loss = 4.38242102, perplexity = 80.03155518
	> validation loss = 4.18226910, perplexity = 65.51434326
	> validation loss = 4.27832127, perplexity = 72.11927032
	> validation loss = 4.47349930, perplexity = 87.66294861
	> validation loss = 4.46437693, perplexity = 86.86688995
	> validation loss = 4.44819832, perplexity = 85.47280884
	> validation loss = 4.18269253, perplexity = 65.54209137
	> validation loss = 4.14211035, perplexity = 62.93549728
	> validation loss = 4.18910503, perplexity = 65.96372986
	> validation loss = 4.60657930, perplexity = 100.14101410
	> validation loss = 4.11243391, perplexity = 61.09523773
	> validation loss = 4.62446737, perplexity = 101.94845581
	> validation loss = 4.48488331, perplexity = 88.66660309
	> validation loss = 4.26356792, perplexity = 71.06307983
at the end of epoch: 20
train loss = 3.85681998, perplexity = 47.31465015
validation loss = 4.42420457, perplexity = 83.44640521
Saved model cv/epoch020_4.4242.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.015625
new learning rate is: 0.0078125
 27872: 21 [    5/ 1327], train_loss/perplexity = 4.05466557/57.6658745 secs/batch = 0.2671s, grad.norm=18.02541542
 27877: 21 [   10/ 1327], train_loss/perplexity = 3.69286275/40.1596489 secs/batch = 0.2664s, grad.norm=17.06980705
 27882: 21 [   15/ 1327], train_loss/perplexity = 4.02040243/55.7235260 secs/batch = 0.2657s, grad.norm=17.09435081
 27887: 21 [   20/ 1327], train_loss/perplexity = 4.24801445/69.9663544 secs/batch = 0.2657s, grad.norm=16.93839836
 27892: 21 [   25/ 1327], train_loss/perplexity = 4.01881027/55.6348763 secs/batch = 0.2656s, grad.norm=18.26209259
 27897: 21 [   30/ 1327], train_loss/perplexity = 4.02026749/55.7160072 secs/batch = 0.2638s, grad.norm=18.11404228
 27902: 21 [   35/ 1327], train_loss/perplexity = 3.87383008/48.1263618 secs/batch = 0.2660s, grad.norm=17.38951683
 27907: 21 [   40/ 1327], train_loss/perplexity = 3.87648535/48.2543182 secs/batch = 0.2651s, grad.norm=17.52138519
 27912: 21 [   45/ 1327], train_loss/perplexity = 3.72983694/41.6723137 secs/batch = 0.2658s, grad.norm=17.00015831
 27917: 21 [   50/ 1327], train_loss/perplexity = 3.87160349/48.0193214 secs/batch = 0.2671s, grad.norm=17.40817261
 27922: 21 [   55/ 1327], train_loss/perplexity = 3.83544016/46.3138084 secs/batch = 0.2657s, grad.norm=18.19524384
 27927: 21 [   60/ 1327], train_loss/perplexity = 3.99846029/54.5141487 secs/batch = 0.2653s, grad.norm=18.07652092
 27932: 21 [   65/ 1327], train_loss/perplexity = 3.67620468/39.4962082 secs/batch = 0.2654s, grad.norm=17.32219124
 27937: 21 [   70/ 1327], train_loss/perplexity = 3.48045993/32.4746552 secs/batch = 0.2662s, grad.norm=17.67811012
 27942: 21 [   75/ 1327], train_loss/perplexity = 3.36911845/29.0529041 secs/batch = 0.2660s, grad.norm=16.50648117
 27947: 21 [   80/ 1327], train_loss/perplexity = 3.78368020/43.9775925 secs/batch = 0.2630s, grad.norm=17.77900696
 27952: 21 [   85/ 1327], train_loss/perplexity = 3.82894969/46.0141830 secs/batch = 0.2656s, grad.norm=17.74417496
 27957: 21 [   90/ 1327], train_loss/perplexity = 3.88095522/48.4704933 secs/batch = 0.2647s, grad.norm=18.33527756
 27962: 21 [   95/ 1327], train_loss/perplexity = 3.75128269/42.5756569 secs/batch = 0.2657s, grad.norm=17.85717583
 27967: 21 [  100/ 1327], train_loss/perplexity = 4.00656366/54.9576912 secs/batch = 0.2640s, grad.norm=18.16791344
 27972: 21 [  105/ 1327], train_loss/perplexity = 3.83935189/46.4953308 secs/batch = 0.2635s, grad.norm=18.53804970
 27977: 21 [  110/ 1327], train_loss/perplexity = 3.68871164/39.9932899 secs/batch = 0.2662s, grad.norm=17.83467484
 27982: 21 [  115/ 1327], train_loss/perplexity = 3.69294381/40.1629066 secs/batch = 0.2666s, grad.norm=18.47771263
 27987: 21 [  120/ 1327], train_loss/perplexity = 3.78098917/43.8594055 secs/batch = 0.2663s, grad.norm=17.67139435
 27992: 21 [  125/ 1327], train_loss/perplexity = 3.77583265/43.6338234 secs/batch = 0.2633s, grad.norm=18.46174240
 27997: 21 [  130/ 1327], train_loss/perplexity = 3.71881962/41.2157135 secs/batch = 0.2599s, grad.norm=18.95088577
 28002: 21 [  135/ 1327], train_loss/perplexity = 3.76756835/43.2747078 secs/batch = 0.2668s, grad.norm=17.63249969
 28007: 21 [  140/ 1327], train_loss/perplexity = 4.07653141/58.9406738 secs/batch = 0.2660s, grad.norm=18.50789452
 28012: 21 [  145/ 1327], train_loss/perplexity = 3.91681051/50.2399483 secs/batch = 0.2649s, grad.norm=18.64127350
 28017: 21 [  150/ 1327], train_loss/perplexity = 4.06121111/58.0445671 secs/batch = 0.2654s, grad.norm=18.90339661
 28022: 21 [  155/ 1327], train_loss/perplexity = 4.26034737/70.8345871 secs/batch = 0.2665s, grad.norm=18.37669945
 28027: 21 [  160/ 1327], train_loss/perplexity = 3.84731841/46.8672142 secs/batch = 0.2629s, grad.norm=17.23501778
 28032: 21 [  165/ 1327], train_loss/perplexity = 4.07223892/58.6882133 secs/batch = 0.2624s, grad.norm=18.18581963
 28037: 21 [  170/ 1327], train_loss/perplexity = 3.77804208/43.7303391 secs/batch = 0.2659s, grad.norm=17.55839348
 28042: 21 [  175/ 1327], train_loss/perplexity = 4.09959126/60.3156281 secs/batch = 0.2669s, grad.norm=17.68663597
 28047: 21 [  180/ 1327], train_loss/perplexity = 3.94931221/51.8996582 secs/batch = 0.2663s, grad.norm=18.03167152
 28052: 21 [  185/ 1327], train_loss/perplexity = 4.26904011/71.4530182 secs/batch = 0.2678s, grad.norm=18.32005692
 28057: 21 [  190/ 1327], train_loss/perplexity = 3.84509397/46.7630768 secs/batch = 0.2631s, grad.norm=17.24227333
 28062: 21 [  195/ 1327], train_loss/perplexity = 4.12338305/61.7678528 secs/batch = 0.2676s, grad.norm=17.36343002
 28067: 21 [  200/ 1327], train_loss/perplexity = 3.95197892/52.0382462 secs/batch = 0.2659s, grad.norm=18.04654884
 28072: 21 [  205/ 1327], train_loss/perplexity = 4.14205742/62.9321671 secs/batch = 0.2668s, grad.norm=17.99198723
 28077: 21 [  210/ 1327], train_loss/perplexity = 4.02760220/56.1261711 secs/batch = 0.2684s, grad.norm=17.34897423
 28082: 21 [  215/ 1327], train_loss/perplexity = 4.15508270/63.7572365 secs/batch = 0.2661s, grad.norm=17.06489944
 28087: 21 [  220/ 1327], train_loss/perplexity = 4.01813316/55.5972176 secs/batch = 0.2611s, grad.norm=17.16440964
 28092: 21 [  225/ 1327], train_loss/perplexity = 4.22014952/68.0436554 secs/batch = 0.2665s, grad.norm=17.64432716
 28097: 21 [  230/ 1327], train_loss/perplexity = 4.05425978/57.6424789 secs/batch = 0.2650s, grad.norm=18.74261856
 28102: 21 [  235/ 1327], train_loss/perplexity = 3.95690870/52.2954140 secs/batch = 0.2670s, grad.norm=18.38124084
 28107: 21 [  240/ 1327], train_loss/perplexity = 3.68091464/39.6826744 secs/batch = 0.2656s, grad.norm=18.05344582
 28112: 21 [  245/ 1327], train_loss/perplexity = 3.98988318/54.0485764 secs/batch = 0.2661s, grad.norm=17.82185745
 28117: 21 [  250/ 1327], train_loss/perplexity = 3.82740641/45.9432259 secs/batch = 0.2669s, grad.norm=17.33305550
 28122: 21 [  255/ 1327], train_loss/perplexity = 3.76589847/43.2025032 secs/batch = 0.2654s, grad.norm=17.61439705
 28127: 21 [  260/ 1327], train_loss/perplexity = 3.93747663/51.2890167 secs/batch = 0.2662s, grad.norm=17.94823647
 28132: 21 [  265/ 1327], train_loss/perplexity = 4.19506550/66.3580780 secs/batch = 0.2666s, grad.norm=17.66418457
 28137: 21 [  270/ 1327], train_loss/perplexity = 4.25159979/70.2176590 secs/batch = 0.2664s, grad.norm=18.01569366
 28142: 21 [  275/ 1327], train_loss/perplexity = 4.24245405/69.5783920 secs/batch = 0.2642s, grad.norm=17.61451721
 28147: 21 [  280/ 1327], train_loss/perplexity = 4.03327656/56.4455566 secs/batch = 0.2644s, grad.norm=17.48859978
 28152: 21 [  285/ 1327], train_loss/perplexity = 4.39668798/81.1815491 secs/batch = 0.2661s, grad.norm=17.32523537
 28157: 21 [  290/ 1327], train_loss/perplexity = 4.02334833/55.8879242 secs/batch = 0.2670s, grad.norm=18.18162727
 28162: 21 [  295/ 1327], train_loss/perplexity = 3.80255461/44.8155251 secs/batch = 0.2596s, grad.norm=17.63938713
 28167: 21 [  300/ 1327], train_loss/perplexity = 3.34979439/28.4968739 secs/batch = 0.2663s, grad.norm=16.84011078
 28172: 21 [  305/ 1327], train_loss/perplexity = 3.90447664/49.6241035 secs/batch = 0.2666s, grad.norm=17.44479370
 28177: 21 [  310/ 1327], train_loss/perplexity = 3.82132721/45.6647758 secs/batch = 0.2679s, grad.norm=17.50049400
 28182: 21 [  315/ 1327], train_loss/perplexity = 3.46927166/32.1133461 secs/batch = 0.2668s, grad.norm=16.63263321
 28187: 21 [  320/ 1327], train_loss/perplexity = 3.34809899/28.4486008 secs/batch = 0.2630s, grad.norm=17.63743401
 28192: 21 [  325/ 1327], train_loss/perplexity = 3.34841275/28.4575291 secs/batch = 0.2659s, grad.norm=16.63332748
 28197: 21 [  330/ 1327], train_loss/perplexity = 3.96693945/52.8226166 secs/batch = 0.2660s, grad.norm=17.51873207
 28202: 21 [  335/ 1327], train_loss/perplexity = 3.37328005/29.1740627 secs/batch = 0.2613s, grad.norm=16.25473976
 28207: 21 [  340/ 1327], train_loss/perplexity = 4.14018011/62.8141327 secs/batch = 0.2660s, grad.norm=17.33542252
 28212: 21 [  345/ 1327], train_loss/perplexity = 3.90252256/49.5272255 secs/batch = 0.2664s, grad.norm=17.12062454
 28217: 21 [  350/ 1327], train_loss/perplexity = 3.81755209/45.4927101 secs/batch = 0.2669s, grad.norm=17.75021553
 28222: 21 [  355/ 1327], train_loss/perplexity = 3.92577195/50.6921959 secs/batch = 0.2672s, grad.norm=17.90741539
 28227: 21 [  360/ 1327], train_loss/perplexity = 3.97237396/53.1104622 secs/batch = 0.2665s, grad.norm=18.69555664
 28232: 21 [  365/ 1327], train_loss/perplexity = 4.04220009/56.9515038 secs/batch = 0.2662s, grad.norm=17.76964569
 28237: 21 [  370/ 1327], train_loss/perplexity = 4.07197189/58.6725464 secs/batch = 0.2657s, grad.norm=17.70269203
 28242: 21 [  375/ 1327], train_loss/perplexity = 3.49663687/33.0042686 secs/batch = 0.2657s, grad.norm=17.40129662
 28247: 21 [  380/ 1327], train_loss/perplexity = 3.58655095/36.1093178 secs/batch = 0.2661s, grad.norm=17.79376602
 28252: 21 [  385/ 1327], train_loss/perplexity = 3.72158742/41.3299484 secs/batch = 0.2656s, grad.norm=17.82601929
 28257: 21 [  390/ 1327], train_loss/perplexity = 3.84754658/46.8779106 secs/batch = 0.2679s, grad.norm=17.51288414
 28262: 21 [  395/ 1327], train_loss/perplexity = 3.94351530/51.5996704 secs/batch = 0.2641s, grad.norm=17.58331871
 28267: 21 [  400/ 1327], train_loss/perplexity = 3.90807891/49.8031845 secs/batch = 0.2662s, grad.norm=17.39764977
 28272: 21 [  405/ 1327], train_loss/perplexity = 4.17021847/64.7295914 secs/batch = 0.2660s, grad.norm=17.55624771
 28277: 21 [  410/ 1327], train_loss/perplexity = 3.79878807/44.6470413 secs/batch = 0.2661s, grad.norm=17.29809570
 28282: 21 [  415/ 1327], train_loss/perplexity = 3.79726982/44.5793076 secs/batch = 0.2664s, grad.norm=17.73462486
 28287: 21 [  420/ 1327], train_loss/perplexity = 3.41324806/30.3637066 secs/batch = 0.2632s, grad.norm=17.73283958
 28292: 21 [  425/ 1327], train_loss/perplexity = 3.79548645/44.4998779 secs/batch = 0.2656s, grad.norm=18.22965240
 28297: 21 [  430/ 1327], train_loss/perplexity = 3.92707849/50.7584686 secs/batch = 0.2669s, grad.norm=18.49481010
 28302: 21 [  435/ 1327], train_loss/perplexity = 4.04364300/57.0337372 secs/batch = 0.2663s, grad.norm=18.40318871
 28307: 21 [  440/ 1327], train_loss/perplexity = 3.52934980/34.1017876 secs/batch = 0.2656s, grad.norm=17.71490860
 28312: 21 [  445/ 1327], train_loss/perplexity = 3.84930849/46.9605789 secs/batch = 0.2614s, grad.norm=17.60331345
 28317: 21 [  450/ 1327], train_loss/perplexity = 3.86508131/47.7071495 secs/batch = 0.2640s, grad.norm=17.63446999
 28322: 21 [  455/ 1327], train_loss/perplexity = 3.87797594/48.3263016 secs/batch = 0.2600s, grad.norm=17.69816589
 28327: 21 [  460/ 1327], train_loss/perplexity = 3.76946068/43.3566742 secs/batch = 0.2668s, grad.norm=17.79383278
 28332: 21 [  465/ 1327], train_loss/perplexity = 3.50228524/33.1912155 secs/batch = 0.2660s, grad.norm=18.33603287
 28337: 21 [  470/ 1327], train_loss/perplexity = 4.26461267/71.1373596 secs/batch = 0.2654s, grad.norm=17.89554596
 28342: 21 [  475/ 1327], train_loss/perplexity = 3.69399691/40.2052231 secs/batch = 0.2657s, grad.norm=17.56545067
 28347: 21 [  480/ 1327], train_loss/perplexity = 3.79456902/44.4590721 secs/batch = 0.2657s, grad.norm=17.48874283
 28352: 21 [  485/ 1327], train_loss/perplexity = 3.74746442/42.4134026 secs/batch = 0.2660s, grad.norm=18.18723869
 28357: 21 [  490/ 1327], train_loss/perplexity = 3.63754559/37.9984589 secs/batch = 0.2662s, grad.norm=18.96912766
 28362: 21 [  495/ 1327], train_loss/perplexity = 3.74375296/42.2562790 secs/batch = 0.2658s, grad.norm=17.64636040
 28367: 21 [  500/ 1327], train_loss/perplexity = 3.84554672/46.7842560 secs/batch = 0.2637s, grad.norm=17.77696800
 28372: 21 [  505/ 1327], train_loss/perplexity = 3.97250462/53.1174049 secs/batch = 0.2660s, grad.norm=16.89072800
 28377: 21 [  510/ 1327], train_loss/perplexity = 4.31188250/74.5807571 secs/batch = 0.2671s, grad.norm=17.47512054
 28382: 21 [  515/ 1327], train_loss/perplexity = 4.00442791/54.8404427 secs/batch = 0.2659s, grad.norm=17.16542244
 28387: 21 [  520/ 1327], train_loss/perplexity = 4.12980938/62.1660728 secs/batch = 0.2647s, grad.norm=17.96990395
 28392: 21 [  525/ 1327], train_loss/perplexity = 3.72417355/41.4369736 secs/batch = 0.2640s, grad.norm=17.28182411
 28397: 21 [  530/ 1327], train_loss/perplexity = 3.75027966/42.5329742 secs/batch = 0.2656s, grad.norm=17.62635994
 28402: 21 [  535/ 1327], train_loss/perplexity = 3.97442627/53.2195740 secs/batch = 0.2658s, grad.norm=17.73830223
 28407: 21 [  540/ 1327], train_loss/perplexity = 3.99271631/54.2019196 secs/batch = 0.2671s, grad.norm=17.33534622
 28412: 21 [  545/ 1327], train_loss/perplexity = 3.92887282/50.8496284 secs/batch = 0.2661s, grad.norm=17.47393227
 28417: 21 [  550/ 1327], train_loss/perplexity = 3.84917879/46.9544868 secs/batch = 0.2665s, grad.norm=17.43763924
 28422: 21 [  555/ 1327], train_loss/perplexity = 3.79148817/44.3223114 secs/batch = 0.2673s, grad.norm=16.88038826
 28427: 21 [  560/ 1327], train_loss/perplexity = 3.86044645/47.4865456 secs/batch = 0.2663s, grad.norm=18.46553421
 28432: 21 [  565/ 1327], train_loss/perplexity = 3.71179676/40.9272766 secs/batch = 0.2652s, grad.norm=18.78535652
 28437: 21 [  570/ 1327], train_loss/perplexity = 3.82462215/45.8154869 secs/batch = 0.2663s, grad.norm=18.16932297
 28442: 21 [  575/ 1327], train_loss/perplexity = 3.59979486/36.5907288 secs/batch = 0.2670s, grad.norm=17.82811356
 28447: 21 [  580/ 1327], train_loss/perplexity = 3.92479372/50.6426315 secs/batch = 0.2656s, grad.norm=17.96050262
 28452: 21 [  585/ 1327], train_loss/perplexity = 3.61189461/37.0361557 secs/batch = 0.2635s, grad.norm=17.22925949
 28457: 21 [  590/ 1327], train_loss/perplexity = 3.90623236/49.7113037 secs/batch = 0.2657s, grad.norm=17.44275475
 28462: 21 [  595/ 1327], train_loss/perplexity = 3.90682793/49.7409210 secs/batch = 0.2655s, grad.norm=17.78754997
 28467: 21 [  600/ 1327], train_loss/perplexity = 4.10922480/60.8994904 secs/batch = 0.2638s, grad.norm=17.22261429
 28472: 21 [  605/ 1327], train_loss/perplexity = 3.95251083/52.0659332 secs/batch = 0.2659s, grad.norm=17.26622772
 28477: 21 [  610/ 1327], train_loss/perplexity = 4.19328499/66.2400284 secs/batch = 0.2662s, grad.norm=17.68064880
 28482: 21 [  615/ 1327], train_loss/perplexity = 3.72428131/41.4414368 secs/batch = 0.2671s, grad.norm=16.77352524
 28487: 21 [  620/ 1327], train_loss/perplexity = 4.11006498/60.9506798 secs/batch = 0.2666s, grad.norm=17.41955566
 28492: 21 [  625/ 1327], train_loss/perplexity = 4.03908491/56.7743645 secs/batch = 0.2666s, grad.norm=17.06089783
 28497: 21 [  630/ 1327], train_loss/perplexity = 4.13259840/62.3396950 secs/batch = 0.2590s, grad.norm=17.67918777
 28502: 21 [  635/ 1327], train_loss/perplexity = 3.86299229/47.6075935 secs/batch = 0.2666s, grad.norm=17.37317848
 28507: 21 [  640/ 1327], train_loss/perplexity = 3.85714674/47.3301125 secs/batch = 0.2664s, grad.norm=17.53937340
 28512: 21 [  645/ 1327], train_loss/perplexity = 4.06330204/58.1660614 secs/batch = 0.2662s, grad.norm=18.37279320
 28517: 21 [  650/ 1327], train_loss/perplexity = 3.63551283/37.9212952 secs/batch = 0.2659s, grad.norm=17.83455849
 28522: 21 [  655/ 1327], train_loss/perplexity = 3.78189135/43.8989906 secs/batch = 0.2661s, grad.norm=17.95223045
 28527: 21 [  660/ 1327], train_loss/perplexity = 3.67565584/39.4745369 secs/batch = 0.2660s, grad.norm=17.43866539
 28532: 21 [  665/ 1327], train_loss/perplexity = 3.83623075/46.3504372 secs/batch = 0.2662s, grad.norm=17.77095413
 28537: 21 [  670/ 1327], train_loss/perplexity = 3.84083748/46.5644531 secs/batch = 0.2656s, grad.norm=17.82024956
 28542: 21 [  675/ 1327], train_loss/perplexity = 3.60203385/36.6727448 secs/batch = 0.2668s, grad.norm=17.92243576
 28547: 21 [  680/ 1327], train_loss/perplexity = 3.78102303/43.8608894 secs/batch = 0.2664s, grad.norm=18.25304222
 28552: 21 [  685/ 1327], train_loss/perplexity = 3.52514935/33.9588432 secs/batch = 0.2669s, grad.norm=17.30853271
 28557: 21 [  690/ 1327], train_loss/perplexity = 4.01388454/55.3615074 secs/batch = 0.2669s, grad.norm=17.26839447
 28562: 21 [  695/ 1327], train_loss/perplexity = 3.88324809/48.5817566 secs/batch = 0.2668s, grad.norm=17.46452904
 28567: 21 [  700/ 1327], train_loss/perplexity = 4.06912613/58.5058136 secs/batch = 0.2663s, grad.norm=17.77949142
 28572: 21 [  705/ 1327], train_loss/perplexity = 3.82186365/45.6892776 secs/batch = 0.2671s, grad.norm=16.82888031
 28577: 21 [  710/ 1327], train_loss/perplexity = 3.74257946/42.2067223 secs/batch = 0.2671s, grad.norm=18.15890884
 28582: 21 [  715/ 1327], train_loss/perplexity = 3.58248758/35.9628906 secs/batch = 0.2653s, grad.norm=17.69031143
 28587: 21 [  720/ 1327], train_loss/perplexity = 3.66179967/38.9313431 secs/batch = 0.2668s, grad.norm=18.36230278
 28592: 21 [  725/ 1327], train_loss/perplexity = 3.65555048/38.6888123 secs/batch = 0.2668s, grad.norm=17.48686790
 28597: 21 [  730/ 1327], train_loss/perplexity = 3.83015108/46.0694962 secs/batch = 0.2600s, grad.norm=18.11615181
 28602: 21 [  735/ 1327], train_loss/perplexity = 3.90470409/49.6353912 secs/batch = 0.2676s, grad.norm=18.85263443
 28607: 21 [  740/ 1327], train_loss/perplexity = 3.43958521/31.1740246 secs/batch = 0.2652s, grad.norm=16.66025543
 28612: 21 [  745/ 1327], train_loss/perplexity = 3.91272712/50.0352173 secs/batch = 0.2666s, grad.norm=18.34280968
 28617: 21 [  750/ 1327], train_loss/perplexity = 3.83546972/46.3151779 secs/batch = 0.2673s, grad.norm=17.69684982
 28622: 21 [  755/ 1327], train_loss/perplexity = 3.64219666/38.1756020 secs/batch = 0.2673s, grad.norm=17.26554108
 28627: 21 [  760/ 1327], train_loss/perplexity = 3.49552774/32.9676819 secs/batch = 0.2656s, grad.norm=16.22847366
 28632: 21 [  765/ 1327], train_loss/perplexity = 3.53868151/34.4215050 secs/batch = 0.2649s, grad.norm=17.19455338
 28637: 21 [  770/ 1327], train_loss/perplexity = 3.62285805/37.4444313 secs/batch = 0.2646s, grad.norm=16.98902893
 28642: 21 [  775/ 1327], train_loss/perplexity = 3.60238791/36.6857338 secs/batch = 0.2680s, grad.norm=17.78203773
 28647: 21 [  780/ 1327], train_loss/perplexity = 3.91662884/50.2308235 secs/batch = 0.2652s, grad.norm=17.88550568
 28652: 21 [  785/ 1327], train_loss/perplexity = 3.83225846/46.1666870 secs/batch = 0.2670s, grad.norm=17.98601913
 28657: 21 [  790/ 1327], train_loss/perplexity = 3.62248826/37.4305878 secs/batch = 0.2668s, grad.norm=17.65886688
 28662: 21 [  795/ 1327], train_loss/perplexity = 4.02963924/56.2406197 secs/batch = 0.2664s, grad.norm=18.00062561
 28667: 21 [  800/ 1327], train_loss/perplexity = 3.84104323/46.5740356 secs/batch = 0.2664s, grad.norm=17.89222527
 28672: 21 [  805/ 1327], train_loss/perplexity = 4.12660408/61.9671288 secs/batch = 0.2651s, grad.norm=17.64533424
 28677: 21 [  810/ 1327], train_loss/perplexity = 3.76708508/43.2537994 secs/batch = 0.2591s, grad.norm=16.52177238
 28682: 21 [  815/ 1327], train_loss/perplexity = 3.73346829/41.8239136 secs/batch = 0.2660s, grad.norm=17.16318512
 28687: 21 [  820/ 1327], train_loss/perplexity = 3.64524031/38.2919731 secs/batch = 0.2653s, grad.norm=16.62706184
 28692: 21 [  825/ 1327], train_loss/perplexity = 3.77245641/43.4867554 secs/batch = 0.2653s, grad.norm=17.54090500
 28697: 21 [  830/ 1327], train_loss/perplexity = 3.44253397/31.2660847 secs/batch = 0.2653s, grad.norm=17.47632027
 28702: 21 [  835/ 1327], train_loss/perplexity = 3.77941251/43.7903061 secs/batch = 0.2608s, grad.norm=17.69120407
 28707: 21 [  840/ 1327], train_loss/perplexity = 3.87581754/48.2221069 secs/batch = 0.2636s, grad.norm=17.71969032
 28712: 21 [  845/ 1327], train_loss/perplexity = 3.63716221/37.9838943 secs/batch = 0.2661s, grad.norm=17.96028137
 28717: 21 [  850/ 1327], train_loss/perplexity = 3.74554992/42.3322792 secs/batch = 0.2654s, grad.norm=17.34081268
 28722: 21 [  855/ 1327], train_loss/perplexity = 3.72447944/41.4496498 secs/batch = 0.2658s, grad.norm=18.07890129
 28727: 21 [  860/ 1327], train_loss/perplexity = 3.53782582/34.3920631 secs/batch = 0.2640s, grad.norm=16.92283821
 28732: 21 [  865/ 1327], train_loss/perplexity = 3.99503088/54.3275185 secs/batch = 0.2644s, grad.norm=17.82691002
 28737: 21 [  870/ 1327], train_loss/perplexity = 3.76586103/43.2008858 secs/batch = 0.2650s, grad.norm=17.77754974
 28742: 21 [  875/ 1327], train_loss/perplexity = 3.50057864/33.1346207 secs/batch = 0.2656s, grad.norm=17.23242378
 28747: 21 [  880/ 1327], train_loss/perplexity = 3.64497399/38.2817764 secs/batch = 0.2651s, grad.norm=17.22241592
 28752: 21 [  885/ 1327], train_loss/perplexity = 3.83188081/46.1492538 secs/batch = 0.2623s, grad.norm=16.94993210
 28757: 21 [  890/ 1327], train_loss/perplexity = 3.98214912/53.6321716 secs/batch = 0.2656s, grad.norm=17.45260811
 28762: 21 [  895/ 1327], train_loss/perplexity = 3.90550375/49.6750984 secs/batch = 0.2658s, grad.norm=17.03036308
 28767: 21 [  900/ 1327], train_loss/perplexity = 3.72328472/41.4001579 secs/batch = 0.2660s, grad.norm=16.74559593
 28772: 21 [  905/ 1327], train_loss/perplexity = 3.71438360/41.0332870 secs/batch = 0.2650s, grad.norm=16.57982445
 28777: 21 [  910/ 1327], train_loss/perplexity = 3.74784684/42.4296265 secs/batch = 0.2654s, grad.norm=16.30658531
 28782: 21 [  915/ 1327], train_loss/perplexity = 3.96278381/52.6035614 secs/batch = 0.2651s, grad.norm=17.05262566
 28787: 21 [  920/ 1327], train_loss/perplexity = 4.08275652/59.3087311 secs/batch = 0.2578s, grad.norm=17.55122948
 28792: 21 [  925/ 1327], train_loss/perplexity = 3.83134604/46.1245804 secs/batch = 0.2654s, grad.norm=17.14517021
 28797: 21 [  930/ 1327], train_loss/perplexity = 3.91912603/50.3564148 secs/batch = 0.2650s, grad.norm=16.81380272
 28802: 21 [  935/ 1327], train_loss/perplexity = 3.92889071/50.8505363 secs/batch = 0.2641s, grad.norm=17.26744461
 28807: 21 [  940/ 1327], train_loss/perplexity = 3.92219830/50.5113602 secs/batch = 0.2599s, grad.norm=16.68924332
 28812: 21 [  945/ 1327], train_loss/perplexity = 4.08968067/59.7208176 secs/batch = 0.2653s, grad.norm=17.17260551
 28817: 21 [  950/ 1327], train_loss/perplexity = 3.91164017/49.9808617 secs/batch = 0.2638s, grad.norm=17.14344788
 28822: 21 [  955/ 1327], train_loss/perplexity = 3.88330436/48.5844917 secs/batch = 0.2647s, grad.norm=17.66715240
 28827: 21 [  960/ 1327], train_loss/perplexity = 4.09714365/60.1681824 secs/batch = 0.2665s, grad.norm=17.56562996
 28832: 21 [  965/ 1327], train_loss/perplexity = 3.84663296/46.8351021 secs/batch = 0.2659s, grad.norm=17.03039169
 28837: 21 [  970/ 1327], train_loss/perplexity = 4.10173798/60.4452477 secs/batch = 0.2657s, grad.norm=17.74964333
 28842: 21 [  975/ 1327], train_loss/perplexity = 3.73693371/41.9691048 secs/batch = 0.2661s, grad.norm=18.45083046
 28847: 21 [  980/ 1327], train_loss/perplexity = 3.63208318/37.7914619 secs/batch = 0.2657s, grad.norm=17.46825218
 28852: 21 [  985/ 1327], train_loss/perplexity = 3.75042391/42.5391121 secs/batch = 0.2648s, grad.norm=18.00530434
 28857: 21 [  990/ 1327], train_loss/perplexity = 3.97618985/53.3135147 secs/batch = 0.2656s, grad.norm=18.09904099
 28862: 21 [  995/ 1327], train_loss/perplexity = 4.05345726/57.5962372 secs/batch = 0.2651s, grad.norm=17.52396965
 28867: 21 [ 1000/ 1327], train_loss/perplexity = 3.57691741/35.7631264 secs/batch = 0.2650s, grad.norm=16.96333694
 28872: 21 [ 1005/ 1327], train_loss/perplexity = 4.01800585/55.5901413 secs/batch = 0.2641s, grad.norm=17.44935036
 28877: 21 [ 1010/ 1327], train_loss/perplexity = 3.58085942/35.9043846 secs/batch = 0.2654s, grad.norm=16.24510765
 28882: 21 [ 1015/ 1327], train_loss/perplexity = 4.01060963/55.1805000 secs/batch = 0.2632s, grad.norm=16.75783539
 28887: 21 [ 1020/ 1327], train_loss/perplexity = 4.16778278/64.5721207 secs/batch = 0.2614s, grad.norm=17.15267754
 28892: 21 [ 1025/ 1327], train_loss/perplexity = 4.08632898/59.5209885 secs/batch = 0.2653s, grad.norm=17.18289185
 28897: 21 [ 1030/ 1327], train_loss/perplexity = 3.80876493/45.0947113 secs/batch = 0.2655s, grad.norm=16.32863617
 28902: 21 [ 1035/ 1327], train_loss/perplexity = 3.74744391/42.4125328 secs/batch = 0.2652s, grad.norm=17.09106255
 28907: 21 [ 1040/ 1327], train_loss/perplexity = 3.93891740/51.3629646 secs/batch = 0.2672s, grad.norm=17.49223518
 28912: 21 [ 1045/ 1327], train_loss/perplexity = 3.51682520/33.6773415 secs/batch = 0.2651s, grad.norm=16.63281822
 28917: 21 [ 1050/ 1327], train_loss/perplexity = 3.61338615/37.0914383 secs/batch = 0.2654s, grad.norm=17.57384491
 28922: 21 [ 1055/ 1327], train_loss/perplexity = 3.71277523/40.9673424 secs/batch = 0.2655s, grad.norm=17.79179955
 28927: 21 [ 1060/ 1327], train_loss/perplexity = 3.27279091/26.3848743 secs/batch = 0.2660s, grad.norm=17.60836983
 28932: 21 [ 1065/ 1327], train_loss/perplexity = 3.48983002/32.7803764 secs/batch = 0.2649s, grad.norm=17.37352943
 28937: 21 [ 1070/ 1327], train_loss/perplexity = 3.86347866/47.6307564 secs/batch = 0.2665s, grad.norm=17.67676735
 28942: 21 [ 1075/ 1327], train_loss/perplexity = 3.55040598/34.8274536 secs/batch = 0.2658s, grad.norm=16.95904541
 28947: 21 [ 1080/ 1327], train_loss/perplexity = 3.51973224/33.7753830 secs/batch = 0.2655s, grad.norm=17.17703438
 28952: 21 [ 1085/ 1327], train_loss/perplexity = 3.42547226/30.7371578 secs/batch = 0.2653s, grad.norm=17.33804703
 28957: 21 [ 1090/ 1327], train_loss/perplexity = 3.61924672/37.3094521 secs/batch = 0.2642s, grad.norm=17.58222008
 28962: 21 [ 1095/ 1327], train_loss/perplexity = 3.83226275/46.1668854 secs/batch = 0.2658s, grad.norm=17.70547867
 28967: 21 [ 1100/ 1327], train_loss/perplexity = 3.51376915/33.5745773 secs/batch = 0.2660s, grad.norm=18.48341179
 28972: 21 [ 1105/ 1327], train_loss/perplexity = 3.47604275/32.3315239 secs/batch = 0.2640s, grad.norm=17.82359123
 28977: 21 [ 1110/ 1327], train_loss/perplexity = 3.73168254/41.7492943 secs/batch = 0.2650s, grad.norm=17.94450951
 28982: 21 [ 1115/ 1327], train_loss/perplexity = 3.55519819/34.9947548 secs/batch = 0.2661s, grad.norm=17.00556946
 28987: 21 [ 1120/ 1327], train_loss/perplexity = 3.84147334/46.5940742 secs/batch = 0.2651s, grad.norm=17.24530411
 28992: 21 [ 1125/ 1327], train_loss/perplexity = 4.01799297/55.5894241 secs/batch = 0.2651s, grad.norm=18.84046936
 28997: 21 [ 1130/ 1327], train_loss/perplexity = 3.71477413/41.0493126 secs/batch = 0.2621s, grad.norm=17.46060944
 29002: 21 [ 1135/ 1327], train_loss/perplexity = 3.69021964/40.0536423 secs/batch = 0.2648s, grad.norm=17.30883598
 29007: 21 [ 1140/ 1327], train_loss/perplexity = 3.96045995/52.4814606 secs/batch = 0.2655s, grad.norm=18.41359901
 29012: 21 [ 1145/ 1327], train_loss/perplexity = 3.74439955/42.2836113 secs/batch = 0.2660s, grad.norm=16.76794434
 29017: 21 [ 1150/ 1327], train_loss/perplexity = 3.70542669/40.6673965 secs/batch = 0.2625s, grad.norm=16.88118362
 29022: 21 [ 1155/ 1327], train_loss/perplexity = 3.77268028/43.4964905 secs/batch = 0.2658s, grad.norm=17.89586639
 29027: 21 [ 1160/ 1327], train_loss/perplexity = 3.68709135/39.9285393 secs/batch = 0.2631s, grad.norm=17.09689903
 29032: 21 [ 1165/ 1327], train_loss/perplexity = 3.73875546/42.0456314 secs/batch = 0.2657s, grad.norm=17.14238739
 29037: 21 [ 1170/ 1327], train_loss/perplexity = 3.61036062/36.9793854 secs/batch = 0.2658s, grad.norm=17.29218292
 29042: 21 [ 1175/ 1327], train_loss/perplexity = 3.47837782/32.4071083 secs/batch = 0.2659s, grad.norm=16.86671448
 29047: 21 [ 1180/ 1327], train_loss/perplexity = 3.46606398/32.0105019 secs/batch = 0.2653s, grad.norm=17.48826790
 29052: 21 [ 1185/ 1327], train_loss/perplexity = 3.64690518/38.3557777 secs/batch = 0.2657s, grad.norm=17.45070648
 29057: 21 [ 1190/ 1327], train_loss/perplexity = 3.72849226/41.6163139 secs/batch = 0.2661s, grad.norm=17.78713608
 29062: 21 [ 1195/ 1327], train_loss/perplexity = 3.58538675/36.0673027 secs/batch = 0.2653s, grad.norm=17.23852730
 29067: 21 [ 1200/ 1327], train_loss/perplexity = 3.48563910/32.6432838 secs/batch = 0.2659s, grad.norm=17.15175819
 29072: 21 [ 1205/ 1327], train_loss/perplexity = 3.52200651/33.8522835 secs/batch = 0.2650s, grad.norm=17.70879555
 29077: 21 [ 1210/ 1327], train_loss/perplexity = 3.17667627/23.9669609 secs/batch = 0.2655s, grad.norm=16.95989799
 29082: 21 [ 1215/ 1327], train_loss/perplexity = 3.39993715/29.9622173 secs/batch = 0.2653s, grad.norm=16.61873817
 29087: 21 [ 1220/ 1327], train_loss/perplexity = 3.60685635/36.8500252 secs/batch = 0.2653s, grad.norm=17.58861732
 29092: 21 [ 1225/ 1327], train_loss/perplexity = 3.20913386/24.7576332 secs/batch = 0.2661s, grad.norm=17.83982086
 29097: 21 [ 1230/ 1327], train_loss/perplexity = 3.55683231/35.0519867 secs/batch = 0.2651s, grad.norm=16.84601021
 29102: 21 [ 1235/ 1327], train_loss/perplexity = 3.55845857/35.1090355 secs/batch = 0.2654s, grad.norm=17.42020416
 29107: 21 [ 1240/ 1327], train_loss/perplexity = 3.75000763/42.5214081 secs/batch = 0.2654s, grad.norm=18.04148483
 29112: 21 [ 1245/ 1327], train_loss/perplexity = 3.63026237/37.7227135 secs/batch = 0.2644s, grad.norm=17.17622757
 29117: 21 [ 1250/ 1327], train_loss/perplexity = 3.79230213/44.3584023 secs/batch = 0.2650s, grad.norm=17.00284004
 29122: 21 [ 1255/ 1327], train_loss/perplexity = 3.77765894/43.7135849 secs/batch = 0.2658s, grad.norm=16.89670372
 29127: 21 [ 1260/ 1327], train_loss/perplexity = 3.54484797/34.6344185 secs/batch = 0.2658s, grad.norm=17.86416435
 29132: 21 [ 1265/ 1327], train_loss/perplexity = 3.82234335/45.7112007 secs/batch = 0.2650s, grad.norm=17.50951004
 29137: 21 [ 1270/ 1327], train_loss/perplexity = 3.53250337/34.2094994 secs/batch = 0.2652s, grad.norm=17.92346001
 29142: 21 [ 1275/ 1327], train_loss/perplexity = 3.73005891/41.6815643 secs/batch = 0.2669s, grad.norm=17.45631790
 29147: 21 [ 1280/ 1327], train_loss/perplexity = 3.66619277/39.1027489 secs/batch = 0.2650s, grad.norm=18.07271767
 29152: 21 [ 1285/ 1327], train_loss/perplexity = 3.50121880/33.1558380 secs/batch = 0.2658s, grad.norm=17.52758026
 29157: 21 [ 1290/ 1327], train_loss/perplexity = 3.79114127/44.3069382 secs/batch = 0.2662s, grad.norm=17.16747665
 29162: 21 [ 1295/ 1327], train_loss/perplexity = 3.73427844/41.8578110 secs/batch = 0.2658s, grad.norm=17.41980171
 29167: 21 [ 1300/ 1327], train_loss/perplexity = 3.91096044/49.9468994 secs/batch = 0.2656s, grad.norm=16.87277603
 29172: 21 [ 1305/ 1327], train_loss/perplexity = 3.88183188/48.5130043 secs/batch = 0.2639s, grad.norm=17.78031158
 29177: 21 [ 1310/ 1327], train_loss/perplexity = 4.20322704/66.9018784 secs/batch = 0.2644s, grad.norm=18.08578682
 29182: 21 [ 1315/ 1327], train_loss/perplexity = 3.93830252/51.3313942 secs/batch = 0.2657s, grad.norm=17.42608070
 29187: 21 [ 1320/ 1327], train_loss/perplexity = 3.97859287/53.4417801 secs/batch = 0.2651s, grad.norm=17.11129379
 29192: 21 [ 1325/ 1327], train_loss/perplexity = 3.83897972/46.4780312 secs/batch = 0.2663s, grad.norm=17.36582565
Epoch training time: 352.2758331298828
	> validation loss = 4.57301760, perplexity = 96.83588409
	> validation loss = 4.51995373, perplexity = 91.83135223
	> validation loss = 4.48134995, perplexity = 88.35386658
	> validation loss = 4.56288385, perplexity = 95.85952759
	> validation loss = 4.66537714, perplexity = 106.20563507
	> validation loss = 4.61989784, perplexity = 101.48366547
	> validation loss = 4.54081774, perplexity = 93.76744843
	> validation loss = 4.38054514, perplexity = 79.88156891
	> validation loss = 4.18032932, perplexity = 65.38738251
	> validation loss = 4.27670765, perplexity = 72.00299072
	> validation loss = 4.47160721, perplexity = 87.49723816
	> validation loss = 4.45901251, perplexity = 86.40214539
	> validation loss = 4.44614744, perplexity = 85.29769897
	> validation loss = 4.17924690, perplexity = 65.31664276
	> validation loss = 4.14068699, perplexity = 62.84598160
	> validation loss = 4.18356752, perplexity = 65.59946442
	> validation loss = 4.60659361, perplexity = 100.14244080
	> validation loss = 4.10966206, perplexity = 60.92612457
	> validation loss = 4.62192106, perplexity = 101.68919373
	> validation loss = 4.48504210, perplexity = 88.68068695
	> validation loss = 4.26122761, perplexity = 70.89696503
at the end of epoch: 21
train loss = 3.84685731, perplexity = 46.84561047
validation loss = 4.42208860, perplexity = 83.27002128
Saved model cv/epoch021_4.4221.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.0078125
new learning rate is: 0.00390625
 29199: 22 [    5/ 1327], train_loss/perplexity = 4.07231092/58.6924400 secs/batch = 0.2654s, grad.norm=17.69974899
 29204: 22 [   10/ 1327], train_loss/perplexity = 3.69940257/40.4231453 secs/batch = 0.2655s, grad.norm=16.80812263
 29209: 22 [   15/ 1327], train_loss/perplexity = 4.03326130/56.4446945 secs/batch = 0.2656s, grad.norm=16.87007523
 29214: 22 [   20/ 1327], train_loss/perplexity = 4.19019222/66.0354843 secs/batch = 0.2647s, grad.norm=16.92089844
 29219: 22 [   25/ 1327], train_loss/perplexity = 4.04544687/57.1367149 secs/batch = 0.2665s, grad.norm=18.33601761
 29224: 22 [   30/ 1327], train_loss/perplexity = 4.04355288/57.0285988 secs/batch = 0.2665s, grad.norm=17.92111206
 29229: 22 [   35/ 1327], train_loss/perplexity = 3.88618636/48.7247124 secs/batch = 0.2662s, grad.norm=17.52117157
 29234: 22 [   40/ 1327], train_loss/perplexity = 3.88185549/48.5141487 secs/batch = 0.2651s, grad.norm=17.41910744
 29239: 22 [   45/ 1327], train_loss/perplexity = 3.68561482/39.8696289 secs/batch = 0.2658s, grad.norm=16.75518036
 29244: 22 [   50/ 1327], train_loss/perplexity = 3.84086561/46.5657654 secs/batch = 0.2647s, grad.norm=17.65016556
 29249: 22 [   55/ 1327], train_loss/perplexity = 3.70413184/40.6147728 secs/batch = 0.2654s, grad.norm=17.98684120
 29254: 22 [   60/ 1327], train_loss/perplexity = 4.08207417/59.2682762 secs/batch = 0.2655s, grad.norm=17.94737053
 29259: 22 [   65/ 1327], train_loss/perplexity = 3.64624834/38.3305931 secs/batch = 0.2659s, grad.norm=17.27567291
 29264: 22 [   70/ 1327], train_loss/perplexity = 3.53898573/34.4319763 secs/batch = 0.2655s, grad.norm=17.57323647
 29269: 22 [   75/ 1327], train_loss/perplexity = 3.36334896/28.8857670 secs/batch = 0.2659s, grad.norm=16.89632988
 29274: 22 [   80/ 1327], train_loss/perplexity = 3.81835938/45.5294495 secs/batch = 0.2660s, grad.norm=17.70388985
 29279: 22 [   85/ 1327], train_loss/perplexity = 3.83772540/46.4197693 secs/batch = 0.2657s, grad.norm=18.40127945
 29284: 22 [   90/ 1327], train_loss/perplexity = 3.85344100/47.1550446 secs/batch = 0.2655s, grad.norm=18.12792587
 29289: 22 [   95/ 1327], train_loss/perplexity = 3.76968527/43.3664131 secs/batch = 0.2650s, grad.norm=18.11002350
 29294: 22 [  100/ 1327], train_loss/perplexity = 4.01365280/55.3486786 secs/batch = 0.2661s, grad.norm=18.05450058
 29299: 22 [  105/ 1327], train_loss/perplexity = 3.79194522/44.3425713 secs/batch = 0.2649s, grad.norm=18.16517639
 29304: 22 [  110/ 1327], train_loss/perplexity = 3.77276826/43.5003166 secs/batch = 0.2639s, grad.norm=18.11764717
 29309: 22 [  115/ 1327], train_loss/perplexity = 3.63317204/37.8326340 secs/batch = 0.2637s, grad.norm=18.03580475
 29314: 22 [  120/ 1327], train_loss/perplexity = 3.74877715/42.4691162 secs/batch = 0.2623s, grad.norm=17.95656013
 29319: 22 [  125/ 1327], train_loss/perplexity = 3.80351114/44.8584137 secs/batch = 0.2605s, grad.norm=18.66338730
 29324: 22 [  130/ 1327], train_loss/perplexity = 3.81974626/45.5926361 secs/batch = 0.2648s, grad.norm=18.53310394
 29329: 22 [  135/ 1327], train_loss/perplexity = 3.76073480/42.9799957 secs/batch = 0.2651s, grad.norm=17.64652252
 29334: 22 [  140/ 1327], train_loss/perplexity = 4.00218105/54.7173615 secs/batch = 0.2659s, grad.norm=18.29068565
 29339: 22 [  145/ 1327], train_loss/perplexity = 3.86485791/47.6964951 secs/batch = 0.2652s, grad.norm=18.73283195
 29344: 22 [  150/ 1327], train_loss/perplexity = 4.01350117/55.3402863 secs/batch = 0.2621s, grad.norm=19.22895050
 29349: 22 [  155/ 1327], train_loss/perplexity = 4.24531984/69.7780762 secs/batch = 0.2657s, grad.norm=17.96492577
 29354: 22 [  160/ 1327], train_loss/perplexity = 3.83387995/46.2416039 secs/batch = 0.2651s, grad.norm=16.99867439
 29359: 22 [  165/ 1327], train_loss/perplexity = 4.03420925/56.4982262 secs/batch = 0.2642s, grad.norm=17.49183083
 29364: 22 [  170/ 1327], train_loss/perplexity = 3.84739828/46.8709602 secs/batch = 0.2663s, grad.norm=17.48595619
 29369: 22 [  175/ 1327], train_loss/perplexity = 4.09778214/60.2066078 secs/batch = 0.2655s, grad.norm=18.05226517
 29374: 22 [  180/ 1327], train_loss/perplexity = 3.96843433/52.9016380 secs/batch = 0.2664s, grad.norm=18.22233582
 29379: 22 [  185/ 1327], train_loss/perplexity = 4.31991482/75.1822205 secs/batch = 0.2641s, grad.norm=18.56765938
 29384: 22 [  190/ 1327], train_loss/perplexity = 3.86339235/47.6266441 secs/batch = 0.2665s, grad.norm=16.78477669
 29389: 22 [  195/ 1327], train_loss/perplexity = 4.11882782/61.4871254 secs/batch = 0.2656s, grad.norm=17.19463730
 29394: 22 [  200/ 1327], train_loss/perplexity = 3.94295907/51.5709763 secs/batch = 0.2666s, grad.norm=18.96509552
 29399: 22 [  205/ 1327], train_loss/perplexity = 4.16875982/64.6352463 secs/batch = 0.2648s, grad.norm=18.21193695
 29404: 22 [  210/ 1327], train_loss/perplexity = 4.03126478/56.3321152 secs/batch = 0.2653s, grad.norm=17.31312752
 29409: 22 [  215/ 1327], train_loss/perplexity = 4.15810394/63.9501534 secs/batch = 0.2655s, grad.norm=17.53639793
 29414: 22 [  220/ 1327], train_loss/perplexity = 4.04213905/56.9480286 secs/batch = 0.2653s, grad.norm=17.19051933
 29419: 22 [  225/ 1327], train_loss/perplexity = 4.23112440/68.7945404 secs/batch = 0.2657s, grad.norm=17.91415787
 29424: 22 [  230/ 1327], train_loss/perplexity = 4.00408077/54.8214073 secs/batch = 0.2660s, grad.norm=18.80635071
 29429: 22 [  235/ 1327], train_loss/perplexity = 3.94416189/51.6330452 secs/batch = 0.2655s, grad.norm=18.37823105
 29434: 22 [  240/ 1327], train_loss/perplexity = 3.72116923/41.3126717 secs/batch = 0.2660s, grad.norm=17.85940361
 29439: 22 [  245/ 1327], train_loss/perplexity = 3.93158102/50.9875259 secs/batch = 0.2657s, grad.norm=17.67462730
 29444: 22 [  250/ 1327], train_loss/perplexity = 3.92375088/50.5898438 secs/batch = 0.2642s, grad.norm=17.49235344
 29449: 22 [  255/ 1327], train_loss/perplexity = 3.80571723/44.9574852 secs/batch = 0.2644s, grad.norm=17.81472588
 29454: 22 [  260/ 1327], train_loss/perplexity = 4.00416565/54.8260612 secs/batch = 0.2662s, grad.norm=18.34080696
 29459: 22 [  265/ 1327], train_loss/perplexity = 4.26013660/70.8196564 secs/batch = 0.2654s, grad.norm=17.88592529
 29464: 22 [  270/ 1327], train_loss/perplexity = 4.25918484/70.7522888 secs/batch = 0.2653s, grad.norm=17.86203003
 29469: 22 [  275/ 1327], train_loss/perplexity = 4.24950314/70.0705872 secs/batch = 0.2659s, grad.norm=17.59897423
 29474: 22 [  280/ 1327], train_loss/perplexity = 3.98703742/53.8949852 secs/batch = 0.2652s, grad.norm=17.15433502
 29479: 22 [  285/ 1327], train_loss/perplexity = 4.28351164/72.4945679 secs/batch = 0.2657s, grad.norm=17.53761101
 29484: 22 [  290/ 1327], train_loss/perplexity = 3.97906423/53.4669762 secs/batch = 0.2621s, grad.norm=18.38685799
 29489: 22 [  295/ 1327], train_loss/perplexity = 3.83644390/46.3603172 secs/batch = 0.2637s, grad.norm=18.00930786
 29494: 22 [  300/ 1327], train_loss/perplexity = 3.34511018/28.3637009 secs/batch = 0.2649s, grad.norm=16.92882156
 29499: 22 [  305/ 1327], train_loss/perplexity = 3.85177827/47.0767021 secs/batch = 0.2646s, grad.norm=17.24534607
 29504: 22 [  310/ 1327], train_loss/perplexity = 3.81436491/45.3479462 secs/batch = 0.2648s, grad.norm=17.33283806
 29509: 22 [  315/ 1327], train_loss/perplexity = 3.40036058/29.9749069 secs/batch = 0.2658s, grad.norm=16.58951759
 29514: 22 [  320/ 1327], train_loss/perplexity = 3.31219363/27.4452648 secs/batch = 0.2645s, grad.norm=17.88153839
 29519: 22 [  325/ 1327], train_loss/perplexity = 3.39179564/29.7192688 secs/batch = 0.2662s, grad.norm=16.90295982
 29524: 22 [  330/ 1327], train_loss/perplexity = 3.96525002/52.7334518 secs/batch = 0.2655s, grad.norm=18.06342316
 29529: 22 [  335/ 1327], train_loss/perplexity = 3.40897560/30.2342567 secs/batch = 0.2647s, grad.norm=16.45514107
 29534: 22 [  340/ 1327], train_loss/perplexity = 4.19743919/66.5157776 secs/batch = 0.2659s, grad.norm=17.89200783
 29539: 22 [  345/ 1327], train_loss/perplexity = 3.88203287/48.5227547 secs/batch = 0.2655s, grad.norm=16.74292564
 29544: 22 [  350/ 1327], train_loss/perplexity = 3.89596319/49.2034225 secs/batch = 0.2657s, grad.norm=18.34298134
 29549: 22 [  355/ 1327], train_loss/perplexity = 3.84903216/46.9476051 secs/batch = 0.2657s, grad.norm=17.32108498
 29554: 22 [  360/ 1327], train_loss/perplexity = 4.01425648/55.3821030 secs/batch = 0.2660s, grad.norm=18.81344032
 29559: 22 [  365/ 1327], train_loss/perplexity = 4.06167936/58.0717545 secs/batch = 0.2627s, grad.norm=17.54513931
 29564: 22 [  370/ 1327], train_loss/perplexity = 4.11923599/61.5122299 secs/batch = 0.2663s, grad.norm=17.94070053
 29569: 22 [  375/ 1327], train_loss/perplexity = 3.55922771/35.1360512 secs/batch = 0.2660s, grad.norm=17.84435844
 29574: 22 [  380/ 1327], train_loss/perplexity = 3.53477764/34.2873917 secs/batch = 0.2653s, grad.norm=17.65926552
 29579: 22 [  385/ 1327], train_loss/perplexity = 3.78410816/43.9964142 secs/batch = 0.2651s, grad.norm=18.25787926
 29584: 22 [  390/ 1327], train_loss/perplexity = 3.95548487/52.2210083 secs/batch = 0.2643s, grad.norm=17.93046570
 29589: 22 [  395/ 1327], train_loss/perplexity = 3.91268444/50.0330811 secs/batch = 0.2657s, grad.norm=17.85087585
 29594: 22 [  400/ 1327], train_loss/perplexity = 3.86800098/47.8466454 secs/batch = 0.2651s, grad.norm=17.09057426
 29599: 22 [  405/ 1327], train_loss/perplexity = 4.11642790/61.3397369 secs/batch = 0.2602s, grad.norm=17.95380783
 29604: 22 [  410/ 1327], train_loss/perplexity = 3.82906008/46.0192642 secs/batch = 0.2659s, grad.norm=17.73900986
 29609: 22 [  415/ 1327], train_loss/perplexity = 3.82107496/45.6532593 secs/batch = 0.2628s, grad.norm=18.11223221
 29614: 22 [  420/ 1327], train_loss/perplexity = 3.42399073/30.6916523 secs/batch = 0.2661s, grad.norm=17.46879768
 29619: 22 [  425/ 1327], train_loss/perplexity = 3.73099947/41.7207870 secs/batch = 0.2655s, grad.norm=18.24739265
 29624: 22 [  430/ 1327], train_loss/perplexity = 3.93994904/51.4159813 secs/batch = 0.2649s, grad.norm=18.01488495
 29629: 22 [  435/ 1327], train_loss/perplexity = 4.01105165/55.2048950 secs/batch = 0.2653s, grad.norm=18.53513527
 29634: 22 [  440/ 1327], train_loss/perplexity = 3.49151587/32.8356857 secs/batch = 0.2663s, grad.norm=17.27884102
 29639: 22 [  445/ 1327], train_loss/perplexity = 3.93467331/51.1454391 secs/batch = 0.2650s, grad.norm=18.31769562
 29644: 22 [  450/ 1327], train_loss/perplexity = 3.87735844/48.2964668 secs/batch = 0.2628s, grad.norm=17.53154945
 29649: 22 [  455/ 1327], train_loss/perplexity = 3.84679794/46.8428307 secs/batch = 0.2645s, grad.norm=17.29498863
 29654: 22 [  460/ 1327], train_loss/perplexity = 3.77142549/43.4419479 secs/batch = 0.2657s, grad.norm=17.92334557
 29659: 22 [  465/ 1327], train_loss/perplexity = 3.43267679/30.9594040 secs/batch = 0.2667s, grad.norm=17.93621635
 29664: 22 [  470/ 1327], train_loss/perplexity = 4.23701525/69.2009964 secs/batch = 0.2660s, grad.norm=17.72700882
 29669: 22 [  475/ 1327], train_loss/perplexity = 3.66911888/39.2173347 secs/batch = 0.2647s, grad.norm=17.46964264
 29674: 22 [  480/ 1327], train_loss/perplexity = 3.83910084/46.4836578 secs/batch = 0.2638s, grad.norm=17.56057739
 29679: 22 [  485/ 1327], train_loss/perplexity = 3.78963804/44.2403831 secs/batch = 0.2658s, grad.norm=17.92598724
 29684: 22 [  490/ 1327], train_loss/perplexity = 3.69288874/40.1606941 secs/batch = 0.2660s, grad.norm=18.96639442
 29689: 22 [  495/ 1327], train_loss/perplexity = 3.74705505/42.3960457 secs/batch = 0.2651s, grad.norm=17.53409195
 29694: 22 [  500/ 1327], train_loss/perplexity = 3.86960340/47.9233742 secs/batch = 0.2641s, grad.norm=17.50676346
 29699: 22 [  505/ 1327], train_loss/perplexity = 4.02140093/55.7791939 secs/batch = 0.2655s, grad.norm=16.69673729
 29704: 22 [  510/ 1327], train_loss/perplexity = 4.28696823/72.7455826 secs/batch = 0.2658s, grad.norm=17.20867920
 29709: 22 [  515/ 1327], train_loss/perplexity = 3.98274207/53.6639824 secs/batch = 0.2636s, grad.norm=17.07669640
 29714: 22 [  520/ 1327], train_loss/perplexity = 4.14641666/63.2070999 secs/batch = 0.2662s, grad.norm=17.75784492
 29719: 22 [  525/ 1327], train_loss/perplexity = 3.71435595/41.0321503 secs/batch = 0.2655s, grad.norm=17.50979424
 29724: 22 [  530/ 1327], train_loss/perplexity = 3.70572853/40.6796722 secs/batch = 0.2650s, grad.norm=17.55077362
 29729: 22 [  535/ 1327], train_loss/perplexity = 3.88686419/48.7577515 secs/batch = 0.2654s, grad.norm=17.36844635
 29734: 22 [  540/ 1327], train_loss/perplexity = 3.97989225/53.5112686 secs/batch = 0.2648s, grad.norm=17.65009308
 29739: 22 [  545/ 1327], train_loss/perplexity = 3.90537548/49.6687279 secs/batch = 0.2660s, grad.norm=17.49334908
 29744: 22 [  550/ 1327], train_loss/perplexity = 3.89619327/49.2147446 secs/batch = 0.2663s, grad.norm=17.52277565
 29749: 22 [  555/ 1327], train_loss/perplexity = 3.79450417/44.4561882 secs/batch = 0.2648s, grad.norm=17.05916405
 29754: 22 [  560/ 1327], train_loss/perplexity = 3.88964128/48.8933449 secs/batch = 0.2659s, grad.norm=18.22019958
 29759: 22 [  565/ 1327], train_loss/perplexity = 3.69577265/40.2766800 secs/batch = 0.2663s, grad.norm=18.61331367
 29764: 22 [  570/ 1327], train_loss/perplexity = 3.75724268/42.8301659 secs/batch = 0.2667s, grad.norm=18.01355743
 29769: 22 [  575/ 1327], train_loss/perplexity = 3.53089714/34.1545944 secs/batch = 0.2660s, grad.norm=17.52964973
 29774: 22 [  580/ 1327], train_loss/perplexity = 3.99200392/54.1633186 secs/batch = 0.2664s, grad.norm=18.86340141
 29779: 22 [  585/ 1327], train_loss/perplexity = 3.59207726/36.3094215 secs/batch = 0.2655s, grad.norm=17.41453743
 29784: 22 [  590/ 1327], train_loss/perplexity = 3.92923570/50.8680840 secs/batch = 0.2661s, grad.norm=17.38084030
 29789: 22 [  595/ 1327], train_loss/perplexity = 3.89608264/49.2093010 secs/batch = 0.2659s, grad.norm=17.93381119
 29794: 22 [  600/ 1327], train_loss/perplexity = 4.08728218/59.5777512 secs/batch = 0.2658s, grad.norm=17.08584404
 29799: 22 [  605/ 1327], train_loss/perplexity = 3.97649622/53.3298492 secs/batch = 0.2646s, grad.norm=17.31325340
 29804: 22 [  610/ 1327], train_loss/perplexity = 4.11449528/61.2213058 secs/batch = 0.2663s, grad.norm=17.53986168
 29809: 22 [  615/ 1327], train_loss/perplexity = 3.68112230/39.6909142 secs/batch = 0.2653s, grad.norm=17.20111084
 29814: 22 [  620/ 1327], train_loss/perplexity = 4.10810852/60.8315468 secs/batch = 0.2608s, grad.norm=17.77486992
 29819: 22 [  625/ 1327], train_loss/perplexity = 4.09782934/60.2094498 secs/batch = 0.2656s, grad.norm=17.26958084
 29824: 22 [  630/ 1327], train_loss/perplexity = 4.14440107/63.0798302 secs/batch = 0.2648s, grad.norm=17.37671852
 29829: 22 [  635/ 1327], train_loss/perplexity = 3.90152407/49.4777985 secs/batch = 0.2653s, grad.norm=17.58736229
 29834: 22 [  640/ 1327], train_loss/perplexity = 3.87566328/48.2146683 secs/batch = 0.2625s, grad.norm=17.18798828
 29839: 22 [  645/ 1327], train_loss/perplexity = 4.06947374/58.5261536 secs/batch = 0.2653s, grad.norm=18.56480408
 29844: 22 [  650/ 1327], train_loss/perplexity = 3.64860296/38.4209518 secs/batch = 0.2654s, grad.norm=17.47361183
 29849: 22 [  655/ 1327], train_loss/perplexity = 3.81787348/45.5073318 secs/batch = 0.2595s, grad.norm=17.25986099
 29854: 22 [  660/ 1327], train_loss/perplexity = 3.71438885/41.0335007 secs/batch = 0.2648s, grad.norm=17.91108322
 29859: 22 [  665/ 1327], train_loss/perplexity = 3.85246992/47.1092758 secs/batch = 0.2657s, grad.norm=17.47854996
 29864: 22 [  670/ 1327], train_loss/perplexity = 3.79920387/44.6656113 secs/batch = 0.2657s, grad.norm=17.90257835
 29869: 22 [  675/ 1327], train_loss/perplexity = 3.56805253/35.4474945 secs/batch = 0.2655s, grad.norm=17.46420670
 29874: 22 [  680/ 1327], train_loss/perplexity = 3.78689289/44.1191025 secs/batch = 0.2648s, grad.norm=18.18707657
 29879: 22 [  685/ 1327], train_loss/perplexity = 3.58062792/35.8960724 secs/batch = 0.2659s, grad.norm=17.30485153
 29884: 22 [  690/ 1327], train_loss/perplexity = 4.07156229/58.6485176 secs/batch = 0.2651s, grad.norm=17.09314346
 29889: 22 [  695/ 1327], train_loss/perplexity = 3.89514661/49.1632614 secs/batch = 0.2654s, grad.norm=17.70738220
 29894: 22 [  700/ 1327], train_loss/perplexity = 4.11531496/61.2715111 secs/batch = 0.2674s, grad.norm=18.40828514
 29899: 22 [  705/ 1327], train_loss/perplexity = 3.80924344/45.1162910 secs/batch = 0.2658s, grad.norm=16.74487305
 29904: 22 [  710/ 1327], train_loss/perplexity = 3.73543453/41.9062309 secs/batch = 0.2648s, grad.norm=18.32417679
 29909: 22 [  715/ 1327], train_loss/perplexity = 3.63094616/37.7485161 secs/batch = 0.2652s, grad.norm=18.11081505
 29914: 22 [  720/ 1327], train_loss/perplexity = 3.62310195/37.4535675 secs/batch = 0.2658s, grad.norm=18.09576797
 29919: 22 [  725/ 1327], train_loss/perplexity = 3.70485663/40.6442184 secs/batch = 0.2658s, grad.norm=17.69042015
 29924: 22 [  730/ 1327], train_loss/perplexity = 3.85477018/47.2177658 secs/batch = 0.2651s, grad.norm=18.03379059
 29929: 22 [  735/ 1327], train_loss/perplexity = 3.87432647/48.1502571 secs/batch = 0.2657s, grad.norm=18.67315674
 29934: 22 [  740/ 1327], train_loss/perplexity = 3.41741323/30.4904404 secs/batch = 0.2618s, grad.norm=16.40255547
 29939: 22 [  745/ 1327], train_loss/perplexity = 3.89181399/48.9996910 secs/batch = 0.2649s, grad.norm=18.47509956
 29944: 22 [  750/ 1327], train_loss/perplexity = 3.73276567/41.7945404 secs/batch = 0.2651s, grad.norm=17.52776527
 29949: 22 [  755/ 1327], train_loss/perplexity = 3.59897947/36.5609055 secs/batch = 0.2655s, grad.norm=17.28976440
 29954: 22 [  760/ 1327], train_loss/perplexity = 3.47678924/32.3556671 secs/batch = 0.2658s, grad.norm=16.15727806
 29959: 22 [  765/ 1327], train_loss/perplexity = 3.59276032/36.3342323 secs/batch = 0.2654s, grad.norm=16.76405144
 29964: 22 [  770/ 1327], train_loss/perplexity = 3.59337544/36.3565903 secs/batch = 0.2650s, grad.norm=17.36396980
 29969: 22 [  775/ 1327], train_loss/perplexity = 3.61922145/37.3085098 secs/batch = 0.2648s, grad.norm=17.99019432
 29974: 22 [  780/ 1327], train_loss/perplexity = 3.91592526/50.1954956 secs/batch = 0.2651s, grad.norm=18.11876869
 29979: 22 [  785/ 1327], train_loss/perplexity = 3.87587023/48.2246475 secs/batch = 0.2659s, grad.norm=18.13269234
 29984: 22 [  790/ 1327], train_loss/perplexity = 3.60500026/36.7816925 secs/batch = 0.2650s, grad.norm=17.77893829
 29989: 22 [  795/ 1327], train_loss/perplexity = 3.96153259/52.5377846 secs/batch = 0.2656s, grad.norm=17.63874435
 29994: 22 [  800/ 1327], train_loss/perplexity = 3.81931877/45.5731506 secs/batch = 0.2657s, grad.norm=17.82152176
 29999: 22 [  805/ 1327], train_loss/perplexity = 4.11463261/61.2297134 secs/batch = 0.2661s, grad.norm=17.88796616
 30004: 22 [  810/ 1327], train_loss/perplexity = 3.81868482/45.5442696 secs/batch = 0.2664s, grad.norm=17.06384659
 30009: 22 [  815/ 1327], train_loss/perplexity = 3.73150539/41.7418976 secs/batch = 0.2653s, grad.norm=17.15312386
 30014: 22 [  820/ 1327], train_loss/perplexity = 3.62141776/37.3905411 secs/batch = 0.2620s, grad.norm=16.67781830
 30019: 22 [  825/ 1327], train_loss/perplexity = 3.75572610/42.7652588 secs/batch = 0.2660s, grad.norm=17.73062515
 30024: 22 [  830/ 1327], train_loss/perplexity = 3.46035004/31.8281155 secs/batch = 0.2650s, grad.norm=17.54460335
 30029: 22 [  835/ 1327], train_loss/perplexity = 3.83900356/46.4791374 secs/batch = 0.2658s, grad.norm=17.81605911
 30034: 22 [  840/ 1327], train_loss/perplexity = 3.91605186/50.2018509 secs/batch = 0.2656s, grad.norm=17.54109383
 30039: 22 [  845/ 1327], train_loss/perplexity = 3.68973231/40.0341301 secs/batch = 0.2628s, grad.norm=18.07029343
 30044: 22 [  850/ 1327], train_loss/perplexity = 3.72687101/41.5488968 secs/batch = 0.2659s, grad.norm=17.14442635
 30049: 22 [  855/ 1327], train_loss/perplexity = 3.74684644/42.3871994 secs/batch = 0.2651s, grad.norm=17.51795006
 30054: 22 [  860/ 1327], train_loss/perplexity = 3.50165987/33.1704636 secs/batch = 0.2653s, grad.norm=16.84626198
 30059: 22 [  865/ 1327], train_loss/perplexity = 4.00399113/54.8164940 secs/batch = 0.2663s, grad.norm=18.15999222
 30064: 22 [  870/ 1327], train_loss/perplexity = 3.79485869/44.4719505 secs/batch = 0.2666s, grad.norm=17.81605339
 30069: 22 [  875/ 1327], train_loss/perplexity = 3.49916697/33.0878754 secs/batch = 0.2656s, grad.norm=17.09718895
 30074: 22 [  880/ 1327], train_loss/perplexity = 3.65006995/38.4773560 secs/batch = 0.2620s, grad.norm=17.61898994
 30079: 22 [  885/ 1327], train_loss/perplexity = 3.85247469/47.1095009 secs/batch = 0.2653s, grad.norm=16.97963524
 30084: 22 [  890/ 1327], train_loss/perplexity = 3.88667345/48.7484512 secs/batch = 0.2656s, grad.norm=17.71844101
 30089: 22 [  895/ 1327], train_loss/perplexity = 3.96248484/52.5878372 secs/batch = 0.2636s, grad.norm=17.13040924
 30094: 22 [  900/ 1327], train_loss/perplexity = 3.79134703/44.3160553 secs/batch = 0.2655s, grad.norm=17.16843987
 30099: 22 [  905/ 1327], train_loss/perplexity = 3.70783424/40.7654228 secs/batch = 0.2655s, grad.norm=16.77639008
 30104: 22 [  910/ 1327], train_loss/perplexity = 3.72457647/41.4536705 secs/batch = 0.2655s, grad.norm=16.54704857
 30109: 22 [  915/ 1327], train_loss/perplexity = 3.91478395/50.1382370 secs/batch = 0.2668s, grad.norm=16.83387947
 30114: 22 [  920/ 1327], train_loss/perplexity = 4.07139111/58.6384773 secs/batch = 0.2652s, grad.norm=17.41103745
 30119: 22 [  925/ 1327], train_loss/perplexity = 3.88990378/48.9061813 secs/batch = 0.2641s, grad.norm=17.07544899
 30124: 22 [  930/ 1327], train_loss/perplexity = 3.93226814/51.0225716 secs/batch = 0.2653s, grad.norm=17.12468719
 30129: 22 [  935/ 1327], train_loss/perplexity = 3.98549271/53.8117981 secs/batch = 0.2668s, grad.norm=17.40067291
 30134: 22 [  940/ 1327], train_loss/perplexity = 3.92638111/50.7230835 secs/batch = 0.2661s, grad.norm=16.99820900
 30139: 22 [  945/ 1327], train_loss/perplexity = 4.11860609/61.4734955 secs/batch = 0.2648s, grad.norm=17.18134308
 30144: 22 [  950/ 1327], train_loss/perplexity = 3.91115046/49.9563904 secs/batch = 0.2658s, grad.norm=17.06100273
 30149: 22 [  955/ 1327], train_loss/perplexity = 3.82981467/46.0540009 secs/batch = 0.2660s, grad.norm=16.95643806
 30154: 22 [  960/ 1327], train_loss/perplexity = 4.09421301/59.9921074 secs/batch = 0.2635s, grad.norm=17.31106567
 30159: 22 [  965/ 1327], train_loss/perplexity = 3.90681624/49.7403374 secs/batch = 0.2658s, grad.norm=17.57166862
 30164: 22 [  970/ 1327], train_loss/perplexity = 4.13303280/62.3667831 secs/batch = 0.2650s, grad.norm=17.84329987
 30169: 22 [  975/ 1327], train_loss/perplexity = 3.75030327/42.5339775 secs/batch = 0.2651s, grad.norm=18.52166939
 30174: 22 [  980/ 1327], train_loss/perplexity = 3.73492360/41.8848267 secs/batch = 0.2662s, grad.norm=17.40727043
 30179: 22 [  985/ 1327], train_loss/perplexity = 3.78819394/44.1765442 secs/batch = 0.2649s, grad.norm=18.10767365
 30184: 22 [  990/ 1327], train_loss/perplexity = 4.07450485/58.8213463 secs/batch = 0.2633s, grad.norm=18.09217453
 30189: 22 [  995/ 1327], train_loss/perplexity = 3.96561122/52.7525024 secs/batch = 0.2655s, grad.norm=17.36790085
 30194: 22 [ 1000/ 1327], train_loss/perplexity = 3.49522805/32.9578018 secs/batch = 0.2655s, grad.norm=16.48238754
 30199: 22 [ 1005/ 1327], train_loss/perplexity = 3.95099449/51.9870415 secs/batch = 0.2649s, grad.norm=17.38677597
 30204: 22 [ 1010/ 1327], train_loss/perplexity = 3.59777594/36.5169296 secs/batch = 0.2644s, grad.norm=16.37657356
 30209: 22 [ 1015/ 1327], train_loss/perplexity = 4.10141754/60.4258842 secs/batch = 0.2638s, grad.norm=17.37245369
 30214: 22 [ 1020/ 1327], train_loss/perplexity = 4.11331415/61.1490402 secs/batch = 0.2649s, grad.norm=17.43535805
 30219: 22 [ 1025/ 1327], train_loss/perplexity = 4.06721783/58.3942719 secs/batch = 0.2628s, grad.norm=17.26046944
 30224: 22 [ 1030/ 1327], train_loss/perplexity = 3.77171540/43.4545441 secs/batch = 0.2654s, grad.norm=16.45187950
 30229: 22 [ 1035/ 1327], train_loss/perplexity = 3.79922771/44.6666756 secs/batch = 0.2638s, grad.norm=16.77048302
 30234: 22 [ 1040/ 1327], train_loss/perplexity = 3.96469998/52.7044563 secs/batch = 0.2656s, grad.norm=17.83443642
 30239: 22 [ 1045/ 1327], train_loss/perplexity = 3.59409857/36.3828888 secs/batch = 0.2657s, grad.norm=16.60458183
 30244: 22 [ 1050/ 1327], train_loss/perplexity = 3.61014175/36.9712944 secs/batch = 0.2657s, grad.norm=17.09715843
 30249: 22 [ 1055/ 1327], train_loss/perplexity = 3.66057992/38.8838844 secs/batch = 0.2647s, grad.norm=17.36740112
 30254: 22 [ 1060/ 1327], train_loss/perplexity = 3.33658504/28.1229248 secs/batch = 0.2653s, grad.norm=18.02603912
 30259: 22 [ 1065/ 1327], train_loss/perplexity = 3.48465657/32.6112251 secs/batch = 0.2664s, grad.norm=17.26035309
 30264: 22 [ 1070/ 1327], train_loss/perplexity = 3.81797624/45.5120087 secs/batch = 0.2657s, grad.norm=17.72113800
 30269: 22 [ 1075/ 1327], train_loss/perplexity = 3.55179334/34.8758049 secs/batch = 0.2664s, grad.norm=17.04526329
 30274: 22 [ 1080/ 1327], train_loss/perplexity = 3.55419588/34.9596977 secs/batch = 0.2656s, grad.norm=17.27360916
 30279: 22 [ 1085/ 1327], train_loss/perplexity = 3.41388893/30.3831730 secs/batch = 0.2659s, grad.norm=17.42430115
 30284: 22 [ 1090/ 1327], train_loss/perplexity = 3.61658382/37.2102356 secs/batch = 0.2658s, grad.norm=17.79088783
 30289: 22 [ 1095/ 1327], train_loss/perplexity = 3.79774213/44.6003685 secs/batch = 0.2663s, grad.norm=17.88538551
 30294: 22 [ 1100/ 1327], train_loss/perplexity = 3.44041348/31.1998558 secs/batch = 0.2659s, grad.norm=18.72309875
 30299: 22 [ 1105/ 1327], train_loss/perplexity = 3.48955846/32.7714729 secs/batch = 0.2651s, grad.norm=17.83308601
 30304: 22 [ 1110/ 1327], train_loss/perplexity = 3.74953103/42.5011444 secs/batch = 0.2635s, grad.norm=18.08699417
 30309: 22 [ 1115/ 1327], train_loss/perplexity = 3.54001808/34.4675407 secs/batch = 0.2657s, grad.norm=16.69554901
 30314: 22 [ 1120/ 1327], train_loss/perplexity = 3.81707716/45.4711075 secs/batch = 0.2659s, grad.norm=17.12769318
 30319: 22 [ 1125/ 1327], train_loss/perplexity = 4.01557446/55.4551430 secs/batch = 0.2656s, grad.norm=18.19854736
 30324: 22 [ 1130/ 1327], train_loss/perplexity = 3.68648052/39.9041595 secs/batch = 0.2668s, grad.norm=17.40018272
 30329: 22 [ 1135/ 1327], train_loss/perplexity = 3.64205956/38.1703720 secs/batch = 0.2660s, grad.norm=17.65416718
 30334: 22 [ 1140/ 1327], train_loss/perplexity = 3.95190144/52.0342140 secs/batch = 0.2662s, grad.norm=17.98233986
 30339: 22 [ 1145/ 1327], train_loss/perplexity = 3.71044731/40.8720856 secs/batch = 0.2647s, grad.norm=17.17410088
 30344: 22 [ 1150/ 1327], train_loss/perplexity = 3.69043350/40.0622101 secs/batch = 0.2654s, grad.norm=17.26602364
 30349: 22 [ 1155/ 1327], train_loss/perplexity = 3.80381989/44.8722649 secs/batch = 0.2659s, grad.norm=17.62663078
 30354: 22 [ 1160/ 1327], train_loss/perplexity = 3.71346927/40.9957848 secs/batch = 0.2651s, grad.norm=17.31028938
 30359: 22 [ 1165/ 1327], train_loss/perplexity = 3.72923732/41.6473312 secs/batch = 0.2658s, grad.norm=17.87374878
 30364: 22 [ 1170/ 1327], train_loss/perplexity = 3.63120008/37.7581024 secs/batch = 0.2651s, grad.norm=17.20919418
 30369: 22 [ 1175/ 1327], train_loss/perplexity = 3.53187490/34.1880074 secs/batch = 0.2660s, grad.norm=16.87553024
 30374: 22 [ 1180/ 1327], train_loss/perplexity = 3.55229902/34.8934479 secs/batch = 0.2662s, grad.norm=17.81894684
 30379: 22 [ 1185/ 1327], train_loss/perplexity = 3.66991663/39.2486343 secs/batch = 0.2655s, grad.norm=17.57327271
 30384: 22 [ 1190/ 1327], train_loss/perplexity = 3.73264217/41.7893791 secs/batch = 0.2662s, grad.norm=17.77648735
 30389: 22 [ 1195/ 1327], train_loss/perplexity = 3.59858513/36.5464897 secs/batch = 0.2660s, grad.norm=17.55201340
 30394: 22 [ 1200/ 1327], train_loss/perplexity = 3.51926661/33.7596588 secs/batch = 0.2653s, grad.norm=17.38272476
 30399: 22 [ 1205/ 1327], train_loss/perplexity = 3.56162620/35.2204247 secs/batch = 0.2660s, grad.norm=17.92156982
 30404: 22 [ 1210/ 1327], train_loss/perplexity = 3.13881016/23.0763931 secs/batch = 0.2645s, grad.norm=17.19920158
 30409: 22 [ 1215/ 1327], train_loss/perplexity = 3.30941772/27.3691845 secs/batch = 0.2637s, grad.norm=16.30504417
 30414: 22 [ 1220/ 1327], train_loss/perplexity = 3.59693098/36.4860878 secs/batch = 0.2636s, grad.norm=17.55826569
 30419: 22 [ 1225/ 1327], train_loss/perplexity = 3.25277758/25.8620739 secs/batch = 0.2653s, grad.norm=18.30741882
 30424: 22 [ 1230/ 1327], train_loss/perplexity = 3.49372959/32.9084549 secs/batch = 0.2654s, grad.norm=16.96335793
 30429: 22 [ 1235/ 1327], train_loss/perplexity = 3.48916364/32.7585373 secs/batch = 0.2644s, grad.norm=16.89825821
 30434: 22 [ 1240/ 1327], train_loss/perplexity = 3.74938035/42.4947433 secs/batch = 0.2655s, grad.norm=17.92654419
 30439: 22 [ 1245/ 1327], train_loss/perplexity = 3.63937998/38.0682259 secs/batch = 0.2639s, grad.norm=17.17381287
 30444: 22 [ 1250/ 1327], train_loss/perplexity = 3.79824114/44.6226311 secs/batch = 0.2650s, grad.norm=16.58227158
 30449: 22 [ 1255/ 1327], train_loss/perplexity = 3.82782745/45.9625740 secs/batch = 0.2643s, grad.norm=16.95750618
 30454: 22 [ 1260/ 1327], train_loss/perplexity = 3.56458950/35.3249512 secs/batch = 0.2657s, grad.norm=17.73260498
 30459: 22 [ 1265/ 1327], train_loss/perplexity = 3.80468416/44.9110641 secs/batch = 0.2657s, grad.norm=17.60386658
 30464: 22 [ 1270/ 1327], train_loss/perplexity = 3.50980139/33.4416237 secs/batch = 0.2658s, grad.norm=17.46337318
 30469: 22 [ 1275/ 1327], train_loss/perplexity = 3.71522474/41.0678139 secs/batch = 0.2656s, grad.norm=17.70657921
 30474: 22 [ 1280/ 1327], train_loss/perplexity = 3.65335131/38.6038246 secs/batch = 0.2657s, grad.norm=18.48488617
 30479: 22 [ 1285/ 1327], train_loss/perplexity = 3.49574041/32.9746933 secs/batch = 0.2651s, grad.norm=17.56280327
 30484: 22 [ 1290/ 1327], train_loss/perplexity = 3.69021869/40.0536041 secs/batch = 0.2655s, grad.norm=17.13553047
 30489: 22 [ 1295/ 1327], train_loss/perplexity = 3.73466611/41.8740425 secs/batch = 0.2655s, grad.norm=17.38769913
 30494: 22 [ 1300/ 1327], train_loss/perplexity = 3.87681293/48.2701302 secs/batch = 0.2636s, grad.norm=16.87698174
 30499: 22 [ 1305/ 1327], train_loss/perplexity = 3.95187593/52.0328865 secs/batch = 0.2643s, grad.norm=17.94717598
 30504: 22 [ 1310/ 1327], train_loss/perplexity = 4.17986155/65.3568039 secs/batch = 0.2666s, grad.norm=17.93291473
 30509: 22 [ 1315/ 1327], train_loss/perplexity = 3.96298385/52.6140862 secs/batch = 0.2663s, grad.norm=17.97812080
 30514: 22 [ 1320/ 1327], train_loss/perplexity = 3.94120431/51.4805641 secs/batch = 0.2666s, grad.norm=17.54032326
 30519: 22 [ 1325/ 1327], train_loss/perplexity = 3.81986666/45.5981293 secs/batch = 0.2662s, grad.norm=17.48963356
Epoch training time: 351.9680178165436
	> validation loss = 4.57319784, perplexity = 96.85334015
	> validation loss = 4.51944876, perplexity = 91.78498840
	> validation loss = 4.48044825, perplexity = 88.27423096
	> validation loss = 4.56086922, perplexity = 95.66659546
	> validation loss = 4.66498995, perplexity = 106.16452026
	> validation loss = 4.61991501, perplexity = 101.48540497
	> validation loss = 4.53858376, perplexity = 93.55820465
	> validation loss = 4.38100147, perplexity = 79.91802979
	> validation loss = 4.17958832, perplexity = 65.33895111
	> validation loss = 4.27638817, perplexity = 71.97998810
	> validation loss = 4.47154760, perplexity = 87.49201965
	> validation loss = 4.45823669, perplexity = 86.33513641
	> validation loss = 4.44667244, perplexity = 85.34249115
	> validation loss = 4.17801380, perplexity = 65.23615265
	> validation loss = 4.13901234, perplexity = 62.74082565
	> validation loss = 4.18265438, perplexity = 65.53958893
	> validation loss = 4.60504150, perplexity = 99.98712921
	> validation loss = 4.10889578, perplexity = 60.87945557
	> validation loss = 4.62058401, perplexity = 101.55332184
	> validation loss = 4.48371649, perplexity = 88.56320953
	> validation loss = 4.26019478, perplexity = 70.82377625
at the end of epoch: 22
train loss = 3.82836953, perplexity = 45.98749583
validation loss = 4.42122952, perplexity = 83.19851648
Saved model cv/epoch022_4.4212.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.00390625
new learning rate is: 0.001953125
 30526: 23 [    5/ 1327], train_loss/perplexity = 4.08657694/59.5357475 secs/batch = 0.2660s, grad.norm=18.05062103
 30531: 23 [   10/ 1327], train_loss/perplexity = 3.63860488/38.0387306 secs/batch = 0.2597s, grad.norm=16.89188004
 30536: 23 [   15/ 1327], train_loss/perplexity = 4.02244997/55.8377380 secs/batch = 0.2664s, grad.norm=16.91365814
 30541: 23 [   20/ 1327], train_loss/perplexity = 4.18476725/65.6782150 secs/batch = 0.2658s, grad.norm=16.87727737
 30546: 23 [   25/ 1327], train_loss/perplexity = 4.06743383/58.4068871 secs/batch = 0.2638s, grad.norm=18.31689262
 30551: 23 [   30/ 1327], train_loss/perplexity = 4.05703163/57.8024788 secs/batch = 0.2667s, grad.norm=18.11297798
 30556: 23 [   35/ 1327], train_loss/perplexity = 3.88005590/48.4269218 secs/batch = 0.2671s, grad.norm=17.20196152
 30561: 23 [   40/ 1327], train_loss/perplexity = 3.84138727/46.5900612 secs/batch = 0.2671s, grad.norm=17.18899345
 30566: 23 [   45/ 1327], train_loss/perplexity = 3.68928027/40.0160370 secs/batch = 0.2640s, grad.norm=16.65513229
 30571: 23 [   50/ 1327], train_loss/perplexity = 3.87150145/48.0144234 secs/batch = 0.2664s, grad.norm=17.41112328
 30576: 23 [   55/ 1327], train_loss/perplexity = 3.66998792/39.2514305 secs/batch = 0.2673s, grad.norm=17.82538795
 30581: 23 [   60/ 1327], train_loss/perplexity = 4.02485037/55.9719315 secs/batch = 0.2658s, grad.norm=18.14062881
 30586: 23 [   65/ 1327], train_loss/perplexity = 3.66978455/39.2434502 secs/batch = 0.2667s, grad.norm=17.50683403
 30591: 23 [   70/ 1327], train_loss/perplexity = 3.49504519/32.9517784 secs/batch = 0.2635s, grad.norm=17.27923965
 30596: 23 [   75/ 1327], train_loss/perplexity = 3.28557515/26.7243500 secs/batch = 0.2664s, grad.norm=16.08998108
 30601: 23 [   80/ 1327], train_loss/perplexity = 3.74319649/42.2327728 secs/batch = 0.2664s, grad.norm=17.86591148
 30606: 23 [   85/ 1327], train_loss/perplexity = 3.83551359/46.3172112 secs/batch = 0.2661s, grad.norm=18.22312355
 30611: 23 [   90/ 1327], train_loss/perplexity = 3.88312030/48.5755501 secs/batch = 0.2657s, grad.norm=17.78384781
 30616: 23 [   95/ 1327], train_loss/perplexity = 3.76428246/43.1327438 secs/batch = 0.2656s, grad.norm=17.74842072
 30621: 23 [  100/ 1327], train_loss/perplexity = 3.96053839/52.4855766 secs/batch = 0.2642s, grad.norm=17.87624359
 30626: 23 [  105/ 1327], train_loss/perplexity = 3.78120494/43.8688698 secs/batch = 0.2666s, grad.norm=18.28078842
 30631: 23 [  110/ 1327], train_loss/perplexity = 3.68696737/39.9235916 secs/batch = 0.2656s, grad.norm=18.23081589
 30636: 23 [  115/ 1327], train_loss/perplexity = 3.64918971/38.4435043 secs/batch = 0.2663s, grad.norm=18.00270462
 30641: 23 [  120/ 1327], train_loss/perplexity = 3.71308899/40.9801979 secs/batch = 0.2660s, grad.norm=17.49031830
 30646: 23 [  125/ 1327], train_loss/perplexity = 3.78384805/43.9849739 secs/batch = 0.2666s, grad.norm=18.31344604
 30651: 23 [  130/ 1327], train_loss/perplexity = 3.78380442/43.9830551 secs/batch = 0.2668s, grad.norm=19.03167725
 30656: 23 [  135/ 1327], train_loss/perplexity = 3.74207139/42.1852837 secs/batch = 0.2661s, grad.norm=17.71977425
 30661: 23 [  140/ 1327], train_loss/perplexity = 4.06549835/58.2939529 secs/batch = 0.2662s, grad.norm=18.65673065
 30666: 23 [  145/ 1327], train_loss/perplexity = 3.83322811/46.2114754 secs/batch = 0.2664s, grad.norm=18.33798027
 30671: 23 [  150/ 1327], train_loss/perplexity = 3.95336151/52.1102409 secs/batch = 0.2664s, grad.norm=18.60781860
 30676: 23 [  155/ 1327], train_loss/perplexity = 4.16762352/64.5618362 secs/batch = 0.2666s, grad.norm=18.17405891
 30681: 23 [  160/ 1327], train_loss/perplexity = 3.89298844/49.0572739 secs/batch = 0.2669s, grad.norm=17.33495522
 30686: 23 [  165/ 1327], train_loss/perplexity = 4.06343746/58.1739388 secs/batch = 0.2666s, grad.norm=17.98884201
 30691: 23 [  170/ 1327], train_loss/perplexity = 3.80356646/44.8608932 secs/batch = 0.2669s, grad.norm=17.62466049
 30696: 23 [  175/ 1327], train_loss/perplexity = 4.10797834/60.8236275 secs/batch = 0.2663s, grad.norm=17.90859795
 30701: 23 [  180/ 1327], train_loss/perplexity = 3.98408151/53.7359123 secs/batch = 0.2667s, grad.norm=18.28467369
 30706: 23 [  185/ 1327], train_loss/perplexity = 4.28163576/72.3587036 secs/batch = 0.2620s, grad.norm=17.98021889
 30711: 23 [  190/ 1327], train_loss/perplexity = 3.84572363/46.7925339 secs/batch = 0.2625s, grad.norm=17.31894875
 30716: 23 [  195/ 1327], train_loss/perplexity = 4.04403257/57.0559616 secs/batch = 0.2642s, grad.norm=17.23451424
 30721: 23 [  200/ 1327], train_loss/perplexity = 3.99755335/54.4647293 secs/batch = 0.2652s, grad.norm=18.56090927
 30726: 23 [  205/ 1327], train_loss/perplexity = 4.12188530/61.6754112 secs/batch = 0.2662s, grad.norm=18.08206177
 30731: 23 [  210/ 1327], train_loss/perplexity = 3.98995352/54.0523758 secs/batch = 0.2618s, grad.norm=17.22762680
 30736: 23 [  215/ 1327], train_loss/perplexity = 4.16367626/64.3075027 secs/batch = 0.2665s, grad.norm=17.28434753
 30741: 23 [  220/ 1327], train_loss/perplexity = 4.03899002/56.7689781 secs/batch = 0.2656s, grad.norm=17.29448509
 30746: 23 [  225/ 1327], train_loss/perplexity = 4.24595022/69.8220749 secs/batch = 0.2643s, grad.norm=18.22622299
 30751: 23 [  230/ 1327], train_loss/perplexity = 4.02622080/56.0486908 secs/batch = 0.2644s, grad.norm=19.00006866
 30756: 23 [  235/ 1327], train_loss/perplexity = 3.92588305/50.6978264 secs/batch = 0.2660s, grad.norm=17.95390129
 30761: 23 [  240/ 1327], train_loss/perplexity = 3.73453665/41.8686218 secs/batch = 0.2673s, grad.norm=18.26733589
 30766: 23 [  245/ 1327], train_loss/perplexity = 3.95914841/52.4126740 secs/batch = 0.2657s, grad.norm=18.02633476
 30771: 23 [  250/ 1327], train_loss/perplexity = 3.84707069/46.8556061 secs/batch = 0.2658s, grad.norm=17.43459892
 30776: 23 [  255/ 1327], train_loss/perplexity = 3.81187439/45.2351494 secs/batch = 0.2665s, grad.norm=17.91061211
 30781: 23 [  260/ 1327], train_loss/perplexity = 3.94388986/51.6190033 secs/batch = 0.2667s, grad.norm=18.49897385
 30786: 23 [  265/ 1327], train_loss/perplexity = 4.25177526/70.2299805 secs/batch = 0.2656s, grad.norm=17.41721344
 30791: 23 [  270/ 1327], train_loss/perplexity = 4.23392534/68.9875031 secs/batch = 0.2668s, grad.norm=18.06278992
 30796: 23 [  275/ 1327], train_loss/perplexity = 4.24170208/69.5260925 secs/batch = 0.2642s, grad.norm=17.69982147
 30801: 23 [  280/ 1327], train_loss/perplexity = 4.02565527/56.0170021 secs/batch = 0.2642s, grad.norm=17.50310516
 30806: 23 [  285/ 1327], train_loss/perplexity = 4.36955690/79.0086136 secs/batch = 0.2669s, grad.norm=18.24589920
 30811: 23 [  290/ 1327], train_loss/perplexity = 3.93902349/51.3684158 secs/batch = 0.2663s, grad.norm=18.39060974
 30816: 23 [  295/ 1327], train_loss/perplexity = 3.81152201/45.2192116 secs/batch = 0.2656s, grad.norm=17.59822655
 30821: 23 [  300/ 1327], train_loss/perplexity = 3.33919001/28.1962795 secs/batch = 0.2661s, grad.norm=16.97310638
 30826: 23 [  305/ 1327], train_loss/perplexity = 3.81722116/45.4776573 secs/batch = 0.2640s, grad.norm=17.23981667
 30831: 23 [  310/ 1327], train_loss/perplexity = 3.82623935/45.8896370 secs/batch = 0.2658s, grad.norm=17.25066757
 30836: 23 [  315/ 1327], train_loss/perplexity = 3.40047002/29.9781876 secs/batch = 0.2640s, grad.norm=16.57762146
 30841: 23 [  320/ 1327], train_loss/perplexity = 3.37021351/29.0847359 secs/batch = 0.2656s, grad.norm=18.01279640
 30846: 23 [  325/ 1327], train_loss/perplexity = 3.34717798/28.4224110 secs/batch = 0.2668s, grad.norm=16.51575851
 30851: 23 [  330/ 1327], train_loss/perplexity = 3.98271513/53.6625366 secs/batch = 0.2669s, grad.norm=17.69159889
 30856: 23 [  335/ 1327], train_loss/perplexity = 3.39954209/29.9503822 secs/batch = 0.2669s, grad.norm=16.33455086
 30861: 23 [  340/ 1327], train_loss/perplexity = 4.15974236/64.0550156 secs/batch = 0.2649s, grad.norm=17.82563400
 30866: 23 [  345/ 1327], train_loss/perplexity = 3.92976475/50.8950043 secs/batch = 0.2661s, grad.norm=17.15069962
 30871: 23 [  350/ 1327], train_loss/perplexity = 3.89685345/49.2472458 secs/batch = 0.2663s, grad.norm=18.09298515
 30876: 23 [  355/ 1327], train_loss/perplexity = 3.88985944/48.9040108 secs/batch = 0.2633s, grad.norm=17.66941833
 30881: 23 [  360/ 1327], train_loss/perplexity = 4.06689358/58.3753433 secs/batch = 0.2644s, grad.norm=19.43932915
 30886: 23 [  365/ 1327], train_loss/perplexity = 4.03468943/56.5253639 secs/batch = 0.2679s, grad.norm=17.96920776
 30891: 23 [  370/ 1327], train_loss/perplexity = 4.06191063/58.0851860 secs/batch = 0.2655s, grad.norm=18.20106697
 30896: 23 [  375/ 1327], train_loss/perplexity = 3.52216196/33.8575478 secs/batch = 0.2670s, grad.norm=17.84689522
 30901: 23 [  380/ 1327], train_loss/perplexity = 3.62784719/37.6317139 secs/batch = 0.2667s, grad.norm=18.06819725
 30906: 23 [  385/ 1327], train_loss/perplexity = 3.70210695/40.5326157 secs/batch = 0.2668s, grad.norm=18.15766525
 30911: 23 [  390/ 1327], train_loss/perplexity = 3.91977930/50.3893242 secs/batch = 0.2666s, grad.norm=17.84639549
 30916: 23 [  395/ 1327], train_loss/perplexity = 3.95851302/52.3793793 secs/batch = 0.2655s, grad.norm=17.52859306
 30921: 23 [  400/ 1327], train_loss/perplexity = 3.89367819/49.0911217 secs/batch = 0.2617s, grad.norm=17.37858391
 30926: 23 [  405/ 1327], train_loss/perplexity = 4.11167002/61.0485840 secs/batch = 0.2661s, grad.norm=17.83220863
 30931: 23 [  410/ 1327], train_loss/perplexity = 3.79123926/44.3112793 secs/batch = 0.2656s, grad.norm=18.13338470
 30936: 23 [  415/ 1327], train_loss/perplexity = 3.79239941/44.3627167 secs/batch = 0.2644s, grad.norm=17.24836159
 30941: 23 [  420/ 1327], train_loss/perplexity = 3.38931394/29.6456070 secs/batch = 0.2659s, grad.norm=17.19839668
 30946: 23 [  425/ 1327], train_loss/perplexity = 3.74252319/42.2043457 secs/batch = 0.2645s, grad.norm=18.58083534
 30951: 23 [  430/ 1327], train_loss/perplexity = 3.90732145/49.7654724 secs/batch = 0.2655s, grad.norm=18.08569145
 30956: 23 [  435/ 1327], train_loss/perplexity = 4.01659012/55.5114937 secs/batch = 0.2668s, grad.norm=18.24978638
 30961: 23 [  440/ 1327], train_loss/perplexity = 3.54813957/34.7486115 secs/batch = 0.2656s, grad.norm=17.54544449
 30966: 23 [  445/ 1327], train_loss/perplexity = 3.88534403/48.6836891 secs/batch = 0.2662s, grad.norm=18.07970428
 30971: 23 [  450/ 1327], train_loss/perplexity = 3.89851665/49.3292236 secs/batch = 0.2664s, grad.norm=17.53936005
 30976: 23 [  455/ 1327], train_loss/perplexity = 3.80420613/44.8895988 secs/batch = 0.2661s, grad.norm=17.33299637
 30981: 23 [  460/ 1327], train_loss/perplexity = 3.78910065/44.2166176 secs/batch = 0.2659s, grad.norm=17.77444077
 30986: 23 [  465/ 1327], train_loss/perplexity = 3.45944953/31.7994671 secs/batch = 0.2663s, grad.norm=17.83111572
 30991: 23 [  470/ 1327], train_loss/perplexity = 4.24523067/69.7718506 secs/batch = 0.2681s, grad.norm=18.17490768
 30996: 23 [  475/ 1327], train_loss/perplexity = 3.70428371/40.6209412 secs/batch = 0.2676s, grad.norm=17.52979660
 31001: 23 [  480/ 1327], train_loss/perplexity = 3.82144332/45.6700783 secs/batch = 0.2667s, grad.norm=17.96038437
 31006: 23 [  485/ 1327], train_loss/perplexity = 3.74160910/42.1657829 secs/batch = 0.2673s, grad.norm=17.84248161
 31011: 23 [  490/ 1327], train_loss/perplexity = 3.63886523/38.0486374 secs/batch = 0.2660s, grad.norm=19.07639503
 31016: 23 [  495/ 1327], train_loss/perplexity = 3.74651361/42.3730965 secs/batch = 0.2668s, grad.norm=17.59131813
 31021: 23 [  500/ 1327], train_loss/perplexity = 3.87972260/48.4107857 secs/batch = 0.2642s, grad.norm=17.57979584
 31026: 23 [  505/ 1327], train_loss/perplexity = 4.00400019/54.8169899 secs/batch = 0.2659s, grad.norm=17.04276657
 31031: 23 [  510/ 1327], train_loss/perplexity = 4.36402988/78.5731354 secs/batch = 0.2662s, grad.norm=17.36426735
 31036: 23 [  515/ 1327], train_loss/perplexity = 3.95991302/52.4527626 secs/batch = 0.2638s, grad.norm=17.24692154
 31041: 23 [  520/ 1327], train_loss/perplexity = 4.10392332/60.5774879 secs/batch = 0.2663s, grad.norm=17.45631790
 31046: 23 [  525/ 1327], train_loss/perplexity = 3.75382471/42.6840248 secs/batch = 0.2670s, grad.norm=17.30953026
 31051: 23 [  530/ 1327], train_loss/perplexity = 3.74460077/42.2921181 secs/batch = 0.2656s, grad.norm=17.87370110
 31056: 23 [  535/ 1327], train_loss/perplexity = 3.94444704/51.6477699 secs/batch = 0.2661s, grad.norm=17.82430077
 31061: 23 [  540/ 1327], train_loss/perplexity = 3.93470955/51.1472931 secs/batch = 0.2642s, grad.norm=17.12348938
 31066: 23 [  545/ 1327], train_loss/perplexity = 3.92244720/50.5239372 secs/batch = 0.2652s, grad.norm=17.64363861
 31071: 23 [  550/ 1327], train_loss/perplexity = 3.90737009/49.7678947 secs/batch = 0.2589s, grad.norm=17.70324707
 31076: 23 [  555/ 1327], train_loss/perplexity = 3.77378273/43.5444717 secs/batch = 0.2640s, grad.norm=17.26568604
 31081: 23 [  560/ 1327], train_loss/perplexity = 3.89355755/49.0851974 secs/batch = 0.2660s, grad.norm=18.89816475
 31086: 23 [  565/ 1327], train_loss/perplexity = 3.69352818/40.1863823 secs/batch = 0.2674s, grad.norm=18.38909721
 31091: 23 [  570/ 1327], train_loss/perplexity = 3.75129795/42.5763092 secs/batch = 0.2658s, grad.norm=18.34778786
 31096: 23 [  575/ 1327], train_loss/perplexity = 3.59539890/36.4302292 secs/batch = 0.2665s, grad.norm=18.02699852
 31101: 23 [  580/ 1327], train_loss/perplexity = 3.89010406/48.9159775 secs/batch = 0.2652s, grad.norm=17.86043358
 31106: 23 [  585/ 1327], train_loss/perplexity = 3.54752207/34.7271576 secs/batch = 0.2660s, grad.norm=17.46074867
 31111: 23 [  590/ 1327], train_loss/perplexity = 4.07062769/58.5937309 secs/batch = 0.2618s, grad.norm=17.62465096
 31116: 23 [  595/ 1327], train_loss/perplexity = 3.82398868/45.7864723 secs/batch = 0.2651s, grad.norm=17.96747971
 31121: 23 [  600/ 1327], train_loss/perplexity = 4.12766075/62.0326424 secs/batch = 0.2660s, grad.norm=17.19206619
 31126: 23 [  605/ 1327], train_loss/perplexity = 3.94822693/51.8433647 secs/batch = 0.2647s, grad.norm=17.20365334
 31131: 23 [  610/ 1327], train_loss/perplexity = 4.12657738/61.9654770 secs/batch = 0.2656s, grad.norm=18.03052711
 31136: 23 [  615/ 1327], train_loss/perplexity = 3.71582246/41.0923691 secs/batch = 0.2667s, grad.norm=16.92344666
 31141: 23 [  620/ 1327], train_loss/perplexity = 4.12433624/61.8267593 secs/batch = 0.2661s, grad.norm=17.63547325
 31146: 23 [  625/ 1327], train_loss/perplexity = 4.07383490/58.7819519 secs/batch = 0.2668s, grad.norm=17.42207146
 31151: 23 [  630/ 1327], train_loss/perplexity = 4.12185669/61.6736450 secs/batch = 0.2666s, grad.norm=17.58363342
 31156: 23 [  635/ 1327], train_loss/perplexity = 3.85739374/47.3418045 secs/batch = 0.2638s, grad.norm=17.56019020
 31161: 23 [  640/ 1327], train_loss/perplexity = 3.91796970/50.2982216 secs/batch = 0.2658s, grad.norm=17.58308029
 31166: 23 [  645/ 1327], train_loss/perplexity = 4.08969402/59.7216148 secs/batch = 0.2660s, grad.norm=18.38311386
 31171: 23 [  650/ 1327], train_loss/perplexity = 3.59933925/36.5740585 secs/batch = 0.2659s, grad.norm=17.57481194
 31176: 23 [  655/ 1327], train_loss/perplexity = 3.81214213/45.2472610 secs/batch = 0.2664s, grad.norm=17.52862167
 31181: 23 [  660/ 1327], train_loss/perplexity = 3.70285416/40.5629120 secs/batch = 0.2658s, grad.norm=17.96211052
 31186: 23 [  665/ 1327], train_loss/perplexity = 3.90740633/49.7696991 secs/batch = 0.2663s, grad.norm=17.92624664
 31191: 23 [  670/ 1327], train_loss/perplexity = 3.85688710/47.3178253 secs/batch = 0.2654s, grad.norm=17.64769554
 31196: 23 [  675/ 1327], train_loss/perplexity = 3.57676601/35.7577133 secs/batch = 0.2674s, grad.norm=17.72019386
 31201: 23 [  680/ 1327], train_loss/perplexity = 3.82424808/45.7983513 secs/batch = 0.2674s, grad.norm=18.50149345
 31206: 23 [  685/ 1327], train_loss/perplexity = 3.53067732/34.1470871 secs/batch = 0.2669s, grad.norm=16.83405113
 31211: 23 [  690/ 1327], train_loss/perplexity = 4.02238703/55.8342247 secs/batch = 0.2664s, grad.norm=17.19101715
 31216: 23 [  695/ 1327], train_loss/perplexity = 3.91694236/50.2465744 secs/batch = 0.2650s, grad.norm=17.38721848
 31221: 23 [  700/ 1327], train_loss/perplexity = 4.15815115/63.9531746 secs/batch = 0.2664s, grad.norm=18.29238892
 31226: 23 [  705/ 1327], train_loss/perplexity = 3.86507893/47.7070389 secs/batch = 0.2628s, grad.norm=17.25667572
 31231: 23 [  710/ 1327], train_loss/perplexity = 3.72490072/41.4671173 secs/batch = 0.2659s, grad.norm=18.18320656
 31236: 23 [  715/ 1327], train_loss/perplexity = 3.58068275/35.8980408 secs/batch = 0.2663s, grad.norm=17.40128899
 31241: 23 [  720/ 1327], train_loss/perplexity = 3.61748552/37.2438011 secs/batch = 0.2639s, grad.norm=18.63906479
 31246: 23 [  725/ 1327], train_loss/perplexity = 3.68194628/39.7236328 secs/batch = 0.2644s, grad.norm=17.79911804
 31251: 23 [  730/ 1327], train_loss/perplexity = 3.92578220/50.6927147 secs/batch = 0.2664s, grad.norm=18.47324944
 31256: 23 [  735/ 1327], train_loss/perplexity = 3.80458879/44.9067802 secs/batch = 0.2659s, grad.norm=18.78652573
 31261: 23 [  740/ 1327], train_loss/perplexity = 3.40868330/30.2254200 secs/batch = 0.2651s, grad.norm=16.35653877
 31266: 23 [  745/ 1327], train_loss/perplexity = 3.86925268/47.9065704 secs/batch = 0.2624s, grad.norm=17.87377548
 31271: 23 [  750/ 1327], train_loss/perplexity = 3.82939935/46.0348778 secs/batch = 0.2668s, grad.norm=17.70990372
 31276: 23 [  755/ 1327], train_loss/perplexity = 3.67573380/39.4776154 secs/batch = 0.2657s, grad.norm=17.27947807
 31281: 23 [  760/ 1327], train_loss/perplexity = 3.51115084/33.4867821 secs/batch = 0.2658s, grad.norm=16.44127846
 31286: 23 [  765/ 1327], train_loss/perplexity = 3.58759475/36.1470299 secs/batch = 0.2666s, grad.norm=16.81815338
 31291: 23 [  770/ 1327], train_loss/perplexity = 3.56771874/35.4356613 secs/batch = 0.2663s, grad.norm=17.18730736
 31296: 23 [  775/ 1327], train_loss/perplexity = 3.63053894/37.7331467 secs/batch = 0.2663s, grad.norm=17.88513565
 31301: 23 [  780/ 1327], train_loss/perplexity = 3.97724152/53.3696136 secs/batch = 0.2663s, grad.norm=17.95243645
 31306: 23 [  785/ 1327], train_loss/perplexity = 3.84645176/46.8266144 secs/batch = 0.2665s, grad.norm=18.52950287
 31311: 23 [  790/ 1327], train_loss/perplexity = 3.62012291/37.3421555 secs/batch = 0.2660s, grad.norm=17.84952164
 31316: 23 [  795/ 1327], train_loss/perplexity = 3.97654605/53.3325081 secs/batch = 0.2660s, grad.norm=17.94201279
 31321: 23 [  800/ 1327], train_loss/perplexity = 3.87881827/48.3670235 secs/batch = 0.2663s, grad.norm=18.28920174
 31326: 23 [  805/ 1327], train_loss/perplexity = 4.18067980/65.4103012 secs/batch = 0.2666s, grad.norm=18.07065010
 31331: 23 [  810/ 1327], train_loss/perplexity = 3.71009469/40.8576736 secs/batch = 0.2664s, grad.norm=16.51648903
 31336: 23 [  815/ 1327], train_loss/perplexity = 3.70843196/40.7897949 secs/batch = 0.2640s, grad.norm=17.16804123
 31341: 23 [  820/ 1327], train_loss/perplexity = 3.61778474/37.2549477 secs/batch = 0.2667s, grad.norm=16.51564026
 31346: 23 [  825/ 1327], train_loss/perplexity = 3.73782277/42.0064316 secs/batch = 0.2645s, grad.norm=17.69439697
 31351: 23 [  830/ 1327], train_loss/perplexity = 3.44476843/31.3360252 secs/batch = 0.2656s, grad.norm=17.67236710
 31356: 23 [  835/ 1327], train_loss/perplexity = 3.78267384/43.9333572 secs/batch = 0.2676s, grad.norm=18.00647926
 31361: 23 [  840/ 1327], train_loss/perplexity = 3.86616659/47.7589569 secs/batch = 0.2679s, grad.norm=17.57113838
 31366: 23 [  845/ 1327], train_loss/perplexity = 3.74733090/42.4077415 secs/batch = 0.2665s, grad.norm=18.30969238
 31371: 23 [  850/ 1327], train_loss/perplexity = 3.72076464/41.2959595 secs/batch = 0.2646s, grad.norm=17.01556396
 31376: 23 [  855/ 1327], train_loss/perplexity = 3.75019932/42.5295563 secs/batch = 0.2663s, grad.norm=18.07515335
 31381: 23 [  860/ 1327], train_loss/perplexity = 3.49728298/33.0256004 secs/batch = 0.2657s, grad.norm=16.92585564
 31386: 23 [  865/ 1327], train_loss/perplexity = 3.95996523/52.4555016 secs/batch = 0.2670s, grad.norm=17.60939407
 31391: 23 [  870/ 1327], train_loss/perplexity = 3.77857041/43.7534485 secs/batch = 0.2658s, grad.norm=17.80831337
 31396: 23 [  875/ 1327], train_loss/perplexity = 3.44179869/31.2431049 secs/batch = 0.2642s, grad.norm=16.82247925
 31401: 23 [  880/ 1327], train_loss/perplexity = 3.61917448/37.3067589 secs/batch = 0.2666s, grad.norm=17.35285187
 31406: 23 [  885/ 1327], train_loss/perplexity = 3.82593942/45.8758774 secs/batch = 0.2657s, grad.norm=17.10553551
 31411: 23 [  890/ 1327], train_loss/perplexity = 3.91890812/50.3454437 secs/batch = 0.2653s, grad.norm=17.78094482
 31416: 23 [  895/ 1327], train_loss/perplexity = 3.90570402/49.6850471 secs/batch = 0.2667s, grad.norm=17.51015091
 31421: 23 [  900/ 1327], train_loss/perplexity = 3.81077385/45.1853905 secs/batch = 0.2641s, grad.norm=16.89398193
 31426: 23 [  905/ 1327], train_loss/perplexity = 3.65618277/38.7132835 secs/batch = 0.2665s, grad.norm=16.52811813
 31431: 23 [  910/ 1327], train_loss/perplexity = 3.71239638/40.9518242 secs/batch = 0.2647s, grad.norm=16.23798943
 31436: 23 [  915/ 1327], train_loss/perplexity = 3.94811440/51.8375282 secs/batch = 0.2657s, grad.norm=16.97915077
 31441: 23 [  920/ 1327], train_loss/perplexity = 4.09731483/60.1784821 secs/batch = 0.2656s, grad.norm=17.89814949
 31446: 23 [  925/ 1327], train_loss/perplexity = 3.86794353/47.8438950 secs/batch = 0.2666s, grad.norm=17.18402672
 31451: 23 [  930/ 1327], train_loss/perplexity = 3.92624640/50.7162514 secs/batch = 0.2661s, grad.norm=17.31142426
 31456: 23 [  935/ 1327], train_loss/perplexity = 3.91523457/50.1608353 secs/batch = 0.2668s, grad.norm=17.19060326
 31461: 23 [  940/ 1327], train_loss/perplexity = 3.88647556/48.7388077 secs/batch = 0.2651s, grad.norm=16.67611694
 31466: 23 [  945/ 1327], train_loss/perplexity = 4.14680624/63.2317314 secs/batch = 0.2665s, grad.norm=17.27602959
 31471: 23 [  950/ 1327], train_loss/perplexity = 3.91511226/50.1547012 secs/batch = 0.2633s, grad.norm=17.03691483
 31476: 23 [  955/ 1327], train_loss/perplexity = 3.80141139/44.7643204 secs/batch = 0.2665s, grad.norm=16.89733505
 31481: 23 [  960/ 1327], train_loss/perplexity = 4.16739130/64.5468521 secs/batch = 0.2606s, grad.norm=18.05124474
 31486: 23 [  965/ 1327], train_loss/perplexity = 3.88246274/48.5436172 secs/batch = 0.2660s, grad.norm=17.59966660
 31491: 23 [  970/ 1327], train_loss/perplexity = 4.11283588/61.1198006 secs/batch = 0.2660s, grad.norm=17.73137283
 31496: 23 [  975/ 1327], train_loss/perplexity = 3.76758385/43.2753792 secs/batch = 0.2665s, grad.norm=18.33154678
 31501: 23 [  980/ 1327], train_loss/perplexity = 3.66103578/38.9016151 secs/batch = 0.2662s, grad.norm=17.08372688
 31506: 23 [  985/ 1327], train_loss/perplexity = 3.81086016/45.1892929 secs/batch = 0.2667s, grad.norm=18.03824234
 31511: 23 [  990/ 1327], train_loss/perplexity = 3.95200038/52.0393600 secs/batch = 0.2605s, grad.norm=17.89788246
 31516: 23 [  995/ 1327], train_loss/perplexity = 4.01811314/55.5961037 secs/batch = 0.2656s, grad.norm=17.46046257
 31521: 23 [ 1000/ 1327], train_loss/perplexity = 3.55207300/34.8855591 secs/batch = 0.2663s, grad.norm=17.17317200
 31526: 23 [ 1005/ 1327], train_loss/perplexity = 4.01975584/55.6875076 secs/batch = 0.2670s, grad.norm=17.26642227
 31531: 23 [ 1010/ 1327], train_loss/perplexity = 3.64655495/38.3423462 secs/batch = 0.2662s, grad.norm=16.22890854
 31536: 23 [ 1015/ 1327], train_loss/perplexity = 4.03369093/56.4689484 secs/batch = 0.2668s, grad.norm=17.17245674
 31541: 23 [ 1020/ 1327], train_loss/perplexity = 4.15530920/63.7716789 secs/batch = 0.2656s, grad.norm=17.17910004
 31546: 23 [ 1025/ 1327], train_loss/perplexity = 4.10057831/60.3751945 secs/batch = 0.2666s, grad.norm=17.60159492
 31551: 23 [ 1030/ 1327], train_loss/perplexity = 3.79836512/44.6281624 secs/batch = 0.2665s, grad.norm=16.72360611
 31556: 23 [ 1035/ 1327], train_loss/perplexity = 3.73164296/41.7476425 secs/batch = 0.2672s, grad.norm=16.63810158
 31561: 23 [ 1040/ 1327], train_loss/perplexity = 3.97793460/53.4066124 secs/batch = 0.2659s, grad.norm=17.38473701
 31566: 23 [ 1045/ 1327], train_loss/perplexity = 3.49566507/32.9722099 secs/batch = 0.2622s, grad.norm=16.39975166
 31571: 23 [ 1050/ 1327], train_loss/perplexity = 3.57806206/35.8040886 secs/batch = 0.2665s, grad.norm=16.98080063
 31576: 23 [ 1055/ 1327], train_loss/perplexity = 3.75141621/42.5813446 secs/batch = 0.2668s, grad.norm=17.78470802
 31581: 23 [ 1060/ 1327], train_loss/perplexity = 3.24257803/25.5996342 secs/batch = 0.2652s, grad.norm=17.47556114
 31586: 23 [ 1065/ 1327], train_loss/perplexity = 3.47014737/32.1414795 secs/batch = 0.2662s, grad.norm=17.33045387
 31591: 23 [ 1070/ 1327], train_loss/perplexity = 3.85123634/47.0511971 secs/batch = 0.2668s, grad.norm=17.41814041
 31596: 23 [ 1075/ 1327], train_loss/perplexity = 3.55510855/34.9916191 secs/batch = 0.2646s, grad.norm=17.01266289
 31601: 23 [ 1080/ 1327], train_loss/perplexity = 3.53343463/34.2413712 secs/batch = 0.2629s, grad.norm=16.93070602
 31606: 23 [ 1085/ 1327], train_loss/perplexity = 3.44258642/31.2677250 secs/batch = 0.2672s, grad.norm=17.28663635
 31611: 23 [ 1090/ 1327], train_loss/perplexity = 3.56640363/35.3890915 secs/batch = 0.2660s, grad.norm=17.74483871
 31616: 23 [ 1095/ 1327], train_loss/perplexity = 3.76254892/43.0580368 secs/batch = 0.2663s, grad.norm=17.63405991
 31621: 23 [ 1100/ 1327], train_loss/perplexity = 3.54726243/34.7181435 secs/batch = 0.2642s, grad.norm=19.68399620
 31626: 23 [ 1105/ 1327], train_loss/perplexity = 3.50975895/33.4402046 secs/batch = 0.2674s, grad.norm=18.26573944
 31631: 23 [ 1110/ 1327], train_loss/perplexity = 3.78971624/44.2438431 secs/batch = 0.2660s, grad.norm=18.04651833
 31636: 23 [ 1115/ 1327], train_loss/perplexity = 3.58887577/36.1933632 secs/batch = 0.2625s, grad.norm=16.63582039
 31641: 23 [ 1120/ 1327], train_loss/perplexity = 3.82795382/45.9683838 secs/batch = 0.2657s, grad.norm=16.99752426
 31646: 23 [ 1125/ 1327], train_loss/perplexity = 4.00387096/54.8099060 secs/batch = 0.2643s, grad.norm=18.60744667
 31651: 23 [ 1130/ 1327], train_loss/perplexity = 3.68396068/39.8037338 secs/batch = 0.2642s, grad.norm=17.66246223
 31656: 23 [ 1135/ 1327], train_loss/perplexity = 3.61090088/36.9993706 secs/batch = 0.2631s, grad.norm=17.40138054
 31661: 23 [ 1140/ 1327], train_loss/perplexity = 3.93904471/51.3695068 secs/batch = 0.2656s, grad.norm=18.51337433
 31666: 23 [ 1145/ 1327], train_loss/perplexity = 3.74883413/42.4715385 secs/batch = 0.2644s, grad.norm=17.11960030
 31671: 23 [ 1150/ 1327], train_loss/perplexity = 3.68414116/39.8109169 secs/batch = 0.2648s, grad.norm=16.98885727
 31676: 23 [ 1155/ 1327], train_loss/perplexity = 3.81079817/45.1864891 secs/batch = 0.2664s, grad.norm=17.75567627
 31681: 23 [ 1160/ 1327], train_loss/perplexity = 3.72012329/41.2694817 secs/batch = 0.2672s, grad.norm=17.36020279
 31686: 23 [ 1165/ 1327], train_loss/perplexity = 3.80692887/45.0119896 secs/batch = 0.2671s, grad.norm=17.97957611
 31691: 23 [ 1170/ 1327], train_loss/perplexity = 3.63624454/37.9490509 secs/batch = 0.2670s, grad.norm=17.49450302
 31696: 23 [ 1175/ 1327], train_loss/perplexity = 3.49324727/32.8925858 secs/batch = 0.2667s, grad.norm=16.84253883
 31701: 23 [ 1180/ 1327], train_loss/perplexity = 3.51348925/33.5651817 secs/batch = 0.2658s, grad.norm=17.91803741
 31706: 23 [ 1185/ 1327], train_loss/perplexity = 3.62244892/37.4291153 secs/batch = 0.2672s, grad.norm=17.65975189
 31711: 23 [ 1190/ 1327], train_loss/perplexity = 3.71008801/40.8574028 secs/batch = 0.2653s, grad.norm=17.50611877
 31716: 23 [ 1195/ 1327], train_loss/perplexity = 3.58774304/36.1523895 secs/batch = 0.2668s, grad.norm=17.49136353
 31721: 23 [ 1200/ 1327], train_loss/perplexity = 3.54947829/34.7951584 secs/batch = 0.2665s, grad.norm=17.12269783
 31726: 23 [ 1205/ 1327], train_loss/perplexity = 3.49941611/33.0961227 secs/batch = 0.2669s, grad.norm=17.74197960
 31731: 23 [ 1210/ 1327], train_loss/perplexity = 3.17621970/23.9560204 secs/batch = 0.2657s, grad.norm=17.31152534
 31736: 23 [ 1215/ 1327], train_loss/perplexity = 3.48557806/32.6412888 secs/batch = 0.2667s, grad.norm=16.64500237
 31741: 23 [ 1220/ 1327], train_loss/perplexity = 3.55151415/34.8660698 secs/batch = 0.2666s, grad.norm=17.58796883
 31746: 23 [ 1225/ 1327], train_loss/perplexity = 3.30932379/27.3666134 secs/batch = 0.2653s, grad.norm=17.63829803
 31751: 23 [ 1230/ 1327], train_loss/perplexity = 3.60594201/36.8163490 secs/batch = 0.2668s, grad.norm=17.40086174
 31756: 23 [ 1235/ 1327], train_loss/perplexity = 3.48453283/32.6071892 secs/batch = 0.2654s, grad.norm=17.14732552
 31761: 23 [ 1240/ 1327], train_loss/perplexity = 3.72594523/41.5104523 secs/batch = 0.2662s, grad.norm=17.59257889
 31766: 23 [ 1245/ 1327], train_loss/perplexity = 3.64892268/38.4332390 secs/batch = 0.2663s, grad.norm=17.39579582
 31771: 23 [ 1250/ 1327], train_loss/perplexity = 3.79956770/44.6818657 secs/batch = 0.2657s, grad.norm=16.98414803
 31776: 23 [ 1255/ 1327], train_loss/perplexity = 3.79010630/44.2611046 secs/batch = 0.2636s, grad.norm=16.91328430
 31781: 23 [ 1260/ 1327], train_loss/perplexity = 3.60491276/36.7784767 secs/batch = 0.2615s, grad.norm=18.12411118
 31786: 23 [ 1265/ 1327], train_loss/perplexity = 3.76920271/43.3454933 secs/batch = 0.2665s, grad.norm=17.39163589
 31791: 23 [ 1270/ 1327], train_loss/perplexity = 3.51330256/33.5589142 secs/batch = 0.2662s, grad.norm=17.38576698
 31796: 23 [ 1275/ 1327], train_loss/perplexity = 3.69137049/40.0997658 secs/batch = 0.2652s, grad.norm=17.55182838
 31801: 23 [ 1280/ 1327], train_loss/perplexity = 3.63843989/38.0324554 secs/batch = 0.2658s, grad.norm=17.89579964
 31806: 23 [ 1285/ 1327], train_loss/perplexity = 3.44119859/31.2243614 secs/batch = 0.2656s, grad.norm=17.17004585
 31811: 23 [ 1290/ 1327], train_loss/perplexity = 3.77268791/43.4968224 secs/batch = 0.2665s, grad.norm=17.11444473
 31816: 23 [ 1295/ 1327], train_loss/perplexity = 3.67394590/39.4070969 secs/batch = 0.2663s, grad.norm=17.34275055
 31821: 23 [ 1300/ 1327], train_loss/perplexity = 3.83408260/46.2509766 secs/batch = 0.2672s, grad.norm=16.81793022
 31826: 23 [ 1305/ 1327], train_loss/perplexity = 3.94650173/51.7540016 secs/batch = 0.2660s, grad.norm=18.01604080
 31831: 23 [ 1310/ 1327], train_loss/perplexity = 4.17880964/65.2880936 secs/batch = 0.2662s, grad.norm=18.00076675
 31836: 23 [ 1315/ 1327], train_loss/perplexity = 3.97670603/53.3410416 secs/batch = 0.2657s, grad.norm=17.54243279
 31841: 23 [ 1320/ 1327], train_loss/perplexity = 3.89440346/49.1267395 secs/batch = 0.2657s, grad.norm=17.37397385
 31846: 23 [ 1325/ 1327], train_loss/perplexity = 3.87990117/48.4194298 secs/batch = 0.2662s, grad.norm=17.91552925
Epoch training time: 352.836546421051
	> validation loss = 4.57270288, perplexity = 96.80541229
	> validation loss = 4.51934528, perplexity = 91.77548981
	> validation loss = 4.48000193, perplexity = 88.23484039
	> validation loss = 4.55981970, perplexity = 95.56624603
	> validation loss = 4.66449308, perplexity = 106.11177826
	> validation loss = 4.61925125, perplexity = 101.41806793
	> validation loss = 4.53809261, perplexity = 93.51226807
	> validation loss = 4.38068295, perplexity = 79.89257812
	> validation loss = 4.17912388, perplexity = 65.30860901
	> validation loss = 4.27560568, perplexity = 71.92369080
	> validation loss = 4.47075748, perplexity = 87.42292023
	> validation loss = 4.45819807, perplexity = 86.33180237
	> validation loss = 4.44635677, perplexity = 85.31555176
	> validation loss = 4.17642164, perplexity = 65.13237000
	> validation loss = 4.13915253, perplexity = 62.74961853
	> validation loss = 4.18227625, perplexity = 65.51480865
	> validation loss = 4.60471725, perplexity = 99.95471954
	> validation loss = 4.10840273, perplexity = 60.84944534
	> validation loss = 4.62019777, perplexity = 101.51410675
	> validation loss = 4.48298264, perplexity = 88.49823761
	> validation loss = 4.25912189, perplexity = 70.74783325
at the end of epoch: 23
train loss = 3.84970371, perplexity = 46.97914183
validation loss = 4.42067151, perplexity = 83.15210414
Saved model cv/epoch023_4.4207.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.00195312
new learning rate is: 0.0009765625
 31853: 24 [    5/ 1327], train_loss/perplexity = 4.10751009/60.7951546 secs/batch = 0.2669s, grad.norm=17.62927437
 31858: 24 [   10/ 1327], train_loss/perplexity = 3.67930222/39.6187401 secs/batch = 0.2667s, grad.norm=17.09854126
 31863: 24 [   15/ 1327], train_loss/perplexity = 3.99629426/54.3961983 secs/batch = 0.2652s, grad.norm=16.88903618
 31868: 24 [   20/ 1327], train_loss/perplexity = 4.18973207/66.0051041 secs/batch = 0.2646s, grad.norm=16.74112511
 31873: 24 [   25/ 1327], train_loss/perplexity = 4.04361820/57.0323257 secs/batch = 0.2667s, grad.norm=18.16394043
 31878: 24 [   30/ 1327], train_loss/perplexity = 4.08517361/59.4522591 secs/batch = 0.2655s, grad.norm=17.77651596
 31883: 24 [   35/ 1327], train_loss/perplexity = 3.88314438/48.5767174 secs/batch = 0.2665s, grad.norm=17.67156410
 31888: 24 [   40/ 1327], train_loss/perplexity = 3.80919075/45.1139145 secs/batch = 0.2659s, grad.norm=17.38099861
 31893: 24 [   45/ 1327], train_loss/perplexity = 3.68650389/39.9050903 secs/batch = 0.2641s, grad.norm=16.83715439
 31898: 24 [   50/ 1327], train_loss/perplexity = 3.84617448/46.8136330 secs/batch = 0.2656s, grad.norm=17.98680115
 31903: 24 [   55/ 1327], train_loss/perplexity = 3.77463889/43.5817680 secs/batch = 0.2629s, grad.norm=18.05645561
 31908: 24 [   60/ 1327], train_loss/perplexity = 4.07201052/58.6748123 secs/batch = 0.2613s, grad.norm=17.87626839
 31913: 24 [   65/ 1327], train_loss/perplexity = 3.65262890/38.5759468 secs/batch = 0.2642s, grad.norm=17.42662239
 31918: 24 [   70/ 1327], train_loss/perplexity = 3.55930948/35.1389236 secs/batch = 0.2661s, grad.norm=17.76178932
 31923: 24 [   75/ 1327], train_loss/perplexity = 3.34548593/28.3743610 secs/batch = 0.2665s, grad.norm=16.49860001
 31928: 24 [   80/ 1327], train_loss/perplexity = 3.76676583/43.2399940 secs/batch = 0.2659s, grad.norm=17.32950783
 31933: 24 [   85/ 1327], train_loss/perplexity = 3.82616353/45.8861580 secs/batch = 0.2663s, grad.norm=18.04957199
 31938: 24 [   90/ 1327], train_loss/perplexity = 3.87128305/48.0039368 secs/batch = 0.2660s, grad.norm=17.80311966
 31943: 24 [   95/ 1327], train_loss/perplexity = 3.73807073/42.0168495 secs/batch = 0.2653s, grad.norm=17.57040596
 31948: 24 [  100/ 1327], train_loss/perplexity = 3.96647358/52.7980156 secs/batch = 0.2661s, grad.norm=17.99599457
 31953: 24 [  105/ 1327], train_loss/perplexity = 3.70316243/40.5754204 secs/batch = 0.2670s, grad.norm=18.23842049
 31958: 24 [  110/ 1327], train_loss/perplexity = 3.65670395/38.7334633 secs/batch = 0.2674s, grad.norm=17.78757477
 31963: 24 [  115/ 1327], train_loss/perplexity = 3.71117783/40.9019547 secs/batch = 0.2647s, grad.norm=18.20908928
 31968: 24 [  120/ 1327], train_loss/perplexity = 3.77081919/43.4156151 secs/batch = 0.2687s, grad.norm=18.23463821
 31973: 24 [  125/ 1327], train_loss/perplexity = 3.84176683/46.6077499 secs/batch = 0.2672s, grad.norm=18.39647102
 31978: 24 [  130/ 1327], train_loss/perplexity = 3.76176548/43.0243187 secs/batch = 0.2667s, grad.norm=18.26789284
 31983: 24 [  135/ 1327], train_loss/perplexity = 3.74130917/42.1531410 secs/batch = 0.2669s, grad.norm=17.74309349
 31988: 24 [  140/ 1327], train_loss/perplexity = 4.05667400/57.7818108 secs/batch = 0.2683s, grad.norm=18.34333038
 31993: 24 [  145/ 1327], train_loss/perplexity = 3.91120625/49.9591789 secs/batch = 0.2648s, grad.norm=18.53116035
 31998: 24 [  150/ 1327], train_loss/perplexity = 3.97801328/53.4108162 secs/batch = 0.2645s, grad.norm=18.73775864
 32003: 24 [  155/ 1327], train_loss/perplexity = 4.25246668/70.2785568 secs/batch = 0.2667s, grad.norm=18.33891869
 32008: 24 [  160/ 1327], train_loss/perplexity = 3.90660548/49.7298546 secs/batch = 0.2663s, grad.norm=17.17577934
 32013: 24 [  165/ 1327], train_loss/perplexity = 4.06667995/58.3628731 secs/batch = 0.2671s, grad.norm=17.95878983
 32018: 24 [  170/ 1327], train_loss/perplexity = 3.82133222/45.6650047 secs/batch = 0.2666s, grad.norm=17.74005890
 32023: 24 [  175/ 1327], train_loss/perplexity = 4.07823896/59.0414047 secs/batch = 0.2666s, grad.norm=17.92880440
 32028: 24 [  180/ 1327], train_loss/perplexity = 3.92516804/50.6615906 secs/batch = 0.2684s, grad.norm=18.03670502
 32033: 24 [  185/ 1327], train_loss/perplexity = 4.32143307/75.2964554 secs/batch = 0.2670s, grad.norm=18.37883949
 32038: 24 [  190/ 1327], train_loss/perplexity = 3.83905649/46.4815979 secs/batch = 0.2661s, grad.norm=17.00058365
 32043: 24 [  195/ 1327], train_loss/perplexity = 4.09724522/60.1742935 secs/batch = 0.2684s, grad.norm=17.50699043
 32048: 24 [  200/ 1327], train_loss/perplexity = 3.91808558/50.3040504 secs/batch = 0.2672s, grad.norm=18.13071823
 32053: 24 [  205/ 1327], train_loss/perplexity = 4.11220503/61.0812569 secs/batch = 0.2650s, grad.norm=18.13010025
 32058: 24 [  210/ 1327], train_loss/perplexity = 4.01217842/55.2671356 secs/batch = 0.2662s, grad.norm=17.06969643
 32063: 24 [  215/ 1327], train_loss/perplexity = 4.16910601/64.6576233 secs/batch = 0.2656s, grad.norm=17.22379303
 32068: 24 [  220/ 1327], train_loss/perplexity = 4.07484388/58.8412933 secs/batch = 0.2658s, grad.norm=17.16919708
 32073: 24 [  225/ 1327], train_loss/perplexity = 4.19354439/66.2572174 secs/batch = 0.2664s, grad.norm=18.16988182
 32078: 24 [  230/ 1327], train_loss/perplexity = 3.96426582/52.6815758 secs/batch = 0.2662s, grad.norm=18.86902046
 32083: 24 [  235/ 1327], train_loss/perplexity = 3.94797611/51.8303604 secs/batch = 0.2660s, grad.norm=17.86134338
 32088: 24 [  240/ 1327], train_loss/perplexity = 3.67386198/39.4037895 secs/batch = 0.2662s, grad.norm=17.92255592
 32093: 24 [  245/ 1327], train_loss/perplexity = 3.99660301/54.4129944 secs/batch = 0.2661s, grad.norm=18.00708961
 32098: 24 [  250/ 1327], train_loss/perplexity = 3.91276336/50.0370331 secs/batch = 0.2650s, grad.norm=17.25430870
 32103: 24 [  255/ 1327], train_loss/perplexity = 3.81925654/45.5703163 secs/batch = 0.2671s, grad.norm=17.80917168
 32108: 24 [  260/ 1327], train_loss/perplexity = 4.04148245/56.9106483 secs/batch = 0.2665s, grad.norm=18.79142952
 32113: 24 [  265/ 1327], train_loss/perplexity = 4.23208952/68.8609695 secs/batch = 0.2661s, grad.norm=17.60640526
 32118: 24 [  270/ 1327], train_loss/perplexity = 4.23354483/68.9612579 secs/batch = 0.2646s, grad.norm=17.87872505
 32123: 24 [  275/ 1327], train_loss/perplexity = 4.18600130/65.7593155 secs/batch = 0.2662s, grad.norm=17.84073639
 32128: 24 [  280/ 1327], train_loss/perplexity = 4.04885864/57.3319817 secs/batch = 0.2665s, grad.norm=17.57241440
 32133: 24 [  285/ 1327], train_loss/perplexity = 4.37251949/79.2430344 secs/batch = 0.2661s, grad.norm=17.88886070
 32138: 24 [  290/ 1327], train_loss/perplexity = 4.01750374/55.5622330 secs/batch = 0.2600s, grad.norm=18.52955437
 32143: 24 [  295/ 1327], train_loss/perplexity = 3.76862669/43.3205299 secs/batch = 0.2656s, grad.norm=17.87991714
 32148: 24 [  300/ 1327], train_loss/perplexity = 3.33975101/28.2121010 secs/batch = 0.2681s, grad.norm=16.60854721
 32153: 24 [  305/ 1327], train_loss/perplexity = 3.80899906/45.1052704 secs/batch = 0.2650s, grad.norm=17.36628151
 32158: 24 [  310/ 1327], train_loss/perplexity = 3.86251640/47.5849419 secs/batch = 0.2674s, grad.norm=17.14218903
 32163: 24 [  315/ 1327], train_loss/perplexity = 3.45283246/31.5897427 secs/batch = 0.2675s, grad.norm=16.93756294
 32168: 24 [  320/ 1327], train_loss/perplexity = 3.35093212/28.5293140 secs/batch = 0.2657s, grad.norm=17.62166405
 32173: 24 [  325/ 1327], train_loss/perplexity = 3.36685729/28.9872856 secs/batch = 0.2668s, grad.norm=17.01616669
 32178: 24 [  330/ 1327], train_loss/perplexity = 4.00137711/54.6733894 secs/batch = 0.2674s, grad.norm=17.90456009
 32183: 24 [  335/ 1327], train_loss/perplexity = 3.46125340/31.8568802 secs/batch = 0.2647s, grad.norm=16.49990082
 32188: 24 [  340/ 1327], train_loss/perplexity = 4.06849051/58.4686394 secs/batch = 0.2683s, grad.norm=17.79986000
 32193: 24 [  345/ 1327], train_loss/perplexity = 3.91989064/50.3949318 secs/batch = 0.2649s, grad.norm=17.18946457
 32198: 24 [  350/ 1327], train_loss/perplexity = 3.89223194/49.0201759 secs/batch = 0.2665s, grad.norm=17.80686760
 32203: 24 [  355/ 1327], train_loss/perplexity = 3.89353538/49.0841103 secs/batch = 0.2657s, grad.norm=17.49970436
 32208: 24 [  360/ 1327], train_loss/perplexity = 4.04515839/57.1202316 secs/batch = 0.2674s, grad.norm=19.03240967
 32213: 24 [  365/ 1327], train_loss/perplexity = 4.03279495/56.4183769 secs/batch = 0.2662s, grad.norm=17.67978287
 32218: 24 [  370/ 1327], train_loss/perplexity = 4.08208561/59.2689514 secs/batch = 0.2657s, grad.norm=17.94665527
 32223: 24 [  375/ 1327], train_loss/perplexity = 3.54006076/34.4690132 secs/batch = 0.2624s, grad.norm=17.57411194
 32228: 24 [  380/ 1327], train_loss/perplexity = 3.54244423/34.5512657 secs/batch = 0.2666s, grad.norm=17.71199608
 32233: 24 [  385/ 1327], train_loss/perplexity = 3.75586915/42.7713776 secs/batch = 0.2659s, grad.norm=17.69628525
 32238: 24 [  390/ 1327], train_loss/perplexity = 3.88852596/48.8388443 secs/batch = 0.2665s, grad.norm=17.85655212
 32243: 24 [  395/ 1327], train_loss/perplexity = 3.94328785/51.5879364 secs/batch = 0.2661s, grad.norm=17.88288498
 32248: 24 [  400/ 1327], train_loss/perplexity = 3.83901858/46.4798355 secs/batch = 0.2659s, grad.norm=16.87295914
 32253: 24 [  405/ 1327], train_loss/perplexity = 4.16556835/64.4292908 secs/batch = 0.2635s, grad.norm=18.01631737
 32258: 24 [  410/ 1327], train_loss/perplexity = 3.79905367/44.6589012 secs/batch = 0.2602s, grad.norm=17.66449356
 32263: 24 [  415/ 1327], train_loss/perplexity = 3.84232593/46.6338158 secs/batch = 0.2635s, grad.norm=17.61529541
 32268: 24 [  420/ 1327], train_loss/perplexity = 3.43778515/31.1179600 secs/batch = 0.2665s, grad.norm=17.35845947
 32273: 24 [  425/ 1327], train_loss/perplexity = 3.78019524/43.8245964 secs/batch = 0.2653s, grad.norm=18.57175636
 32278: 24 [  430/ 1327], train_loss/perplexity = 3.88705182/48.7668991 secs/batch = 0.2659s, grad.norm=17.84827423
 32283: 24 [  435/ 1327], train_loss/perplexity = 3.93567872/51.1968880 secs/batch = 0.2672s, grad.norm=18.26994133
 32288: 24 [  440/ 1327], train_loss/perplexity = 3.53585148/34.3242302 secs/batch = 0.2663s, grad.norm=17.59608269
 32293: 24 [  445/ 1327], train_loss/perplexity = 3.83332610/46.2160034 secs/batch = 0.2667s, grad.norm=18.09892273
 32298: 24 [  450/ 1327], train_loss/perplexity = 3.91007328/49.9026070 secs/batch = 0.2648s, grad.norm=17.87095261
 32303: 24 [  455/ 1327], train_loss/perplexity = 3.83482957/46.2855377 secs/batch = 0.2662s, grad.norm=17.30446243
 32308: 24 [  460/ 1327], train_loss/perplexity = 3.80543804/44.9449348 secs/batch = 0.2654s, grad.norm=17.72626114
 32313: 24 [  465/ 1327], train_loss/perplexity = 3.52176976/33.8442726 secs/batch = 0.2658s, grad.norm=18.25715828
 32318: 24 [  470/ 1327], train_loss/perplexity = 4.24304962/69.6198425 secs/batch = 0.2681s, grad.norm=18.01642036
 32323: 24 [  475/ 1327], train_loss/perplexity = 3.64083862/38.1237946 secs/batch = 0.2673s, grad.norm=17.65293503
 32328: 24 [  480/ 1327], train_loss/perplexity = 3.79514360/44.4846230 secs/batch = 0.2653s, grad.norm=18.00163460
 32333: 24 [  485/ 1327], train_loss/perplexity = 3.77486229/43.5915031 secs/batch = 0.2652s, grad.norm=18.14395714
 32338: 24 [  490/ 1327], train_loss/perplexity = 3.62316227/37.4558258 secs/batch = 0.2668s, grad.norm=18.91466713
 32343: 24 [  495/ 1327], train_loss/perplexity = 3.71801496/41.1825638 secs/batch = 0.2665s, grad.norm=17.61145020
 32348: 24 [  500/ 1327], train_loss/perplexity = 3.84606695/46.8086014 secs/batch = 0.2669s, grad.norm=17.55921745
 32353: 24 [  505/ 1327], train_loss/perplexity = 3.98367071/53.7138405 secs/batch = 0.2667s, grad.norm=17.09707069
 32358: 24 [  510/ 1327], train_loss/perplexity = 4.34289455/76.9298935 secs/batch = 0.2618s, grad.norm=17.29291153
 32363: 24 [  515/ 1327], train_loss/perplexity = 3.96240497/52.5836372 secs/batch = 0.2658s, grad.norm=17.57819939
 32368: 24 [  520/ 1327], train_loss/perplexity = 4.07265139/58.7124252 secs/batch = 0.2652s, grad.norm=17.62824821
 32373: 24 [  525/ 1327], train_loss/perplexity = 3.74373269/42.2554207 secs/batch = 0.2670s, grad.norm=17.37780952
 32378: 24 [  530/ 1327], train_loss/perplexity = 3.75594330/42.7745514 secs/batch = 0.2648s, grad.norm=18.04327011
 32383: 24 [  535/ 1327], train_loss/perplexity = 3.83510804/46.2984314 secs/batch = 0.2667s, grad.norm=17.69702148
 32388: 24 [  540/ 1327], train_loss/perplexity = 3.97744393/53.3804169 secs/batch = 0.2648s, grad.norm=17.50668526
 32393: 24 [  545/ 1327], train_loss/perplexity = 3.96102381/52.5110588 secs/batch = 0.2622s, grad.norm=17.88207436
 32398: 24 [  550/ 1327], train_loss/perplexity = 3.90940571/49.8693047 secs/batch = 0.2669s, grad.norm=17.68577194
 32403: 24 [  555/ 1327], train_loss/perplexity = 3.83199120/46.1543503 secs/batch = 0.2649s, grad.norm=17.46507263
 32408: 24 [  560/ 1327], train_loss/perplexity = 3.91121578/49.9596558 secs/batch = 0.2667s, grad.norm=18.48744774
 32413: 24 [  565/ 1327], train_loss/perplexity = 3.65136480/38.5272141 secs/batch = 0.2658s, grad.norm=18.46084023
 32418: 24 [  570/ 1327], train_loss/perplexity = 3.71598840/41.0991898 secs/batch = 0.2666s, grad.norm=18.14434242
 32423: 24 [  575/ 1327], train_loss/perplexity = 3.58496952/36.0522575 secs/batch = 0.2661s, grad.norm=17.70815659
 32428: 24 [  580/ 1327], train_loss/perplexity = 3.94374394/51.6114693 secs/batch = 0.2657s, grad.norm=18.18782043
 32433: 24 [  585/ 1327], train_loss/perplexity = 3.49202299/32.8523407 secs/batch = 0.2666s, grad.norm=17.20810127
 32438: 24 [  590/ 1327], train_loss/perplexity = 3.95239234/52.0597610 secs/batch = 0.2660s, grad.norm=17.62693596
 32443: 24 [  595/ 1327], train_loss/perplexity = 3.92883730/50.8478241 secs/batch = 0.2679s, grad.norm=18.05613899
 32448: 24 [  600/ 1327], train_loss/perplexity = 4.13879633/62.7272720 secs/batch = 0.2664s, grad.norm=17.09575844
 32453: 24 [  605/ 1327], train_loss/perplexity = 3.98860002/53.9792671 secs/batch = 0.2669s, grad.norm=17.45696068
 32458: 24 [  610/ 1327], train_loss/perplexity = 4.18940353/65.9834213 secs/batch = 0.2620s, grad.norm=17.87582970
 32463: 24 [  615/ 1327], train_loss/perplexity = 3.73750019/41.9928856 secs/batch = 0.2657s, grad.norm=17.13274574
 32468: 24 [  620/ 1327], train_loss/perplexity = 4.09299755/59.9192352 secs/batch = 0.2653s, grad.norm=17.80330086
 32473: 24 [  625/ 1327], train_loss/perplexity = 3.99656343/54.4108429 secs/batch = 0.2669s, grad.norm=17.29883575
 32478: 24 [  630/ 1327], train_loss/perplexity = 4.07675600/58.9539146 secs/batch = 0.2658s, grad.norm=17.31169701
 32483: 24 [  635/ 1327], train_loss/perplexity = 3.79198480/44.3443260 secs/batch = 0.2659s, grad.norm=17.24412727
 32488: 24 [  640/ 1327], train_loss/perplexity = 3.85366821/47.1657600 secs/batch = 0.2665s, grad.norm=17.39577484
 32493: 24 [  645/ 1327], train_loss/perplexity = 4.10700703/60.7645798 secs/batch = 0.2669s, grad.norm=18.47613525
 32498: 24 [  650/ 1327], train_loss/perplexity = 3.61596632/37.1872635 secs/batch = 0.2671s, grad.norm=17.39920044
 32503: 24 [  655/ 1327], train_loss/perplexity = 3.74803758/42.4377213 secs/batch = 0.2661s, grad.norm=17.90209961
 32508: 24 [  660/ 1327], train_loss/perplexity = 3.66894817/39.2106400 secs/batch = 0.2660s, grad.norm=17.59309006
 32513: 24 [  665/ 1327], train_loss/perplexity = 3.81634355/45.4377632 secs/batch = 0.2646s, grad.norm=17.60972214
 32518: 24 [  670/ 1327], train_loss/perplexity = 3.82229900/45.7091713 secs/batch = 0.2657s, grad.norm=17.67485237
 32523: 24 [  675/ 1327], train_loss/perplexity = 3.57292509/35.6206360 secs/batch = 0.2660s, grad.norm=17.85149765
 32528: 24 [  680/ 1327], train_loss/perplexity = 3.81166887/45.2258530 secs/batch = 0.2674s, grad.norm=18.59556198
 32533: 24 [  685/ 1327], train_loss/perplexity = 3.58888626/36.1937447 secs/batch = 0.2652s, grad.norm=17.33981514
 32538: 24 [  690/ 1327], train_loss/perplexity = 4.03698730/56.6553993 secs/batch = 0.2661s, grad.norm=16.99058151
 32543: 24 [  695/ 1327], train_loss/perplexity = 3.87004757/47.9446678 secs/batch = 0.2663s, grad.norm=17.66588974
 32548: 24 [  700/ 1327], train_loss/perplexity = 4.14387798/63.0468445 secs/batch = 0.2662s, grad.norm=18.26755714
 32553: 24 [  705/ 1327], train_loss/perplexity = 3.80644250/44.9901009 secs/batch = 0.2655s, grad.norm=16.91305542
 32558: 24 [  710/ 1327], train_loss/perplexity = 3.75383663/42.6845322 secs/batch = 0.2658s, grad.norm=18.03304863
 32563: 24 [  715/ 1327], train_loss/perplexity = 3.56149960/35.2159691 secs/batch = 0.2659s, grad.norm=17.79533958
 32568: 24 [  720/ 1327], train_loss/perplexity = 3.63345432/37.8433151 secs/batch = 0.2670s, grad.norm=19.35536957
 32573: 24 [  725/ 1327], train_loss/perplexity = 3.70867848/40.7998543 secs/batch = 0.2657s, grad.norm=17.84952736
 32578: 24 [  730/ 1327], train_loss/perplexity = 3.77326012/43.5217209 secs/batch = 0.2679s, grad.norm=17.97268677
 32583: 24 [  735/ 1327], train_loss/perplexity = 3.78688645/44.1188202 secs/batch = 0.2647s, grad.norm=18.40721512
 32588: 24 [  740/ 1327], train_loss/perplexity = 3.44304919/31.2821980 secs/batch = 0.2640s, grad.norm=16.86226463
 32593: 24 [  745/ 1327], train_loss/perplexity = 3.83712673/46.3919868 secs/batch = 0.2661s, grad.norm=18.12652969
 32598: 24 [  750/ 1327], train_loss/perplexity = 3.75650954/42.7987785 secs/batch = 0.2650s, grad.norm=17.82359886
 32603: 24 [  755/ 1327], train_loss/perplexity = 3.62022543/37.3459854 secs/batch = 0.2638s, grad.norm=17.16656303
 32608: 24 [  760/ 1327], train_loss/perplexity = 3.46363449/31.9328251 secs/batch = 0.2666s, grad.norm=16.45367432
 32613: 24 [  765/ 1327], train_loss/perplexity = 3.64176726/38.1592140 secs/batch = 0.2669s, grad.norm=16.87495613
 32618: 24 [  770/ 1327], train_loss/perplexity = 3.64053440/38.1121979 secs/batch = 0.2660s, grad.norm=17.52263069
 32623: 24 [  775/ 1327], train_loss/perplexity = 3.55857968/35.1132889 secs/batch = 0.2666s, grad.norm=17.91711807
 32628: 24 [  780/ 1327], train_loss/perplexity = 3.91877413/50.3386993 secs/batch = 0.2620s, grad.norm=18.13945007
 32633: 24 [  785/ 1327], train_loss/perplexity = 3.81469131/45.3627510 secs/batch = 0.2652s, grad.norm=17.97630692
 32638: 24 [  790/ 1327], train_loss/perplexity = 3.59099197/36.2700386 secs/batch = 0.2662s, grad.norm=17.98699188
 32643: 24 [  795/ 1327], train_loss/perplexity = 3.96600223/52.7731323 secs/batch = 0.2643s, grad.norm=17.71563339
 32648: 24 [  800/ 1327], train_loss/perplexity = 3.83494568/46.2909126 secs/batch = 0.2664s, grad.norm=17.74563026
 32653: 24 [  805/ 1327], train_loss/perplexity = 4.12609005/61.9352837 secs/batch = 0.2620s, grad.norm=17.66661835
 32658: 24 [  810/ 1327], train_loss/perplexity = 3.76061392/42.9748001 secs/batch = 0.2661s, grad.norm=16.83743858
 32663: 24 [  815/ 1327], train_loss/perplexity = 3.75736976/42.8356094 secs/batch = 0.2670s, grad.norm=17.43468666
 32668: 24 [  820/ 1327], train_loss/perplexity = 3.64843369/38.4144516 secs/batch = 0.2671s, grad.norm=16.71658897
 32673: 24 [  825/ 1327], train_loss/perplexity = 3.73974633/42.0873108 secs/batch = 0.2668s, grad.norm=17.98705101
 32678: 24 [  830/ 1327], train_loss/perplexity = 3.48558187/32.6414146 secs/batch = 0.2670s, grad.norm=17.94968033
 32683: 24 [  835/ 1327], train_loss/perplexity = 3.79971862/44.6886101 secs/batch = 0.2659s, grad.norm=18.04715157
 32688: 24 [  840/ 1327], train_loss/perplexity = 3.85582900/47.2677841 secs/batch = 0.2662s, grad.norm=17.58042145
 32693: 24 [  845/ 1327], train_loss/perplexity = 3.67631817/39.5006905 secs/batch = 0.2646s, grad.norm=17.88514137
 32698: 24 [  850/ 1327], train_loss/perplexity = 3.84165144/46.6023712 secs/batch = 0.2657s, grad.norm=17.64241600
 32703: 24 [  855/ 1327], train_loss/perplexity = 3.76791286/43.2896194 secs/batch = 0.2660s, grad.norm=18.09468842
 32708: 24 [  860/ 1327], train_loss/perplexity = 3.53207588/34.1948776 secs/batch = 0.2668s, grad.norm=17.26454735
 32713: 24 [  865/ 1327], train_loss/perplexity = 3.96401882/52.6685677 secs/batch = 0.2661s, grad.norm=17.67706871
 32718: 24 [  870/ 1327], train_loss/perplexity = 3.81780529/45.5042305 secs/batch = 0.2664s, grad.norm=18.01434517
 32723: 24 [  875/ 1327], train_loss/perplexity = 3.45909429/31.7881718 secs/batch = 0.2640s, grad.norm=17.11282349
 32728: 24 [  880/ 1327], train_loss/perplexity = 3.68733168/39.9381371 secs/batch = 0.2659s, grad.norm=17.44146729
 32733: 24 [  885/ 1327], train_loss/perplexity = 3.81692219/45.4640617 secs/batch = 0.2663s, grad.norm=17.31877708
 32738: 24 [  890/ 1327], train_loss/perplexity = 3.94003606/51.4204559 secs/batch = 0.2664s, grad.norm=17.48744583
 32743: 24 [  895/ 1327], train_loss/perplexity = 3.92673826/50.7412033 secs/batch = 0.2665s, grad.norm=17.91228485
 32748: 24 [  900/ 1327], train_loss/perplexity = 3.80113649/44.7520142 secs/batch = 0.2639s, grad.norm=16.86297798
 32753: 24 [  905/ 1327], train_loss/perplexity = 3.65505791/38.6697617 secs/batch = 0.2658s, grad.norm=16.62463760
 32758: 24 [  910/ 1327], train_loss/perplexity = 3.70001984/40.4481049 secs/batch = 0.2659s, grad.norm=15.99011993
 32763: 24 [  915/ 1327], train_loss/perplexity = 3.92747784/50.7787437 secs/batch = 0.2658s, grad.norm=17.11927414
 32768: 24 [  920/ 1327], train_loss/perplexity = 4.07206059/58.6777496 secs/batch = 0.2666s, grad.norm=17.56245422
 32773: 24 [  925/ 1327], train_loss/perplexity = 3.81343555/45.3058243 secs/batch = 0.2680s, grad.norm=17.17941856
 32778: 24 [  930/ 1327], train_loss/perplexity = 3.97361541/53.1764374 secs/batch = 0.2668s, grad.norm=17.35014153
 32783: 24 [  935/ 1327], train_loss/perplexity = 3.92204332/50.5035362 secs/batch = 0.2656s, grad.norm=17.03506660
 32788: 24 [  940/ 1327], train_loss/perplexity = 3.95405436/52.1463585 secs/batch = 0.2664s, grad.norm=17.17630577
 32793: 24 [  945/ 1327], train_loss/perplexity = 4.10550547/60.6734047 secs/batch = 0.2667s, grad.norm=17.09388924
 32798: 24 [  950/ 1327], train_loss/perplexity = 3.88723040/48.7756119 secs/batch = 0.2669s, grad.norm=17.38261604
 32803: 24 [  955/ 1327], train_loss/perplexity = 3.75147533/42.5838623 secs/batch = 0.2637s, grad.norm=17.00247765
 32808: 24 [  960/ 1327], train_loss/perplexity = 4.15465164/63.7297592 secs/batch = 0.2636s, grad.norm=17.62390518
 32813: 24 [  965/ 1327], train_loss/perplexity = 3.91677856/50.2383461 secs/batch = 0.2645s, grad.norm=17.82142639
 32818: 24 [  970/ 1327], train_loss/perplexity = 4.12725639/62.0075645 secs/batch = 0.2660s, grad.norm=17.94735146
 32823: 24 [  975/ 1327], train_loss/perplexity = 3.77680302/43.6761856 secs/batch = 0.2656s, grad.norm=18.72049522
 32828: 24 [  980/ 1327], train_loss/perplexity = 3.65269089/38.5783348 secs/batch = 0.2658s, grad.norm=17.75078773
 32833: 24 [  985/ 1327], train_loss/perplexity = 3.73083210/41.7138023 secs/batch = 0.2634s, grad.norm=17.76068306
 32838: 24 [  990/ 1327], train_loss/perplexity = 3.97054100/53.0132027 secs/batch = 0.2666s, grad.norm=18.35206795
 32843: 24 [  995/ 1327], train_loss/perplexity = 4.02482605/55.9705734 secs/batch = 0.2661s, grad.norm=17.76866531
 32848: 24 [ 1000/ 1327], train_loss/perplexity = 3.51465678/33.6043930 secs/batch = 0.2651s, grad.norm=16.84591675
 32853: 24 [ 1005/ 1327], train_loss/perplexity = 3.98834991/53.9657669 secs/batch = 0.2670s, grad.norm=17.41508293
 32858: 24 [ 1010/ 1327], train_loss/perplexity = 3.55662227/35.0446243 secs/batch = 0.2659s, grad.norm=16.32838249
 32863: 24 [ 1015/ 1327], train_loss/perplexity = 4.02388239/55.9177780 secs/batch = 0.2659s, grad.norm=17.40964890
 32868: 24 [ 1020/ 1327], train_loss/perplexity = 4.12914133/62.1245537 secs/batch = 0.2618s, grad.norm=17.31085396
 32873: 24 [ 1025/ 1327], train_loss/perplexity = 4.08651161/59.5318604 secs/batch = 0.2666s, grad.norm=17.30716133
 32878: 24 [ 1030/ 1327], train_loss/perplexity = 3.81921244/45.5683060 secs/batch = 0.2658s, grad.norm=16.55924797
 32883: 24 [ 1035/ 1327], train_loss/perplexity = 3.80775928/45.0493813 secs/batch = 0.2668s, grad.norm=17.21035576
 32888: 24 [ 1040/ 1327], train_loss/perplexity = 3.93245268/51.0319901 secs/batch = 0.2659s, grad.norm=17.82263947
 32893: 24 [ 1045/ 1327], train_loss/perplexity = 3.50395870/33.2468071 secs/batch = 0.2659s, grad.norm=16.43918419
 32898: 24 [ 1050/ 1327], train_loss/perplexity = 3.58258104/35.9662514 secs/batch = 0.2658s, grad.norm=17.02474022
 32903: 24 [ 1055/ 1327], train_loss/perplexity = 3.70854211/40.7942886 secs/batch = 0.2667s, grad.norm=18.07340431
 32908: 24 [ 1060/ 1327], train_loss/perplexity = 3.29424810/26.9571381 secs/batch = 0.2671s, grad.norm=17.55924225
 32913: 24 [ 1065/ 1327], train_loss/perplexity = 3.44995213/31.4988842 secs/batch = 0.2659s, grad.norm=17.48686981
 32918: 24 [ 1070/ 1327], train_loss/perplexity = 3.76118946/42.9995422 secs/batch = 0.2659s, grad.norm=17.43728828
 32923: 24 [ 1075/ 1327], train_loss/perplexity = 3.58434725/36.0298309 secs/batch = 0.2671s, grad.norm=17.20222282
 32928: 24 [ 1080/ 1327], train_loss/perplexity = 3.55042386/34.8280754 secs/batch = 0.2663s, grad.norm=17.38672638
 32933: 24 [ 1085/ 1327], train_loss/perplexity = 3.45148969/31.5473537 secs/batch = 0.2653s, grad.norm=17.74458504
 32938: 24 [ 1090/ 1327], train_loss/perplexity = 3.55311799/34.9220352 secs/batch = 0.2646s, grad.norm=17.67478752
 32943: 24 [ 1095/ 1327], train_loss/perplexity = 3.85011125/46.9982910 secs/batch = 0.2641s, grad.norm=18.21532059
 32948: 24 [ 1100/ 1327], train_loss/perplexity = 3.47098207/32.1683197 secs/batch = 0.2662s, grad.norm=18.43693352
 32953: 24 [ 1105/ 1327], train_loss/perplexity = 3.50226498/33.1905441 secs/batch = 0.2659s, grad.norm=18.02371597
 32958: 24 [ 1110/ 1327], train_loss/perplexity = 3.67975855/39.6368217 secs/batch = 0.2654s, grad.norm=17.90896606
 32963: 24 [ 1115/ 1327], train_loss/perplexity = 3.57325244/35.6322975 secs/batch = 0.2644s, grad.norm=16.59192467
 32968: 24 [ 1120/ 1327], train_loss/perplexity = 3.87344599/48.1078796 secs/batch = 0.2628s, grad.norm=17.43577385
 32973: 24 [ 1125/ 1327], train_loss/perplexity = 4.00213480/54.7148323 secs/batch = 0.2670s, grad.norm=18.39869308
 32978: 24 [ 1130/ 1327], train_loss/perplexity = 3.65642929/38.7228279 secs/batch = 0.2663s, grad.norm=17.24096107
 32983: 24 [ 1135/ 1327], train_loss/perplexity = 3.62123418/37.3836784 secs/batch = 0.2664s, grad.norm=17.32221603
 32988: 24 [ 1140/ 1327], train_loss/perplexity = 3.93114400/50.9652481 secs/batch = 0.2648s, grad.norm=18.34384155
 32993: 24 [ 1145/ 1327], train_loss/perplexity = 3.76712132/43.2553673 secs/batch = 0.2658s, grad.norm=17.21023178
 32998: 24 [ 1150/ 1327], train_loss/perplexity = 3.69090652/40.0811653 secs/batch = 0.2654s, grad.norm=16.90619278
 33003: 24 [ 1155/ 1327], train_loss/perplexity = 3.80235267/44.8064766 secs/batch = 0.2669s, grad.norm=17.58630943
 33008: 24 [ 1160/ 1327], train_loss/perplexity = 3.72426367/41.4407082 secs/batch = 0.2665s, grad.norm=17.23722649
 33013: 24 [ 1165/ 1327], train_loss/perplexity = 3.74320388/42.2330818 secs/batch = 0.2664s, grad.norm=17.27159119
 33018: 24 [ 1170/ 1327], train_loss/perplexity = 3.62504268/37.5263252 secs/batch = 0.2666s, grad.norm=17.02409744
 33023: 24 [ 1175/ 1327], train_loss/perplexity = 3.51562452/33.6369286 secs/batch = 0.2661s, grad.norm=17.16353035
 33028: 24 [ 1180/ 1327], train_loss/perplexity = 3.47263265/32.2214584 secs/batch = 0.2665s, grad.norm=17.38039589
 33033: 24 [ 1185/ 1327], train_loss/perplexity = 3.64179397/38.1602325 secs/batch = 0.2652s, grad.norm=17.64003754
 33038: 24 [ 1190/ 1327], train_loss/perplexity = 3.74840307/42.4532318 secs/batch = 0.2667s, grad.norm=17.92253304
 33043: 24 [ 1195/ 1327], train_loss/perplexity = 3.63934135/38.0667572 secs/batch = 0.2667s, grad.norm=17.11437035
 33048: 24 [ 1200/ 1327], train_loss/perplexity = 3.52667189/34.0105896 secs/batch = 0.2654s, grad.norm=17.15829468
 33053: 24 [ 1205/ 1327], train_loss/perplexity = 3.59464908/36.4029236 secs/batch = 0.2676s, grad.norm=18.00802422
 33058: 24 [ 1210/ 1327], train_loss/perplexity = 3.14753985/23.2787247 secs/batch = 0.2633s, grad.norm=16.89195824
 33063: 24 [ 1215/ 1327], train_loss/perplexity = 3.37933683/29.3512993 secs/batch = 0.2599s, grad.norm=16.52748108
 33068: 24 [ 1220/ 1327], train_loss/perplexity = 3.58714652/36.1308289 secs/batch = 0.2651s, grad.norm=17.95860863
 33073: 24 [ 1225/ 1327], train_loss/perplexity = 3.28810048/26.7919235 secs/batch = 0.2656s, grad.norm=18.35962105
 33078: 24 [ 1230/ 1327], train_loss/perplexity = 3.52514219/33.9586029 secs/batch = 0.2663s, grad.norm=17.00000000
 33083: 24 [ 1235/ 1327], train_loss/perplexity = 3.52959037/34.1099930 secs/batch = 0.2677s, grad.norm=17.35358620
 33088: 24 [ 1240/ 1327], train_loss/perplexity = 3.72491741/41.4678078 secs/batch = 0.2663s, grad.norm=17.95660973
 33093: 24 [ 1245/ 1327], train_loss/perplexity = 3.62859964/37.6600418 secs/batch = 0.2657s, grad.norm=17.57898331
 33098: 24 [ 1250/ 1327], train_loss/perplexity = 3.80304885/44.8376808 secs/batch = 0.2667s, grad.norm=16.65453720
 33103: 24 [ 1255/ 1327], train_loss/perplexity = 3.78547192/44.0564575 secs/batch = 0.2667s, grad.norm=16.97744560
 33108: 24 [ 1260/ 1327], train_loss/perplexity = 3.62882662/37.6685905 secs/batch = 0.2656s, grad.norm=18.08501625
 33113: 24 [ 1265/ 1327], train_loss/perplexity = 3.75177813/42.5967560 secs/batch = 0.2669s, grad.norm=17.57986069
 33118: 24 [ 1270/ 1327], train_loss/perplexity = 3.49043941/32.8003578 secs/batch = 0.2660s, grad.norm=17.58942986
 33123: 24 [ 1275/ 1327], train_loss/perplexity = 3.68915915/40.0111885 secs/batch = 0.2650s, grad.norm=17.45302963
 33128: 24 [ 1280/ 1327], train_loss/perplexity = 3.59821987/36.5331421 secs/batch = 0.2663s, grad.norm=17.90849304
 33133: 24 [ 1285/ 1327], train_loss/perplexity = 3.51315284/33.5538902 secs/batch = 0.2660s, grad.norm=17.42723465
 33138: 24 [ 1290/ 1327], train_loss/perplexity = 3.75642538/42.7951775 secs/batch = 0.2666s, grad.norm=16.89179039
 33143: 24 [ 1295/ 1327], train_loss/perplexity = 3.70570278/40.6786270 secs/batch = 0.2671s, grad.norm=17.29750252
 33148: 24 [ 1300/ 1327], train_loss/perplexity = 3.92173576/50.4880028 secs/batch = 0.2647s, grad.norm=16.86637497
 33153: 24 [ 1305/ 1327], train_loss/perplexity = 3.86728644/47.8124695 secs/batch = 0.2644s, grad.norm=17.42753220
 33158: 24 [ 1310/ 1327], train_loss/perplexity = 4.24428415/69.7058411 secs/batch = 0.2666s, grad.norm=18.33961487
 33163: 24 [ 1315/ 1327], train_loss/perplexity = 3.95117950/51.9966621 secs/batch = 0.2669s, grad.norm=17.46241188
 33168: 24 [ 1320/ 1327], train_loss/perplexity = 3.90053654/49.4289627 secs/batch = 0.2667s, grad.norm=17.35320663
 33173: 24 [ 1325/ 1327], train_loss/perplexity = 3.89264822/49.0405846 secs/batch = 0.2663s, grad.norm=17.43986893
Epoch training time: 352.98090410232544
	> validation loss = 4.57260704, perplexity = 96.79613495
	> validation loss = 4.51935768, perplexity = 91.77662659
	> validation loss = 4.47929955, perplexity = 88.17288971
	> validation loss = 4.55934906, perplexity = 95.52127838
	> validation loss = 4.66468716, perplexity = 106.13237762
	> validation loss = 4.61944962, perplexity = 101.43818665
	> validation loss = 4.53794861, perplexity = 93.49880219
	> validation loss = 4.38003969, perplexity = 79.84120178
	> validation loss = 4.17881536, perplexity = 65.28846741
	> validation loss = 4.27559090, perplexity = 71.92262268
	> validation loss = 4.47067928, perplexity = 87.41608429
	> validation loss = 4.45808411, perplexity = 86.32196808
	> validation loss = 4.44597101, perplexity = 85.28264618
	> validation loss = 4.17606592, perplexity = 65.10920715
	> validation loss = 4.13907623, perplexity = 62.74483490
	> validation loss = 4.18213987, perplexity = 65.50587463
	> validation loss = 4.60429668, perplexity = 99.91268921
	> validation loss = 4.10852718, perplexity = 60.85702133
	> validation loss = 4.61990929, perplexity = 101.48482513
	> validation loss = 4.48295975, perplexity = 88.49620819
	> validation loss = 4.25899410, perplexity = 70.73879242
at the end of epoch: 24
train loss = 3.83822750, perplexity = 46.44308112
validation loss = 4.42053408, perplexity = 83.14067723
Saved model cv/epoch024_4.4205.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.000976562
new learning rate is: 0.00048828125
