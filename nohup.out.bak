I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.7.5 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:1e.0
Total memory: 11.17GiB
Free memory: 11.11GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0)
reading train
reading valid
reading test

actual longest token length is: 21
size of word vocabulary: 10000
size of char vocabulary: 51
number of tokens in train: 929589
number of tokens in valid: 73760
number of tokens in test: 82430
initialized all dataset readers
Created and initialized fresh model. Size: 19367965
     5: 0 [    5/ 1327], train_loss/perplexity = 8.01293850/3019.7775879 secs/batch = 0.2904s, grad.norm=10.42542744
    10: 0 [   10/ 1327], train_loss/perplexity = 14.47045135/1925028.5000000 secs/batch = 0.2905s, grad.norm=191.87536621
    15: 0 [   15/ 1327], train_loss/perplexity = 7.97726345/2913.9460449 secs/batch = 0.2915s, grad.norm=33.05992889
    20: 0 [   20/ 1327], train_loss/perplexity = 7.76806355/2363.8894043 secs/batch = 0.2961s, grad.norm=12.41167164
    25: 0 [   25/ 1327], train_loss/perplexity = 7.43098450/1687.4680176 secs/batch = 0.2901s, grad.norm=7.98627377
    30: 0 [   30/ 1327], train_loss/perplexity = 7.19650030/1334.7513428 secs/batch = 0.2917s, grad.norm=11.29461384
    35: 0 [   35/ 1327], train_loss/perplexity = 7.46632576/1748.1716309 secs/batch = 0.2975s, grad.norm=38.11754990
    40: 0 [   40/ 1327], train_loss/perplexity = 7.18405819/1318.2470703 secs/batch = 0.2902s, grad.norm=14.08061409
    45: 0 [   45/ 1327], train_loss/perplexity = 7.25606298/1416.6680908 secs/batch = 0.2899s, grad.norm=22.19133568
    50: 0 [   50/ 1327], train_loss/perplexity = 7.80122375/2443.5905762 secs/batch = 0.2907s, grad.norm=50.27272034
    55: 0 [   55/ 1327], train_loss/perplexity = 7.27692127/1446.5277100 secs/batch = 0.2906s, grad.norm=11.91872311
    60: 0 [   60/ 1327], train_loss/perplexity = 7.08311701/1191.6772461 secs/batch = 0.2965s, grad.norm=7.04880953
    65: 0 [   65/ 1327], train_loss/perplexity = 6.79107285/889.8677368 secs/batch = 0.2968s, grad.norm=10.45193481
    70: 0 [   70/ 1327], train_loss/perplexity = 7.00305080/1099.9838867 secs/batch = 0.2974s, grad.norm=8.05864811
    75: 0 [   75/ 1327], train_loss/perplexity = 6.83945608/933.9809570 secs/batch = 0.2912s, grad.norm=9.29597378
    80: 0 [   80/ 1327], train_loss/perplexity = 7.01593161/1114.2441406 secs/batch = 0.2903s, grad.norm=14.28967476
    85: 0 [   85/ 1327], train_loss/perplexity = 6.96518517/1059.1109619 secs/batch = 0.2958s, grad.norm=11.05343437
    90: 0 [   90/ 1327], train_loss/perplexity = 6.92922115/1021.6979370 secs/batch = 0.2912s, grad.norm=7.21544313
    95: 0 [   95/ 1327], train_loss/perplexity = 6.86633205/959.4229736 secs/batch = 0.2979s, grad.norm=5.62116051
   100: 0 [  100/ 1327], train_loss/perplexity = 6.96465921/1058.5540771 secs/batch = 0.2906s, grad.norm=6.72304058
   105: 0 [  105/ 1327], train_loss/perplexity = 7.04533005/1147.4875488 secs/batch = 0.2980s, grad.norm=4.24077988
   110: 0 [  110/ 1327], train_loss/perplexity = 6.83931780/933.8518677 secs/batch = 0.2972s, grad.norm=6.87828445
   115: 0 [  115/ 1327], train_loss/perplexity = 6.96368408/1057.5223389 secs/batch = 0.2976s, grad.norm=18.48983002
   120: 0 [  120/ 1327], train_loss/perplexity = 6.93942642/1032.1779785 secs/batch = 0.2914s, grad.norm=5.29181480
   125: 0 [  125/ 1327], train_loss/perplexity = 6.84417629/938.4000244 secs/batch = 0.2984s, grad.norm=3.41193318
   130: 0 [  130/ 1327], train_loss/perplexity = 6.96370792/1057.5476074 secs/batch = 0.2914s, grad.norm=4.74317122
   135: 0 [  135/ 1327], train_loss/perplexity = 6.85404587/947.7074585 secs/batch = 0.2988s, grad.norm=4.73538208
   140: 0 [  140/ 1327], train_loss/perplexity = 6.82060146/916.5361328 secs/batch = 0.2916s, grad.norm=3.29353452
   145: 0 [  145/ 1327], train_loss/perplexity = 7.09547377/1206.4937744 secs/batch = 0.2916s, grad.norm=4.67410612
   150: 0 [  150/ 1327], train_loss/perplexity = 6.76615810/867.9708252 secs/batch = 0.2893s, grad.norm=8.34880829
   155: 0 [  155/ 1327], train_loss/perplexity = 6.74591255/850.5749512 secs/batch = 0.2971s, grad.norm=2.83777070
   160: 0 [  160/ 1327], train_loss/perplexity = 6.72368479/831.8771362 secs/batch = 0.2976s, grad.norm=15.61221027
   165: 0 [  165/ 1327], train_loss/perplexity = 6.73792839/843.8108521 secs/batch = 0.2920s, grad.norm=5.46551037
   170: 0 [  170/ 1327], train_loss/perplexity = 6.78848410/887.5670776 secs/batch = 0.2913s, grad.norm=4.30076075
   175: 0 [  175/ 1327], train_loss/perplexity = 6.77257061/873.5545654 secs/batch = 0.2914s, grad.norm=5.82182837
   180: 0 [  180/ 1327], train_loss/perplexity = 6.74564457/850.3470459 secs/batch = 0.2914s, grad.norm=3.30411959
   185: 0 [  185/ 1327], train_loss/perplexity = 6.91062737/1002.8762207 secs/batch = 0.2915s, grad.norm=4.97500134
   190: 0 [  190/ 1327], train_loss/perplexity = 6.81392050/910.4331665 secs/batch = 0.2921s, grad.norm=7.25356388
   195: 0 [  195/ 1327], train_loss/perplexity = 6.73657322/842.6681519 secs/batch = 0.2976s, grad.norm=10.72701454
   200: 0 [  200/ 1327], train_loss/perplexity = 6.88800573/980.4442139 secs/batch = 0.2924s, grad.norm=7.43924522
   205: 0 [  205/ 1327], train_loss/perplexity = 6.80465126/902.0331421 secs/batch = 0.2898s, grad.norm=6.11104059
   210: 0 [  210/ 1327], train_loss/perplexity = 6.63447189/760.8771362 secs/batch = 0.2988s, grad.norm=5.45285702
   215: 0 [  215/ 1327], train_loss/perplexity = 6.69359350/807.2177734 secs/batch = 0.2929s, grad.norm=4.88712692
   220: 0 [  220/ 1327], train_loss/perplexity = 6.69117880/805.2709351 secs/batch = 0.2921s, grad.norm=5.25233364
   225: 0 [  225/ 1327], train_loss/perplexity = 6.88313532/975.6806641 secs/batch = 0.2989s, grad.norm=5.17388487
   230: 0 [  230/ 1327], train_loss/perplexity = 6.69146252/805.4994507 secs/batch = 0.2939s, grad.norm=6.90940428
   235: 0 [  235/ 1327], train_loss/perplexity = 6.75408649/857.5560303 secs/batch = 0.2976s, grad.norm=11.81987572
   240: 0 [  240/ 1327], train_loss/perplexity = 6.44498253/629.5357056 secs/batch = 0.2920s, grad.norm=4.77741385
   245: 0 [  245/ 1327], train_loss/perplexity = 6.62499142/753.6977539 secs/batch = 0.2927s, grad.norm=5.44334698
   250: 0 [  250/ 1327], train_loss/perplexity = 6.68251896/798.3275146 secs/batch = 0.2973s, grad.norm=5.20939636
   255: 0 [  255/ 1327], train_loss/perplexity = 6.71173859/821.9985352 secs/batch = 0.2918s, grad.norm=4.31574535
   260: 0 [  260/ 1327], train_loss/perplexity = 6.78947592/888.4478149 secs/batch = 0.2920s, grad.norm=4.28581476
   265: 0 [  265/ 1327], train_loss/perplexity = 6.83228683/927.3090210 secs/batch = 0.2919s, grad.norm=13.98180485
   270: 0 [  270/ 1327], train_loss/perplexity = 6.81833935/914.4651489 secs/batch = 0.2905s, grad.norm=12.47551060
   275: 0 [  275/ 1327], train_loss/perplexity = 6.93864727/1031.3741455 secs/batch = 0.2966s, grad.norm=9.54551029
   280: 0 [  280/ 1327], train_loss/perplexity = 6.55117321/700.0650024 secs/batch = 0.2938s, grad.norm=5.99462414
   285: 0 [  285/ 1327], train_loss/perplexity = 6.79570198/893.9965820 secs/batch = 0.2913s, grad.norm=8.04433060
   290: 0 [  290/ 1327], train_loss/perplexity = 6.62514591/753.8142090 secs/batch = 0.2959s, grad.norm=5.75225687
   295: 0 [  295/ 1327], train_loss/perplexity = 6.60856199/741.4160767 secs/batch = 0.2934s, grad.norm=7.54227972
   300: 0 [  300/ 1327], train_loss/perplexity = 6.24696636/516.4437256 secs/batch = 0.2906s, grad.norm=5.67982483
   305: 0 [  305/ 1327], train_loss/perplexity = 6.48327923/654.1124268 secs/batch = 0.2944s, grad.norm=10.47778893
   310: 0 [  310/ 1327], train_loss/perplexity = 6.52806711/684.0747070 secs/batch = 0.2936s, grad.norm=5.45395041
   315: 0 [  315/ 1327], train_loss/perplexity = 6.22392654/504.6809998 secs/batch = 0.2920s, grad.norm=5.11334562
   320: 0 [  320/ 1327], train_loss/perplexity = 6.64605236/769.7396851 secs/batch = 0.2948s, grad.norm=16.91667938
   325: 0 [  325/ 1327], train_loss/perplexity = 6.20109224/493.2875366 secs/batch = 0.2935s, grad.norm=7.05724096
   330: 0 [  330/ 1327], train_loss/perplexity = 6.52769852/683.8225708 secs/batch = 0.2972s, grad.norm=5.53144598
   335: 0 [  335/ 1327], train_loss/perplexity = 5.77096176/320.8461609 secs/batch = 0.2978s, grad.norm=6.45880175
   340: 0 [  340/ 1327], train_loss/perplexity = 6.56511974/709.8969116 secs/batch = 0.2968s, grad.norm=6.88814116
   345: 0 [  345/ 1327], train_loss/perplexity = 6.34983730/572.3995972 secs/batch = 0.2938s, grad.norm=4.99611282
   350: 0 [  350/ 1327], train_loss/perplexity = 6.50545549/668.7802124 secs/batch = 0.2941s, grad.norm=7.42428398
   355: 0 [  355/ 1327], train_loss/perplexity = 6.51343155/674.1358032 secs/batch = 0.3002s, grad.norm=4.68768215
   360: 0 [  360/ 1327], train_loss/perplexity = 6.55955696/705.9588623 secs/batch = 0.2933s, grad.norm=5.81527948
   365: 0 [  365/ 1327], train_loss/perplexity = 6.53086758/685.9931030 secs/batch = 0.3002s, grad.norm=6.91386127
   370: 0 [  370/ 1327], train_loss/perplexity = 6.56153440/707.3562622 secs/batch = 0.2939s, grad.norm=5.90070772
   375: 0 [  375/ 1327], train_loss/perplexity = 6.03483534/417.7300110 secs/batch = 0.2949s, grad.norm=4.73260927
   380: 0 [  380/ 1327], train_loss/perplexity = 6.33229637/562.4467163 secs/batch = 0.2935s, grad.norm=8.47176647
   385: 0 [  385/ 1327], train_loss/perplexity = 6.42506933/617.1235962 secs/batch = 0.2930s, grad.norm=9.38771343
   390: 0 [  390/ 1327], train_loss/perplexity = 6.38485384/592.7980957 secs/batch = 0.2938s, grad.norm=5.26870155
   395: 0 [  395/ 1327], train_loss/perplexity = 6.56320238/708.5370483 secs/batch = 0.2939s, grad.norm=6.65146351
   400: 0 [  400/ 1327], train_loss/perplexity = 6.19772243/491.6280518 secs/batch = 0.2922s, grad.norm=6.87144661
   405: 0 [  405/ 1327], train_loss/perplexity = 6.39329147/597.8210449 secs/batch = 0.2945s, grad.norm=5.05413961
   410: 0 [  410/ 1327], train_loss/perplexity = 6.28387785/535.8626099 secs/batch = 0.2953s, grad.norm=5.50581884
   415: 0 [  415/ 1327], train_loss/perplexity = 6.27163982/529.3447266 secs/batch = 0.2968s, grad.norm=5.98401594
   420: 0 [  420/ 1327], train_loss/perplexity = 6.11484003/452.5236511 secs/batch = 0.2932s, grad.norm=6.10698271
   425: 0 [  425/ 1327], train_loss/perplexity = 6.43960714/626.1607666 secs/batch = 0.2986s, grad.norm=6.53519058
   430: 0 [  430/ 1327], train_loss/perplexity = 6.39610863/599.5075684 secs/batch = 0.2942s, grad.norm=7.21475363
   435: 0 [  435/ 1327], train_loss/perplexity = 6.50482273/668.3571777 secs/batch = 0.2926s, grad.norm=7.76406050
   440: 0 [  440/ 1327], train_loss/perplexity = 6.31081581/550.4938354 secs/batch = 0.2920s, grad.norm=9.86321449
   445: 0 [  445/ 1327], train_loss/perplexity = 6.21669197/501.0430298 secs/batch = 0.2984s, grad.norm=6.62428761
   450: 0 [  450/ 1327], train_loss/perplexity = 6.18806839/486.9046936 secs/batch = 0.2939s, grad.norm=7.02688599
   455: 0 [  455/ 1327], train_loss/perplexity = 5.82498598/338.6563721 secs/batch = 0.2938s, grad.norm=6.02984571
   460: 0 [  460/ 1327], train_loss/perplexity = 6.18364334/484.7548828 secs/batch = 0.3020s, grad.norm=6.65867949
   465: 0 [  465/ 1327], train_loss/perplexity = 6.09428358/443.3163452 secs/batch = 0.3001s, grad.norm=6.65639734
   470: 0 [  470/ 1327], train_loss/perplexity = 6.40381813/604.1473389 secs/batch = 0.2954s, grad.norm=6.68959951
   475: 0 [  475/ 1327], train_loss/perplexity = 6.30941963/549.7258301 secs/batch = 0.2944s, grad.norm=7.16636467
   480: 0 [  480/ 1327], train_loss/perplexity = 6.10828495/449.5670166 secs/batch = 0.2994s, grad.norm=6.46785545
   485: 0 [  485/ 1327], train_loss/perplexity = 5.99948645/403.2216492 secs/batch = 0.2950s, grad.norm=6.34026957
   490: 0 [  490/ 1327], train_loss/perplexity = 6.17349958/479.8624878 secs/batch = 0.2943s, grad.norm=7.88982201
   495: 0 [  495/ 1327], train_loss/perplexity = 5.87580109/356.3099670 secs/batch = 0.2994s, grad.norm=10.74971867
   500: 0 [  500/ 1327], train_loss/perplexity = 6.24468660/515.2677002 secs/batch = 0.2984s, grad.norm=7.38277388
   505: 0 [  505/ 1327], train_loss/perplexity = 6.02176046/412.3038025 secs/batch = 0.2946s, grad.norm=7.59541988
   510: 0 [  510/ 1327], train_loss/perplexity = 6.30290127/546.1541748 secs/batch = 0.2984s, grad.norm=6.29898882
   515: 0 [  515/ 1327], train_loss/perplexity = 6.00133562/403.9679871 secs/batch = 0.2963s, grad.norm=7.44078016
   520: 0 [  520/ 1327], train_loss/perplexity = 6.32266474/557.0554199 secs/batch = 0.2944s, grad.norm=10.08641243
   525: 0 [  525/ 1327], train_loss/perplexity = 5.96328640/388.8860474 secs/batch = 0.2954s, grad.norm=6.63120317
   530: 0 [  530/ 1327], train_loss/perplexity = 5.93277836/377.2010498 secs/batch = 0.2944s, grad.norm=7.11887932
   535: 0 [  535/ 1327], train_loss/perplexity = 6.12408543/456.7268066 secs/batch = 0.2960s, grad.norm=9.33505249
   540: 0 [  540/ 1327], train_loss/perplexity = 6.04507065/422.0275574 secs/batch = 0.2952s, grad.norm=6.37402630
   545: 0 [  545/ 1327], train_loss/perplexity = 6.16809607/477.2765503 secs/batch = 0.2946s, grad.norm=7.85455418
   550: 0 [  550/ 1327], train_loss/perplexity = 6.09204054/442.3230591 secs/batch = 0.2955s, grad.norm=7.38402510
   555: 0 [  555/ 1327], train_loss/perplexity = 6.07445240/434.6114502 secs/batch = 0.3011s, grad.norm=8.17764378
   560: 0 [  560/ 1327], train_loss/perplexity = 6.05051041/424.3295593 secs/batch = 0.2940s, grad.norm=7.97641277
   565: 0 [  565/ 1327], train_loss/perplexity = 6.03922272/419.5667725 secs/batch = 0.2953s, grad.norm=7.05553818
   570: 0 [  570/ 1327], train_loss/perplexity = 5.94719028/382.6766052 secs/batch = 0.2994s, grad.norm=8.17770100
   575: 0 [  575/ 1327], train_loss/perplexity = 5.97742844/394.4247742 secs/batch = 0.2960s, grad.norm=9.11530685
   580: 0 [  580/ 1327], train_loss/perplexity = 6.02833939/415.0252686 secs/batch = 0.2944s, grad.norm=7.21383429
   585: 0 [  585/ 1327], train_loss/perplexity = 5.75137615/314.6233215 secs/batch = 0.2945s, grad.norm=7.60989475
   590: 0 [  590/ 1327], train_loss/perplexity = 6.12797928/458.5086975 secs/batch = 0.2925s, grad.norm=7.13128519
   595: 0 [  595/ 1327], train_loss/perplexity = 5.98342180/396.7958069 secs/batch = 0.3018s, grad.norm=7.72419071
   600: 0 [  600/ 1327], train_loss/perplexity = 6.17409039/480.1460876 secs/batch = 0.2969s, grad.norm=7.74334812
   605: 0 [  605/ 1327], train_loss/perplexity = 6.08375168/438.6718750 secs/batch = 0.2998s, grad.norm=7.17189837
   610: 0 [  610/ 1327], train_loss/perplexity = 6.24922180/517.6098633 secs/batch = 0.2963s, grad.norm=7.66725588
   615: 0 [  615/ 1327], train_loss/perplexity = 5.66244221/287.8507690 secs/batch = 0.2988s, grad.norm=9.10275269
   620: 0 [  620/ 1327], train_loss/perplexity = 5.88255024/358.7229004 secs/batch = 0.2956s, grad.norm=8.15163994
   625: 0 [  625/ 1327], train_loss/perplexity = 6.07334042/434.1284180 secs/batch = 0.2967s, grad.norm=7.24322844
   630: 0 [  630/ 1327], train_loss/perplexity = 6.07341623/434.1613464 secs/batch = 0.2960s, grad.norm=6.82494497
   635: 0 [  635/ 1327], train_loss/perplexity = 5.82026482/337.0613098 secs/batch = 0.2948s, grad.norm=7.40049553
   640: 0 [  640/ 1327], train_loss/perplexity = 6.03735542/418.7840576 secs/batch = 0.2938s, grad.norm=8.95516396
   645: 0 [  645/ 1327], train_loss/perplexity = 6.07964325/436.8733215 secs/batch = 0.2934s, grad.norm=7.66924381
   650: 0 [  650/ 1327], train_loss/perplexity = 5.82150984/337.4812012 secs/batch = 0.3008s, grad.norm=8.99605465
   655: 0 [  655/ 1327], train_loss/perplexity = 5.84356594/345.0074158 secs/batch = 0.2947s, grad.norm=8.21461201
   660: 0 [  660/ 1327], train_loss/perplexity = 5.73876810/310.6814575 secs/batch = 0.2930s, grad.norm=7.19983101
   665: 0 [  665/ 1327], train_loss/perplexity = 5.96415997/389.2259216 secs/batch = 0.2975s, grad.norm=7.66356468
   670: 0 [  670/ 1327], train_loss/perplexity = 5.88028812/357.9123535 secs/batch = 0.2992s, grad.norm=9.10779095
   675: 0 [  675/ 1327], train_loss/perplexity = 5.65715981/286.3342285 secs/batch = 0.2945s, grad.norm=9.18742943
   680: 0 [  680/ 1327], train_loss/perplexity = 5.95039225/383.9039001 secs/batch = 0.2965s, grad.norm=7.87540913
   685: 0 [  685/ 1327], train_loss/perplexity = 5.87970161/357.7024841 secs/batch = 0.2950s, grad.norm=8.46397495
   690: 0 [  690/ 1327], train_loss/perplexity = 6.01097870/407.8823242 secs/batch = 0.3024s, grad.norm=8.76751995
   695: 0 [  695/ 1327], train_loss/perplexity = 5.75866079/316.9236145 secs/batch = 0.2996s, grad.norm=8.00504303
   700: 0 [  700/ 1327], train_loss/perplexity = 5.97149563/392.0916443 secs/batch = 0.2959s, grad.norm=7.27089024
   705: 0 [  705/ 1327], train_loss/perplexity = 5.71872568/304.5166321 secs/batch = 0.2946s, grad.norm=8.67480659
   710: 0 [  710/ 1327], train_loss/perplexity = 5.85346794/348.4406738 secs/batch = 0.2945s, grad.norm=8.20757961
   715: 0 [  715/ 1327], train_loss/perplexity = 5.79341269/328.1309204 secs/batch = 0.2945s, grad.norm=8.81947708
   720: 0 [  720/ 1327], train_loss/perplexity = 5.81672382/335.8698730 secs/batch = 0.2940s, grad.norm=8.86456776
   725: 0 [  725/ 1327], train_loss/perplexity = 5.56956196/262.3191528 secs/batch = 0.2926s, grad.norm=9.81480217
   730: 0 [  730/ 1327], train_loss/perplexity = 5.74205732/311.7050171 secs/batch = 0.3000s, grad.norm=8.55775833
   735: 0 [  735/ 1327], train_loss/perplexity = 5.85870552/350.2704468 secs/batch = 0.2957s, grad.norm=8.36600113
   740: 0 [  740/ 1327], train_loss/perplexity = 5.31712914/203.7979584 secs/batch = 0.2945s, grad.norm=9.24409199
   745: 0 [  745/ 1327], train_loss/perplexity = 5.79733849/329.4216309 secs/batch = 0.2956s, grad.norm=8.68831825
   750: 0 [  750/ 1327], train_loss/perplexity = 5.61493492/274.4955139 secs/batch = 0.3024s, grad.norm=8.91751480
   755: 0 [  755/ 1327], train_loss/perplexity = 5.61222458/273.7525330 secs/batch = 0.3028s, grad.norm=7.80462790
   760: 0 [  760/ 1327], train_loss/perplexity = 5.64458752/282.7568970 secs/batch = 0.2962s, grad.norm=8.95015526
   765: 0 [  765/ 1327], train_loss/perplexity = 5.60911798/272.9034119 secs/batch = 0.2945s, grad.norm=8.76686478
   770: 0 [  770/ 1327], train_loss/perplexity = 5.63920164/281.2380981 secs/batch = 0.2933s, grad.norm=8.29747486
   775: 0 [  775/ 1327], train_loss/perplexity = 5.71508884/303.4111633 secs/batch = 0.2950s, grad.norm=8.90152264
   780: 0 [  780/ 1327], train_loss/perplexity = 5.88107014/358.1923523 secs/batch = 0.2943s, grad.norm=8.42349815
   785: 0 [  785/ 1327], train_loss/perplexity = 5.73132992/308.3791199 secs/batch = 0.2938s, grad.norm=8.92122936
   790: 0 [  790/ 1327], train_loss/perplexity = 5.55885983/259.5267639 secs/batch = 0.2933s, grad.norm=10.23282242
   795: 0 [  795/ 1327], train_loss/perplexity = 5.86877203/353.8142395 secs/batch = 0.2957s, grad.norm=8.89124489
   800: 0 [  800/ 1327], train_loss/perplexity = 5.79542780/328.7928162 secs/batch = 0.2991s, grad.norm=9.22293854
   805: 0 [  805/ 1327], train_loss/perplexity = 6.06382036/430.0151062 secs/batch = 0.3001s, grad.norm=9.22394180
   810: 0 [  810/ 1327], train_loss/perplexity = 5.81724548/336.0451355 secs/batch = 0.2959s, grad.norm=8.88952255
   815: 0 [  815/ 1327], train_loss/perplexity = 5.68170452/293.4491882 secs/batch = 0.2945s, grad.norm=8.71951103
   820: 0 [  820/ 1327], train_loss/perplexity = 5.27584219/195.5550995 secs/batch = 0.2958s, grad.norm=8.42529011
   825: 0 [  825/ 1327], train_loss/perplexity = 5.45299673/233.4567261 secs/batch = 0.3015s, grad.norm=8.28945255
   830: 0 [  830/ 1327], train_loss/perplexity = 5.32483912/205.3753204 secs/batch = 0.3019s, grad.norm=9.28942966
   835: 0 [  835/ 1327], train_loss/perplexity = 5.59948540/270.2872925 secs/batch = 0.2942s, grad.norm=8.49296665
   840: 0 [  840/ 1327], train_loss/perplexity = 5.70650578/300.8181152 secs/batch = 0.2955s, grad.norm=8.51887608
   845: 0 [  845/ 1327], train_loss/perplexity = 5.58925629/267.5365601 secs/batch = 0.2947s, grad.norm=8.37776184
   850: 0 [  850/ 1327], train_loss/perplexity = 5.65230274/284.9468689 secs/batch = 0.2965s, grad.norm=9.11549473
   855: 0 [  855/ 1327], train_loss/perplexity = 5.61271572/273.8870239 secs/batch = 0.2956s, grad.norm=9.36306286
   860: 0 [  860/ 1327], train_loss/perplexity = 5.34281731/209.1009827 secs/batch = 0.2945s, grad.norm=9.15335846
   865: 0 [  865/ 1327], train_loss/perplexity = 5.73641062/309.9498901 secs/batch = 0.2964s, grad.norm=9.03665161
   870: 0 [  870/ 1327], train_loss/perplexity = 5.89346409/362.6593933 secs/batch = 0.2948s, grad.norm=9.77715397
   875: 0 [  875/ 1327], train_loss/perplexity = 5.36100817/212.9395142 secs/batch = 0.2995s, grad.norm=9.03293991
   880: 0 [  880/ 1327], train_loss/perplexity = 5.43082094/228.3366241 secs/batch = 0.2955s, grad.norm=8.80555153
   885: 0 [  885/ 1327], train_loss/perplexity = 5.50967789/247.0715332 secs/batch = 0.2956s, grad.norm=9.59808826
   890: 0 [  890/ 1327], train_loss/perplexity = 5.74629116/313.0275269 secs/batch = 0.2965s, grad.norm=9.34318352
   895: 0 [  895/ 1327], train_loss/perplexity = 5.83539391/342.1994934 secs/batch = 0.3025s, grad.norm=10.84504032
   900: 0 [  900/ 1327], train_loss/perplexity = 5.67081976/290.2723999 secs/batch = 0.2968s, grad.norm=8.60294342
   905: 0 [  905/ 1327], train_loss/perplexity = 5.47725964/239.1903381 secs/batch = 0.2965s, grad.norm=8.85539722
   910: 0 [  910/ 1327], train_loss/perplexity = 5.57779932/264.4889221 secs/batch = 0.2967s, grad.norm=9.17731667
   915: 0 [  915/ 1327], train_loss/perplexity = 5.86314058/351.8273621 secs/batch = 0.2938s, grad.norm=8.96951199
   920: 0 [  920/ 1327], train_loss/perplexity = 5.92304802/373.5485535 secs/batch = 0.2966s, grad.norm=9.06474113
   925: 0 [  925/ 1327], train_loss/perplexity = 5.63970613/281.3800049 secs/batch = 0.2934s, grad.norm=8.39040852
   930: 0 [  930/ 1327], train_loss/perplexity = 5.55919409/259.6135254 secs/batch = 0.2934s, grad.norm=9.84539795
   935: 0 [  935/ 1327], train_loss/perplexity = 5.59616089/269.3901978 secs/batch = 0.3005s, grad.norm=9.08026505
   940: 0 [  940/ 1327], train_loss/perplexity = 5.60146141/270.8218994 secs/batch = 0.2946s, grad.norm=10.31246948
   945: 0 [  945/ 1327], train_loss/perplexity = 5.79106617/327.3618469 secs/batch = 0.2995s, grad.norm=9.56205845
   950: 0 [  950/ 1327], train_loss/perplexity = 5.53643465/253.7716064 secs/batch = 0.2944s, grad.norm=9.04889297
   955: 0 [  955/ 1327], train_loss/perplexity = 5.75731230/316.4965515 secs/batch = 0.2932s, grad.norm=8.92421436
   960: 0 [  960/ 1327], train_loss/perplexity = 5.87567139/356.2637634 secs/batch = 0.2955s, grad.norm=9.14882851
   965: 0 [  965/ 1327], train_loss/perplexity = 5.62335348/276.8161316 secs/batch = 0.2951s, grad.norm=9.02596378
   970: 0 [  970/ 1327], train_loss/perplexity = 5.81196213/334.2743835 secs/batch = 0.2966s, grad.norm=8.17927647
   975: 0 [  975/ 1327], train_loss/perplexity = 5.58369255/266.0522156 secs/batch = 0.2949s, grad.norm=9.91958904
   980: 0 [  980/ 1327], train_loss/perplexity = 5.43973351/230.3807831 secs/batch = 0.3009s, grad.norm=9.23288822
   985: 0 [  985/ 1327], train_loss/perplexity = 5.63455963/279.9356079 secs/batch = 0.2991s, grad.norm=10.08242321
   990: 0 [  990/ 1327], train_loss/perplexity = 5.73053646/308.1345215 secs/batch = 0.2949s, grad.norm=8.88379574
   995: 0 [  995/ 1327], train_loss/perplexity = 5.73967886/310.9645386 secs/batch = 0.3003s, grad.norm=9.08232498
  1000: 0 [ 1000/ 1327], train_loss/perplexity = 5.18568707/178.6961823 secs/batch = 0.2940s, grad.norm=9.39633274
  1005: 0 [ 1005/ 1327], train_loss/perplexity = 5.75391769/315.4239807 secs/batch = 0.2945s, grad.norm=10.43569756
  1010: 0 [ 1010/ 1327], train_loss/perplexity = 5.22290134/185.4715271 secs/batch = 0.2958s, grad.norm=9.44058323
  1015: 0 [ 1015/ 1327], train_loss/perplexity = 5.70166779/299.3662720 secs/batch = 0.2955s, grad.norm=9.11976242
  1020: 0 [ 1020/ 1327], train_loss/perplexity = 5.89337826/362.6282654 secs/batch = 0.2943s, grad.norm=9.03062057
  1025: 0 [ 1025/ 1327], train_loss/perplexity = 5.67184830/290.5711060 secs/batch = 0.2959s, grad.norm=9.35180855
  1030: 0 [ 1030/ 1327], train_loss/perplexity = 5.55615902/258.8267822 secs/batch = 0.2954s, grad.norm=9.42503548
  1035: 0 [ 1035/ 1327], train_loss/perplexity = 5.45583153/234.1194611 secs/batch = 0.2958s, grad.norm=9.43454838
  1040: 0 [ 1040/ 1327], train_loss/perplexity = 5.67821741/292.4276733 secs/batch = 0.2952s, grad.norm=8.31525517
  1045: 0 [ 1045/ 1327], train_loss/perplexity = 5.34469128/209.4931946 secs/batch = 0.3001s, grad.norm=9.38987255
  1050: 0 [ 1050/ 1327], train_loss/perplexity = 5.42333794/226.6343536 secs/batch = 0.2941s, grad.norm=11.23145580
  1055: 0 [ 1055/ 1327], train_loss/perplexity = 5.59025383/267.8035889 secs/batch = 0.2972s, grad.norm=9.89193249
  1060: 0 [ 1060/ 1327], train_loss/perplexity = 5.24588966/189.7845917 secs/batch = 0.2952s, grad.norm=10.81735420
  1065: 0 [ 1065/ 1327], train_loss/perplexity = 5.30171967/200.6816254 secs/batch = 0.2933s, grad.norm=9.84258461
  1070: 0 [ 1070/ 1327], train_loss/perplexity = 5.71241760/302.6017456 secs/batch = 0.2939s, grad.norm=10.06800270
  1075: 0 [ 1075/ 1327], train_loss/perplexity = 5.48082495/240.0446472 secs/batch = 0.2995s, grad.norm=10.84337425
  1080: 0 [ 1080/ 1327], train_loss/perplexity = 5.28741598/197.8315582 secs/batch = 0.2940s, grad.norm=10.28289127
  1085: 0 [ 1085/ 1327], train_loss/perplexity = 5.20542765/182.2588043 secs/batch = 0.2931s, grad.norm=9.86422920
  1090: 0 [ 1090/ 1327], train_loss/perplexity = 5.48760319/241.6772614 secs/batch = 0.2979s, grad.norm=10.03815365
  1095: 0 [ 1095/ 1327], train_loss/perplexity = 5.59550762/269.2142639 secs/batch = 0.2951s, grad.norm=9.95504093
  1100: 0 [ 1100/ 1327], train_loss/perplexity = 5.60055399/270.5762634 secs/batch = 0.2944s, grad.norm=10.14382839
  1105: 0 [ 1105/ 1327], train_loss/perplexity = 5.37183428/215.2573547 secs/batch = 0.2924s, grad.norm=10.26859951
  1110: 0 [ 1110/ 1327], train_loss/perplexity = 5.88022375/357.8893127 secs/batch = 0.2965s, grad.norm=9.09574699
  1115: 0 [ 1115/ 1327], train_loss/perplexity = 5.35149956/210.9243469 secs/batch = 0.3006s, grad.norm=9.49911404
  1120: 0 [ 1120/ 1327], train_loss/perplexity = 5.54592752/256.1920776 secs/batch = 0.2941s, grad.norm=9.70406818
  1125: 0 [ 1125/ 1327], train_loss/perplexity = 5.74467802/312.5229797 secs/batch = 0.2931s, grad.norm=9.57599068
  1130: 0 [ 1130/ 1327], train_loss/perplexity = 5.44782495/232.2524567 secs/batch = 0.2940s, grad.norm=9.58164883
  1135: 0 [ 1135/ 1327], train_loss/perplexity = 5.41421652/224.5765228 secs/batch = 0.2933s, grad.norm=9.71865654
  1140: 0 [ 1140/ 1327], train_loss/perplexity = 5.67453051/291.3515320 secs/batch = 0.2953s, grad.norm=9.59961605
  1145: 0 [ 1145/ 1327], train_loss/perplexity = 5.43680096/229.7061615 secs/batch = 0.2933s, grad.norm=10.35483074
  1150: 0 [ 1150/ 1327], train_loss/perplexity = 5.37208128/215.3105164 secs/batch = 0.2956s, grad.norm=9.66263294
  1155: 0 [ 1155/ 1327], train_loss/perplexity = 5.53367853/253.0731354 secs/batch = 0.2951s, grad.norm=9.89175320
  1160: 0 [ 1160/ 1327], train_loss/perplexity = 5.54468489/255.8739319 secs/batch = 0.2957s, grad.norm=10.16207218
  1165: 0 [ 1165/ 1327], train_loss/perplexity = 5.61160755/273.5836792 secs/batch = 0.2998s, grad.norm=10.23176956
  1170: 0 [ 1170/ 1327], train_loss/perplexity = 5.49898195/244.4429474 secs/batch = 0.2954s, grad.norm=10.58216572
  1175: 0 [ 1175/ 1327], train_loss/perplexity = 5.25297928/191.1348724 secs/batch = 0.2962s, grad.norm=11.23749828
  1180: 0 [ 1180/ 1327], train_loss/perplexity = 5.18773174/179.0619354 secs/batch = 0.2948s, grad.norm=10.75174332
  1185: 0 [ 1185/ 1327], train_loss/perplexity = 5.43891716/230.1927795 secs/batch = 0.2941s, grad.norm=10.22704792
  1190: 0 [ 1190/ 1327], train_loss/perplexity = 5.43673563/229.6911621 secs/batch = 0.2994s, grad.norm=9.81217957
  1195: 0 [ 1195/ 1327], train_loss/perplexity = 5.27796841/195.9713440 secs/batch = 0.2958s, grad.norm=9.98629761
  1200: 0 [ 1200/ 1327], train_loss/perplexity = 5.15793467/173.8051147 secs/batch = 0.2944s, grad.norm=10.00376892
  1205: 0 [ 1205/ 1327], train_loss/perplexity = 5.33064365/206.5708923 secs/batch = 0.2986s, grad.norm=10.83920383
  1210: 0 [ 1210/ 1327], train_loss/perplexity = 5.08440065/161.4831238 secs/batch = 0.2988s, grad.norm=10.82659912
  1215: 0 [ 1215/ 1327], train_loss/perplexity = 5.11791134/166.9862213 secs/batch = 0.3033s, grad.norm=10.31983757
  1220: 0 [ 1220/ 1327], train_loss/perplexity = 5.27357674/195.1125793 secs/batch = 0.2953s, grad.norm=10.43917465
  1225: 0 [ 1225/ 1327], train_loss/perplexity = 5.25335884/191.2074280 secs/batch = 0.3005s, grad.norm=11.61283779
  1230: 0 [ 1230/ 1327], train_loss/perplexity = 5.31020117/202.3909454 secs/batch = 0.2938s, grad.norm=10.48982334
  1235: 0 [ 1235/ 1327], train_loss/perplexity = 5.36545801/213.8891754 secs/batch = 0.2989s, grad.norm=10.75909805
  1240: 0 [ 1240/ 1327], train_loss/perplexity = 5.43850660/230.0982971 secs/batch = 0.3009s, grad.norm=9.55787754
  1245: 0 [ 1245/ 1327], train_loss/perplexity = 5.28053951/196.4758453 secs/batch = 0.2997s, grad.norm=11.25134373
  1250: 0 [ 1250/ 1327], train_loss/perplexity = 5.43391228/229.0435791 secs/batch = 0.2941s, grad.norm=9.93581390
  1255: 0 [ 1255/ 1327], train_loss/perplexity = 5.36785173/214.4017792 secs/batch = 0.2921s, grad.norm=9.75391197
  1260: 0 [ 1260/ 1327], train_loss/perplexity = 5.35982227/212.6871490 secs/batch = 0.2945s, grad.norm=10.83176804
  1265: 0 [ 1265/ 1327], train_loss/perplexity = 5.52326775/250.4521179 secs/batch = 0.2956s, grad.norm=10.15710068
  1270: 0 [ 1270/ 1327], train_loss/perplexity = 5.26740742/193.9125824 secs/batch = 0.2941s, grad.norm=9.86153603
  1275: 0 [ 1275/ 1327], train_loss/perplexity = 5.60444355/271.6307373 secs/batch = 0.2988s, grad.norm=10.75855541
  1280: 0 [ 1280/ 1327], train_loss/perplexity = 5.26235580/192.9354706 secs/batch = 0.2990s, grad.norm=10.12966347
  1285: 0 [ 1285/ 1327], train_loss/perplexity = 5.30663490/201.6704407 secs/batch = 0.2952s, grad.norm=10.24597931
  1290: 0 [ 1290/ 1327], train_loss/perplexity = 5.34855461/210.3041077 secs/batch = 0.2946s, grad.norm=9.62075138
  1295: 0 [ 1295/ 1327], train_loss/perplexity = 5.48532057/241.1262360 secs/batch = 0.2927s, grad.norm=10.84751034
  1300: 0 [ 1300/ 1327], train_loss/perplexity = 5.59285641/268.5014648 secs/batch = 0.2952s, grad.norm=10.68122292
  1305: 0 [ 1305/ 1327], train_loss/perplexity = 5.68743801/295.1365051 secs/batch = 0.2950s, grad.norm=10.16239548
  1310: 0 [ 1310/ 1327], train_loss/perplexity = 5.84085178/344.0722961 secs/batch = 0.2939s, grad.norm=9.60547447
  1315: 0 [ 1315/ 1327], train_loss/perplexity = 5.71880150/304.5397034 secs/batch = 0.2946s, grad.norm=10.19216824
  1320: 0 [ 1320/ 1327], train_loss/perplexity = 5.69967937/298.7716064 secs/batch = 0.2943s, grad.norm=9.96347618
  1325: 0 [ 1325/ 1327], train_loss/perplexity = 5.51933002/249.4678497 secs/batch = 0.3003s, grad.norm=10.18863010
Epoch training time: 393.5706307888031
	> validation loss = 5.59527016, perplexity = 269.15036011
	> validation loss = 5.46912193, perplexity = 237.25178528
	> validation loss = 5.32319832, perplexity = 205.03862000
	> validation loss = 5.43384838, perplexity = 229.02894592
	> validation loss = 5.67696571, perplexity = 292.06188965
	> validation loss = 5.41054106, perplexity = 223.75262451
	> validation loss = 5.39202023, perplexity = 219.64666748
	> validation loss = 5.35942173, perplexity = 212.60197449
	> validation loss = 5.61873531, perplexity = 275.54067993
	> validation loss = 5.37067318, perplexity = 215.00755310
	> validation loss = 5.41879034, perplexity = 225.60604858
	> validation loss = 5.48672390, perplexity = 241.46484375
	> validation loss = 5.39005709, perplexity = 219.21589661
	> validation loss = 5.35367489, perplexity = 211.38368225
	> validation loss = 5.08742094, perplexity = 161.97158813
	> validation loss = 5.17463398, perplexity = 176.73191833
	> validation loss = 5.45764208, perplexity = 234.54373169
	> validation loss = 5.11401320, perplexity = 166.33656311
	> validation loss = 5.54605627, perplexity = 256.22506714
	> validation loss = 5.50629520, perplexity = 246.23718262
	> validation loss = 5.31750679, perplexity = 203.87493896
at the end of epoch: 0
train loss = 5.52040395, perplexity = 249.73589783
validation loss = 5.38538056, perplexity = 218.19312460
Saved model cv/epoch000_5.3854.model
  1332: 1 [    5/ 1327], train_loss/perplexity = 5.72507524/306.4563293 secs/batch = 0.2945s, grad.norm=9.67725086
  1337: 1 [   10/ 1327], train_loss/perplexity = 5.27227020/194.8578339 secs/batch = 0.2990s, grad.norm=11.88463211
  1342: 1 [   15/ 1327], train_loss/perplexity = 5.28583717/197.5194702 secs/batch = 0.2983s, grad.norm=10.12249184
  1347: 1 [   20/ 1327], train_loss/perplexity = 5.77886200/323.3909607 secs/batch = 0.2961s, grad.norm=10.49187088
  1352: 1 [   25/ 1327], train_loss/perplexity = 5.56604099/261.3971863 secs/batch = 0.2941s, grad.norm=10.32298374
  1357: 1 [   30/ 1327], train_loss/perplexity = 5.40108681/221.6471710 secs/batch = 0.2950s, grad.norm=10.26700497
  1362: 1 [   35/ 1327], train_loss/perplexity = 5.29028034/198.3990326 secs/batch = 0.2929s, grad.norm=9.85660267
  1367: 1 [   40/ 1327], train_loss/perplexity = 5.37895870/216.7964020 secs/batch = 0.2957s, grad.norm=10.82123375
  1372: 1 [   45/ 1327], train_loss/perplexity = 5.09760952/163.6302795 secs/batch = 0.2975s, grad.norm=10.86035919
  1377: 1 [   50/ 1327], train_loss/perplexity = 5.47522068/238.7031403 secs/batch = 0.2938s, grad.norm=9.67454243
  1382: 1 [   55/ 1327], train_loss/perplexity = 5.42470789/226.9450378 secs/batch = 0.2989s, grad.norm=11.08526230
  1387: 1 [   60/ 1327], train_loss/perplexity = 5.57031679/262.5172424 secs/batch = 0.2944s, grad.norm=10.78427696
  1392: 1 [   65/ 1327], train_loss/perplexity = 5.16307449/174.7007446 secs/batch = 0.2942s, grad.norm=10.35247898
  1397: 1 [   70/ 1327], train_loss/perplexity = 4.98304558/145.9181061 secs/batch = 0.2948s, grad.norm=11.02852440
  1402: 1 [   75/ 1327], train_loss/perplexity = 5.02034760/151.4639435 secs/batch = 0.2940s, grad.norm=10.69035149
  1407: 1 [   80/ 1327], train_loss/perplexity = 5.41021824/223.6804047 secs/batch = 0.2939s, grad.norm=11.07501698
  1412: 1 [   85/ 1327], train_loss/perplexity = 5.36655569/214.1240845 secs/batch = 0.3007s, grad.norm=9.96315575
  1417: 1 [   90/ 1327], train_loss/perplexity = 5.38384724/217.8588257 secs/batch = 0.3010s, grad.norm=11.12634373
  1422: 1 [   95/ 1327], train_loss/perplexity = 5.19595480/180.5404358 secs/batch = 0.2932s, grad.norm=11.08730984
  1427: 1 [  100/ 1327], train_loss/perplexity = 5.48499584/241.0479431 secs/batch = 0.2957s, grad.norm=10.24355602
  1432: 1 [  105/ 1327], train_loss/perplexity = 5.49730110/244.0324249 secs/batch = 0.3005s, grad.norm=10.73710537
  1437: 1 [  110/ 1327], train_loss/perplexity = 5.26850367/194.1252747 secs/batch = 0.2940s, grad.norm=10.20786667
  1442: 1 [  115/ 1327], train_loss/perplexity = 5.17714834/177.1768494 secs/batch = 0.2952s, grad.norm=11.71120739
  1447: 1 [  120/ 1327], train_loss/perplexity = 5.34415960/209.3818512 secs/batch = 0.3000s, grad.norm=11.50828075
  1452: 1 [  125/ 1327], train_loss/perplexity = 5.40403700/222.3020477 secs/batch = 0.2947s, grad.norm=10.36868095
  1457: 1 [  130/ 1327], train_loss/perplexity = 5.26721525/193.8753204 secs/batch = 0.2952s, grad.norm=10.89841270
  1462: 1 [  135/ 1327], train_loss/perplexity = 5.35826349/212.3558655 secs/batch = 0.2904s, grad.norm=10.44399452
  1467: 1 [  140/ 1327], train_loss/perplexity = 5.59634352/269.4393921 secs/batch = 0.2983s, grad.norm=10.01089859
  1472: 1 [  145/ 1327], train_loss/perplexity = 5.56102610/260.0895691 secs/batch = 0.3016s, grad.norm=10.52130222
  1477: 1 [  150/ 1327], train_loss/perplexity = 5.44597816/231.8239288 secs/batch = 0.2960s, grad.norm=10.72173119
  1482: 1 [  155/ 1327], train_loss/perplexity = 5.70223331/299.5356140 secs/batch = 0.2946s, grad.norm=10.84416199
  1487: 1 [  160/ 1327], train_loss/perplexity = 5.31587124/203.5417633 secs/batch = 0.2997s, grad.norm=10.29705620
  1492: 1 [  165/ 1327], train_loss/perplexity = 5.53144836/252.5093689 secs/batch = 0.2957s, grad.norm=10.18796444
  1497: 1 [  170/ 1327], train_loss/perplexity = 5.35656643/211.9957886 secs/batch = 0.3010s, grad.norm=10.12521744
  1502: 1 [  175/ 1327], train_loss/perplexity = 5.57105112/262.7100830 secs/batch = 0.3007s, grad.norm=10.21432686
  1507: 1 [  180/ 1327], train_loss/perplexity = 5.41083336/223.8180237 secs/batch = 0.2997s, grad.norm=10.77332497
  1512: 1 [  185/ 1327], train_loss/perplexity = 5.68736362/295.1145630 secs/batch = 0.2963s, grad.norm=10.02481747
  1517: 1 [  190/ 1327], train_loss/perplexity = 5.18488741/178.5533447 secs/batch = 0.3012s, grad.norm=10.54722023
  1522: 1 [  195/ 1327], train_loss/perplexity = 5.38166475/217.3838654 secs/batch = 0.3002s, grad.norm=9.76265812
  1527: 1 [  200/ 1327], train_loss/perplexity = 5.42405462/226.7968292 secs/batch = 0.3014s, grad.norm=10.89484215
  1532: 1 [  205/ 1327], train_loss/perplexity = 5.41718006/225.2430573 secs/batch = 0.2948s, grad.norm=10.14078331
  1537: 1 [  210/ 1327], train_loss/perplexity = 5.33776712/208.0476532 secs/batch = 0.2919s, grad.norm=10.07605457
  1542: 1 [  215/ 1327], train_loss/perplexity = 5.48343754/240.6726074 secs/batch = 0.2983s, grad.norm=10.53646088
  1547: 1 [  220/ 1327], train_loss/perplexity = 5.50910378/246.9297180 secs/batch = 0.2998s, grad.norm=9.98190022
  1552: 1 [  225/ 1327], train_loss/perplexity = 5.68257236/293.7039795 secs/batch = 0.2961s, grad.norm=9.90341759
  1557: 1 [  230/ 1327], train_loss/perplexity = 5.45525742/233.9850922 secs/batch = 0.2942s, grad.norm=10.34872723
  1562: 1 [  235/ 1327], train_loss/perplexity = 5.33503199/207.4793854 secs/batch = 0.2960s, grad.norm=10.36378956
  1567: 1 [  240/ 1327], train_loss/perplexity = 5.20436287/182.0648346 secs/batch = 0.2949s, grad.norm=10.77433586
  1572: 1 [  245/ 1327], train_loss/perplexity = 5.44105721/230.6859436 secs/batch = 0.2957s, grad.norm=10.29318333
  1577: 1 [  250/ 1327], train_loss/perplexity = 5.16007900/174.1782227 secs/batch = 0.3019s, grad.norm=10.54684162
  1582: 1 [  255/ 1327], train_loss/perplexity = 5.28568077/197.4885864 secs/batch = 0.2967s, grad.norm=10.73587894
  1587: 1 [  260/ 1327], train_loss/perplexity = 5.61388350/274.2070618 secs/batch = 0.2957s, grad.norm=11.15302467
  1592: 1 [  265/ 1327], train_loss/perplexity = 5.55669498/258.9655457 secs/batch = 0.2953s, grad.norm=9.94792366
  1597: 1 [  270/ 1327], train_loss/perplexity = 5.63775492/280.8315125 secs/batch = 0.2943s, grad.norm=10.34694195
  1602: 1 [  275/ 1327], train_loss/perplexity = 5.76962376/320.4171448 secs/batch = 0.2949s, grad.norm=10.66927910
  1607: 1 [  280/ 1327], train_loss/perplexity = 5.38735151/218.6235962 secs/batch = 0.3015s, grad.norm=10.15950775
  1612: 1 [  285/ 1327], train_loss/perplexity = 5.64538002/282.9810791 secs/batch = 0.2995s, grad.norm=11.01960659
  1617: 1 [  290/ 1327], train_loss/perplexity = 5.51287889/247.8636780 secs/batch = 0.2999s, grad.norm=10.73314571
  1622: 1 [  295/ 1327], train_loss/perplexity = 5.25203991/190.9553986 secs/batch = 0.2994s, grad.norm=10.32317066
  1627: 1 [  300/ 1327], train_loss/perplexity = 4.81486988/123.3307648 secs/batch = 0.2950s, grad.norm=10.79090595
  1632: 1 [  305/ 1327], train_loss/perplexity = 5.29006958/198.3572235 secs/batch = 0.2970s, grad.norm=10.56601906
  1637: 1 [  310/ 1327], train_loss/perplexity = 5.35599089/211.8738098 secs/batch = 0.3005s, grad.norm=10.36072826
  1642: 1 [  315/ 1327], train_loss/perplexity = 5.09880972/163.8267975 secs/batch = 0.2947s, grad.norm=10.81215858
  1647: 1 [  320/ 1327], train_loss/perplexity = 5.17153692/176.1854095 secs/batch = 0.2994s, grad.norm=11.67184448
  1652: 1 [  325/ 1327], train_loss/perplexity = 4.99354458/147.4581757 secs/batch = 0.2928s, grad.norm=11.40063763
  1657: 1 [  330/ 1327], train_loss/perplexity = 5.40849733/223.2957916 secs/batch = 0.2941s, grad.norm=11.04216194
  1662: 1 [  335/ 1327], train_loss/perplexity = 4.66872406/106.5616913 secs/batch = 0.3011s, grad.norm=10.74073696
  1667: 1 [  340/ 1327], train_loss/perplexity = 5.48511124/241.0757599 secs/batch = 0.2945s, grad.norm=10.91244984
  1672: 1 [  345/ 1327], train_loss/perplexity = 5.33408833/207.2836914 secs/batch = 0.2971s, grad.norm=10.88194561
  1677: 1 [  350/ 1327], train_loss/perplexity = 5.44652510/231.9507599 secs/batch = 0.2999s, grad.norm=10.85206223
  1682: 1 [  355/ 1327], train_loss/perplexity = 5.52389622/250.6095734 secs/batch = 0.2952s, grad.norm=10.36192226
  1687: 1 [  360/ 1327], train_loss/perplexity = 5.58976030/267.6714478 secs/batch = 0.3012s, grad.norm=10.88906860
  1692: 1 [  365/ 1327], train_loss/perplexity = 5.49009800/242.2809448 secs/batch = 0.3020s, grad.norm=10.42073631
  1697: 1 [  370/ 1327], train_loss/perplexity = 5.41798210/225.4237823 secs/batch = 0.2936s, grad.norm=11.02240181
  1702: 1 [  375/ 1327], train_loss/perplexity = 4.79961300/121.4634018 secs/batch = 0.2966s, grad.norm=10.93684959
  1707: 1 [  380/ 1327], train_loss/perplexity = 5.12288904/167.8195038 secs/batch = 0.3021s, grad.norm=11.44387627
  1712: 1 [  385/ 1327], train_loss/perplexity = 5.28806019/197.9590454 secs/batch = 0.3012s, grad.norm=10.99972153
  1717: 1 [  390/ 1327], train_loss/perplexity = 5.33014297/206.4674988 secs/batch = 0.2953s, grad.norm=10.30053425
  1722: 1 [  395/ 1327], train_loss/perplexity = 5.56678820/261.5925598 secs/batch = 0.2953s, grad.norm=10.71601963
  1727: 1 [  400/ 1327], train_loss/perplexity = 5.31925535/204.2317505 secs/batch = 0.2939s, grad.norm=11.41872025
  1732: 1 [  405/ 1327], train_loss/perplexity = 5.63074684/278.8703003 secs/batch = 0.3008s, grad.norm=10.80508518
  1737: 1 [  410/ 1327], train_loss/perplexity = 5.35476875/211.6150360 secs/batch = 0.3014s, grad.norm=10.54700279
  1742: 1 [  415/ 1327], train_loss/perplexity = 5.17266369/176.3840485 secs/batch = 0.3021s, grad.norm=10.96536064
  1747: 1 [  420/ 1327], train_loss/perplexity = 5.05984211/157.5656433 secs/batch = 0.2948s, grad.norm=11.58576202
  1752: 1 [  425/ 1327], train_loss/perplexity = 5.31393528/203.1481018 secs/batch = 0.2955s, grad.norm=11.72565842
  1757: 1 [  430/ 1327], train_loss/perplexity = 5.41617727/225.0173035 secs/batch = 0.2951s, grad.norm=11.07246399
  1762: 1 [  435/ 1327], train_loss/perplexity = 5.49769831/244.1293793 secs/batch = 0.2960s, grad.norm=10.61056232
  1767: 1 [  440/ 1327], train_loss/perplexity = 5.22549915/185.9539642 secs/batch = 0.2939s, grad.norm=11.61570168
  1772: 1 [  445/ 1327], train_loss/perplexity = 5.40417385/222.3324585 secs/batch = 0.3017s, grad.norm=12.70892811
  1777: 1 [  450/ 1327], train_loss/perplexity = 5.23371840/187.4886627 secs/batch = 0.2961s, grad.norm=10.87611866
  1782: 1 [  455/ 1327], train_loss/perplexity = 5.01136827/150.1099854 secs/batch = 0.2953s, grad.norm=10.86133003
  1787: 1 [  460/ 1327], train_loss/perplexity = 5.31779623/203.9339600 secs/batch = 0.3034s, grad.norm=11.46519470
  1792: 1 [  465/ 1327], train_loss/perplexity = 5.18653345/178.8474884 secs/batch = 0.2962s, grad.norm=12.05385685
  1797: 1 [  470/ 1327], train_loss/perplexity = 5.49286652/242.9526367 secs/batch = 0.3004s, grad.norm=10.51830006
  1802: 1 [  475/ 1327], train_loss/perplexity = 5.16811562/175.5836639 secs/batch = 0.2953s, grad.norm=10.73149776
  1807: 1 [  480/ 1327], train_loss/perplexity = 5.34176636/208.8813477 secs/batch = 0.2959s, grad.norm=11.01169872
  1812: 1 [  485/ 1327], train_loss/perplexity = 5.16385984/174.8379974 secs/batch = 0.2945s, grad.norm=11.07084942
  1817: 1 [  490/ 1327], train_loss/perplexity = 5.13353920/169.6163635 secs/batch = 0.2986s, grad.norm=11.98387527
  1822: 1 [  495/ 1327], train_loss/perplexity = 5.08662081/161.8420410 secs/batch = 0.2958s, grad.norm=11.46976471
  1827: 1 [  500/ 1327], train_loss/perplexity = 5.44068909/230.6010284 secs/batch = 0.2957s, grad.norm=11.03465939
  1832: 1 [  505/ 1327], train_loss/perplexity = 5.29546261/199.4298553 secs/batch = 0.3011s, grad.norm=10.94130611
  1837: 1 [  510/ 1327], train_loss/perplexity = 5.65542507/285.8379517 secs/batch = 0.3011s, grad.norm=10.14025211
  1842: 1 [  515/ 1327], train_loss/perplexity = 5.33432341/207.3324280 secs/batch = 0.2921s, grad.norm=10.20658875
  1847: 1 [  520/ 1327], train_loss/perplexity = 5.54394579/255.6848907 secs/batch = 0.2945s, grad.norm=10.97019196
  1852: 1 [  525/ 1327], train_loss/perplexity = 5.10540867/164.9114532 secs/batch = 0.2980s, grad.norm=11.65553284
  1857: 1 [  530/ 1327], train_loss/perplexity = 5.21039867/183.1670685 secs/batch = 0.2959s, grad.norm=11.90958595
  1862: 1 [  535/ 1327], train_loss/perplexity = 5.22702360/186.2376556 secs/batch = 0.2947s, grad.norm=10.99048424
  1867: 1 [  540/ 1327], train_loss/perplexity = 5.35181189/210.9902344 secs/batch = 0.2955s, grad.norm=10.19323349
  1872: 1 [  545/ 1327], train_loss/perplexity = 5.44208431/230.9230042 secs/batch = 0.2933s, grad.norm=10.45643330
  1877: 1 [  550/ 1327], train_loss/perplexity = 5.34196138/208.9220886 secs/batch = 0.3031s, grad.norm=11.11273766
  1882: 1 [  555/ 1327], train_loss/perplexity = 5.23582268/187.8836060 secs/batch = 0.2955s, grad.norm=10.87406063
  1887: 1 [  560/ 1327], train_loss/perplexity = 5.29068756/198.4798431 secs/batch = 0.2950s, grad.norm=13.08139324
  1892: 1 [  565/ 1327], train_loss/perplexity = 5.33151388/206.7507324 secs/batch = 0.2954s, grad.norm=12.28428841
  1897: 1 [  570/ 1327], train_loss/perplexity = 5.12433338/168.0620728 secs/batch = 0.2951s, grad.norm=12.23125744
  1902: 1 [  575/ 1327], train_loss/perplexity = 5.13934422/170.6038513 secs/batch = 0.2994s, grad.norm=11.76542377
  1907: 1 [  580/ 1327], train_loss/perplexity = 5.39525557/220.3584595 secs/batch = 0.3005s, grad.norm=11.21404457
  1912: 1 [  585/ 1327], train_loss/perplexity = 5.03157997/153.1748352 secs/batch = 0.2999s, grad.norm=11.98956776
  1917: 1 [  590/ 1327], train_loss/perplexity = 5.30140495/200.6184692 secs/batch = 0.2988s, grad.norm=10.64360714
  1922: 1 [  595/ 1327], train_loss/perplexity = 5.24047375/188.7595062 secs/batch = 0.2936s, grad.norm=12.70572662
  1927: 1 [  600/ 1327], train_loss/perplexity = 5.48644924/241.3985291 secs/batch = 0.2965s, grad.norm=10.63425350
  1932: 1 [  605/ 1327], train_loss/perplexity = 5.43561077/229.4329376 secs/batch = 0.2944s, grad.norm=11.38946629
  1937: 1 [  610/ 1327], train_loss/perplexity = 5.50372601/245.6053619 secs/batch = 0.2956s, grad.norm=11.38821316
  1942: 1 [  615/ 1327], train_loss/perplexity = 4.95892668/142.4408264 secs/batch = 0.2959s, grad.norm=11.34497929
  1947: 1 [  620/ 1327], train_loss/perplexity = 5.32484865/205.3772736 secs/batch = 0.2949s, grad.norm=12.05844307
  1952: 1 [  625/ 1327], train_loss/perplexity = 5.49098492/242.4959259 secs/batch = 0.2962s, grad.norm=11.42002106
  1957: 1 [  630/ 1327], train_loss/perplexity = 5.48070240/240.0152283 secs/batch = 0.3010s, grad.norm=10.39088154
  1962: 1 [  635/ 1327], train_loss/perplexity = 5.17077923/176.0519714 secs/batch = 0.2957s, grad.norm=11.11613369
  1967: 1 [  640/ 1327], train_loss/perplexity = 5.28496265/197.3468170 secs/batch = 0.2958s, grad.norm=11.46236897
  1972: 1 [  645/ 1327], train_loss/perplexity = 5.47869253/239.5333252 secs/batch = 0.2951s, grad.norm=11.75078106
  1977: 1 [  650/ 1327], train_loss/perplexity = 5.14039040/170.7824249 secs/batch = 0.2949s, grad.norm=12.23167610
  1982: 1 [  655/ 1327], train_loss/perplexity = 5.22192812/185.2911072 secs/batch = 0.2946s, grad.norm=11.78015709
  1987: 1 [  660/ 1327], train_loss/perplexity = 5.08364487/161.3611298 secs/batch = 0.2988s, grad.norm=10.99584675
  1992: 1 [  665/ 1327], train_loss/perplexity = 5.35470247/211.6010132 secs/batch = 0.2952s, grad.norm=11.30467606
  1997: 1 [  670/ 1327], train_loss/perplexity = 5.19233942/179.8889008 secs/batch = 0.3008s, grad.norm=11.19686127
  2002: 1 [  675/ 1327], train_loss/perplexity = 4.96221256/142.9096375 secs/batch = 0.2959s, grad.norm=12.68548203
  2007: 1 [  680/ 1327], train_loss/perplexity = 5.30198622/200.7351227 secs/batch = 0.2944s, grad.norm=11.81545353
  2012: 1 [  685/ 1327], train_loss/perplexity = 5.22702074/186.2371216 secs/batch = 0.3031s, grad.norm=11.33686638
  2017: 1 [  690/ 1327], train_loss/perplexity = 5.41693687/225.1882782 secs/batch = 0.2964s, grad.norm=11.11783504
  2022: 1 [  695/ 1327], train_loss/perplexity = 5.15246677/172.8573608 secs/batch = 0.2950s, grad.norm=11.08698654
  2027: 1 [  700/ 1327], train_loss/perplexity = 5.39067316/219.3509979 secs/batch = 0.3009s, grad.norm=10.31947041
  2032: 1 [  705/ 1327], train_loss/perplexity = 5.12034988/167.3939209 secs/batch = 0.2957s, grad.norm=11.20395470
  2037: 1 [  710/ 1327], train_loss/perplexity = 5.14969492/172.3788910 secs/batch = 0.2945s, grad.norm=10.94866085
  2042: 1 [  715/ 1327], train_loss/perplexity = 5.10397291/164.6748505 secs/batch = 0.2940s, grad.norm=11.27514744
  2047: 1 [  720/ 1327], train_loss/perplexity = 5.18105555/177.8704681 secs/batch = 0.2945s, grad.norm=11.04423237
  2052: 1 [  725/ 1327], train_loss/perplexity = 4.98882484/146.7638550 secs/batch = 0.2939s, grad.norm=11.66844654
  2057: 1 [  730/ 1327], train_loss/perplexity = 5.16966248/175.8554688 secs/batch = 0.2964s, grad.norm=10.62526512
  2062: 1 [  735/ 1327], train_loss/perplexity = 5.30081367/200.4998779 secs/batch = 0.2961s, grad.norm=11.40547943
  2067: 1 [  740/ 1327], train_loss/perplexity = 4.68587160/108.4047165 secs/batch = 0.3012s, grad.norm=12.25527191
  2072: 1 [  745/ 1327], train_loss/perplexity = 5.20828485/182.7802887 secs/batch = 0.2952s, grad.norm=11.54806519
  2077: 1 [  750/ 1327], train_loss/perplexity = 5.02447844/152.0909119 secs/batch = 0.2991s, grad.norm=11.19644547
  2082: 1 [  755/ 1327], train_loss/perplexity = 5.01077652/150.0211792 secs/batch = 0.2959s, grad.norm=11.30496883
  2087: 1 [  760/ 1327], train_loss/perplexity = 4.99729967/148.0129395 secs/batch = 0.3016s, grad.norm=11.78158760
  2092: 1 [  765/ 1327], train_loss/perplexity = 5.05923796/157.4704742 secs/batch = 0.2945s, grad.norm=10.86802006
  2097: 1 [  770/ 1327], train_loss/perplexity = 5.00348282/148.9309540 secs/batch = 0.2999s, grad.norm=11.86434078
  2102: 1 [  775/ 1327], train_loss/perplexity = 5.08409786/161.4342346 secs/batch = 0.2991s, grad.norm=12.55352211
  2107: 1 [  780/ 1327], train_loss/perplexity = 5.41051102/223.7458954 secs/batch = 0.2948s, grad.norm=11.03359222
  2112: 1 [  785/ 1327], train_loss/perplexity = 5.17748976/177.2373505 secs/batch = 0.2953s, grad.norm=11.89763165
  2117: 1 [  790/ 1327], train_loss/perplexity = 5.01067734/150.0063019 secs/batch = 0.2951s, grad.norm=11.13499069
  2122: 1 [  795/ 1327], train_loss/perplexity = 5.40853930/223.3051605 secs/batch = 0.2959s, grad.norm=11.50867081
  2127: 1 [  800/ 1327], train_loss/perplexity = 5.26523352/193.4914856 secs/batch = 0.2970s, grad.norm=12.55191040
  2132: 1 [  805/ 1327], train_loss/perplexity = 5.55393457/258.2516785 secs/batch = 0.3004s, grad.norm=11.33641624
  2137: 1 [  810/ 1327], train_loss/perplexity = 5.25866604/192.2248993 secs/batch = 0.2945s, grad.norm=11.76398659
  2142: 1 [  815/ 1327], train_loss/perplexity = 5.09612560/163.3876495 secs/batch = 0.2967s, grad.norm=10.99190807
  2147: 1 [  820/ 1327], train_loss/perplexity = 4.77657890/118.6975784 secs/batch = 0.2997s, grad.norm=11.77811813
  2152: 1 [  825/ 1327], train_loss/perplexity = 4.98833752/146.6923523 secs/batch = 0.2952s, grad.norm=11.03948879
  2157: 1 [  830/ 1327], train_loss/perplexity = 4.87258625/130.6584015 secs/batch = 0.2952s, grad.norm=12.05464554
  2162: 1 [  835/ 1327], train_loss/perplexity = 5.10741377/165.2424469 secs/batch = 0.2951s, grad.norm=11.88142586
  2167: 1 [  840/ 1327], train_loss/perplexity = 5.23386240/187.5156708 secs/batch = 0.2993s, grad.norm=12.33814430
  2172: 1 [  845/ 1327], train_loss/perplexity = 5.05165434/156.2807922 secs/batch = 0.2944s, grad.norm=10.75530243
  2177: 1 [  850/ 1327], train_loss/perplexity = 5.12542152/168.2450409 secs/batch = 0.2952s, grad.norm=11.86309624
  2182: 1 [  855/ 1327], train_loss/perplexity = 5.06305504/158.0726929 secs/batch = 0.2958s, grad.norm=11.55989552
  2187: 1 [  860/ 1327], train_loss/perplexity = 4.79531050/120.9419250 secs/batch = 0.2976s, grad.norm=11.20605850
  2192: 1 [  865/ 1327], train_loss/perplexity = 5.29945707/200.2280731 secs/batch = 0.2946s, grad.norm=11.74779129
  2197: 1 [  870/ 1327], train_loss/perplexity = 5.32180500/204.7531281 secs/batch = 0.2950s, grad.norm=11.77507114
  2202: 1 [  875/ 1327], train_loss/perplexity = 4.79915810/121.4081573 secs/batch = 0.3003s, grad.norm=11.83552647
  2207: 1 [  880/ 1327], train_loss/perplexity = 5.01640654/150.8681946 secs/batch = 0.2945s, grad.norm=11.41475391
  2212: 1 [  885/ 1327], train_loss/perplexity = 5.07866669/160.5598297 secs/batch = 0.2946s, grad.norm=11.76466465
  2217: 1 [  890/ 1327], train_loss/perplexity = 5.24707603/190.0098724 secs/batch = 0.2960s, grad.norm=11.37800217
  2222: 1 [  895/ 1327], train_loss/perplexity = 5.32491064/205.3899994 secs/batch = 0.3023s, grad.norm=10.86033821
  2227: 1 [  900/ 1327], train_loss/perplexity = 5.18551064/178.6646576 secs/batch = 0.2947s, grad.norm=11.48365688
  2232: 1 [  905/ 1327], train_loss/perplexity = 5.03962803/154.4125671 secs/batch = 0.2998s, grad.norm=12.31152248
  2237: 1 [  910/ 1327], train_loss/perplexity = 5.09745073/163.6043091 secs/batch = 0.2952s, grad.norm=12.00500202
  2242: 1 [  915/ 1327], train_loss/perplexity = 5.32589483/205.5922546 secs/batch = 0.2949s, grad.norm=11.14748383
  2247: 1 [  920/ 1327], train_loss/perplexity = 5.44171906/230.8386688 secs/batch = 0.2944s, grad.norm=11.32104778
  2252: 1 [  925/ 1327], train_loss/perplexity = 5.24164534/188.9807892 secs/batch = 0.2938s, grad.norm=10.69169044
  2257: 1 [  930/ 1327], train_loss/perplexity = 5.19718266/180.7622528 secs/batch = 0.2986s, grad.norm=11.67291641
  2262: 1 [  935/ 1327], train_loss/perplexity = 5.21297312/183.6392212 secs/batch = 0.2962s, grad.norm=10.79436207
  2267: 1 [  940/ 1327], train_loss/perplexity = 5.19075775/179.6045990 secs/batch = 0.3004s, grad.norm=12.17611313
  2272: 1 [  945/ 1327], train_loss/perplexity = 5.40426254/222.3521881 secs/batch = 0.3000s, grad.norm=11.92645741
  2277: 1 [  950/ 1327], train_loss/perplexity = 5.14036655/170.7783508 secs/batch = 0.3003s, grad.norm=12.08497620
  2282: 1 [  955/ 1327], train_loss/perplexity = 5.20695353/182.5371094 secs/batch = 0.2960s, grad.norm=11.05844402
  2287: 1 [  960/ 1327], train_loss/perplexity = 5.44412804/231.3954163 secs/batch = 0.2945s, grad.norm=10.93135738
  2292: 1 [  965/ 1327], train_loss/perplexity = 5.17589140/176.9542847 secs/batch = 0.2989s, grad.norm=11.30391884
  2297: 1 [  970/ 1327], train_loss/perplexity = 5.44098043/230.6682281 secs/batch = 0.2932s, grad.norm=11.05568027
  2302: 1 [  975/ 1327], train_loss/perplexity = 5.19321299/180.0461121 secs/batch = 0.2946s, grad.norm=12.31577778
  2307: 1 [  980/ 1327], train_loss/perplexity = 4.92732859/138.0103302 secs/batch = 0.2997s, grad.norm=11.23840046
  2312: 1 [  985/ 1327], train_loss/perplexity = 5.14878702/172.2224579 secs/batch = 0.2959s, grad.norm=12.50771523
  2317: 1 [  990/ 1327], train_loss/perplexity = 5.27903843/196.1811371 secs/batch = 0.2977s, grad.norm=11.39429665
  2322: 1 [  995/ 1327], train_loss/perplexity = 5.31960106/204.3023682 secs/batch = 0.2979s, grad.norm=11.10092449
  2327: 1 [ 1000/ 1327], train_loss/perplexity = 4.77779770/118.8423386 secs/batch = 0.2952s, grad.norm=11.19316483
  2332: 1 [ 1005/ 1327], train_loss/perplexity = 5.26837254/194.0998077 secs/batch = 0.2944s, grad.norm=11.81188488
  2337: 1 [ 1010/ 1327], train_loss/perplexity = 4.82450962/124.5253906 secs/batch = 0.2998s, grad.norm=11.04724884
  2342: 1 [ 1015/ 1327], train_loss/perplexity = 5.29977655/200.2920532 secs/batch = 0.2970s, grad.norm=10.83679390
  2347: 1 [ 1020/ 1327], train_loss/perplexity = 5.52023649/249.6940765 secs/batch = 0.2958s, grad.norm=10.95750237
  2352: 1 [ 1025/ 1327], train_loss/perplexity = 5.28218651/196.7997131 secs/batch = 0.2961s, grad.norm=11.39722824
  2357: 1 [ 1030/ 1327], train_loss/perplexity = 5.11804295/167.0082092 secs/batch = 0.2942s, grad.norm=11.36650467
  2362: 1 [ 1035/ 1327], train_loss/perplexity = 4.95514965/141.9038391 secs/batch = 0.2925s, grad.norm=10.71650791
  2367: 1 [ 1040/ 1327], train_loss/perplexity = 5.31102943/202.5586395 secs/batch = 0.3012s, grad.norm=11.08300781
  2372: 1 [ 1045/ 1327], train_loss/perplexity = 4.94051027/139.8415833 secs/batch = 0.2938s, grad.norm=11.49544048
  2377: 1 [ 1050/ 1327], train_loss/perplexity = 4.96006536/142.6031189 secs/batch = 0.3018s, grad.norm=11.94964886
  2382: 1 [ 1055/ 1327], train_loss/perplexity = 5.10528994/164.8918762 secs/batch = 0.2987s, grad.norm=11.90113640
  2387: 1 [ 1060/ 1327], train_loss/perplexity = 4.72457743/112.6828690 secs/batch = 0.2990s, grad.norm=12.56124783
  2392: 1 [ 1065/ 1327], train_loss/perplexity = 4.85114145/127.8862839 secs/batch = 0.2950s, grad.norm=11.52582169
  2397: 1 [ 1070/ 1327], train_loss/perplexity = 5.22246695/185.3909760 secs/batch = 0.2963s, grad.norm=11.60952759
  2402: 1 [ 1075/ 1327], train_loss/perplexity = 5.00855255/149.6879120 secs/batch = 0.2954s, grad.norm=12.13036537
  2407: 1 [ 1080/ 1327], train_loss/perplexity = 4.88572836/132.3868561 secs/batch = 0.2956s, grad.norm=11.53899002
  2412: 1 [ 1085/ 1327], train_loss/perplexity = 4.67969465/107.7371674 secs/batch = 0.2970s, grad.norm=11.76185608
  2417: 1 [ 1090/ 1327], train_loss/perplexity = 5.03014755/152.9555817 secs/batch = 0.2959s, grad.norm=12.46164036
  2422: 1 [ 1095/ 1327], train_loss/perplexity = 5.12145185/167.5784912 secs/batch = 0.2951s, grad.norm=12.30859184
  2427: 1 [ 1100/ 1327], train_loss/perplexity = 5.09004736/162.3975525 secs/batch = 0.2942s, grad.norm=13.07668591
  2432: 1 [ 1105/ 1327], train_loss/perplexity = 4.86135530/129.1991882 secs/batch = 0.2984s, grad.norm=12.07499695
  2437: 1 [ 1110/ 1327], train_loss/perplexity = 5.45243454/233.3255157 secs/batch = 0.2958s, grad.norm=11.42822456
  2442: 1 [ 1115/ 1327], train_loss/perplexity = 4.95951986/142.5253448 secs/batch = 0.2938s, grad.norm=13.95830059
  2447: 1 [ 1120/ 1327], train_loss/perplexity = 5.67194128/290.5981140 secs/batch = 0.2982s, grad.norm=26.02194595
  2452: 1 [ 1125/ 1327], train_loss/perplexity = 5.38422537/217.9412079 secs/batch = 0.2985s, grad.norm=12.46331310
  2457: 1 [ 1130/ 1327], train_loss/perplexity = 5.05104876/156.1861725 secs/batch = 0.2996s, grad.norm=12.41869545
  2462: 1 [ 1135/ 1327], train_loss/perplexity = 5.08570528/161.6939392 secs/batch = 0.2946s, grad.norm=11.77311325
  2467: 1 [ 1140/ 1327], train_loss/perplexity = 5.32729006/205.8793030 secs/batch = 0.2945s, grad.norm=12.35490799
  2472: 1 [ 1145/ 1327], train_loss/perplexity = 5.05889130/157.4158936 secs/batch = 0.2939s, grad.norm=12.21526241
  2477: 1 [ 1150/ 1327], train_loss/perplexity = 5.01337433/150.4114227 secs/batch = 0.2946s, grad.norm=12.12379360
  2482: 1 [ 1155/ 1327], train_loss/perplexity = 5.15500116/173.2960052 secs/batch = 0.2957s, grad.norm=11.90624142
  2487: 1 [ 1160/ 1327], train_loss/perplexity = 5.12317324/167.8672028 secs/batch = 0.2938s, grad.norm=11.87087822
  2492: 1 [ 1165/ 1327], train_loss/perplexity = 5.19651937/180.6423950 secs/batch = 0.2984s, grad.norm=11.53252316
  2497: 1 [ 1170/ 1327], train_loss/perplexity = 5.11999846/167.3351135 secs/batch = 0.2942s, grad.norm=12.35659790
  2502: 1 [ 1175/ 1327], train_loss/perplexity = 4.77453089/118.4547348 secs/batch = 0.3001s, grad.norm=12.87336159
  2507: 1 [ 1180/ 1327], train_loss/perplexity = 4.80993557/122.7237091 secs/batch = 0.3000s, grad.norm=12.69529533
  2512: 1 [ 1185/ 1327], train_loss/perplexity = 5.01123238/150.0895844 secs/batch = 0.2994s, grad.norm=11.49035454
  2517: 1 [ 1190/ 1327], train_loss/perplexity = 5.07940149/160.6778564 secs/batch = 0.2957s, grad.norm=12.13169956
  2522: 1 [ 1195/ 1327], train_loss/perplexity = 4.91243029/135.9694519 secs/batch = 0.2979s, grad.norm=11.55656338
  2527: 1 [ 1200/ 1327], train_loss/perplexity = 4.81066465/122.8132172 secs/batch = 0.3004s, grad.norm=11.33001614
  2532: 1 [ 1205/ 1327], train_loss/perplexity = 4.88132524/131.8052216 secs/batch = 0.2928s, grad.norm=11.93273640
  2537: 1 [ 1210/ 1327], train_loss/perplexity = 4.62273979/101.7724838 secs/batch = 0.2960s, grad.norm=12.41700268
  2542: 1 [ 1215/ 1327], train_loss/perplexity = 4.72190142/112.3817368 secs/batch = 0.2953s, grad.norm=11.80578518
  2547: 1 [ 1220/ 1327], train_loss/perplexity = 4.89796877/134.0172882 secs/batch = 0.2995s, grad.norm=12.36774349
  2552: 1 [ 1225/ 1327], train_loss/perplexity = 4.73166895/113.4848022 secs/batch = 0.2948s, grad.norm=13.36532974
  2557: 1 [ 1230/ 1327], train_loss/perplexity = 4.84573984/127.1973495 secs/batch = 0.3017s, grad.norm=12.42369080
  2562: 1 [ 1235/ 1327], train_loss/perplexity = 4.96694279/143.5872345 secs/batch = 0.2949s, grad.norm=12.33015823
  2567: 1 [ 1240/ 1327], train_loss/perplexity = 5.04830408/155.7580872 secs/batch = 0.3011s, grad.norm=11.84885406
  2572: 1 [ 1245/ 1327], train_loss/perplexity = 4.94635630/140.6614990 secs/batch = 0.2979s, grad.norm=11.79981518
  2577: 1 [ 1250/ 1327], train_loss/perplexity = 5.05909252/157.4475708 secs/batch = 0.2940s, grad.norm=11.41427994
  2582: 1 [ 1255/ 1327], train_loss/perplexity = 5.07110882/159.3509216 secs/batch = 0.2990s, grad.norm=11.31016064
  2587: 1 [ 1260/ 1327], train_loss/perplexity = 4.96726179/143.6330566 secs/batch = 0.2950s, grad.norm=12.51421738
  2592: 1 [ 1265/ 1327], train_loss/perplexity = 5.14501286/171.5736847 secs/batch = 0.2979s, grad.norm=12.41885471
  2597: 1 [ 1270/ 1327], train_loss/perplexity = 4.84334469/126.8930588 secs/batch = 0.2927s, grad.norm=11.67407131
  2602: 1 [ 1275/ 1327], train_loss/perplexity = 5.10967731/165.6168976 secs/batch = 0.3012s, grad.norm=13.02248764
  2607: 1 [ 1280/ 1327], train_loss/perplexity = 4.89922953/134.1863556 secs/batch = 0.2945s, grad.norm=11.79623699
  2612: 1 [ 1285/ 1327], train_loss/perplexity = 4.92883015/138.2177277 secs/batch = 0.2974s, grad.norm=11.69698238
  2617: 1 [ 1290/ 1327], train_loss/perplexity = 5.05186272/156.3133545 secs/batch = 0.2961s, grad.norm=11.95217991
  2622: 1 [ 1295/ 1327], train_loss/perplexity = 5.12812090/168.6998138 secs/batch = 0.2957s, grad.norm=12.65150356
  2627: 1 [ 1300/ 1327], train_loss/perplexity = 5.21635056/184.2605133 secs/batch = 0.2939s, grad.norm=11.64949608
  2632: 1 [ 1305/ 1327], train_loss/perplexity = 5.37990427/217.0014954 secs/batch = 0.3006s, grad.norm=11.84639835
  2637: 1 [ 1310/ 1327], train_loss/perplexity = 5.49690437/243.9356232 secs/batch = 0.2992s, grad.norm=11.90411568
  2642: 1 [ 1315/ 1327], train_loss/perplexity = 5.32159042/204.7091980 secs/batch = 0.2995s, grad.norm=11.57715893
  2647: 1 [ 1320/ 1327], train_loss/perplexity = 5.30709219/201.7626801 secs/batch = 0.2945s, grad.norm=12.06372929
  2652: 1 [ 1325/ 1327], train_loss/perplexity = 5.15895605/173.9827271 secs/batch = 0.3013s, grad.norm=11.54788303
Epoch training time: 393.47483825683594
	> validation loss = 5.27404976, perplexity = 195.20489502
	> validation loss = 5.13380003, perplexity = 169.66061401
	> validation loss = 5.02729464, perplexity = 152.51983643
	> validation loss = 5.12976933, perplexity = 168.97813416
	> validation loss = 5.37777519, perplexity = 216.53997803
	> validation loss = 5.13826942, perplexity = 170.42059326
	> validation loss = 5.09511757, perplexity = 163.22303772
	> validation loss = 5.02955103, perplexity = 152.86436462
	> validation loss = 5.18743229, perplexity = 179.00831604
	> validation loss = 5.01152658, perplexity = 150.13375854
	> validation loss = 5.09212875, perplexity = 162.73591614
	> validation loss = 5.15730000, perplexity = 173.69483948
	> validation loss = 5.01835251, perplexity = 151.16206360
	> validation loss = 4.98063755, perplexity = 145.56715393
	> validation loss = 4.76042318, perplexity = 116.79534149
	> validation loss = 4.80512810, perplexity = 122.13513184
	> validation loss = 5.18561935, perplexity = 178.68408203
	> validation loss = 4.81927252, perplexity = 123.87493896
	> validation loss = 5.17695522, perplexity = 177.14262390
	> validation loss = 5.08792067, perplexity = 162.05255127
	> validation loss = 4.94999599, perplexity = 141.17439270
at the end of epoch: 1
train loss = 5.17533812, perplexity = 176.85640253
validation loss = 5.05177012, perplexity = 156.29888817
Saved model cv/epoch001_5.0518.model
  2659: 2 [    5/ 1327], train_loss/perplexity = 5.37289238/215.4852295 secs/batch = 0.2951s, grad.norm=11.31337643
  2664: 2 [   10/ 1327], train_loss/perplexity = 4.95452404/141.8150940 secs/batch = 0.2933s, grad.norm=13.18737316
  2669: 2 [   15/ 1327], train_loss/perplexity = 5.08959627/162.3243103 secs/batch = 0.2923s, grad.norm=11.22681427
  2674: 2 [   20/ 1327], train_loss/perplexity = 5.30672741/201.6891022 secs/batch = 0.2947s, grad.norm=12.69203568
  2679: 2 [   25/ 1327], train_loss/perplexity = 5.23049879/186.8860016 secs/batch = 0.2944s, grad.norm=12.07417583
  2684: 2 [   30/ 1327], train_loss/perplexity = 5.14056396/170.8120728 secs/batch = 0.2950s, grad.norm=11.98613262
  2689: 2 [   35/ 1327], train_loss/perplexity = 4.95543289/141.9440460 secs/batch = 0.2995s, grad.norm=11.08538151
  2694: 2 [   40/ 1327], train_loss/perplexity = 5.07000732/159.1754913 secs/batch = 0.2938s, grad.norm=12.02390671
  2699: 2 [   45/ 1327], train_loss/perplexity = 4.78508377/119.7113876 secs/batch = 0.2956s, grad.norm=12.27765274
  2704: 2 [   50/ 1327], train_loss/perplexity = 5.02056599/151.4970245 secs/batch = 0.2942s, grad.norm=11.61235142
  2709: 2 [   55/ 1327], train_loss/perplexity = 5.01239729/150.2645264 secs/batch = 0.2943s, grad.norm=11.69920635
  2714: 2 [   60/ 1327], train_loss/perplexity = 5.30251551/200.8414001 secs/batch = 0.2944s, grad.norm=13.15967751
  2719: 2 [   65/ 1327], train_loss/perplexity = 4.79955387/121.4562225 secs/batch = 0.2998s, grad.norm=11.84196949
  2724: 2 [   70/ 1327], train_loss/perplexity = 4.63926744/103.4685211 secs/batch = 0.2945s, grad.norm=12.39379406
  2729: 2 [   75/ 1327], train_loss/perplexity = 4.65685987/105.3048935 secs/batch = 0.2988s, grad.norm=12.55006981
  2734: 2 [   80/ 1327], train_loss/perplexity = 5.05331230/156.5401154 secs/batch = 0.2952s, grad.norm=13.07316494
  2739: 2 [   85/ 1327], train_loss/perplexity = 5.03033400/152.9841003 secs/batch = 0.2961s, grad.norm=12.45158958
  2744: 2 [   90/ 1327], train_loss/perplexity = 4.99602079/147.8237610 secs/batch = 0.2966s, grad.norm=12.60767078
  2749: 2 [   95/ 1327], train_loss/perplexity = 4.83846140/126.2749176 secs/batch = 0.2960s, grad.norm=12.01921177
  2754: 2 [  100/ 1327], train_loss/perplexity = 5.15347481/173.0316925 secs/batch = 0.2931s, grad.norm=11.98400116
  2759: 2 [  105/ 1327], train_loss/perplexity = 5.14481306/171.5394135 secs/batch = 0.2946s, grad.norm=12.62238407
  2764: 2 [  110/ 1327], train_loss/perplexity = 4.96102667/142.7402649 secs/batch = 0.2953s, grad.norm=11.78545094
  2769: 2 [  115/ 1327], train_loss/perplexity = 4.87991142/131.6190033 secs/batch = 0.2984s, grad.norm=13.19327831
  2774: 2 [  120/ 1327], train_loss/perplexity = 5.01536942/150.7118073 secs/batch = 0.2934s, grad.norm=13.25323391
  2779: 2 [  125/ 1327], train_loss/perplexity = 5.04749346/155.6318817 secs/batch = 0.2951s, grad.norm=11.81050873
  2784: 2 [  130/ 1327], train_loss/perplexity = 4.93872595/139.5922852 secs/batch = 0.3014s, grad.norm=12.68418026
  2789: 2 [  135/ 1327], train_loss/perplexity = 4.93735552/139.4011230 secs/batch = 0.3009s, grad.norm=12.13787746
  2794: 2 [  140/ 1327], train_loss/perplexity = 5.32694244/205.8077393 secs/batch = 0.2954s, grad.norm=11.88664532
  2799: 2 [  145/ 1327], train_loss/perplexity = 5.19213200/179.8515930 secs/batch = 0.2967s, grad.norm=12.88876629
  2804: 2 [  150/ 1327], train_loss/perplexity = 5.09737253/163.5915070 secs/batch = 0.2956s, grad.norm=12.43258572
  2809: 2 [  155/ 1327], train_loss/perplexity = 5.40391493/222.2749023 secs/batch = 0.2958s, grad.norm=11.61411381
  2814: 2 [  160/ 1327], train_loss/perplexity = 5.09654284/163.4558411 secs/batch = 0.2939s, grad.norm=11.74892998
  2819: 2 [  165/ 1327], train_loss/perplexity = 5.22669506/186.1764832 secs/batch = 0.2964s, grad.norm=11.58927917
  2824: 2 [  170/ 1327], train_loss/perplexity = 5.04336739/154.9910583 secs/batch = 0.3005s, grad.norm=11.48870850
  2829: 2 [  175/ 1327], train_loss/perplexity = 5.26664352/193.7645111 secs/batch = 0.2961s, grad.norm=11.65872002
  2834: 2 [  180/ 1327], train_loss/perplexity = 5.10302258/164.5184326 secs/batch = 0.2992s, grad.norm=12.78662109
  2839: 2 [  185/ 1327], train_loss/perplexity = 5.40080976/221.5857697 secs/batch = 0.2996s, grad.norm=12.12510395
  2844: 2 [  190/ 1327], train_loss/perplexity = 4.83268499/125.5475998 secs/batch = 0.2945s, grad.norm=11.62639618
  2849: 2 [  195/ 1327], train_loss/perplexity = 5.17191887/176.2527161 secs/batch = 0.3014s, grad.norm=10.89498138
  2854: 2 [  200/ 1327], train_loss/perplexity = 5.09768820/163.6431580 secs/batch = 0.2953s, grad.norm=12.27217770
  2859: 2 [  205/ 1327], train_loss/perplexity = 5.13565350/169.9753571 secs/batch = 0.2941s, grad.norm=11.61762714
  2864: 2 [  210/ 1327], train_loss/perplexity = 5.09885406/163.8340607 secs/batch = 0.2920s, grad.norm=11.44223881
  2869: 2 [  215/ 1327], train_loss/perplexity = 5.22448063/185.7646637 secs/batch = 0.2960s, grad.norm=11.70705128
  2874: 2 [  220/ 1327], train_loss/perplexity = 5.21725082/184.4264679 secs/batch = 0.3019s, grad.norm=11.52903461
  2879: 2 [  225/ 1327], train_loss/perplexity = 5.40104675/221.6382904 secs/batch = 0.2959s, grad.norm=11.63141727
  2884: 2 [  230/ 1327], train_loss/perplexity = 5.19339895/180.0795898 secs/batch = 0.3025s, grad.norm=11.72952652
  2889: 2 [  235/ 1327], train_loss/perplexity = 5.03699112/154.0059357 secs/batch = 0.2949s, grad.norm=11.73527431
  2894: 2 [  240/ 1327], train_loss/perplexity = 4.88287067/132.0090790 secs/batch = 0.2931s, grad.norm=12.39475918
  2899: 2 [  245/ 1327], train_loss/perplexity = 5.17974091/177.6367798 secs/batch = 0.3018s, grad.norm=11.60617924
  2904: 2 [  250/ 1327], train_loss/perplexity = 4.89065933/133.0412598 secs/batch = 0.2948s, grad.norm=11.84193134
  2909: 2 [  255/ 1327], train_loss/perplexity = 4.96336651/143.0746460 secs/batch = 0.2945s, grad.norm=12.14560127
  2914: 2 [  260/ 1327], train_loss/perplexity = 5.32657337/205.7317963 secs/batch = 0.3006s, grad.norm=13.17297077
  2919: 2 [  265/ 1327], train_loss/perplexity = 5.31070614/202.4931641 secs/batch = 0.2942s, grad.norm=11.20940685
  2924: 2 [  270/ 1327], train_loss/perplexity = 5.35271931/211.1817932 secs/batch = 0.2970s, grad.norm=11.59678173
  2929: 2 [  275/ 1327], train_loss/perplexity = 5.46461821/236.1856689 secs/batch = 0.3005s, grad.norm=11.59322643
  2934: 2 [  280/ 1327], train_loss/perplexity = 5.14804792/172.0952148 secs/batch = 0.2988s, grad.norm=11.86427975
  2939: 2 [  285/ 1327], train_loss/perplexity = 5.39077139/219.3725433 secs/batch = 0.2945s, grad.norm=11.80723095
  2944: 2 [  290/ 1327], train_loss/perplexity = 5.22065735/185.0557861 secs/batch = 0.2944s, grad.norm=11.42056465
  2949: 2 [  295/ 1327], train_loss/perplexity = 4.98844671/146.7083588 secs/batch = 0.2940s, grad.norm=11.55933189
  2954: 2 [  300/ 1327], train_loss/perplexity = 4.57972097/97.4871902 secs/batch = 0.2941s, grad.norm=11.56534576
  2959: 2 [  305/ 1327], train_loss/perplexity = 5.03688955/153.9902954 secs/batch = 0.2937s, grad.norm=11.47564888
  2964: 2 [  310/ 1327], train_loss/perplexity = 5.00980616/149.8756866 secs/batch = 0.2932s, grad.norm=11.49294472
  2969: 2 [  315/ 1327], train_loss/perplexity = 4.66727734/106.4076385 secs/batch = 0.2946s, grad.norm=12.65028286
  2974: 2 [  320/ 1327], train_loss/perplexity = 4.81249619/123.0383606 secs/batch = 0.2947s, grad.norm=14.49242592
  2979: 2 [  325/ 1327], train_loss/perplexity = 4.62890959/102.4023438 secs/batch = 0.2938s, grad.norm=12.02617645
  2984: 2 [  330/ 1327], train_loss/perplexity = 5.05489588/156.7882080 secs/batch = 0.2953s, grad.norm=12.50639820
  2989: 2 [  335/ 1327], train_loss/perplexity = 4.40010977/81.4598083 secs/batch = 0.3008s, grad.norm=11.84464169
  2994: 2 [  340/ 1327], train_loss/perplexity = 5.26415920/193.2837219 secs/batch = 0.2948s, grad.norm=11.19594765
  2999: 2 [  345/ 1327], train_loss/perplexity = 5.03271675/153.3490601 secs/batch = 0.2946s, grad.norm=11.34577656
  3004: 2 [  350/ 1327], train_loss/perplexity = 5.13316059/169.5521545 secs/batch = 0.2993s, grad.norm=12.13500214
  3009: 2 [  355/ 1327], train_loss/perplexity = 5.22573328/185.9975128 secs/batch = 0.2963s, grad.norm=12.44843960
  3014: 2 [  360/ 1327], train_loss/perplexity = 5.33037376/206.5151520 secs/batch = 0.2999s, grad.norm=12.39689255
  3019: 2 [  365/ 1327], train_loss/perplexity = 5.21151161/183.3710327 secs/batch = 0.2984s, grad.norm=11.61359310
  3024: 2 [  370/ 1327], train_loss/perplexity = 5.20731974/182.6039734 secs/batch = 0.3012s, grad.norm=12.33237171
  3029: 2 [  375/ 1327], train_loss/perplexity = 4.59127998/98.6205826 secs/batch = 0.2999s, grad.norm=12.26861668
  3034: 2 [  380/ 1327], train_loss/perplexity = 4.79643822/121.0783920 secs/batch = 0.2965s, grad.norm=12.71742535
  3039: 2 [  385/ 1327], train_loss/perplexity = 4.98554325/146.2830200 secs/batch = 0.2950s, grad.norm=12.68913841
  3044: 2 [  390/ 1327], train_loss/perplexity = 5.02986956/152.9130707 secs/batch = 0.2949s, grad.norm=12.26730347
  3049: 2 [  395/ 1327], train_loss/perplexity = 5.26062870/192.6025391 secs/batch = 0.2948s, grad.norm=12.44161129
  3054: 2 [  400/ 1327], train_loss/perplexity = 5.03222752/153.2740479 secs/batch = 0.2990s, grad.norm=12.01689148
  3059: 2 [  405/ 1327], train_loss/perplexity = 5.34959507/210.5230408 secs/batch = 0.2980s, grad.norm=12.37386417
  3064: 2 [  410/ 1327], train_loss/perplexity = 5.05234241/156.3883667 secs/batch = 0.3013s, grad.norm=11.80407429
  3069: 2 [  415/ 1327], train_loss/perplexity = 4.88242865/131.9507294 secs/batch = 0.3004s, grad.norm=12.36084461
  3074: 2 [  420/ 1327], train_loss/perplexity = 4.69116020/108.9795456 secs/batch = 0.2945s, grad.norm=12.82792473
  3079: 2 [  425/ 1327], train_loss/perplexity = 4.89086103/133.0681000 secs/batch = 0.2937s, grad.norm=12.96569729
  3084: 2 [  430/ 1327], train_loss/perplexity = 5.16767740/175.5067291 secs/batch = 0.2939s, grad.norm=12.11635113
  3089: 2 [  435/ 1327], train_loss/perplexity = 5.16123581/174.3798218 secs/batch = 0.2944s, grad.norm=12.42419147
  3094: 2 [  440/ 1327], train_loss/perplexity = 4.90212822/134.5758820 secs/batch = 0.2956s, grad.norm=13.13595390
  3099: 2 [  445/ 1327], train_loss/perplexity = 5.05274534/156.4513855 secs/batch = 0.2998s, grad.norm=13.14945889
  3104: 2 [  450/ 1327], train_loss/perplexity = 4.95974636/142.5576324 secs/batch = 0.2941s, grad.norm=12.44837570
  3109: 2 [  455/ 1327], train_loss/perplexity = 4.78573847/119.7897949 secs/batch = 0.2937s, grad.norm=12.03926754
  3114: 2 [  460/ 1327], train_loss/perplexity = 4.98199129/145.7643585 secs/batch = 0.2979s, grad.norm=13.36086464
  3119: 2 [  465/ 1327], train_loss/perplexity = 4.82802773/124.9642563 secs/batch = 0.2989s, grad.norm=14.09654331
  3124: 2 [  470/ 1327], train_loss/perplexity = 5.32002068/204.3881073 secs/batch = 0.3006s, grad.norm=11.97099304
  3129: 2 [  475/ 1327], train_loss/perplexity = 4.86929226/130.2287140 secs/batch = 0.3019s, grad.norm=12.45915222
  3134: 2 [  480/ 1327], train_loss/perplexity = 5.06742048/158.7642670 secs/batch = 0.2944s, grad.norm=12.84059334
  3139: 2 [  485/ 1327], train_loss/perplexity = 4.92536116/137.7390747 secs/batch = 0.2989s, grad.norm=12.72080708
  3144: 2 [  490/ 1327], train_loss/perplexity = 4.84784985/127.4660263 secs/batch = 0.2926s, grad.norm=13.53929710
  3149: 2 [  495/ 1327], train_loss/perplexity = 4.81175709/122.9474564 secs/batch = 0.3003s, grad.norm=12.41492653
  3154: 2 [  500/ 1327], train_loss/perplexity = 5.19982672/181.2408295 secs/batch = 0.2931s, grad.norm=12.96378899
  3159: 2 [  505/ 1327], train_loss/perplexity = 5.13875198/170.5028381 secs/batch = 0.2958s, grad.norm=11.77796936
  3164: 2 [  510/ 1327], train_loss/perplexity = 5.44790030/232.2699585 secs/batch = 0.2928s, grad.norm=10.88020420
  3169: 2 [  515/ 1327], train_loss/perplexity = 5.11507988/166.5140839 secs/batch = 0.2942s, grad.norm=11.62739086
  3174: 2 [  520/ 1327], train_loss/perplexity = 5.30774212/201.8938599 secs/batch = 0.2943s, grad.norm=11.91871643
  3179: 2 [  525/ 1327], train_loss/perplexity = 4.85350323/128.1886749 secs/batch = 0.2922s, grad.norm=12.52909470
  3184: 2 [  530/ 1327], train_loss/perplexity = 4.88417292/132.1810913 secs/batch = 0.2987s, grad.norm=12.55889893
  3189: 2 [  535/ 1327], train_loss/perplexity = 5.08674431/161.8620300 secs/batch = 0.2983s, grad.norm=15.12860775
  3194: 2 [  540/ 1327], train_loss/perplexity = 5.10534906/164.9016266 secs/batch = 0.2941s, grad.norm=12.21312141
  3199: 2 [  545/ 1327], train_loss/perplexity = 5.17431974/176.6763916 secs/batch = 0.3003s, grad.norm=12.12956715
  3204: 2 [  550/ 1327], train_loss/perplexity = 5.13610458/170.0520477 secs/batch = 0.2939s, grad.norm=12.33685017
  3209: 2 [  555/ 1327], train_loss/perplexity = 4.92577887/137.7966309 secs/batch = 0.2948s, grad.norm=12.56965446
  3214: 2 [  560/ 1327], train_loss/perplexity = 5.04967499/155.9717560 secs/batch = 0.3001s, grad.norm=13.37767315
  3219: 2 [  565/ 1327], train_loss/perplexity = 4.99881744/148.2377625 secs/batch = 0.2939s, grad.norm=13.45796967
  3224: 2 [  570/ 1327], train_loss/perplexity = 4.87532520/131.0167542 secs/batch = 0.2957s, grad.norm=12.74004173
  3229: 2 [  575/ 1327], train_loss/perplexity = 4.77438641/118.4376221 secs/batch = 0.2945s, grad.norm=12.78990364
  3234: 2 [  580/ 1327], train_loss/perplexity = 5.09043169/162.4599762 secs/batch = 0.2958s, grad.norm=12.35537338
  3239: 2 [  585/ 1327], train_loss/perplexity = 4.65730333/105.3516006 secs/batch = 0.2935s, grad.norm=13.08889484
  3244: 2 [  590/ 1327], train_loss/perplexity = 4.98406029/146.0662537 secs/batch = 0.2936s, grad.norm=12.03421402
  3249: 2 [  595/ 1327], train_loss/perplexity = 4.94968033/141.1298370 secs/batch = 0.2982s, grad.norm=12.63198090
  3254: 2 [  600/ 1327], train_loss/perplexity = 5.25732994/191.9682465 secs/batch = 0.2991s, grad.norm=11.68376827
  3259: 2 [  605/ 1327], train_loss/perplexity = 5.15897560/173.9861298 secs/batch = 0.2948s, grad.norm=12.39853287
  3264: 2 [  610/ 1327], train_loss/perplexity = 5.28257275/196.8757324 secs/batch = 0.2989s, grad.norm=12.81882572
  3269: 2 [  615/ 1327], train_loss/perplexity = 4.80058861/121.5819626 secs/batch = 0.2946s, grad.norm=12.52361584
  3274: 2 [  620/ 1327], train_loss/perplexity = 5.13056517/169.1126709 secs/batch = 0.3001s, grad.norm=12.18604279
  3279: 2 [  625/ 1327], train_loss/perplexity = 5.19862986/181.0240479 secs/batch = 0.2947s, grad.norm=11.89860249
  3284: 2 [  630/ 1327], train_loss/perplexity = 5.23426580/187.5913239 secs/batch = 0.2933s, grad.norm=11.92498493
  3289: 2 [  635/ 1327], train_loss/perplexity = 4.98999500/146.9356842 secs/batch = 0.2960s, grad.norm=12.84257126
  3294: 2 [  640/ 1327], train_loss/perplexity = 5.01657963/150.8943024 secs/batch = 0.2967s, grad.norm=12.42632008
  3299: 2 [  645/ 1327], train_loss/perplexity = 5.25086164/190.7305450 secs/batch = 0.2954s, grad.norm=12.70212460
  3304: 2 [  650/ 1327], train_loss/perplexity = 4.83035231/125.2550812 secs/batch = 0.2953s, grad.norm=12.32922745
  3309: 2 [  655/ 1327], train_loss/perplexity = 4.97632885/144.9412994 secs/batch = 0.2942s, grad.norm=12.03968430
  3314: 2 [  660/ 1327], train_loss/perplexity = 4.77942371/119.0357285 secs/batch = 0.2993s, grad.norm=12.26410484
  3319: 2 [  665/ 1327], train_loss/perplexity = 5.08377600/161.3822784 secs/batch = 0.2943s, grad.norm=12.02335835
  3324: 2 [  670/ 1327], train_loss/perplexity = 4.93654251/139.2878265 secs/batch = 0.3009s, grad.norm=12.73542881
  3329: 2 [  675/ 1327], train_loss/perplexity = 4.74303484/114.7820206 secs/batch = 0.2945s, grad.norm=13.17782974
  3334: 2 [  680/ 1327], train_loss/perplexity = 5.00832844/149.6543732 secs/batch = 0.2933s, grad.norm=12.62560749
  3339: 2 [  685/ 1327], train_loss/perplexity = 4.90741014/135.2885895 secs/batch = 0.2954s, grad.norm=12.66062737
  3344: 2 [  690/ 1327], train_loss/perplexity = 5.22234344/185.3680725 secs/batch = 0.2984s, grad.norm=12.01956272
  3349: 2 [  695/ 1327], train_loss/perplexity = 4.96850586/143.8118591 secs/batch = 0.2942s, grad.norm=13.07854652
  3354: 2 [  700/ 1327], train_loss/perplexity = 5.20352650/181.9126282 secs/batch = 0.2950s, grad.norm=12.15995693
  3359: 2 [  705/ 1327], train_loss/perplexity = 4.93247175/138.7219696 secs/batch = 0.2957s, grad.norm=11.96989441
  3364: 2 [  710/ 1327], train_loss/perplexity = 4.94169283/140.0070496 secs/batch = 0.2950s, grad.norm=13.06910610
  3369: 2 [  715/ 1327], train_loss/perplexity = 4.85824013/128.7973328 secs/batch = 0.2952s, grad.norm=12.16369343
  3374: 2 [  720/ 1327], train_loss/perplexity = 4.96850920/143.8123322 secs/batch = 0.2992s, grad.norm=12.74133873
  3379: 2 [  725/ 1327], train_loss/perplexity = 4.80457306/122.0673676 secs/batch = 0.2917s, grad.norm=13.10875511
  3384: 2 [  730/ 1327], train_loss/perplexity = 5.02074862/151.5246887 secs/batch = 0.2954s, grad.norm=12.11808586
  3389: 2 [  735/ 1327], train_loss/perplexity = 5.07668686/160.2422638 secs/batch = 0.2985s, grad.norm=12.58038139
  3394: 2 [  740/ 1327], train_loss/perplexity = 4.43378353/84.2495728 secs/batch = 0.2968s, grad.norm=12.52330971
  3399: 2 [  745/ 1327], train_loss/perplexity = 5.02095461/151.5559082 secs/batch = 0.2987s, grad.norm=12.23973846
  3404: 2 [  750/ 1327], train_loss/perplexity = 4.79831886/121.3063126 secs/batch = 0.2952s, grad.norm=12.65737915
  3409: 2 [  755/ 1327], train_loss/perplexity = 4.79292774/120.6540985 secs/batch = 0.2961s, grad.norm=12.57800102
  3414: 2 [  760/ 1327], train_loss/perplexity = 4.62906551/102.4183121 secs/batch = 0.2990s, grad.norm=12.49274731
  3419: 2 [  765/ 1327], train_loss/perplexity = 4.80914307/122.6264877 secs/batch = 0.3011s, grad.norm=14.14346886
  3424: 2 [  770/ 1327], train_loss/perplexity = 4.65981579/105.6166229 secs/batch = 0.2992s, grad.norm=12.74536991
  3429: 2 [  775/ 1327], train_loss/perplexity = 4.84414673/126.9948730 secs/batch = 0.2966s, grad.norm=12.63085651
  3434: 2 [  780/ 1327], train_loss/perplexity = 5.15660810/173.5747070 secs/batch = 0.2957s, grad.norm=12.13963223
  3439: 2 [  785/ 1327], train_loss/perplexity = 4.94966316/141.1274261 secs/batch = 0.2929s, grad.norm=13.36830139
  3444: 2 [  790/ 1327], train_loss/perplexity = 4.78059196/119.1748734 secs/batch = 0.2938s, grad.norm=12.93128490
  3449: 2 [  795/ 1327], train_loss/perplexity = 5.15719223/173.6761322 secs/batch = 0.2943s, grad.norm=13.18267632
  3454: 2 [  800/ 1327], train_loss/perplexity = 5.00716877/149.4809265 secs/batch = 0.2943s, grad.norm=13.02680111
  3459: 2 [  805/ 1327], train_loss/perplexity = 5.35761166/212.2174988 secs/batch = 0.2985s, grad.norm=12.76138020
  3464: 2 [  810/ 1327], train_loss/perplexity = 4.95856333/142.3890839 secs/batch = 0.2952s, grad.norm=12.34078312
  3469: 2 [  815/ 1327], train_loss/perplexity = 4.90704632/135.2393646 secs/batch = 0.2952s, grad.norm=12.26100349
  3474: 2 [  820/ 1327], train_loss/perplexity = 4.56197357/95.7723083 secs/batch = 0.2920s, grad.norm=12.24827003
  3479: 2 [  825/ 1327], train_loss/perplexity = 4.83349133/125.6488800 secs/batch = 0.2950s, grad.norm=12.34435654
  3484: 2 [  830/ 1327], train_loss/perplexity = 4.63444281/102.9705276 secs/batch = 0.2947s, grad.norm=12.73261929
  3489: 2 [  835/ 1327], train_loss/perplexity = 4.90254354/134.6317902 secs/batch = 0.2937s, grad.norm=12.80342865
  3494: 2 [  840/ 1327], train_loss/perplexity = 5.05149841/156.2564240 secs/batch = 0.2987s, grad.norm=12.80137730
  3499: 2 [  845/ 1327], train_loss/perplexity = 4.83319759/125.6119766 secs/batch = 0.2949s, grad.norm=12.50732803
  3504: 2 [  850/ 1327], train_loss/perplexity = 4.91904640/136.8720245 secs/batch = 0.2952s, grad.norm=12.26908398
  3509: 2 [  855/ 1327], train_loss/perplexity = 4.89387321/133.4695282 secs/batch = 0.2937s, grad.norm=13.08353901
  3514: 2 [  860/ 1327], train_loss/perplexity = 4.61597109/101.0859451 secs/batch = 0.2931s, grad.norm=12.37424564
  3519: 2 [  865/ 1327], train_loss/perplexity = 5.09679461/163.4969940 secs/batch = 0.2923s, grad.norm=12.50043011
  3524: 2 [  870/ 1327], train_loss/perplexity = 5.07929420/160.6606293 secs/batch = 0.2953s, grad.norm=12.88586044
  3529: 2 [  875/ 1327], train_loss/perplexity = 4.54121494/93.8046951 secs/batch = 0.3004s, grad.norm=12.41638470
  3534: 2 [  880/ 1327], train_loss/perplexity = 4.82282352/124.3156052 secs/batch = 0.2944s, grad.norm=11.67489815
  3539: 2 [  885/ 1327], train_loss/perplexity = 4.92694569/137.9575043 secs/batch = 0.2943s, grad.norm=11.83335972
  3544: 2 [  890/ 1327], train_loss/perplexity = 5.07863712/160.5550842 secs/batch = 0.2986s, grad.norm=12.16321468
  3549: 2 [  895/ 1327], train_loss/perplexity = 5.17212200/176.2885284 secs/batch = 0.2939s, grad.norm=11.74523544
  3554: 2 [  900/ 1327], train_loss/perplexity = 4.94240141/140.1062927 secs/batch = 0.2928s, grad.norm=12.67741013
  3559: 2 [  905/ 1327], train_loss/perplexity = 4.79802132/121.2702255 secs/batch = 0.2948s, grad.norm=13.66628838
  3564: 2 [  910/ 1327], train_loss/perplexity = 4.82906103/125.0934448 secs/batch = 0.2936s, grad.norm=12.78979588
  3569: 2 [  915/ 1327], train_loss/perplexity = 5.11609983/166.6840057 secs/batch = 0.2948s, grad.norm=14.53870392
  3574: 2 [  920/ 1327], train_loss/perplexity = 5.24693060/189.9822388 secs/batch = 0.2932s, grad.norm=12.11413002
  3579: 2 [  925/ 1327], train_loss/perplexity = 4.99931049/148.3108673 secs/batch = 0.2993s, grad.norm=11.77879429
  3584: 2 [  930/ 1327], train_loss/perplexity = 4.98516941/146.2283478 secs/batch = 0.2947s, grad.norm=12.74974918
  3589: 2 [  935/ 1327], train_loss/perplexity = 5.02267647/151.8170929 secs/batch = 0.2937s, grad.norm=11.95161438
  3594: 2 [  940/ 1327], train_loss/perplexity = 5.00389290/148.9920349 secs/batch = 0.2994s, grad.norm=12.42097092
  3599: 2 [  945/ 1327], train_loss/perplexity = 5.23143148/187.0603790 secs/batch = 0.2946s, grad.norm=13.05471706
  3604: 2 [  950/ 1327], train_loss/perplexity = 4.98258448/145.8508453 secs/batch = 0.2999s, grad.norm=12.69435787
  3609: 2 [  955/ 1327], train_loss/perplexity = 5.03719378/154.0371399 secs/batch = 0.2928s, grad.norm=12.01678467
  3614: 2 [  960/ 1327], train_loss/perplexity = 5.22311211/185.5106201 secs/batch = 0.2952s, grad.norm=12.12147617
  3619: 2 [  965/ 1327], train_loss/perplexity = 5.04207373/154.7906799 secs/batch = 0.3006s, grad.norm=12.90421486
  3624: 2 [  970/ 1327], train_loss/perplexity = 5.23615885/187.9467773 secs/batch = 0.2926s, grad.norm=12.18151855
  3629: 2 [  975/ 1327], train_loss/perplexity = 4.96013451/142.6129761 secs/batch = 0.2942s, grad.norm=12.67190552
  3634: 2 [  980/ 1327], train_loss/perplexity = 4.71367550/111.4610825 secs/batch = 0.2947s, grad.norm=11.81286907
  3639: 2 [  985/ 1327], train_loss/perplexity = 4.94031858/139.8147888 secs/batch = 0.2953s, grad.norm=12.98438549
  3644: 2 [  990/ 1327], train_loss/perplexity = 5.14759350/172.0170288 secs/batch = 0.2984s, grad.norm=12.01947117
  3649: 2 [  995/ 1327], train_loss/perplexity = 5.07434416/159.8673096 secs/batch = 0.2947s, grad.norm=11.67683315
  3654: 2 [ 1000/ 1327], train_loss/perplexity = 4.55754614/95.3492203 secs/batch = 0.2936s, grad.norm=11.51523495
  3659: 2 [ 1005/ 1327], train_loss/perplexity = 5.08048916/160.8527222 secs/batch = 0.2989s, grad.norm=12.50541401
  3664: 2 [ 1010/ 1327], train_loss/perplexity = 4.62354136/101.8540955 secs/batch = 0.2918s, grad.norm=12.00225258
  3669: 2 [ 1015/ 1327], train_loss/perplexity = 5.10134268/164.2422791 secs/batch = 0.2948s, grad.norm=11.73684883
  3674: 2 [ 1020/ 1327], train_loss/perplexity = 5.26947403/194.3137360 secs/batch = 0.2983s, grad.norm=11.84232521
  3679: 2 [ 1025/ 1327], train_loss/perplexity = 5.12020206/167.3691864 secs/batch = 0.2917s, grad.norm=12.07961845
  3684: 2 [ 1030/ 1327], train_loss/perplexity = 4.92775488/138.0691833 secs/batch = 0.2950s, grad.norm=11.94775486
  3689: 2 [ 1035/ 1327], train_loss/perplexity = 4.82530022/124.6238785 secs/batch = 0.2944s, grad.norm=12.13174629
  3694: 2 [ 1040/ 1327], train_loss/perplexity = 5.15063667/172.5413055 secs/batch = 0.2942s, grad.norm=12.38663101
  3699: 2 [ 1045/ 1327], train_loss/perplexity = 4.68913174/108.7587051 secs/batch = 0.2931s, grad.norm=12.68303871
  3704: 2 [ 1050/ 1327], train_loss/perplexity = 4.71673918/111.8030853 secs/batch = 0.2927s, grad.norm=13.84927177
  3709: 2 [ 1055/ 1327], train_loss/perplexity = 4.93974686/139.7348785 secs/batch = 0.2949s, grad.norm=13.63730049
  3714: 2 [ 1060/ 1327], train_loss/perplexity = 4.50993156/90.9155960 secs/batch = 0.2945s, grad.norm=13.31853294
  3719: 2 [ 1065/ 1327], train_loss/perplexity = 4.63342619/102.8658981 secs/batch = 0.2984s, grad.norm=12.63759232
  3724: 2 [ 1070/ 1327], train_loss/perplexity = 5.03781223/154.1324463 secs/batch = 0.2966s, grad.norm=12.52804279
  3729: 2 [ 1075/ 1327], train_loss/perplexity = 4.74637413/115.1659470 secs/batch = 0.2973s, grad.norm=12.55831528
  3734: 2 [ 1080/ 1327], train_loss/perplexity = 4.70195436/110.1622620 secs/batch = 0.2930s, grad.norm=12.60962391
  3739: 2 [ 1085/ 1327], train_loss/perplexity = 4.63748646/103.2844086 secs/batch = 0.2955s, grad.norm=13.35197163
  3744: 2 [ 1090/ 1327], train_loss/perplexity = 4.73800182/114.2057724 secs/batch = 0.2992s, grad.norm=13.09776592
  3749: 2 [ 1095/ 1327], train_loss/perplexity = 4.90048170/134.3544769 secs/batch = 0.2992s, grad.norm=13.88578796
  3754: 2 [ 1100/ 1327], train_loss/perplexity = 4.77299929/118.2734451 secs/batch = 0.2916s, grad.norm=13.85091496
  3759: 2 [ 1105/ 1327], train_loss/perplexity = 4.65349770/104.9514313 secs/batch = 0.2941s, grad.norm=12.63777828
  3764: 2 [ 1110/ 1327], train_loss/perplexity = 5.18365955/178.3342438 secs/batch = 0.2985s, grad.norm=12.96375561
  3769: 2 [ 1115/ 1327], train_loss/perplexity = 4.65276051/104.8740921 secs/batch = 0.2923s, grad.norm=12.56950665
  3774: 2 [ 1120/ 1327], train_loss/perplexity = 4.98347855/145.9813080 secs/batch = 0.2988s, grad.norm=11.65913677
  3779: 2 [ 1125/ 1327], train_loss/perplexity = 5.13552761/169.9539642 secs/batch = 0.2942s, grad.norm=12.87991238
  3784: 2 [ 1130/ 1327], train_loss/perplexity = 4.80412102/122.0121994 secs/batch = 0.2940s, grad.norm=12.93328381
  3789: 2 [ 1135/ 1327], train_loss/perplexity = 4.84614754/127.2492218 secs/batch = 0.2938s, grad.norm=12.68248463
  3794: 2 [ 1140/ 1327], train_loss/perplexity = 5.12701273/168.5129700 secs/batch = 0.2929s, grad.norm=12.98466206
  3799: 2 [ 1145/ 1327], train_loss/perplexity = 4.86474228/129.6375275 secs/batch = 0.2939s, grad.norm=12.49062729
  3804: 2 [ 1150/ 1327], train_loss/perplexity = 4.87339449/130.7640381 secs/batch = 0.2916s, grad.norm=13.35193825
  3809: 2 [ 1155/ 1327], train_loss/perplexity = 4.99093533/147.0739136 secs/batch = 0.2984s, grad.norm=12.63069630
  3814: 2 [ 1160/ 1327], train_loss/perplexity = 4.91185760/135.8916168 secs/batch = 0.2942s, grad.norm=12.19849491
  3819: 2 [ 1165/ 1327], train_loss/perplexity = 4.95326710/141.6369476 secs/batch = 0.2921s, grad.norm=12.41791058
  3824: 2 [ 1170/ 1327], train_loss/perplexity = 4.88199282/131.8932343 secs/batch = 0.2986s, grad.norm=13.13618755
  3829: 2 [ 1175/ 1327], train_loss/perplexity = 4.58719587/98.2186279 secs/batch = 0.3003s, grad.norm=13.51940441
  3834: 2 [ 1180/ 1327], train_loss/perplexity = 4.60479069/99.9620590 secs/batch = 0.2940s, grad.norm=12.79044819
  3839: 2 [ 1185/ 1327], train_loss/perplexity = 4.75484133/116.1452255 secs/batch = 0.2996s, grad.norm=12.02760601
  3844: 2 [ 1190/ 1327], train_loss/perplexity = 4.85568094/128.4681396 secs/batch = 0.2928s, grad.norm=12.55031300
  3849: 2 [ 1195/ 1327], train_loss/perplexity = 4.71404696/111.5024948 secs/batch = 0.2960s, grad.norm=12.17260933
  3854: 2 [ 1200/ 1327], train_loss/perplexity = 4.62306261/101.8053436 secs/batch = 0.2938s, grad.norm=12.71975803
  3859: 2 [ 1205/ 1327], train_loss/perplexity = 4.68618727/108.4389420 secs/batch = 0.2922s, grad.norm=12.68558979
  3864: 2 [ 1210/ 1327], train_loss/perplexity = 4.41349411/82.5574265 secs/batch = 0.2944s, grad.norm=12.51900005
  3869: 2 [ 1215/ 1327], train_loss/perplexity = 4.60303402/99.7866135 secs/batch = 0.2944s, grad.norm=12.90092945
  3874: 2 [ 1220/ 1327], train_loss/perplexity = 4.70067120/110.0209961 secs/batch = 0.2942s, grad.norm=13.58794975
  3879: 2 [ 1225/ 1327], train_loss/perplexity = 4.54486990/94.1481781 secs/batch = 0.2929s, grad.norm=13.88507748
  3884: 2 [ 1230/ 1327], train_loss/perplexity = 4.69222927/109.0961151 secs/batch = 0.2986s, grad.norm=13.29402637
  3889: 2 [ 1235/ 1327], train_loss/perplexity = 4.76051092/116.8055878 secs/batch = 0.2944s, grad.norm=12.77627754
  3894: 2 [ 1240/ 1327], train_loss/perplexity = 4.78747416/119.9978867 secs/batch = 0.2956s, grad.norm=12.58619213
  3899: 2 [ 1245/ 1327], train_loss/perplexity = 4.81275845/123.0706329 secs/batch = 0.2946s, grad.norm=12.43013859
  3904: 2 [ 1250/ 1327], train_loss/perplexity = 4.93939924/139.6863098 secs/batch = 0.2916s, grad.norm=12.53858471
  3909: 2 [ 1255/ 1327], train_loss/perplexity = 4.91104841/135.7816925 secs/batch = 0.2982s, grad.norm=12.56403065
  3914: 2 [ 1260/ 1327], train_loss/perplexity = 4.80223656/121.7824860 secs/batch = 0.2952s, grad.norm=13.20734692
  3919: 2 [ 1265/ 1327], train_loss/perplexity = 4.97457695/144.6876068 secs/batch = 0.2986s, grad.norm=12.81698799
  3924: 2 [ 1270/ 1327], train_loss/perplexity = 4.69075918/108.9358521 secs/batch = 0.2925s, grad.norm=13.27793598
  3929: 2 [ 1275/ 1327], train_loss/perplexity = 4.97737885/145.0935669 secs/batch = 0.2932s, grad.norm=13.27170372
  3934: 2 [ 1280/ 1327], train_loss/perplexity = 4.72205257/112.3987198 secs/batch = 0.2974s, grad.norm=13.44283867
  3939: 2 [ 1285/ 1327], train_loss/perplexity = 4.69345522/109.2299423 secs/batch = 0.2955s, grad.norm=13.14932442
  3944: 2 [ 1290/ 1327], train_loss/perplexity = 4.86682606/129.9079437 secs/batch = 0.2921s, grad.norm=12.74174118
  3949: 2 [ 1295/ 1327], train_loss/perplexity = 4.88051653/131.6986694 secs/batch = 0.2963s, grad.norm=13.21951771
  3954: 2 [ 1300/ 1327], train_loss/perplexity = 5.03642941/153.9194489 secs/batch = 0.2943s, grad.norm=12.50617504
  3959: 2 [ 1305/ 1327], train_loss/perplexity = 5.15637684/173.5345764 secs/batch = 0.2941s, grad.norm=12.80764580
  3964: 2 [ 1310/ 1327], train_loss/perplexity = 5.34414530/209.3788452 secs/batch = 0.2926s, grad.norm=12.20762920
  3969: 2 [ 1315/ 1327], train_loss/perplexity = 5.17302942/176.4485626 secs/batch = 0.2965s, grad.norm=12.09621811
  3974: 2 [ 1320/ 1327], train_loss/perplexity = 5.16072226/174.2902985 secs/batch = 0.2923s, grad.norm=12.73844051
  3979: 2 [ 1325/ 1327], train_loss/perplexity = 5.07193756/159.4830322 secs/batch = 0.2978s, grad.norm=12.39370060
Epoch training time: 392.7401957511902
	> validation loss = 5.11174631, perplexity = 165.95991516
	> validation loss = 4.96726656, perplexity = 143.63374329
	> validation loss = 4.91661263, perplexity = 136.53932190
	> validation loss = 4.93397093, perplexity = 138.93009949
	> validation loss = 5.16471720, perplexity = 174.98796082
	> validation loss = 4.99464083, perplexity = 147.61991882
	> validation loss = 4.95918083, perplexity = 142.47703552
	> validation loss = 4.81730127, perplexity = 123.63099670
	> validation loss = 4.86285400, perplexity = 129.39295959
	> validation loss = 4.76998949, perplexity = 117.91799927
	> validation loss = 4.84764147, perplexity = 127.43946838
	> validation loss = 4.94057274, perplexity = 139.85032654
	> validation loss = 4.83862591, perplexity = 126.29569244
	> validation loss = 4.76034737, perplexity = 116.78648376
	> validation loss = 4.59375191, perplexity = 98.86466980
	> validation loss = 4.58621025, perplexity = 98.12186432
	> validation loss = 5.02638054, perplexity = 152.38047791
	> validation loss = 4.64911938, perplexity = 104.49292755
	> validation loss = 5.01332808, perplexity = 150.40446472
	> validation loss = 4.89938545, perplexity = 134.20727539
	> validation loss = 4.73332739, perplexity = 113.67317200
at the end of epoch: 2
train loss = 4.99188308, perplexity = 147.21337740
validation loss = 4.87461041, perplexity = 130.92313639
Saved model cv/epoch002_4.8746.model
  3986: 3 [    5/ 1327], train_loss/perplexity = 5.17602491/176.9779053 secs/batch = 0.2976s, grad.norm=12.40822792
  3991: 3 [   10/ 1327], train_loss/perplexity = 4.69693470/109.6106644 secs/batch = 0.2918s, grad.norm=14.11813927
  3996: 3 [   15/ 1327], train_loss/perplexity = 4.90328884/134.7321625 secs/batch = 0.2916s, grad.norm=12.06899166
  4001: 3 [   20/ 1327], train_loss/perplexity = 5.08013916/160.7964325 secs/batch = 0.2927s, grad.norm=12.80364037
  4006: 3 [   25/ 1327], train_loss/perplexity = 5.04349041/155.0101166 secs/batch = 0.2982s, grad.norm=12.54591179
  4011: 3 [   30/ 1327], train_loss/perplexity = 4.96208382/142.8912506 secs/batch = 0.2983s, grad.norm=12.45588207
  4016: 3 [   35/ 1327], train_loss/perplexity = 4.85722399/128.6665192 secs/batch = 0.2924s, grad.norm=12.08873844
  4021: 3 [   40/ 1327], train_loss/perplexity = 4.87816095/131.3888092 secs/batch = 0.2930s, grad.norm=12.39412880
  4026: 3 [   45/ 1327], train_loss/perplexity = 4.63711548/103.2461014 secs/batch = 0.2920s, grad.norm=11.94666672
  4031: 3 [   50/ 1327], train_loss/perplexity = 4.84518003/127.1261673 secs/batch = 0.2931s, grad.norm=11.90267372
  4036: 3 [   55/ 1327], train_loss/perplexity = 4.86518431/129.6948395 secs/batch = 0.2934s, grad.norm=12.82693577
  4041: 3 [   60/ 1327], train_loss/perplexity = 5.12449837/168.0897980 secs/batch = 0.2981s, grad.norm=12.96990681
  4046: 3 [   65/ 1327], train_loss/perplexity = 4.69815397/109.7443924 secs/batch = 0.2938s, grad.norm=12.29213619
  4051: 3 [   70/ 1327], train_loss/perplexity = 4.54464293/94.1268082 secs/batch = 0.2946s, grad.norm=13.46711063
  4056: 3 [   75/ 1327], train_loss/perplexity = 4.39732981/81.2336655 secs/batch = 0.2939s, grad.norm=13.31752682
  4061: 3 [   80/ 1327], train_loss/perplexity = 4.81332254/123.1400757 secs/batch = 0.2918s, grad.norm=13.06179810
  4066: 3 [   85/ 1327], train_loss/perplexity = 4.83673239/126.0567703 secs/batch = 0.2916s, grad.norm=12.67033482
  4071: 3 [   90/ 1327], train_loss/perplexity = 4.86587858/129.7849121 secs/batch = 0.2983s, grad.norm=13.00772953
  4076: 3 [   95/ 1327], train_loss/perplexity = 4.71717978/111.8523560 secs/batch = 0.2990s, grad.norm=13.08843994
  4081: 3 [  100/ 1327], train_loss/perplexity = 4.98163128/145.7118835 secs/batch = 0.3007s, grad.norm=12.55591393
  4086: 3 [  105/ 1327], train_loss/perplexity = 4.89883471/134.1333771 secs/batch = 0.2964s, grad.norm=13.89254570
  4091: 3 [  110/ 1327], train_loss/perplexity = 4.75890589/116.6182632 secs/batch = 0.2983s, grad.norm=12.49288845
  4096: 3 [  115/ 1327], train_loss/perplexity = 4.64117861/103.6664581 secs/batch = 0.2936s, grad.norm=13.79879093
  4101: 3 [  120/ 1327], train_loss/perplexity = 4.76798248/117.6815796 secs/batch = 0.2986s, grad.norm=13.31018257
  4106: 3 [  125/ 1327], train_loss/perplexity = 4.88785028/132.6680756 secs/batch = 0.2960s, grad.norm=12.79477501
  4111: 3 [  130/ 1327], train_loss/perplexity = 4.74117088/114.5682678 secs/batch = 0.2951s, grad.norm=13.82085037
  4116: 3 [  135/ 1327], train_loss/perplexity = 4.80598164/122.2394257 secs/batch = 0.2938s, grad.norm=12.80954647
  4121: 3 [  140/ 1327], train_loss/perplexity = 5.03401709/153.5485992 secs/batch = 0.2973s, grad.norm=12.85143375
  4126: 3 [  145/ 1327], train_loss/perplexity = 5.09213543/162.7369995 secs/batch = 0.2935s, grad.norm=13.97697544
  4131: 3 [  150/ 1327], train_loss/perplexity = 4.96381187/143.1383820 secs/batch = 0.2937s, grad.norm=13.29893112
  4136: 3 [  155/ 1327], train_loss/perplexity = 5.30375195/201.0898743 secs/batch = 0.2938s, grad.norm=12.65768719
  4141: 3 [  160/ 1327], train_loss/perplexity = 4.85699272/128.6367798 secs/batch = 0.2978s, grad.norm=12.19056797
  4146: 3 [  165/ 1327], train_loss/perplexity = 5.03797150/154.1569824 secs/batch = 0.2950s, grad.norm=11.79275227
  4151: 3 [  170/ 1327], train_loss/perplexity = 4.87102365/130.4543915 secs/batch = 0.2956s, grad.norm=12.39288139
  4156: 3 [  175/ 1327], train_loss/perplexity = 5.10372591/164.6341705 secs/batch = 0.2988s, grad.norm=12.44939995
  4161: 3 [  180/ 1327], train_loss/perplexity = 4.95116520/141.3395538 secs/batch = 0.2955s, grad.norm=12.94801235
  4166: 3 [  185/ 1327], train_loss/perplexity = 5.23227835/187.2188721 secs/batch = 0.3014s, grad.norm=12.52163792
  4171: 3 [  190/ 1327], train_loss/perplexity = 4.70717192/110.7385406 secs/batch = 0.2998s, grad.norm=12.02433300
  4176: 3 [  195/ 1327], train_loss/perplexity = 4.97286081/144.4395142 secs/batch = 0.2933s, grad.norm=11.52073765
  4181: 3 [  200/ 1327], train_loss/perplexity = 4.90797997/135.3656921 secs/batch = 0.2925s, grad.norm=12.96580505
  4186: 3 [  205/ 1327], train_loss/perplexity = 4.99844408/148.1824188 secs/batch = 0.2990s, grad.norm=12.55029583
  4191: 3 [  210/ 1327], train_loss/perplexity = 4.91438055/136.2348938 secs/batch = 0.2943s, grad.norm=12.14119244
  4196: 3 [  215/ 1327], train_loss/perplexity = 5.07289219/159.6353607 secs/batch = 0.2924s, grad.norm=12.36933517
  4201: 3 [  220/ 1327], train_loss/perplexity = 5.01654816/150.8895569 secs/batch = 0.2979s, grad.norm=12.26281452
  4206: 3 [  225/ 1327], train_loss/perplexity = 5.20416164/182.0281982 secs/batch = 0.2947s, grad.norm=12.37017250
  4211: 3 [  230/ 1327], train_loss/perplexity = 5.06834984/158.9118805 secs/batch = 0.2939s, grad.norm=12.93893719
  4216: 3 [  235/ 1327], train_loss/perplexity = 4.81444311/123.2781372 secs/batch = 0.2954s, grad.norm=11.93360996
  4221: 3 [  240/ 1327], train_loss/perplexity = 4.70508146/110.5072861 secs/batch = 0.2942s, grad.norm=14.17600632
  4226: 3 [  245/ 1327], train_loss/perplexity = 5.01110697/150.0707703 secs/batch = 0.2973s, grad.norm=14.37292385
  4231: 3 [  250/ 1327], train_loss/perplexity = 4.72548914/112.7856522 secs/batch = 0.2953s, grad.norm=12.18322277
  4236: 3 [  255/ 1327], train_loss/perplexity = 4.76513052/117.3464355 secs/batch = 0.2939s, grad.norm=12.71588326
  4241: 3 [  260/ 1327], train_loss/perplexity = 5.09803772/163.7003632 secs/batch = 0.2945s, grad.norm=13.68679142
  4246: 3 [  265/ 1327], train_loss/perplexity = 5.12689018/168.4923248 secs/batch = 0.2936s, grad.norm=12.08946800
  4251: 3 [  270/ 1327], train_loss/perplexity = 5.16147041/174.4207306 secs/batch = 0.2947s, grad.norm=12.13723946
  4256: 3 [  275/ 1327], train_loss/perplexity = 5.28998470/198.3403931 secs/batch = 0.2956s, grad.norm=12.97150326
  4261: 3 [  280/ 1327], train_loss/perplexity = 4.99894190/148.2562103 secs/batch = 0.2954s, grad.norm=12.35896015
  4266: 3 [  285/ 1327], train_loss/perplexity = 5.19544888/180.4491272 secs/batch = 0.2995s, grad.norm=12.62892342
  4271: 3 [  290/ 1327], train_loss/perplexity = 5.05922842/157.4689636 secs/batch = 0.2921s, grad.norm=14.02210331
  4276: 3 [  295/ 1327], train_loss/perplexity = 4.82852364/125.0262375 secs/batch = 0.2932s, grad.norm=12.16255474
  4281: 3 [  300/ 1327], train_loss/perplexity = 4.43471003/84.3276672 secs/batch = 0.3010s, grad.norm=12.85329819
  4286: 3 [  305/ 1327], train_loss/perplexity = 4.84782743/127.4631653 secs/batch = 0.2951s, grad.norm=12.77917194
  4291: 3 [  310/ 1327], train_loss/perplexity = 4.87401867/130.8456879 secs/batch = 0.2981s, grad.norm=13.25523281
  4296: 3 [  315/ 1327], train_loss/perplexity = 4.49395800/89.4748840 secs/batch = 0.2991s, grad.norm=13.54201031
  4301: 3 [  320/ 1327], train_loss/perplexity = 4.47859144/88.1104736 secs/batch = 0.2954s, grad.norm=14.75877953
  4306: 3 [  325/ 1327], train_loss/perplexity = 4.43228579/84.1234894 secs/batch = 0.2943s, grad.norm=12.29708862
  4311: 3 [  330/ 1327], train_loss/perplexity = 4.91713142/136.6101685 secs/batch = 0.2941s, grad.norm=13.45410061
  4316: 3 [  335/ 1327], train_loss/perplexity = 4.33354330/76.2138596 secs/batch = 0.2941s, grad.norm=13.69477272
  4321: 3 [  340/ 1327], train_loss/perplexity = 5.06768990/158.8070374 secs/batch = 0.2954s, grad.norm=12.17969894
  4326: 3 [  345/ 1327], train_loss/perplexity = 4.92726421/138.0014496 secs/batch = 0.2954s, grad.norm=12.22833729
  4331: 3 [  350/ 1327], train_loss/perplexity = 4.97322750/144.4924927 secs/batch = 0.2996s, grad.norm=12.85959911
  4336: 3 [  355/ 1327], train_loss/perplexity = 5.05332851/156.5426483 secs/batch = 0.3001s, grad.norm=12.98762131
  4341: 3 [  360/ 1327], train_loss/perplexity = 5.19606304/180.5599823 secs/batch = 0.2941s, grad.norm=13.29217529
  4346: 3 [  365/ 1327], train_loss/perplexity = 5.06215572/157.9306030 secs/batch = 0.2965s, grad.norm=12.97741699
  4351: 3 [  370/ 1327], train_loss/perplexity = 5.02878332/152.7470551 secs/batch = 0.2958s, grad.norm=12.77213573
  4356: 3 [  375/ 1327], train_loss/perplexity = 4.42374897/83.4083939 secs/batch = 0.2926s, grad.norm=13.22362041
  4361: 3 [  380/ 1327], train_loss/perplexity = 4.61490154/100.9778824 secs/batch = 0.2941s, grad.norm=13.64355183
  4366: 3 [  385/ 1327], train_loss/perplexity = 4.81008387/122.7419128 secs/batch = 0.2965s, grad.norm=12.93064690
  4371: 3 [  390/ 1327], train_loss/perplexity = 4.85597038/128.5053253 secs/batch = 0.2936s, grad.norm=12.68888378
  4376: 3 [  395/ 1327], train_loss/perplexity = 5.09010029/162.4061432 secs/batch = 0.2985s, grad.norm=13.19706249
  4381: 3 [  400/ 1327], train_loss/perplexity = 4.87357235/130.7873077 secs/batch = 0.2943s, grad.norm=12.85077190
  4386: 3 [  405/ 1327], train_loss/perplexity = 5.25167894/190.8864899 secs/batch = 0.2986s, grad.norm=12.65099335
  4391: 3 [  410/ 1327], train_loss/perplexity = 4.86770201/130.0217896 secs/batch = 0.2982s, grad.norm=12.86923981
  4396: 3 [  415/ 1327], train_loss/perplexity = 4.70476437/110.4722519 secs/batch = 0.2950s, grad.norm=13.38364315
  4401: 3 [  420/ 1327], train_loss/perplexity = 4.54911232/94.5484390 secs/batch = 0.2952s, grad.norm=14.09518528
  4406: 3 [  425/ 1327], train_loss/perplexity = 4.75382519/116.0272598 secs/batch = 0.2930s, grad.norm=14.41739750
  4411: 3 [  430/ 1327], train_loss/perplexity = 4.96972847/143.9877777 secs/batch = 0.2914s, grad.norm=13.41725636
  4416: 3 [  435/ 1327], train_loss/perplexity = 5.04171371/154.7349548 secs/batch = 0.2954s, grad.norm=13.80217552
  4421: 3 [  440/ 1327], train_loss/perplexity = 4.68971205/108.8218384 secs/batch = 0.2940s, grad.norm=14.47173119
  4426: 3 [  445/ 1327], train_loss/perplexity = 4.99017811/146.9626007 secs/batch = 0.2937s, grad.norm=14.50134087
  4431: 3 [  450/ 1327], train_loss/perplexity = 4.77447224/118.4477844 secs/batch = 0.2990s, grad.norm=12.95337296
  4436: 3 [  455/ 1327], train_loss/perplexity = 4.72105598/112.2867661 secs/batch = 0.2942s, grad.norm=13.26987648
  4441: 3 [  460/ 1327], train_loss/perplexity = 4.79413319/120.7996292 secs/batch = 0.2989s, grad.norm=13.47460938
  4446: 3 [  465/ 1327], train_loss/perplexity = 4.60353374/99.8364868 secs/batch = 0.2934s, grad.norm=14.16077614
  4451: 3 [  470/ 1327], train_loss/perplexity = 5.17343712/176.5205231 secs/batch = 0.2984s, grad.norm=12.17245102
  4456: 3 [  475/ 1327], train_loss/perplexity = 4.64676142/104.2468262 secs/batch = 0.2952s, grad.norm=12.65938187
  4461: 3 [  480/ 1327], train_loss/perplexity = 4.85701418/128.6395264 secs/batch = 0.2995s, grad.norm=13.25066090
  4466: 3 [  485/ 1327], train_loss/perplexity = 4.76309633/117.1079712 secs/batch = 0.2949s, grad.norm=12.82996082
  4471: 3 [  490/ 1327], train_loss/perplexity = 4.69702196/109.6202316 secs/batch = 0.2930s, grad.norm=14.61891556
  4476: 3 [  495/ 1327], train_loss/perplexity = 4.67956591/107.7232971 secs/batch = 0.2986s, grad.norm=13.07716084
  4481: 3 [  500/ 1327], train_loss/perplexity = 5.00044727/148.4795532 secs/batch = 0.2940s, grad.norm=12.54198837
  4486: 3 [  505/ 1327], train_loss/perplexity = 4.95862246/142.3975067 secs/batch = 0.3000s, grad.norm=11.87020016
  4491: 3 [  510/ 1327], train_loss/perplexity = 5.31423378/203.2087555 secs/batch = 0.2955s, grad.norm=11.90046978
  4496: 3 [  515/ 1327], train_loss/perplexity = 4.93387270/138.9164581 secs/batch = 0.3002s, grad.norm=11.56245708
  4501: 3 [  520/ 1327], train_loss/perplexity = 5.12012768/167.3567352 secs/batch = 0.2956s, grad.norm=12.09403419
  4506: 3 [  525/ 1327], train_loss/perplexity = 4.71773815/111.9148331 secs/batch = 0.2944s, grad.norm=13.56246376
  4511: 3 [  530/ 1327], train_loss/perplexity = 4.66204643/105.8524780 secs/batch = 0.2924s, grad.norm=12.81476879
  4516: 3 [  535/ 1327], train_loss/perplexity = 4.85388803/128.2380219 secs/batch = 0.2957s, grad.norm=12.35999775
  4521: 3 [  540/ 1327], train_loss/perplexity = 4.96062136/142.6824188 secs/batch = 0.2944s, grad.norm=12.73431110
  4526: 3 [  545/ 1327], train_loss/perplexity = 5.02107573/151.5742645 secs/batch = 0.2949s, grad.norm=12.56614494
  4531: 3 [  550/ 1327], train_loss/perplexity = 4.92642832/137.8861389 secs/batch = 0.2947s, grad.norm=13.10216713
  4536: 3 [  555/ 1327], train_loss/perplexity = 4.71268177/111.3503723 secs/batch = 0.2984s, grad.norm=12.16839981
  4541: 3 [  560/ 1327], train_loss/perplexity = 4.79376602/120.7552795 secs/batch = 0.2942s, grad.norm=14.48595142
  4546: 3 [  565/ 1327], train_loss/perplexity = 4.80917311/122.6301727 secs/batch = 0.3009s, grad.norm=13.36544132
  4551: 3 [  570/ 1327], train_loss/perplexity = 4.82507324/124.5955963 secs/batch = 0.2992s, grad.norm=13.78917885
  4556: 3 [  575/ 1327], train_loss/perplexity = 4.61478615/100.9662323 secs/batch = 0.2922s, grad.norm=13.06020832
  4561: 3 [  580/ 1327], train_loss/perplexity = 4.98222780/145.7988281 secs/batch = 0.2983s, grad.norm=12.80946541
  4566: 3 [  585/ 1327], train_loss/perplexity = 4.50681543/90.6327362 secs/batch = 0.2957s, grad.norm=13.01983261
  4571: 3 [  590/ 1327], train_loss/perplexity = 4.81305218/123.1067886 secs/batch = 0.2925s, grad.norm=13.00184822
  4576: 3 [  595/ 1327], train_loss/perplexity = 4.81223583/123.0063324 secs/batch = 0.2967s, grad.norm=13.34525394
  4581: 3 [  600/ 1327], train_loss/perplexity = 5.08408165/161.4316254 secs/batch = 0.2955s, grad.norm=12.58234119
  4586: 3 [  605/ 1327], train_loss/perplexity = 5.04460430/155.1828766 secs/batch = 0.2944s, grad.norm=13.27106667
  4591: 3 [  610/ 1327], train_loss/perplexity = 5.08180809/161.0650177 secs/batch = 0.2986s, grad.norm=13.57788944
  4596: 3 [  615/ 1327], train_loss/perplexity = 4.60493374/99.9763565 secs/batch = 0.2941s, grad.norm=13.25087261
  4601: 3 [  620/ 1327], train_loss/perplexity = 4.98125219/145.6566620 secs/batch = 0.2956s, grad.norm=13.08951759
  4606: 3 [  625/ 1327], train_loss/perplexity = 5.00361061/148.9499969 secs/batch = 0.2981s, grad.norm=13.13469315
  4611: 3 [  630/ 1327], train_loss/perplexity = 5.11016703/165.6980286 secs/batch = 0.2937s, grad.norm=12.35725689
  4616: 3 [  635/ 1327], train_loss/perplexity = 4.84160423/126.6724014 secs/batch = 0.2968s, grad.norm=12.82878304
  4621: 3 [  640/ 1327], train_loss/perplexity = 4.85552788/128.4484711 secs/batch = 0.3004s, grad.norm=13.33324051
  4626: 3 [  645/ 1327], train_loss/perplexity = 5.08842754/162.1347046 secs/batch = 0.3001s, grad.norm=13.33016682
  4631: 3 [  650/ 1327], train_loss/perplexity = 4.64713430/104.2857056 secs/batch = 0.3002s, grad.norm=12.81282711
  4636: 3 [  655/ 1327], train_loss/perplexity = 4.78459597/119.6530075 secs/batch = 0.2952s, grad.norm=12.67746830
  4641: 3 [  660/ 1327], train_loss/perplexity = 4.72113419/112.2955475 secs/batch = 0.2986s, grad.norm=12.91788197
  4646: 3 [  665/ 1327], train_loss/perplexity = 4.94655943/140.6900787 secs/batch = 0.2933s, grad.norm=12.57821941
  4651: 3 [  670/ 1327], train_loss/perplexity = 4.76005554/116.7524109 secs/batch = 0.3005s, grad.norm=12.72987938
  4656: 3 [  675/ 1327], train_loss/perplexity = 4.58567095/98.0689621 secs/batch = 0.2933s, grad.norm=13.63917732
  4661: 3 [  680/ 1327], train_loss/perplexity = 4.89073420/133.0512238 secs/batch = 0.2987s, grad.norm=13.33291054
  4666: 3 [  685/ 1327], train_loss/perplexity = 4.72743988/113.0058823 secs/batch = 0.2938s, grad.norm=13.51016426
  4671: 3 [  690/ 1327], train_loss/perplexity = 5.05189991/156.3191681 secs/batch = 0.2944s, grad.norm=12.67057419
  4676: 3 [  695/ 1327], train_loss/perplexity = 4.81285095/123.0820160 secs/batch = 0.2960s, grad.norm=13.35088348
  4681: 3 [  700/ 1327], train_loss/perplexity = 5.07584715/160.1077728 secs/batch = 0.2943s, grad.norm=12.38379383
  4686: 3 [  705/ 1327], train_loss/perplexity = 4.77224731/118.1845398 secs/batch = 0.2949s, grad.norm=13.05883026
  4691: 3 [  710/ 1327], train_loss/perplexity = 4.75860882/116.5836258 secs/batch = 0.2957s, grad.norm=12.99994850
  4696: 3 [  715/ 1327], train_loss/perplexity = 4.69868898/109.8031235 secs/batch = 0.2939s, grad.norm=12.45317078
  4701: 3 [  720/ 1327], train_loss/perplexity = 4.72297096/112.5019989 secs/batch = 0.2984s, grad.norm=13.38839245
  4706: 3 [  725/ 1327], train_loss/perplexity = 4.65850163/105.4779205 secs/batch = 0.3003s, grad.norm=12.95625401
  4711: 3 [  730/ 1327], train_loss/perplexity = 4.79948616/121.4479980 secs/batch = 0.2950s, grad.norm=12.67072964
  4716: 3 [  735/ 1327], train_loss/perplexity = 4.93759394/139.4343567 secs/batch = 0.2986s, grad.norm=13.40946579
  4721: 3 [  740/ 1327], train_loss/perplexity = 4.34172106/76.8396683 secs/batch = 0.2998s, grad.norm=12.89086342
  4726: 3 [  745/ 1327], train_loss/perplexity = 4.85524750/128.4124756 secs/batch = 0.2941s, grad.norm=12.81212711
  4731: 3 [  750/ 1327], train_loss/perplexity = 4.64849377/104.4275742 secs/batch = 0.2944s, grad.norm=13.08762836
  4736: 3 [  755/ 1327], train_loss/perplexity = 4.61391068/100.8778763 secs/batch = 0.2941s, grad.norm=13.48383617
  4741: 3 [  760/ 1327], train_loss/perplexity = 4.50964689/90.8897171 secs/batch = 0.2986s, grad.norm=13.79219151
  4746: 3 [  765/ 1327], train_loss/perplexity = 4.57348204/96.8808670 secs/batch = 0.2958s, grad.norm=13.29875755
  4751: 3 [  770/ 1327], train_loss/perplexity = 4.52089977/91.9182663 secs/batch = 0.2977s, grad.norm=13.37384415
  4756: 3 [  775/ 1327], train_loss/perplexity = 4.72158337/112.3459930 secs/batch = 0.2999s, grad.norm=13.20200920
  4761: 3 [  780/ 1327], train_loss/perplexity = 5.01769257/151.0623322 secs/batch = 0.2936s, grad.norm=12.62020206
  4766: 3 [  785/ 1327], train_loss/perplexity = 4.78600121/119.8212662 secs/batch = 0.3002s, grad.norm=13.68477917
  4771: 3 [  790/ 1327], train_loss/perplexity = 4.60289812/99.7730484 secs/batch = 0.2985s, grad.norm=13.47113514
  4776: 3 [  795/ 1327], train_loss/perplexity = 5.08048868/160.8526459 secs/batch = 0.2986s, grad.norm=13.92147732
  4781: 3 [  800/ 1327], train_loss/perplexity = 4.91485739/136.2998657 secs/batch = 0.2926s, grad.norm=14.10263538
  4786: 3 [  805/ 1327], train_loss/perplexity = 5.24534655/189.6815338 secs/batch = 0.2948s, grad.norm=13.45161629
  4791: 3 [  810/ 1327], train_loss/perplexity = 4.84468889/127.0637436 secs/batch = 0.2954s, grad.norm=12.71014023
  4796: 3 [  815/ 1327], train_loss/perplexity = 4.77536392/118.5534515 secs/batch = 0.2993s, grad.norm=12.18637466
  4801: 3 [  820/ 1327], train_loss/perplexity = 4.50243855/90.2369080 secs/batch = 0.2987s, grad.norm=12.79559231
  4806: 3 [  825/ 1327], train_loss/perplexity = 4.74457788/114.9592667 secs/batch = 0.2939s, grad.norm=12.59811974
  4811: 3 [  830/ 1327], train_loss/perplexity = 4.50851393/90.7868042 secs/batch = 0.2947s, grad.norm=13.44181919
  4816: 3 [  835/ 1327], train_loss/perplexity = 4.77304840/118.2792587 secs/batch = 0.2930s, grad.norm=13.53273678
  4821: 3 [  840/ 1327], train_loss/perplexity = 4.83211422/125.4759674 secs/batch = 0.2942s, grad.norm=13.10817051
  4826: 3 [  845/ 1327], train_loss/perplexity = 4.66740513/106.4212341 secs/batch = 0.2979s, grad.norm=12.87709236
  4831: 3 [  850/ 1327], train_loss/perplexity = 4.69559717/109.4641571 secs/batch = 0.2926s, grad.norm=12.85631943
  4836: 3 [  855/ 1327], train_loss/perplexity = 4.77054358/117.9833603 secs/batch = 0.2985s, grad.norm=13.67134190
  4841: 3 [  860/ 1327], train_loss/perplexity = 4.48274231/88.4769745 secs/batch = 0.2939s, grad.norm=12.47856045
  4846: 3 [  865/ 1327], train_loss/perplexity = 5.02340126/151.9271698 secs/batch = 0.2918s, grad.norm=12.99699783
  4851: 3 [  870/ 1327], train_loss/perplexity = 4.88635874/132.4703369 secs/batch = 0.2952s, grad.norm=13.80896759
  4856: 3 [  875/ 1327], train_loss/perplexity = 4.40352535/81.7385178 secs/batch = 0.2956s, grad.norm=13.17116356
  4861: 3 [  880/ 1327], train_loss/perplexity = 4.61471558/100.9591064 secs/batch = 0.2943s, grad.norm=11.97522259
  4866: 3 [  885/ 1327], train_loss/perplexity = 4.70628357/110.6402054 secs/batch = 0.2930s, grad.norm=12.48249531
  4871: 3 [  890/ 1327], train_loss/perplexity = 4.96271658/142.9816895 secs/batch = 0.2947s, grad.norm=13.20593739
  4876: 3 [  895/ 1327], train_loss/perplexity = 4.94905472/141.0415802 secs/batch = 0.2952s, grad.norm=12.38810539
  4881: 3 [  900/ 1327], train_loss/perplexity = 4.74659920/115.1918716 secs/batch = 0.2941s, grad.norm=13.19050026
  4886: 3 [  905/ 1327], train_loss/perplexity = 4.55929232/95.5158615 secs/batch = 0.2947s, grad.norm=12.97824287
  4891: 3 [  910/ 1327], train_loss/perplexity = 4.66766548/106.4489441 secs/batch = 0.2985s, grad.norm=12.93003941
  4896: 3 [  915/ 1327], train_loss/perplexity = 4.94936991/141.0860443 secs/batch = 0.2940s, grad.norm=13.37606049
  4901: 3 [  920/ 1327], train_loss/perplexity = 5.12786531/168.6567078 secs/batch = 0.2921s, grad.norm=13.53925610
  4906: 3 [  925/ 1327], train_loss/perplexity = 4.89135838/133.1342926 secs/batch = 0.2937s, grad.norm=12.65647221
  4911: 3 [  930/ 1327], train_loss/perplexity = 4.88473892/132.2559357 secs/batch = 0.3005s, grad.norm=12.81591415
  4916: 3 [  935/ 1327], train_loss/perplexity = 4.89593267/133.7446899 secs/batch = 0.2943s, grad.norm=12.51157665
  4921: 3 [  940/ 1327], train_loss/perplexity = 4.85486794/128.3637390 secs/batch = 0.2933s, grad.norm=12.97846222
  4926: 3 [  945/ 1327], train_loss/perplexity = 5.05071831/156.1345825 secs/batch = 0.2950s, grad.norm=13.00768185
  4931: 3 [  950/ 1327], train_loss/perplexity = 4.84472990/127.0689545 secs/batch = 0.2943s, grad.norm=12.49714661
  4936: 3 [  955/ 1327], train_loss/perplexity = 4.89361525/133.4351044 secs/batch = 0.2947s, grad.norm=13.17992115
  4941: 3 [  960/ 1327], train_loss/perplexity = 5.07268906/159.6029358 secs/batch = 0.2945s, grad.norm=12.59407330
  4946: 3 [  965/ 1327], train_loss/perplexity = 4.94694614/140.7444916 secs/batch = 0.2944s, grad.norm=12.55045319
  4951: 3 [  970/ 1327], train_loss/perplexity = 5.09582901/163.3392029 secs/batch = 0.2947s, grad.norm=12.52574348
  4956: 3 [  975/ 1327], train_loss/perplexity = 4.87626934/131.1405029 secs/batch = 0.2922s, grad.norm=13.25594139
  4961: 3 [  980/ 1327], train_loss/perplexity = 4.65629005/105.2449036 secs/batch = 0.2985s, grad.norm=11.78675652
  4966: 3 [  985/ 1327], train_loss/perplexity = 4.84011412/126.4837875 secs/batch = 0.2940s, grad.norm=13.47589016
  4971: 3 [  990/ 1327], train_loss/perplexity = 4.96365738/143.1162720 secs/batch = 0.3002s, grad.norm=12.83631039
  4976: 3 [  995/ 1327], train_loss/perplexity = 4.96898842/143.8812714 secs/batch = 0.3006s, grad.norm=12.31804180
  4981: 3 [ 1000/ 1327], train_loss/perplexity = 4.45310450/85.8931885 secs/batch = 0.2992s, grad.norm=12.59829712
  4986: 3 [ 1005/ 1327], train_loss/perplexity = 4.91260767/135.9935760 secs/batch = 0.2950s, grad.norm=12.83489990
  4991: 3 [ 1010/ 1327], train_loss/perplexity = 4.49851513/89.8835678 secs/batch = 0.3001s, grad.norm=11.73335934
  4996: 3 [ 1015/ 1327], train_loss/perplexity = 4.99073696/147.0447540 secs/batch = 0.3003s, grad.norm=12.26542282
  5001: 3 [ 1020/ 1327], train_loss/perplexity = 5.19920683/181.1285248 secs/batch = 0.2933s, grad.norm=12.24567032
  5006: 3 [ 1025/ 1327], train_loss/perplexity = 4.94870663/140.9924927 secs/batch = 0.2944s, grad.norm=12.33670235
  5011: 3 [ 1030/ 1327], train_loss/perplexity = 4.76803541/117.6878052 secs/batch = 0.2929s, grad.norm=12.19496918
  5016: 3 [ 1035/ 1327], train_loss/perplexity = 4.68155432/107.9377136 secs/batch = 0.2947s, grad.norm=11.85581207
  5021: 3 [ 1040/ 1327], train_loss/perplexity = 4.95481634/141.8565521 secs/batch = 0.2992s, grad.norm=12.85410213
  5026: 3 [ 1045/ 1327], train_loss/perplexity = 4.59251070/98.7420273 secs/batch = 0.2955s, grad.norm=12.73203182
  5031: 3 [ 1050/ 1327], train_loss/perplexity = 4.58913422/98.4091949 secs/batch = 0.2953s, grad.norm=13.55587006
  5036: 3 [ 1055/ 1327], train_loss/perplexity = 4.68994379/108.8470612 secs/batch = 0.2971s, grad.norm=13.47908878
  5041: 3 [ 1060/ 1327], train_loss/perplexity = 4.36450624/78.6105728 secs/batch = 0.2944s, grad.norm=14.23214817
  5046: 3 [ 1065/ 1327], train_loss/perplexity = 4.55968380/95.5532608 secs/batch = 0.2949s, grad.norm=13.08621311
  5051: 3 [ 1070/ 1327], train_loss/perplexity = 4.82961130/125.1623001 secs/batch = 0.2985s, grad.norm=13.35068512
  5056: 3 [ 1075/ 1327], train_loss/perplexity = 4.58516502/98.0193634 secs/batch = 0.2991s, grad.norm=13.15910816
  5061: 3 [ 1080/ 1327], train_loss/perplexity = 4.52769709/92.5451889 secs/batch = 0.2941s, grad.norm=13.24046612
  5066: 3 [ 1085/ 1327], train_loss/perplexity = 4.41609812/82.7726822 secs/batch = 0.2928s, grad.norm=13.00408077
  5071: 3 [ 1090/ 1327], train_loss/perplexity = 4.64054823/103.6011276 secs/batch = 0.2944s, grad.norm=13.90476131
  5076: 3 [ 1095/ 1327], train_loss/perplexity = 4.74743366/115.2880402 secs/batch = 0.3012s, grad.norm=13.48783970
  5081: 3 [ 1100/ 1327], train_loss/perplexity = 4.54810238/94.4530029 secs/batch = 0.2945s, grad.norm=14.95575523
  5086: 3 [ 1105/ 1327], train_loss/perplexity = 4.50179958/90.1792679 secs/batch = 0.2930s, grad.norm=13.58876801
  5091: 3 [ 1110/ 1327], train_loss/perplexity = 4.92632294/137.8716125 secs/batch = 0.2950s, grad.norm=13.97174072
  5096: 3 [ 1115/ 1327], train_loss/perplexity = 4.53764105/93.4700470 secs/batch = 0.2948s, grad.norm=13.18054104
  5101: 3 [ 1120/ 1327], train_loss/perplexity = 4.79184246/120.5232239 secs/batch = 0.2942s, grad.norm=13.25664139
  5106: 3 [ 1125/ 1327], train_loss/perplexity = 5.05586195/156.9397430 secs/batch = 0.2927s, grad.norm=13.31175804
  5111: 3 [ 1130/ 1327], train_loss/perplexity = 4.68682289/108.5078888 secs/batch = 0.2992s, grad.norm=13.01744175
  5116: 3 [ 1135/ 1327], train_loss/perplexity = 4.71206617/111.2818527 secs/batch = 0.2985s, grad.norm=13.11279202
  5121: 3 [ 1140/ 1327], train_loss/perplexity = 4.99823332/148.1511993 secs/batch = 0.2989s, grad.norm=13.80858612
  5126: 3 [ 1145/ 1327], train_loss/perplexity = 4.69706154/109.6245728 secs/batch = 0.2983s, grad.norm=12.96452522
  5131: 3 [ 1150/ 1327], train_loss/perplexity = 4.68666458/108.4907150 secs/batch = 0.2938s, grad.norm=13.26545620
  5136: 3 [ 1155/ 1327], train_loss/perplexity = 4.81420517/123.2488098 secs/batch = 0.2988s, grad.norm=12.90703011
  5141: 3 [ 1160/ 1327], train_loss/perplexity = 4.75280952/115.9094772 secs/batch = 0.2937s, grad.norm=12.72250366
  5146: 3 [ 1165/ 1327], train_loss/perplexity = 4.78098011/119.2211456 secs/batch = 0.2924s, grad.norm=13.13527679
  5151: 3 [ 1170/ 1327], train_loss/perplexity = 4.68200111/107.9859467 secs/batch = 0.2954s, grad.norm=13.24247742
  5156: 3 [ 1175/ 1327], train_loss/perplexity = 4.45896053/86.3976517 secs/batch = 0.2953s, grad.norm=14.03131294
  5161: 3 [ 1180/ 1327], train_loss/perplexity = 4.47751808/88.0159531 secs/batch = 0.2941s, grad.norm=14.14630222
  5166: 3 [ 1185/ 1327], train_loss/perplexity = 4.69775581/109.7007065 secs/batch = 0.2982s, grad.norm=13.11325169
  5171: 3 [ 1190/ 1327], train_loss/perplexity = 4.71900082/112.0562286 secs/batch = 0.2952s, grad.norm=13.45905590
  5176: 3 [ 1195/ 1327], train_loss/perplexity = 4.58263493/97.7716751 secs/batch = 0.2929s, grad.norm=12.90679359
  5181: 3 [ 1200/ 1327], train_loss/perplexity = 4.48820066/88.9612274 secs/batch = 0.2973s, grad.norm=13.44594765
  5186: 3 [ 1205/ 1327], train_loss/perplexity = 4.50863123/90.7974548 secs/batch = 0.2931s, grad.norm=14.17827892
  5191: 3 [ 1210/ 1327], train_loss/perplexity = 4.26898956/71.4494019 secs/batch = 0.2991s, grad.norm=13.64714241
  5196: 3 [ 1215/ 1327], train_loss/perplexity = 4.41004801/82.2734146 secs/batch = 0.3010s, grad.norm=12.77472210
  5201: 3 [ 1220/ 1327], train_loss/perplexity = 4.53463602/93.1895905 secs/batch = 0.3009s, grad.norm=13.62140656
  5206: 3 [ 1225/ 1327], train_loss/perplexity = 4.31072140/74.4942093 secs/batch = 0.2921s, grad.norm=13.63439655
  5211: 3 [ 1230/ 1327], train_loss/perplexity = 4.55833626/95.4245834 secs/batch = 0.2943s, grad.norm=13.43050766
  5216: 3 [ 1235/ 1327], train_loss/perplexity = 4.63155460/102.6735535 secs/batch = 0.2996s, grad.norm=13.07892227
  5221: 3 [ 1240/ 1327], train_loss/perplexity = 4.72348976/112.5603790 secs/batch = 0.2952s, grad.norm=13.51188278
  5226: 3 [ 1245/ 1327], train_loss/perplexity = 4.68225861/108.0137558 secs/batch = 0.2923s, grad.norm=12.72980309
  5231: 3 [ 1250/ 1327], train_loss/perplexity = 4.75695705/116.3912125 secs/batch = 0.2950s, grad.norm=12.51764965
  5236: 3 [ 1255/ 1327], train_loss/perplexity = 4.80451918/122.0607910 secs/batch = 0.2949s, grad.norm=12.47107029
  5241: 3 [ 1260/ 1327], train_loss/perplexity = 4.65114403/104.7047043 secs/batch = 0.2949s, grad.norm=13.89984512
  5246: 3 [ 1265/ 1327], train_loss/perplexity = 4.77799273/118.8655167 secs/batch = 0.2953s, grad.norm=13.38933849
  5251: 3 [ 1270/ 1327], train_loss/perplexity = 4.57473278/97.0021133 secs/batch = 0.2937s, grad.norm=13.30640793
  5256: 3 [ 1275/ 1327], train_loss/perplexity = 4.76909876/117.8130188 secs/batch = 0.2938s, grad.norm=13.70274448
  5261: 3 [ 1280/ 1327], train_loss/perplexity = 4.58754826/98.2532425 secs/batch = 0.2946s, grad.norm=13.38600826
  5266: 3 [ 1285/ 1327], train_loss/perplexity = 4.51964045/91.8025818 secs/batch = 0.2994s, grad.norm=13.75290298
  5271: 3 [ 1290/ 1327], train_loss/perplexity = 4.74457407/114.9588318 secs/batch = 0.2941s, grad.norm=12.58365154
  5276: 3 [ 1295/ 1327], train_loss/perplexity = 4.76371002/117.1798630 secs/batch = 0.2924s, grad.norm=14.45362663
  5281: 3 [ 1300/ 1327], train_loss/perplexity = 4.89588976/133.7389526 secs/batch = 0.2958s, grad.norm=13.18931866
  5286: 3 [ 1305/ 1327], train_loss/perplexity = 5.04712582/155.5746765 secs/batch = 0.2956s, grad.norm=14.37727928
  5291: 3 [ 1310/ 1327], train_loss/perplexity = 5.20813513/182.7529297 secs/batch = 0.2945s, grad.norm=12.80850410
  5296: 3 [ 1315/ 1327], train_loss/perplexity = 4.99202442/147.2341919 secs/batch = 0.2934s, grad.norm=12.96704102
  5301: 3 [ 1320/ 1327], train_loss/perplexity = 5.04899311/155.8654480 secs/batch = 0.2947s, grad.norm=13.27392006
  5306: 3 [ 1325/ 1327], train_loss/perplexity = 4.96574640/143.4155579 secs/batch = 0.2943s, grad.norm=12.85692692
Epoch training time: 392.5559823513031
	> validation loss = 5.04187012, perplexity = 154.75915527
	> validation loss = 4.91634512, perplexity = 136.50279236
	> validation loss = 4.84460974, perplexity = 127.05368805
	> validation loss = 4.80234623, perplexity = 121.79584503
	> validation loss = 5.07438755, perplexity = 159.87425232
	> validation loss = 4.90720224, perplexity = 135.26045227
	> validation loss = 4.90311623, perplexity = 134.70890808
	> validation loss = 4.77709627, perplexity = 118.75900269
	> validation loss = 4.67992830, perplexity = 107.76234436
	> validation loss = 4.71384430, perplexity = 111.47989655
	> validation loss = 4.74772739, perplexity = 115.32190704
	> validation loss = 4.85123014, perplexity = 127.89762878
	> validation loss = 4.76641464, perplexity = 117.49721527
	> validation loss = 4.65630198, perplexity = 105.24616241
	> validation loss = 4.51173019, perplexity = 91.07926941
	> validation loss = 4.51271200, perplexity = 91.16873169
	> validation loss = 4.97028255, perplexity = 144.06758118
	> validation loss = 4.55034113, perplexity = 94.66469574
	> validation loss = 4.91492558, perplexity = 136.30915833
	> validation loss = 4.82994938, perplexity = 125.20462036
	> validation loss = 4.66394997, perplexity = 106.05416870
at the end of epoch: 3
train loss = 4.86004394, perplexity = 129.02987146
validation loss = 4.78329900, perplexity = 119.49792427
Saved model cv/epoch003_4.7833.model
  5313: 4 [    5/ 1327], train_loss/perplexity = 5.00624275/149.3425598 secs/batch = 0.2969s, grad.norm=13.16835976
  5318: 4 [   10/ 1327], train_loss/perplexity = 4.53181362/92.9269409 secs/batch = 0.2996s, grad.norm=14.08289242
  5323: 4 [   15/ 1327], train_loss/perplexity = 4.85054016/127.8094101 secs/batch = 0.2929s, grad.norm=12.36742401
  5328: 4 [   20/ 1327], train_loss/perplexity = 4.98413849/146.0776672 secs/batch = 0.2949s, grad.norm=13.77181149
  5333: 4 [   25/ 1327], train_loss/perplexity = 4.92653942/137.9014740 secs/batch = 0.2942s, grad.norm=12.79901028
  5338: 4 [   30/ 1327], train_loss/perplexity = 4.90033531/134.3348236 secs/batch = 0.2930s, grad.norm=13.01752853
  5343: 4 [   35/ 1327], train_loss/perplexity = 4.71495867/111.6041946 secs/batch = 0.2993s, grad.norm=12.47092342
  5348: 4 [   40/ 1327], train_loss/perplexity = 4.67020512/106.7196274 secs/batch = 0.2944s, grad.norm=12.77402592
  5353: 4 [   45/ 1327], train_loss/perplexity = 4.55474091/95.0821152 secs/batch = 0.2984s, grad.norm=13.61123753
  5358: 4 [   50/ 1327], train_loss/perplexity = 4.75854254/116.5758972 secs/batch = 0.2920s, grad.norm=13.08469582
  5363: 4 [   55/ 1327], train_loss/perplexity = 4.67568302/107.3058319 secs/batch = 0.2955s, grad.norm=13.28796577
  5368: 4 [   60/ 1327], train_loss/perplexity = 4.96068382/142.6913452 secs/batch = 0.3009s, grad.norm=13.28630352
  5373: 4 [   65/ 1327], train_loss/perplexity = 4.51037502/90.9559250 secs/batch = 0.2951s, grad.norm=12.25539684
  5378: 4 [   70/ 1327], train_loss/perplexity = 4.39905071/81.3735886 secs/batch = 0.2999s, grad.norm=13.75280762
  5383: 4 [   75/ 1327], train_loss/perplexity = 4.27612019/71.9607010 secs/batch = 0.2973s, grad.norm=13.21700764
  5388: 4 [   80/ 1327], train_loss/perplexity = 4.66909218/106.6009216 secs/batch = 0.2946s, grad.norm=13.23413372
  5393: 4 [   85/ 1327], train_loss/perplexity = 4.69408512/109.2987671 secs/batch = 0.2953s, grad.norm=13.29774570
  5398: 4 [   90/ 1327], train_loss/perplexity = 4.69374847/109.2619781 secs/batch = 0.2942s, grad.norm=13.81214714
  5403: 4 [   95/ 1327], train_loss/perplexity = 4.57501507/97.0295029 secs/batch = 0.2923s, grad.norm=12.92251587
  5408: 4 [  100/ 1327], train_loss/perplexity = 4.89190769/133.2074432 secs/batch = 0.3003s, grad.norm=12.78449917
  5413: 4 [  105/ 1327], train_loss/perplexity = 4.75832748/116.5508270 secs/batch = 0.2971s, grad.norm=13.71381474
  5418: 4 [  110/ 1327], train_loss/perplexity = 4.67377949/107.1017685 secs/batch = 0.2920s, grad.norm=12.01858425
  5423: 4 [  115/ 1327], train_loss/perplexity = 4.53570414/93.2891769 secs/batch = 0.2982s, grad.norm=13.80343056
  5428: 4 [  120/ 1327], train_loss/perplexity = 4.66661930/106.3376389 secs/batch = 0.2966s, grad.norm=14.17634392
  5433: 4 [  125/ 1327], train_loss/perplexity = 4.74262810/114.7353439 secs/batch = 0.2928s, grad.norm=13.46020031
  5438: 4 [  130/ 1327], train_loss/perplexity = 4.68649864/108.4727097 secs/batch = 0.2935s, grad.norm=14.00702190
  5443: 4 [  135/ 1327], train_loss/perplexity = 4.64144516/103.6940918 secs/batch = 0.2938s, grad.norm=12.83945465
  5448: 4 [  140/ 1327], train_loss/perplexity = 4.94863272/140.9820709 secs/batch = 0.2944s, grad.norm=13.79718876
  5453: 4 [  145/ 1327], train_loss/perplexity = 4.90763807/135.3194275 secs/batch = 0.2953s, grad.norm=14.54118347
  5458: 4 [  150/ 1327], train_loss/perplexity = 4.83438110/125.7607269 secs/batch = 0.2935s, grad.norm=14.14353466
  5463: 4 [  155/ 1327], train_loss/perplexity = 5.04355860/155.0206909 secs/batch = 0.2946s, grad.norm=12.51031017
  5468: 4 [  160/ 1327], train_loss/perplexity = 4.72649097/112.8987045 secs/batch = 0.2982s, grad.norm=12.85441017
  5473: 4 [  165/ 1327], train_loss/perplexity = 4.86818409/130.0844879 secs/batch = 0.2997s, grad.norm=12.93366623
  5478: 4 [  170/ 1327], train_loss/perplexity = 4.71337223/111.4272842 secs/batch = 0.2929s, grad.norm=12.78987694
  5483: 4 [  175/ 1327], train_loss/perplexity = 4.96522427/143.3406982 secs/batch = 0.2942s, grad.norm=13.17895889
  5488: 4 [  180/ 1327], train_loss/perplexity = 4.80390024/121.9852600 secs/batch = 0.2928s, grad.norm=13.70529270
  5493: 4 [  185/ 1327], train_loss/perplexity = 5.15117168/172.6336365 secs/batch = 0.2960s, grad.norm=12.97120667
  5498: 4 [  190/ 1327], train_loss/perplexity = 4.65974522/105.6091690 secs/batch = 0.2945s, grad.norm=12.36699867
  5503: 4 [  195/ 1327], train_loss/perplexity = 4.87470579/130.9356232 secs/batch = 0.2939s, grad.norm=12.47935104
  5508: 4 [  200/ 1327], train_loss/perplexity = 4.76735830/117.6081467 secs/batch = 0.2979s, grad.norm=13.34085751
  5513: 4 [  205/ 1327], train_loss/perplexity = 4.89186192/133.2013550 secs/batch = 0.2956s, grad.norm=13.42151070
  5518: 4 [  210/ 1327], train_loss/perplexity = 4.81172991/122.9441147 secs/batch = 0.2947s, grad.norm=13.13600063
  5523: 4 [  215/ 1327], train_loss/perplexity = 4.94389629/140.3159027 secs/batch = 0.2933s, grad.norm=12.48712254
  5528: 4 [  220/ 1327], train_loss/perplexity = 4.91639519/136.5096283 secs/batch = 0.2921s, grad.norm=12.37843800
  5533: 4 [  225/ 1327], train_loss/perplexity = 5.10607862/165.0219727 secs/batch = 0.2996s, grad.norm=12.86792660
  5538: 4 [  230/ 1327], train_loss/perplexity = 4.93175316/138.6223297 secs/batch = 0.2920s, grad.norm=13.45272064
  5543: 4 [  235/ 1327], train_loss/perplexity = 4.76327181/117.1285248 secs/batch = 0.2995s, grad.norm=12.63273430
  5548: 4 [  240/ 1327], train_loss/perplexity = 4.65643167/105.2598114 secs/batch = 0.2927s, grad.norm=13.93400860
  5553: 4 [  245/ 1327], train_loss/perplexity = 4.93174791/138.6215973 secs/batch = 0.2931s, grad.norm=12.84922791
  5558: 4 [  250/ 1327], train_loss/perplexity = 4.65656137/105.2734604 secs/batch = 0.2927s, grad.norm=12.68142223
  5563: 4 [  255/ 1327], train_loss/perplexity = 4.68352985/108.1511536 secs/batch = 0.2938s, grad.norm=13.48652840
  5568: 4 [  260/ 1327], train_loss/perplexity = 4.90991211/135.6274872 secs/batch = 0.2991s, grad.norm=13.45006466
  5573: 4 [  265/ 1327], train_loss/perplexity = 5.03305197/153.4004669 secs/batch = 0.2987s, grad.norm=12.36031628
  5578: 4 [  270/ 1327], train_loss/perplexity = 5.12294149/167.8283081 secs/batch = 0.2932s, grad.norm=13.26223946
  5583: 4 [  275/ 1327], train_loss/perplexity = 5.14134407/170.9453735 secs/batch = 0.2959s, grad.norm=13.16956329
  5588: 4 [  280/ 1327], train_loss/perplexity = 4.88176060/131.8626099 secs/batch = 0.3006s, grad.norm=12.73024082
  5593: 4 [  285/ 1327], train_loss/perplexity = 5.09697676/163.5267792 secs/batch = 0.2937s, grad.norm=12.77478123
  5598: 4 [  290/ 1327], train_loss/perplexity = 4.91409636/136.1961823 secs/batch = 0.2926s, grad.norm=13.23258972
  5603: 4 [  295/ 1327], train_loss/perplexity = 4.61201572/100.6869049 secs/batch = 0.2938s, grad.norm=12.45150852
  5608: 4 [  300/ 1327], train_loss/perplexity = 4.25519943/70.4708710 secs/batch = 0.2955s, grad.norm=12.72059059
  5613: 4 [  305/ 1327], train_loss/perplexity = 4.77554560/118.5749893 secs/batch = 0.2949s, grad.norm=13.18334103
  5618: 4 [  310/ 1327], train_loss/perplexity = 4.69339895/109.2237930 secs/batch = 0.2947s, grad.norm=12.76630211
  5623: 4 [  315/ 1327], train_loss/perplexity = 4.37129784/79.1462860 secs/batch = 0.2954s, grad.norm=14.12702179
  5628: 4 [  320/ 1327], train_loss/perplexity = 4.29925966/73.6452484 secs/batch = 0.2949s, grad.norm=15.34764004
  5633: 4 [  325/ 1327], train_loss/perplexity = 4.30591393/74.1369400 secs/batch = 0.2980s, grad.norm=12.89595509
  5638: 4 [  330/ 1327], train_loss/perplexity = 4.81333256/123.1413116 secs/batch = 0.2984s, grad.norm=14.20740318
  5643: 4 [  335/ 1327], train_loss/perplexity = 4.21244717/67.5215759 secs/batch = 0.2990s, grad.norm=13.37517738
  5648: 4 [  340/ 1327], train_loss/perplexity = 4.95713568/142.1859436 secs/batch = 0.2937s, grad.norm=12.30457306
  5653: 4 [  345/ 1327], train_loss/perplexity = 4.79471779/120.8702698 secs/batch = 0.2964s, grad.norm=12.53669262
  5658: 4 [  350/ 1327], train_loss/perplexity = 4.82159901/124.1634674 secs/batch = 0.2981s, grad.norm=13.50857639
  5663: 4 [  355/ 1327], train_loss/perplexity = 4.88790512/132.6753387 secs/batch = 0.2956s, grad.norm=13.12634659
  5668: 4 [  360/ 1327], train_loss/perplexity = 5.03706837/154.0178223 secs/batch = 0.2936s, grad.norm=13.48386002
  5673: 4 [  365/ 1327], train_loss/perplexity = 4.95060682/141.2606506 secs/batch = 0.2982s, grad.norm=13.27730465
  5678: 4 [  370/ 1327], train_loss/perplexity = 4.91057348/135.7172241 secs/batch = 0.2947s, grad.norm=13.52466869
  5683: 4 [  375/ 1327], train_loss/perplexity = 4.25861549/70.7120132 secs/batch = 0.2979s, grad.norm=13.30959415
  5688: 4 [  380/ 1327], train_loss/perplexity = 4.44248962/84.9862595 secs/batch = 0.2938s, grad.norm=14.08276081
  5693: 4 [  385/ 1327], train_loss/perplexity = 4.70401859/110.3898926 secs/batch = 0.2995s, grad.norm=13.69656849
  5698: 4 [  390/ 1327], train_loss/perplexity = 4.83631134/126.0037079 secs/batch = 0.2988s, grad.norm=13.14503002
  5703: 4 [  395/ 1327], train_loss/perplexity = 4.91125536/135.8097992 secs/batch = 0.2952s, grad.norm=13.60312653
  5708: 4 [  400/ 1327], train_loss/perplexity = 4.72564173/112.8028641 secs/batch = 0.2946s, grad.norm=12.96397591
  5713: 4 [  405/ 1327], train_loss/perplexity = 5.13484669/169.8382721 secs/batch = 0.2944s, grad.norm=13.23502827
  5718: 4 [  410/ 1327], train_loss/perplexity = 4.77913713/119.0016251 secs/batch = 0.2948s, grad.norm=13.92347431
  5723: 4 [  415/ 1327], train_loss/perplexity = 4.60193586/99.6770935 secs/batch = 0.2951s, grad.norm=13.67016983
  5728: 4 [  420/ 1327], train_loss/perplexity = 4.35200119/77.6336670 secs/batch = 0.2958s, grad.norm=13.19529438
  5733: 4 [  425/ 1327], train_loss/perplexity = 4.59737587/99.2235947 secs/batch = 0.2985s, grad.norm=14.07224369
  5738: 4 [  430/ 1327], train_loss/perplexity = 4.83759546/126.1656189 secs/batch = 0.2944s, grad.norm=13.52702045
  5743: 4 [  435/ 1327], train_loss/perplexity = 4.89318657/133.3779144 secs/batch = 0.2939s, grad.norm=13.91886997
  5748: 4 [  440/ 1327], train_loss/perplexity = 4.56931686/96.4781799 secs/batch = 0.2947s, grad.norm=14.39601707
  5753: 4 [  445/ 1327], train_loss/perplexity = 4.84533072/127.1453247 secs/batch = 0.2983s, grad.norm=13.62139225
  5758: 4 [  450/ 1327], train_loss/perplexity = 4.70892000/110.9322891 secs/batch = 0.2940s, grad.norm=13.56176853
  5763: 4 [  455/ 1327], train_loss/perplexity = 4.61397743/100.8846130 secs/batch = 0.2962s, grad.norm=13.36933708
  5768: 4 [  460/ 1327], train_loss/perplexity = 4.66156197/105.8012085 secs/batch = 0.2957s, grad.norm=13.74019337
  5773: 4 [  465/ 1327], train_loss/perplexity = 4.48855257/88.9925385 secs/batch = 0.3011s, grad.norm=16.37494278
  5778: 4 [  470/ 1327], train_loss/perplexity = 5.03424168/153.5830841 secs/batch = 0.2939s, grad.norm=12.51899052
  5783: 4 [  475/ 1327], train_loss/perplexity = 4.52237320/92.0538025 secs/batch = 0.2940s, grad.norm=13.35042858
  5788: 4 [  480/ 1327], train_loss/perplexity = 4.74216413/114.6821213 secs/batch = 0.2953s, grad.norm=13.31821156
  5793: 4 [  485/ 1327], train_loss/perplexity = 4.64704990/104.2769012 secs/batch = 0.2953s, grad.norm=13.52358246
  5798: 4 [  490/ 1327], train_loss/perplexity = 4.57388973/96.9203720 secs/batch = 0.2941s, grad.norm=14.74564838
  5803: 4 [  495/ 1327], train_loss/perplexity = 4.55382061/94.9946518 secs/batch = 0.2959s, grad.norm=13.45683670
  5808: 4 [  500/ 1327], train_loss/perplexity = 4.84437132/127.0233994 secs/batch = 0.2956s, grad.norm=13.63236809
  5813: 4 [  505/ 1327], train_loss/perplexity = 4.86760616/130.0093231 secs/batch = 0.2959s, grad.norm=12.56984901
  5818: 4 [  510/ 1327], train_loss/perplexity = 5.27100134/194.6107330 secs/batch = 0.2981s, grad.norm=12.98095989
  5823: 4 [  515/ 1327], train_loss/perplexity = 4.83362770/125.6660156 secs/batch = 0.2947s, grad.norm=12.64758587
  5828: 4 [  520/ 1327], train_loss/perplexity = 5.04551554/155.3243561 secs/batch = 0.2990s, grad.norm=12.78037167
  5833: 4 [  525/ 1327], train_loss/perplexity = 4.57786703/97.3066177 secs/batch = 0.2959s, grad.norm=13.14865780
  5838: 4 [  530/ 1327], train_loss/perplexity = 4.64880371/104.4599457 secs/batch = 0.2928s, grad.norm=14.42352295
  5843: 4 [  535/ 1327], train_loss/perplexity = 4.77010536/117.9316635 secs/batch = 0.2994s, grad.norm=12.48662090
  5848: 4 [  540/ 1327], train_loss/perplexity = 4.85845041/128.8244171 secs/batch = 0.2985s, grad.norm=12.52886677
  5853: 4 [  545/ 1327], train_loss/perplexity = 4.89763784/133.9729462 secs/batch = 0.3015s, grad.norm=13.21448803
  5858: 4 [  550/ 1327], train_loss/perplexity = 4.82895184/125.0797882 secs/batch = 0.3000s, grad.norm=13.64371777
  5863: 4 [  555/ 1327], train_loss/perplexity = 4.59078121/98.5714035 secs/batch = 0.2932s, grad.norm=13.47416210
  5868: 4 [  560/ 1327], train_loss/perplexity = 4.78507900/119.7108231 secs/batch = 0.2929s, grad.norm=15.55743122
  5873: 4 [  565/ 1327], train_loss/perplexity = 4.67476273/107.2071228 secs/batch = 0.3013s, grad.norm=14.29920483
  5878: 4 [  570/ 1327], train_loss/perplexity = 4.56567860/96.1278076 secs/batch = 0.3001s, grad.norm=13.89965820
  5883: 4 [  575/ 1327], train_loss/perplexity = 4.48077250/88.3028564 secs/batch = 0.2945s, grad.norm=14.76989365
  5888: 4 [  580/ 1327], train_loss/perplexity = 4.86469698/129.6316528 secs/batch = 0.2921s, grad.norm=13.82639122
  5893: 4 [  585/ 1327], train_loss/perplexity = 4.36489391/78.6410599 secs/batch = 0.2951s, grad.norm=13.40832043
  5898: 4 [  590/ 1327], train_loss/perplexity = 4.74368286/114.8564224 secs/batch = 0.2962s, grad.norm=12.81525421
  5903: 4 [  595/ 1327], train_loss/perplexity = 4.71792746/111.9360199 secs/batch = 0.2941s, grad.norm=14.08060360
  5908: 4 [  600/ 1327], train_loss/perplexity = 4.97370195/144.5610504 secs/batch = 0.2956s, grad.norm=12.86865616
  5913: 4 [  605/ 1327], train_loss/perplexity = 4.85108852/127.8795166 secs/batch = 0.2951s, grad.norm=13.31776142
  5918: 4 [  610/ 1327], train_loss/perplexity = 5.02180624/151.6850433 secs/batch = 0.2951s, grad.norm=12.90805912
  5923: 4 [  615/ 1327], train_loss/perplexity = 4.55631638/95.2320328 secs/batch = 0.2984s, grad.norm=13.40274525
  5928: 4 [  620/ 1327], train_loss/perplexity = 4.86969471/130.2811432 secs/batch = 0.2948s, grad.norm=13.22589874
  5933: 4 [  625/ 1327], train_loss/perplexity = 4.87311506/130.7275085 secs/batch = 0.2961s, grad.norm=13.44169331
  5938: 4 [  630/ 1327], train_loss/perplexity = 5.02580309/152.2925110 secs/batch = 0.2953s, grad.norm=13.86900330
  5943: 4 [  635/ 1327], train_loss/perplexity = 4.74697542/115.2352219 secs/batch = 0.2957s, grad.norm=14.04456997
  5948: 4 [  640/ 1327], train_loss/perplexity = 4.76585484/117.4314575 secs/batch = 0.2924s, grad.norm=14.24210835
  5953: 4 [  645/ 1327], train_loss/perplexity = 5.00129938/148.6061249 secs/batch = 0.3003s, grad.norm=13.45007324
  5958: 4 [  650/ 1327], train_loss/perplexity = 4.51648521/91.5133820 secs/batch = 0.2990s, grad.norm=13.25724506
  5963: 4 [  655/ 1327], train_loss/perplexity = 4.69444847/109.3384857 secs/batch = 0.2964s, grad.norm=13.95672226
  5968: 4 [  660/ 1327], train_loss/perplexity = 4.62933588/102.4460068 secs/batch = 0.2920s, grad.norm=13.35418987
  5973: 4 [  665/ 1327], train_loss/perplexity = 4.77532911/118.5493240 secs/batch = 0.2959s, grad.norm=13.67737675
  5978: 4 [  670/ 1327], train_loss/perplexity = 4.67674208/107.4195404 secs/batch = 0.2958s, grad.norm=13.66262913
  5983: 4 [  675/ 1327], train_loss/perplexity = 4.51676893/91.5393524 secs/batch = 0.3017s, grad.norm=14.12216282
  5988: 4 [  680/ 1327], train_loss/perplexity = 4.76581097/117.4263077 secs/batch = 0.2924s, grad.norm=13.70031929
  5993: 4 [  685/ 1327], train_loss/perplexity = 4.64302826/103.8583832 secs/batch = 0.2972s, grad.norm=13.90666866
  5998: 4 [  690/ 1327], train_loss/perplexity = 4.95056868/141.2552643 secs/batch = 0.2945s, grad.norm=13.03618050
  6003: 4 [  695/ 1327], train_loss/perplexity = 4.74061680/114.5048065 secs/batch = 0.2990s, grad.norm=13.80984974
  6008: 4 [  700/ 1327], train_loss/perplexity = 5.03179502/153.2077789 secs/batch = 0.2935s, grad.norm=12.90851402
  6013: 4 [  705/ 1327], train_loss/perplexity = 4.69336748/109.2203598 secs/batch = 0.2934s, grad.norm=13.00483131
  6018: 4 [  710/ 1327], train_loss/perplexity = 4.66639900/106.3142166 secs/batch = 0.2979s, grad.norm=13.13665199
  6023: 4 [  715/ 1327], train_loss/perplexity = 4.64660692/104.2307205 secs/batch = 0.2940s, grad.norm=13.35167122
  6028: 4 [  720/ 1327], train_loss/perplexity = 4.58693743/98.1932449 secs/batch = 0.3004s, grad.norm=14.97635841
  6033: 4 [  725/ 1327], train_loss/perplexity = 4.50664043/90.6168747 secs/batch = 0.2995s, grad.norm=13.58347893
  6038: 4 [  730/ 1327], train_loss/perplexity = 4.78147507/119.2801666 secs/batch = 0.2950s, grad.norm=13.41797161
  6043: 4 [  735/ 1327], train_loss/perplexity = 4.83433723/125.7552109 secs/batch = 0.2987s, grad.norm=13.19693279
  6048: 4 [  740/ 1327], train_loss/perplexity = 4.15732002/63.9000435 secs/batch = 0.2967s, grad.norm=12.87359810
  6053: 4 [  745/ 1327], train_loss/perplexity = 4.76544094/117.3828659 secs/batch = 0.3002s, grad.norm=13.51422024
  6058: 4 [  750/ 1327], train_loss/perplexity = 4.57996416/97.5109024 secs/batch = 0.2941s, grad.norm=12.96051311
  6063: 4 [  755/ 1327], train_loss/perplexity = 4.43017006/83.9456940 secs/batch = 0.2947s, grad.norm=13.75195122
  6068: 4 [  760/ 1327], train_loss/perplexity = 4.40645409/81.9782562 secs/batch = 0.2956s, grad.norm=15.34168720
  6073: 4 [  765/ 1327], train_loss/perplexity = 4.47666883/87.9412384 secs/batch = 0.2982s, grad.norm=15.50612831
  6078: 4 [  770/ 1327], train_loss/perplexity = 4.39328814/80.9060135 secs/batch = 0.2946s, grad.norm=13.90234470
  6083: 4 [  775/ 1327], train_loss/perplexity = 4.58235121/97.7439423 secs/batch = 0.2988s, grad.norm=14.10919952
  6088: 4 [  780/ 1327], train_loss/perplexity = 4.91590643/136.4429321 secs/batch = 0.2960s, grad.norm=13.11431694
  6093: 4 [  785/ 1327], train_loss/perplexity = 4.69340515/109.2244720 secs/batch = 0.2958s, grad.norm=14.33536148
  6098: 4 [  790/ 1327], train_loss/perplexity = 4.53353786/93.0873108 secs/batch = 0.2948s, grad.norm=14.18216133
  6103: 4 [  795/ 1327], train_loss/perplexity = 4.85912657/128.9115601 secs/batch = 0.2943s, grad.norm=13.92802620
  6108: 4 [  800/ 1327], train_loss/perplexity = 4.78967905/120.2627640 secs/batch = 0.2961s, grad.norm=13.61481476
  6113: 4 [  805/ 1327], train_loss/perplexity = 5.11992264/167.3224182 secs/batch = 0.2961s, grad.norm=13.71186638
  6118: 4 [  810/ 1327], train_loss/perplexity = 4.73060465/113.3640900 secs/batch = 0.2969s, grad.norm=12.94431210
  6123: 4 [  815/ 1327], train_loss/perplexity = 4.63436127/102.9621353 secs/batch = 0.2947s, grad.norm=13.07890129
  6128: 4 [  820/ 1327], train_loss/perplexity = 4.39999199/81.4502182 secs/batch = 0.3006s, grad.norm=12.87209797
  6133: 4 [  825/ 1327], train_loss/perplexity = 4.57641935/97.1658554 secs/batch = 0.3025s, grad.norm=13.49450111
  6138: 4 [  830/ 1327], train_loss/perplexity = 4.40363693/81.7476425 secs/batch = 0.2945s, grad.norm=14.03779125
  6143: 4 [  835/ 1327], train_loss/perplexity = 4.70908642/110.9507523 secs/batch = 0.2948s, grad.norm=13.80819035
  6148: 4 [  840/ 1327], train_loss/perplexity = 4.75540018/116.2101517 secs/batch = 0.2990s, grad.norm=13.89537430
  6153: 4 [  845/ 1327], train_loss/perplexity = 4.63748503/103.2842636 secs/batch = 0.2994s, grad.norm=13.91847515
  6158: 4 [  850/ 1327], train_loss/perplexity = 4.60045290/99.5293808 secs/batch = 0.3005s, grad.norm=13.12720776
  6163: 4 [  855/ 1327], train_loss/perplexity = 4.67062950/106.7649307 secs/batch = 0.2962s, grad.norm=14.16383457
  6168: 4 [  860/ 1327], train_loss/perplexity = 4.33680201/76.4626236 secs/batch = 0.2936s, grad.norm=12.74520206
  6173: 4 [  865/ 1327], train_loss/perplexity = 4.91507149/136.3290558 secs/batch = 0.2955s, grad.norm=13.41650677
  6178: 4 [  870/ 1327], train_loss/perplexity = 4.83896589/126.3386383 secs/batch = 0.2949s, grad.norm=13.17405701
  6183: 4 [  875/ 1327], train_loss/perplexity = 4.27820539/72.1109161 secs/batch = 0.2974s, grad.norm=13.22049713
  6188: 4 [  880/ 1327], train_loss/perplexity = 4.52440453/92.2409821 secs/batch = 0.2960s, grad.norm=12.47839355
  6193: 4 [  885/ 1327], train_loss/perplexity = 4.70659781/110.6749802 secs/batch = 0.2936s, grad.norm=12.92346764
  6198: 4 [  890/ 1327], train_loss/perplexity = 4.80944681/122.6637421 secs/batch = 0.2929s, grad.norm=13.17963696
  6203: 4 [  895/ 1327], train_loss/perplexity = 4.84450817/127.0407867 secs/batch = 0.2993s, grad.norm=13.36420441
  6208: 4 [  900/ 1327], train_loss/perplexity = 4.68903494/108.7481842 secs/batch = 0.2958s, grad.norm=14.40703106
  6213: 4 [  905/ 1327], train_loss/perplexity = 4.52112150/91.9386520 secs/batch = 0.2940s, grad.norm=13.36892986
  6218: 4 [  910/ 1327], train_loss/perplexity = 4.49993467/90.0112534 secs/batch = 0.2996s, grad.norm=13.55328083
  6223: 4 [  915/ 1327], train_loss/perplexity = 4.79700518/121.1470566 secs/batch = 0.3005s, grad.norm=13.17811203
  6228: 4 [  920/ 1327], train_loss/perplexity = 4.98910999/146.8057098 secs/batch = 0.2998s, grad.norm=12.86880875
  6233: 4 [  925/ 1327], train_loss/perplexity = 4.81635857/123.5145035 secs/batch = 0.3022s, grad.norm=12.84517384
  6238: 4 [  930/ 1327], train_loss/perplexity = 4.74484348/114.9898071 secs/batch = 0.2941s, grad.norm=13.22662163
  6243: 4 [  935/ 1327], train_loss/perplexity = 4.80794525/122.4796906 secs/batch = 0.2954s, grad.norm=12.87353706
  6248: 4 [  940/ 1327], train_loss/perplexity = 4.81341362/123.1512909 secs/batch = 0.2959s, grad.norm=13.84861755
  6253: 4 [  945/ 1327], train_loss/perplexity = 4.94870996/140.9929657 secs/batch = 0.2999s, grad.norm=12.64057732
  6258: 4 [  950/ 1327], train_loss/perplexity = 4.81733370/123.6350021 secs/batch = 0.2960s, grad.norm=13.92243195
  6263: 4 [  955/ 1327], train_loss/perplexity = 4.72759247/113.0231247 secs/batch = 0.3003s, grad.norm=13.14833736
  6268: 4 [  960/ 1327], train_loss/perplexity = 5.02874374/152.7410126 secs/batch = 0.2947s, grad.norm=13.31652737
  6273: 4 [  965/ 1327], train_loss/perplexity = 4.89965963/134.2440796 secs/batch = 0.2950s, grad.norm=13.67545986
  6278: 4 [  970/ 1327], train_loss/perplexity = 5.01092434/150.0433655 secs/batch = 0.2967s, grad.norm=13.60627174
  6283: 4 [  975/ 1327], train_loss/perplexity = 4.80466080/122.0780716 secs/batch = 0.2969s, grad.norm=13.87128258
  6288: 4 [  980/ 1327], train_loss/perplexity = 4.53933334/93.6283646 secs/batch = 0.2943s, grad.norm=12.86073971
  6293: 4 [  985/ 1327], train_loss/perplexity = 4.65663290/105.2809906 secs/batch = 0.2995s, grad.norm=14.11441898
  6298: 4 [  990/ 1327], train_loss/perplexity = 4.99232817/147.2789154 secs/batch = 0.2917s, grad.norm=13.74382687
  6303: 4 [  995/ 1327], train_loss/perplexity = 4.92497587/137.6860199 secs/batch = 0.2966s, grad.norm=13.36571312
  6308: 4 [ 1000/ 1327], train_loss/perplexity = 4.37043667/79.0781555 secs/batch = 0.2969s, grad.norm=12.64448166
  6313: 4 [ 1005/ 1327], train_loss/perplexity = 4.88592243/132.4125519 secs/batch = 0.2957s, grad.norm=14.07846928
  6318: 4 [ 1010/ 1327], train_loss/perplexity = 4.38619518/80.3341827 secs/batch = 0.2959s, grad.norm=12.43962193
  6323: 4 [ 1015/ 1327], train_loss/perplexity = 4.94603825/140.6167755 secs/batch = 0.2994s, grad.norm=12.89613724
  6328: 4 [ 1020/ 1327], train_loss/perplexity = 5.10635376/165.0673828 secs/batch = 0.2937s, grad.norm=12.63776493
  6333: 4 [ 1025/ 1327], train_loss/perplexity = 4.89080238/133.0603027 secs/batch = 0.2946s, grad.norm=12.79846954
  6338: 4 [ 1030/ 1327], train_loss/perplexity = 4.71367216/111.4607086 secs/batch = 0.2992s, grad.norm=12.99491310
  6343: 4 [ 1035/ 1327], train_loss/perplexity = 4.62852049/102.3625031 secs/batch = 0.2964s, grad.norm=12.54038906
  6348: 4 [ 1040/ 1327], train_loss/perplexity = 4.90957928/135.5823669 secs/batch = 0.2948s, grad.norm=13.34502792
  6353: 4 [ 1045/ 1327], train_loss/perplexity = 4.44756889/85.4190292 secs/batch = 0.3009s, grad.norm=13.50045872
  6358: 4 [ 1050/ 1327], train_loss/perplexity = 4.50074911/90.0845871 secs/batch = 0.3001s, grad.norm=13.86939716
  6363: 4 [ 1055/ 1327], train_loss/perplexity = 4.61109781/100.5945206 secs/batch = 0.3002s, grad.norm=14.69398403
  6368: 4 [ 1060/ 1327], train_loss/perplexity = 4.30803585/74.2944183 secs/batch = 0.2960s, grad.norm=14.28319550
  6373: 4 [ 1065/ 1327], train_loss/perplexity = 4.46139097/86.6078949 secs/batch = 0.2990s, grad.norm=13.27873802
  6378: 4 [ 1070/ 1327], train_loss/perplexity = 4.76020336/116.7696686 secs/batch = 0.2940s, grad.norm=13.93207550
  6383: 4 [ 1075/ 1327], train_loss/perplexity = 4.47418165/87.7227859 secs/batch = 0.2941s, grad.norm=13.73187351
  6388: 4 [ 1080/ 1327], train_loss/perplexity = 4.49688721/89.7373657 secs/batch = 0.2958s, grad.norm=13.47533989
  6393: 4 [ 1085/ 1327], train_loss/perplexity = 4.33426476/76.2688599 secs/batch = 0.2947s, grad.norm=13.70906639
  6398: 4 [ 1090/ 1327], train_loss/perplexity = 4.51478910/91.3582993 secs/batch = 0.2945s, grad.norm=13.52101612
  6403: 4 [ 1095/ 1327], train_loss/perplexity = 4.63105249/102.6220169 secs/batch = 0.2931s, grad.norm=13.48443985
  6408: 4 [ 1100/ 1327], train_loss/perplexity = 4.39069033/80.6961060 secs/batch = 0.2949s, grad.norm=15.26112270
  6413: 4 [ 1105/ 1327], train_loss/perplexity = 4.36459064/78.6172104 secs/batch = 0.2950s, grad.norm=13.98747063
  6418: 4 [ 1110/ 1327], train_loss/perplexity = 4.76232243/117.0173721 secs/batch = 0.3003s, grad.norm=14.24072266
  6423: 4 [ 1115/ 1327], train_loss/perplexity = 4.47633171/87.9115982 secs/batch = 0.2941s, grad.norm=13.90451813
  6428: 4 [ 1120/ 1327], train_loss/perplexity = 4.70055866/110.0086136 secs/batch = 0.2956s, grad.norm=12.91719246
  6433: 4 [ 1125/ 1327], train_loss/perplexity = 4.89788008/134.0054016 secs/batch = 0.3013s, grad.norm=13.92972755
  6438: 4 [ 1130/ 1327], train_loss/perplexity = 4.60122681/99.6064377 secs/batch = 0.2988s, grad.norm=14.79445171
  6443: 4 [ 1135/ 1327], train_loss/perplexity = 4.60434198/99.9172134 secs/batch = 0.2964s, grad.norm=13.55362129
  6448: 4 [ 1140/ 1327], train_loss/perplexity = 4.84596252/127.2256775 secs/batch = 0.2954s, grad.norm=13.75174904
  6453: 4 [ 1145/ 1327], train_loss/perplexity = 4.64794350/104.3701248 secs/batch = 0.2987s, grad.norm=13.55305004
  6458: 4 [ 1150/ 1327], train_loss/perplexity = 4.61548996/101.0373230 secs/batch = 0.2959s, grad.norm=13.82952976
  6463: 4 [ 1155/ 1327], train_loss/perplexity = 4.71955395/112.1182327 secs/batch = 0.2944s, grad.norm=14.28995228
  6468: 4 [ 1160/ 1327], train_loss/perplexity = 4.69368029/109.2545319 secs/batch = 0.3007s, grad.norm=13.99766254
  6473: 4 [ 1165/ 1327], train_loss/perplexity = 4.70238113/110.2092819 secs/batch = 0.2950s, grad.norm=13.80031395
  6478: 4 [ 1170/ 1327], train_loss/perplexity = 4.61940384/101.4335403 secs/batch = 0.3009s, grad.norm=13.56939411
  6483: 4 [ 1175/ 1327], train_loss/perplexity = 4.26972961/71.5022964 secs/batch = 0.2944s, grad.norm=13.48475456
  6488: 4 [ 1180/ 1327], train_loss/perplexity = 4.42359638/83.3956680 secs/batch = 0.3001s, grad.norm=14.78971672
  6493: 4 [ 1185/ 1327], train_loss/perplexity = 4.60323095/99.8062668 secs/batch = 0.2942s, grad.norm=13.83987808
  6498: 4 [ 1190/ 1327], train_loss/perplexity = 4.59369802/98.8593369 secs/batch = 0.2993s, grad.norm=13.23749352
  6503: 4 [ 1195/ 1327], train_loss/perplexity = 4.43923044/84.7097244 secs/batch = 0.2965s, grad.norm=13.74291039
  6508: 4 [ 1200/ 1327], train_loss/perplexity = 4.41852331/82.9736710 secs/batch = 0.2957s, grad.norm=13.53349304
  6513: 4 [ 1205/ 1327], train_loss/perplexity = 4.40675879/82.0032425 secs/batch = 0.2955s, grad.norm=14.18381596
  6518: 4 [ 1210/ 1327], train_loss/perplexity = 4.12787676/62.0460434 secs/batch = 0.2933s, grad.norm=13.99929142
  6523: 4 [ 1215/ 1327], train_loss/perplexity = 4.33263159/76.1444016 secs/batch = 0.2954s, grad.norm=13.64548397
  6528: 4 [ 1220/ 1327], train_loss/perplexity = 4.50796986/90.7374191 secs/batch = 0.2949s, grad.norm=13.93779469
  6533: 4 [ 1225/ 1327], train_loss/perplexity = 4.20273399/66.8689041 secs/batch = 0.2945s, grad.norm=14.98082161
  6538: 4 [ 1230/ 1327], train_loss/perplexity = 4.46154928/86.6216049 secs/batch = 0.2997s, grad.norm=13.69196033
  6543: 4 [ 1235/ 1327], train_loss/perplexity = 4.40159702/81.5810547 secs/batch = 0.2991s, grad.norm=13.75035477
  6548: 4 [ 1240/ 1327], train_loss/perplexity = 4.59440804/98.9295578 secs/batch = 0.2952s, grad.norm=14.08234692
  6553: 4 [ 1245/ 1327], train_loss/perplexity = 4.54468393/94.1306686 secs/batch = 0.3010s, grad.norm=13.33800411
  6558: 4 [ 1250/ 1327], train_loss/perplexity = 4.67016363/106.7152023 secs/batch = 0.2938s, grad.norm=12.90226555
  6563: 4 [ 1255/ 1327], train_loss/perplexity = 4.73641777/114.0250092 secs/batch = 0.2982s, grad.norm=12.80764008
  6568: 4 [ 1260/ 1327], train_loss/perplexity = 4.55430126/95.0403214 secs/batch = 0.2972s, grad.norm=13.87112713
  6573: 4 [ 1265/ 1327], train_loss/perplexity = 4.70664215/110.6798859 secs/batch = 0.2940s, grad.norm=14.10019875
  6578: 4 [ 1270/ 1327], train_loss/perplexity = 4.48165417/88.3807449 secs/batch = 0.2968s, grad.norm=13.61176777
  6583: 4 [ 1275/ 1327], train_loss/perplexity = 4.74322844/114.8042450 secs/batch = 0.2941s, grad.norm=14.79263783
  6588: 4 [ 1280/ 1327], train_loss/perplexity = 4.46536255/86.9525452 secs/batch = 0.2955s, grad.norm=13.56332207
  6593: 4 [ 1285/ 1327], train_loss/perplexity = 4.40947819/82.2265472 secs/batch = 0.2946s, grad.norm=13.49328136
  6598: 4 [ 1290/ 1327], train_loss/perplexity = 4.65183640/104.7772217 secs/batch = 0.2942s, grad.norm=13.37769985
  6603: 4 [ 1295/ 1327], train_loss/perplexity = 4.63049412/102.5647278 secs/batch = 0.2948s, grad.norm=13.79362774
  6608: 4 [ 1300/ 1327], train_loss/perplexity = 4.79195261/120.5364990 secs/batch = 0.2947s, grad.norm=13.37761593
  6613: 4 [ 1305/ 1327], train_loss/perplexity = 4.97714186/145.0591888 secs/batch = 0.2943s, grad.norm=14.15338039
  6618: 4 [ 1310/ 1327], train_loss/perplexity = 5.16775417/175.5202026 secs/batch = 0.3009s, grad.norm=13.81955051
  6623: 4 [ 1315/ 1327], train_loss/perplexity = 4.97359085/144.5449982 secs/batch = 0.2933s, grad.norm=13.84706783
  6628: 4 [ 1320/ 1327], train_loss/perplexity = 4.90434313/134.8742828 secs/batch = 0.2979s, grad.norm=13.16839218
  6633: 4 [ 1325/ 1327], train_loss/perplexity = 4.84966755/127.6979294 secs/batch = 0.2951s, grad.norm=13.20643234
Epoch training time: 393.0559594631195
	> validation loss = 4.93593168, perplexity = 139.20277405
	> validation loss = 4.82057190, perplexity = 124.03600311
	> validation loss = 4.76498938, perplexity = 117.32987213
	> validation loss = 4.74738503, perplexity = 115.28243256
	> validation loss = 4.96440125, perplexity = 143.22276306
	> validation loss = 4.83079433, perplexity = 125.31046295
	> validation loss = 4.83203793, perplexity = 125.46639252
	> validation loss = 4.72154331, perplexity = 112.34149933
	> validation loss = 4.55301857, perplexity = 94.91849518
	> validation loss = 4.57987595, perplexity = 97.50229645
	> validation loss = 4.71489048, perplexity = 111.59658813
	> validation loss = 4.77914000, perplexity = 119.00196075
	> validation loss = 4.67183638, perplexity = 106.89385986
	> validation loss = 4.56643772, perplexity = 96.20080566
	> validation loss = 4.43620253, perplexity = 84.45362091
	> validation loss = 4.44788933, perplexity = 85.44640350
	> validation loss = 4.88426399, perplexity = 132.19313049
	> validation loss = 4.45111465, perplexity = 85.72244263
	> validation loss = 4.85237312, perplexity = 128.04389954
	> validation loss = 4.74766302, perplexity = 115.31448364
	> validation loss = 4.55695820, perplexity = 95.29317474
at the end of epoch: 4
train loss = 4.76735049, perplexity = 117.60722737
validation loss = 4.70705684, perplexity = 110.72579566
Saved model cv/epoch004_4.7071.model
  6640: 5 [    5/ 1327], train_loss/perplexity = 4.90422916/134.8589172 secs/batch = 0.2945s, grad.norm=13.32238579
  6645: 5 [   10/ 1327], train_loss/perplexity = 4.43799210/84.6048889 secs/batch = 0.3004s, grad.norm=13.37780952
  6650: 5 [   15/ 1327], train_loss/perplexity = 4.78478670/119.6758347 secs/batch = 0.2944s, grad.norm=13.00021458
  6655: 5 [   20/ 1327], train_loss/perplexity = 4.86181211/129.2582245 secs/batch = 0.2946s, grad.norm=13.64714432
  6660: 5 [   25/ 1327], train_loss/perplexity = 4.81571198/123.4346619 secs/batch = 0.2924s, grad.norm=13.68047237
  6665: 5 [   30/ 1327], train_loss/perplexity = 4.73290062/113.6246643 secs/batch = 0.2954s, grad.norm=13.44327164
  6670: 5 [   35/ 1327], train_loss/perplexity = 4.60497093/99.9800797 secs/batch = 0.2998s, grad.norm=12.72216415
  6675: 5 [   40/ 1327], train_loss/perplexity = 4.57517958/97.0454636 secs/batch = 0.2933s, grad.norm=14.04317379
  6680: 5 [   45/ 1327], train_loss/perplexity = 4.39889097/81.3605881 secs/batch = 0.2933s, grad.norm=12.86831093
  6685: 5 [   50/ 1327], train_loss/perplexity = 4.60164070/99.6476746 secs/batch = 0.2951s, grad.norm=13.07102394
  6690: 5 [   55/ 1327], train_loss/perplexity = 4.55070448/94.6990967 secs/batch = 0.2948s, grad.norm=13.74273109
  6695: 5 [   60/ 1327], train_loss/perplexity = 4.77162123/118.1105728 secs/batch = 0.2947s, grad.norm=13.42733669
  6700: 5 [   65/ 1327], train_loss/perplexity = 4.41796255/82.9271545 secs/batch = 0.2944s, grad.norm=13.47336102
  6705: 5 [   70/ 1327], train_loss/perplexity = 4.28075886/72.2952805 secs/batch = 0.2942s, grad.norm=13.86999130
  6710: 5 [   75/ 1327], train_loss/perplexity = 4.13222647/62.3165131 secs/batch = 0.2947s, grad.norm=13.97149181
  6715: 5 [   80/ 1327], train_loss/perplexity = 4.63465738/102.9926224 secs/batch = 0.2940s, grad.norm=14.33962727
  6720: 5 [   85/ 1327], train_loss/perplexity = 4.61848831/101.3407211 secs/batch = 0.2989s, grad.norm=13.43949509
  6725: 5 [   90/ 1327], train_loss/perplexity = 4.62638760/102.1444092 secs/batch = 0.2958s, grad.norm=14.12452126
  6730: 5 [   95/ 1327], train_loss/perplexity = 4.46779394/87.1642227 secs/batch = 0.3006s, grad.norm=13.68480015
  6735: 5 [  100/ 1327], train_loss/perplexity = 4.78580475/119.7977295 secs/batch = 0.2949s, grad.norm=14.27205181
  6740: 5 [  105/ 1327], train_loss/perplexity = 4.68213749/108.0006790 secs/batch = 0.2953s, grad.norm=14.66103745
  6745: 5 [  110/ 1327], train_loss/perplexity = 4.51652813/91.5173111 secs/batch = 0.2935s, grad.norm=13.55260181
  6750: 5 [  115/ 1327], train_loss/perplexity = 4.45287228/85.8732452 secs/batch = 0.2956s, grad.norm=14.67321301
  6755: 5 [  120/ 1327], train_loss/perplexity = 4.54547024/94.2047119 secs/batch = 0.2936s, grad.norm=15.99322796
  6760: 5 [  125/ 1327], train_loss/perplexity = 4.68174314/107.9580917 secs/batch = 0.3012s, grad.norm=14.15140247
  6765: 5 [  130/ 1327], train_loss/perplexity = 4.55777502/95.3710480 secs/batch = 0.2944s, grad.norm=13.72162914
  6770: 5 [  135/ 1327], train_loss/perplexity = 4.52238274/92.0546799 secs/batch = 0.2988s, grad.norm=13.18142414
  6775: 5 [  140/ 1327], train_loss/perplexity = 4.83503819/125.8433914 secs/batch = 0.3008s, grad.norm=13.35426521
  6780: 5 [  145/ 1327], train_loss/perplexity = 4.74958038/115.5357971 secs/batch = 0.2963s, grad.norm=14.80652332
  6785: 5 [  150/ 1327], train_loss/perplexity = 4.69854259/109.7870483 secs/batch = 0.2951s, grad.norm=14.00819206
  6790: 5 [  155/ 1327], train_loss/perplexity = 5.02275419/151.8288879 secs/batch = 0.2999s, grad.norm=12.95280075
  6795: 5 [  160/ 1327], train_loss/perplexity = 4.65448141/105.0547256 secs/batch = 0.2982s, grad.norm=13.12588501
  6800: 5 [  165/ 1327], train_loss/perplexity = 4.81636143/123.5148544 secs/batch = 0.2968s, grad.norm=13.06311893
  6805: 5 [  170/ 1327], train_loss/perplexity = 4.63356876/102.8805695 secs/batch = 0.2946s, grad.norm=13.23099899
  6810: 5 [  175/ 1327], train_loss/perplexity = 4.87180233/130.5560150 secs/batch = 0.2988s, grad.norm=13.35543919
  6815: 5 [  180/ 1327], train_loss/perplexity = 4.73209810/113.5335159 secs/batch = 0.3004s, grad.norm=13.93239307
  6820: 5 [  185/ 1327], train_loss/perplexity = 5.04762173/155.6518402 secs/batch = 0.2933s, grad.norm=13.58611679
  6825: 5 [  190/ 1327], train_loss/perplexity = 4.54977560/94.6111755 secs/batch = 0.2997s, grad.norm=12.95633793
  6830: 5 [  195/ 1327], train_loss/perplexity = 4.80668449/122.3253708 secs/batch = 0.2957s, grad.norm=13.04037857
  6835: 5 [  200/ 1327], train_loss/perplexity = 4.72717810/112.9763031 secs/batch = 0.2941s, grad.norm=13.69412708
  6840: 5 [  205/ 1327], train_loss/perplexity = 4.86348200/129.4742432 secs/batch = 0.2938s, grad.norm=13.41600227
  6845: 5 [  210/ 1327], train_loss/perplexity = 4.69684839/109.6012039 secs/batch = 0.2959s, grad.norm=13.12426949
  6850: 5 [  215/ 1327], train_loss/perplexity = 4.89874935/134.1219330 secs/batch = 0.2955s, grad.norm=13.57888412
  6855: 5 [  220/ 1327], train_loss/perplexity = 4.83295679/125.5817337 secs/batch = 0.3025s, grad.norm=13.12927532
  6860: 5 [  225/ 1327], train_loss/perplexity = 4.93562222/139.1596985 secs/batch = 0.2949s, grad.norm=13.12676716
  6865: 5 [  230/ 1327], train_loss/perplexity = 4.83728313/126.1262207 secs/batch = 0.2941s, grad.norm=13.50995159
  6870: 5 [  235/ 1327], train_loss/perplexity = 4.64348745/103.9060822 secs/batch = 0.2945s, grad.norm=12.99823475
  6875: 5 [  240/ 1327], train_loss/perplexity = 4.47789717/88.0493240 secs/batch = 0.2949s, grad.norm=14.33281040
  6880: 5 [  245/ 1327], train_loss/perplexity = 4.84377575/126.9477692 secs/batch = 0.3014s, grad.norm=14.53705502
  6885: 5 [  250/ 1327], train_loss/perplexity = 4.52292395/92.1045151 secs/batch = 0.2967s, grad.norm=13.18626404
  6890: 5 [  255/ 1327], train_loss/perplexity = 4.56970692/96.5158157 secs/batch = 0.2972s, grad.norm=13.73047829
  6895: 5 [  260/ 1327], train_loss/perplexity = 4.87235832/130.6286163 secs/batch = 0.3025s, grad.norm=14.95134735
  6900: 5 [  265/ 1327], train_loss/perplexity = 4.95630503/142.0678864 secs/batch = 0.2971s, grad.norm=12.73789597
  6905: 5 [  270/ 1327], train_loss/perplexity = 4.99654579/147.9013977 secs/batch = 0.3005s, grad.norm=13.77063847
  6910: 5 [  275/ 1327], train_loss/perplexity = 5.01941967/151.3234558 secs/batch = 0.2951s, grad.norm=14.07534981
  6915: 5 [  280/ 1327], train_loss/perplexity = 4.76933670/117.8410492 secs/batch = 0.2929s, grad.norm=13.21369743
  6920: 5 [  285/ 1327], train_loss/perplexity = 5.07394695/159.8038177 secs/batch = 0.2959s, grad.norm=13.02472878
  6925: 5 [  290/ 1327], train_loss/perplexity = 4.81677294/123.5656891 secs/batch = 0.2986s, grad.norm=13.60509968
  6930: 5 [  295/ 1327], train_loss/perplexity = 4.58873510/98.3699265 secs/batch = 0.3005s, grad.norm=13.46747684
  6935: 5 [  300/ 1327], train_loss/perplexity = 4.24657822/69.8659363 secs/batch = 0.3005s, grad.norm=13.23691940
  6940: 5 [  305/ 1327], train_loss/perplexity = 4.57799006/97.3185959 secs/batch = 0.3001s, grad.norm=13.12686539
  6945: 5 [  310/ 1327], train_loss/perplexity = 4.69355583/109.2409286 secs/batch = 0.2954s, grad.norm=13.56391144
  6950: 5 [  315/ 1327], train_loss/perplexity = 4.22825909/68.5977020 secs/batch = 0.2991s, grad.norm=13.70267868
  6955: 5 [  320/ 1327], train_loss/perplexity = 4.21325302/67.5760117 secs/batch = 0.3010s, grad.norm=14.95637989
  6960: 5 [  325/ 1327], train_loss/perplexity = 4.20293045/66.8820419 secs/batch = 0.2954s, grad.norm=12.75046635
  6965: 5 [  330/ 1327], train_loss/perplexity = 4.72596645/112.8395004 secs/batch = 0.2969s, grad.norm=14.44616985
  6970: 5 [  335/ 1327], train_loss/perplexity = 4.18352890/65.5969315 secs/batch = 0.3004s, grad.norm=13.20551968
  6975: 5 [  340/ 1327], train_loss/perplexity = 4.82238054/124.2605438 secs/batch = 0.2955s, grad.norm=12.92118931
  6980: 5 [  345/ 1327], train_loss/perplexity = 4.66313934/105.9682312 secs/batch = 0.3016s, grad.norm=12.98400688
  6985: 5 [  350/ 1327], train_loss/perplexity = 4.77471495/118.4765396 secs/batch = 0.2940s, grad.norm=14.19739056
  6990: 5 [  355/ 1327], train_loss/perplexity = 4.79446459/120.8396683 secs/batch = 0.2942s, grad.norm=13.50881004
  6995: 5 [  360/ 1327], train_loss/perplexity = 4.86921024/130.2180328 secs/batch = 0.2929s, grad.norm=13.82384682
  7000: 5 [  365/ 1327], train_loss/perplexity = 4.81354666/123.1676788 secs/batch = 0.2957s, grad.norm=13.14438725
  7005: 5 [  370/ 1327], train_loss/perplexity = 4.83612633/125.9804001 secs/batch = 0.2938s, grad.norm=13.41091537
  7010: 5 [  375/ 1327], train_loss/perplexity = 4.29348278/73.2210388 secs/batch = 0.3011s, grad.norm=13.92431736
  7015: 5 [  380/ 1327], train_loss/perplexity = 4.40715694/82.0358963 secs/batch = 0.2987s, grad.norm=14.35398769
  7020: 5 [  385/ 1327], train_loss/perplexity = 4.60788202/100.2715530 secs/batch = 0.3017s, grad.norm=14.30024624
  7025: 5 [  390/ 1327], train_loss/perplexity = 4.67341375/107.0626068 secs/batch = 0.2960s, grad.norm=13.16997814
  7030: 5 [  395/ 1327], train_loss/perplexity = 4.77089500/118.0248260 secs/batch = 0.2992s, grad.norm=13.39435196
  7035: 5 [  400/ 1327], train_loss/perplexity = 4.62252903/101.7510376 secs/batch = 0.2953s, grad.norm=13.07231998
  7040: 5 [  405/ 1327], train_loss/perplexity = 4.94844675/140.9558563 secs/batch = 0.2991s, grad.norm=13.76965714
  7045: 5 [  410/ 1327], train_loss/perplexity = 4.64164495/103.7148132 secs/batch = 0.2955s, grad.norm=13.67867947
  7050: 5 [  415/ 1327], train_loss/perplexity = 4.50511885/90.4790955 secs/batch = 0.2980s, grad.norm=13.59068966
  7055: 5 [  420/ 1327], train_loss/perplexity = 4.24883890/70.0240631 secs/batch = 0.2958s, grad.norm=14.32603359
  7060: 5 [  425/ 1327], train_loss/perplexity = 4.47783756/88.0440750 secs/batch = 0.2976s, grad.norm=14.92088223
  7065: 5 [  430/ 1327], train_loss/perplexity = 4.78465414/119.6599731 secs/batch = 0.2959s, grad.norm=14.35477257
  7070: 5 [  435/ 1327], train_loss/perplexity = 4.74455357/114.9564743 secs/batch = 0.2944s, grad.norm=13.97194290
  7075: 5 [  440/ 1327], train_loss/perplexity = 4.47650194/87.9265594 secs/batch = 0.2996s, grad.norm=15.45133495
  7080: 5 [  445/ 1327], train_loss/perplexity = 4.74821377/115.3780060 secs/batch = 0.2944s, grad.norm=14.05902863
  7085: 5 [  450/ 1327], train_loss/perplexity = 4.59373093/98.8625946 secs/batch = 0.2952s, grad.norm=13.86880398
  7090: 5 [  455/ 1327], train_loss/perplexity = 4.54839325/94.4804764 secs/batch = 0.3006s, grad.norm=13.61057758
  7095: 5 [  460/ 1327], train_loss/perplexity = 4.60904598/100.3883286 secs/batch = 0.2963s, grad.norm=13.95533466
  7100: 5 [  465/ 1327], train_loss/perplexity = 4.33527374/76.3458557 secs/batch = 0.2970s, grad.norm=14.51787758
  7105: 5 [  470/ 1327], train_loss/perplexity = 4.95617342/142.0491943 secs/batch = 0.2942s, grad.norm=13.24297047
  7110: 5 [  475/ 1327], train_loss/perplexity = 4.45554447/86.1030197 secs/batch = 0.2959s, grad.norm=14.34657288
  7115: 5 [  480/ 1327], train_loss/perplexity = 4.66337109/105.9927902 secs/batch = 0.3005s, grad.norm=14.56174469
  7120: 5 [  485/ 1327], train_loss/perplexity = 4.76302958/117.1001511 secs/batch = 0.2950s, grad.norm=20.76061821
  7125: 5 [  490/ 1327], train_loss/perplexity = 4.38748121/80.4375610 secs/batch = 0.2934s, grad.norm=15.04339886
  7130: 5 [  495/ 1327], train_loss/perplexity = 4.47527122/87.8184128 secs/batch = 0.2951s, grad.norm=14.20746326
  7135: 5 [  500/ 1327], train_loss/perplexity = 4.87270832/130.6743469 secs/batch = 0.3003s, grad.norm=14.93758488
  7140: 5 [  505/ 1327], train_loss/perplexity = 4.77945328/119.0392532 secs/batch = 0.3008s, grad.norm=12.57495975
  7145: 5 [  510/ 1327], train_loss/perplexity = 5.15982294/174.1336212 secs/batch = 0.2943s, grad.norm=12.43681622
  7150: 5 [  515/ 1327], train_loss/perplexity = 4.75417662/116.0680466 secs/batch = 0.2990s, grad.norm=13.26714230
  7155: 5 [  520/ 1327], train_loss/perplexity = 4.99291611/147.3655243 secs/batch = 0.2963s, grad.norm=13.15958881
  7160: 5 [  525/ 1327], train_loss/perplexity = 4.48854923/88.9922485 secs/batch = 0.2944s, grad.norm=13.43985653
  7165: 5 [  530/ 1327], train_loss/perplexity = 4.50727749/90.6746216 secs/batch = 0.2939s, grad.norm=14.44537067
  7170: 5 [  535/ 1327], train_loss/perplexity = 4.66880035/106.5698166 secs/batch = 0.2997s, grad.norm=13.52473640
  7175: 5 [  540/ 1327], train_loss/perplexity = 4.76740789/117.6139755 secs/batch = 0.2952s, grad.norm=13.12880325
  7180: 5 [  545/ 1327], train_loss/perplexity = 4.75479317/116.1396255 secs/batch = 0.2945s, grad.norm=13.56617641
  7185: 5 [  550/ 1327], train_loss/perplexity = 4.72922659/113.2079697 secs/batch = 0.2937s, grad.norm=13.67302036
  7190: 5 [  555/ 1327], train_loss/perplexity = 4.58018827/97.5327530 secs/batch = 0.2939s, grad.norm=13.88242626
  7195: 5 [  560/ 1327], train_loss/perplexity = 4.63540459/103.0696106 secs/batch = 0.3008s, grad.norm=14.89429665
  7200: 5 [  565/ 1327], train_loss/perplexity = 4.63837004/103.3757095 secs/batch = 0.3001s, grad.norm=14.84162426
  7205: 5 [  570/ 1327], train_loss/perplexity = 4.56056643/95.6376343 secs/batch = 0.2974s, grad.norm=14.92830849
  7210: 5 [  575/ 1327], train_loss/perplexity = 4.38427305/80.1799164 secs/batch = 0.2946s, grad.norm=14.28064537
  7215: 5 [  580/ 1327], train_loss/perplexity = 4.76002455/116.7487946 secs/batch = 0.2979s, grad.norm=13.77115250
  7220: 5 [  585/ 1327], train_loss/perplexity = 4.29399204/73.2583389 secs/batch = 0.2969s, grad.norm=14.06281090
  7225: 5 [  590/ 1327], train_loss/perplexity = 4.62628412/102.1338425 secs/batch = 0.2996s, grad.norm=13.40135288
  7230: 5 [  595/ 1327], train_loss/perplexity = 4.59803343/99.2888641 secs/batch = 0.2943s, grad.norm=14.65588760
  7235: 5 [  600/ 1327], train_loss/perplexity = 4.91257715/135.9894257 secs/batch = 0.2991s, grad.norm=13.12745094
  7240: 5 [  605/ 1327], train_loss/perplexity = 4.73007870/113.3044815 secs/batch = 0.2981s, grad.norm=14.68583393
  7245: 5 [  610/ 1327], train_loss/perplexity = 4.86313486/129.4293060 secs/batch = 0.2950s, grad.norm=14.15835381
  7250: 5 [  615/ 1327], train_loss/perplexity = 4.48849678/88.9875793 secs/batch = 0.2930s, grad.norm=13.34875107
  7255: 5 [  620/ 1327], train_loss/perplexity = 4.78424072/119.6105118 secs/batch = 0.2988s, grad.norm=13.64241219
  7260: 5 [  625/ 1327], train_loss/perplexity = 4.85026455/127.7741852 secs/batch = 0.2980s, grad.norm=13.90364456
  7265: 5 [  630/ 1327], train_loss/perplexity = 4.96809196/143.7523346 secs/batch = 0.2938s, grad.norm=13.15918636
  7270: 5 [  635/ 1327], train_loss/perplexity = 4.67264557/106.9803925 secs/batch = 0.2992s, grad.norm=13.90596485
  7275: 5 [  640/ 1327], train_loss/perplexity = 4.61897469/101.3900223 secs/batch = 0.2928s, grad.norm=14.04944897
  7280: 5 [  645/ 1327], train_loss/perplexity = 4.91046810/135.7029266 secs/batch = 0.2955s, grad.norm=14.47187424
  7285: 5 [  650/ 1327], train_loss/perplexity = 4.40629292/81.9650497 secs/batch = 0.2985s, grad.norm=13.45252323
  7290: 5 [  655/ 1327], train_loss/perplexity = 4.63532257/103.0611572 secs/batch = 0.2942s, grad.norm=13.73447132
  7295: 5 [  660/ 1327], train_loss/perplexity = 4.44704533/85.3743210 secs/batch = 0.2960s, grad.norm=13.49996853
  7300: 5 [  665/ 1327], train_loss/perplexity = 4.66951275/106.6457672 secs/batch = 0.2954s, grad.norm=13.84691620
  7305: 5 [  670/ 1327], train_loss/perplexity = 4.59773874/99.2596130 secs/batch = 0.3001s, grad.norm=13.72130871
  7310: 5 [  675/ 1327], train_loss/perplexity = 4.33972025/76.6860809 secs/batch = 0.2941s, grad.norm=14.23342609
  7315: 5 [  680/ 1327], train_loss/perplexity = 4.66167688/105.8133698 secs/batch = 0.3005s, grad.norm=14.64219284
  7320: 5 [  685/ 1327], train_loss/perplexity = 4.45884800/86.3879318 secs/batch = 0.2957s, grad.norm=13.66963196
  7325: 5 [  690/ 1327], train_loss/perplexity = 4.77446556/118.4469986 secs/batch = 0.2952s, grad.norm=13.02541065
  7330: 5 [  695/ 1327], train_loss/perplexity = 4.67899132/107.6614227 secs/batch = 0.3000s, grad.norm=13.31225109
  7335: 5 [  700/ 1327], train_loss/perplexity = 4.88103294/131.7667084 secs/batch = 0.2948s, grad.norm=13.75171757
  7340: 5 [  705/ 1327], train_loss/perplexity = 4.63475037/103.0022049 secs/batch = 0.2939s, grad.norm=13.76396751
  7345: 5 [  710/ 1327], train_loss/perplexity = 4.52856779/92.6258087 secs/batch = 0.2963s, grad.norm=14.36721325
  7350: 5 [  715/ 1327], train_loss/perplexity = 4.47741699/88.0070572 secs/batch = 0.2940s, grad.norm=13.49110603
  7355: 5 [  720/ 1327], train_loss/perplexity = 4.45716190/86.2423935 secs/batch = 0.2985s, grad.norm=14.08254242
  7360: 5 [  725/ 1327], train_loss/perplexity = 4.48097086/88.3203812 secs/batch = 0.2944s, grad.norm=14.27750492
  7365: 5 [  730/ 1327], train_loss/perplexity = 4.57725286/97.2468796 secs/batch = 0.2951s, grad.norm=13.81944275
  7370: 5 [  735/ 1327], train_loss/perplexity = 4.72483826/112.7122650 secs/batch = 0.2958s, grad.norm=14.27782631
  7375: 5 [  740/ 1327], train_loss/perplexity = 4.13147640/62.2697906 secs/batch = 0.2987s, grad.norm=13.11388683
  7380: 5 [  745/ 1327], train_loss/perplexity = 4.58761644/98.2599411 secs/batch = 0.2952s, grad.norm=13.78215981
  7385: 5 [  750/ 1327], train_loss/perplexity = 4.50509357/90.4768066 secs/batch = 0.3008s, grad.norm=13.56827450
  7390: 5 [  755/ 1327], train_loss/perplexity = 4.38793802/80.4743118 secs/batch = 0.2979s, grad.norm=13.68231392
  7395: 5 [  760/ 1327], train_loss/perplexity = 4.24942636/70.0652084 secs/batch = 0.2991s, grad.norm=13.17562675
  7400: 5 [  765/ 1327], train_loss/perplexity = 4.40467167/81.8322678 secs/batch = 0.2910s, grad.norm=14.71962261
  7405: 5 [  770/ 1327], train_loss/perplexity = 4.33405495/76.2528610 secs/batch = 0.3015s, grad.norm=13.96722317
  7410: 5 [  775/ 1327], train_loss/perplexity = 4.46059704/86.5391617 secs/batch = 0.2926s, grad.norm=14.63174629
  7415: 5 [  780/ 1327], train_loss/perplexity = 4.85042524/127.7947235 secs/batch = 0.2934s, grad.norm=13.46534920
  7420: 5 [  785/ 1327], train_loss/perplexity = 4.63665247/103.1983109 secs/batch = 0.2984s, grad.norm=14.80849266
  7425: 5 [  790/ 1327], train_loss/perplexity = 4.40361023/81.7454529 secs/batch = 0.2987s, grad.norm=14.00471497
  7430: 5 [  795/ 1327], train_loss/perplexity = 4.82618046/124.7336273 secs/batch = 0.2994s, grad.norm=14.57736111
  7435: 5 [  800/ 1327], train_loss/perplexity = 4.90349531/134.7599792 secs/batch = 0.2990s, grad.norm=19.81626511
  7440: 5 [  805/ 1327], train_loss/perplexity = 5.07738543/160.3542480 secs/batch = 0.2937s, grad.norm=15.51144314
  7445: 5 [  810/ 1327], train_loss/perplexity = 4.65770864/105.3943100 secs/batch = 0.2993s, grad.norm=13.06951714
  7450: 5 [  815/ 1327], train_loss/perplexity = 4.54927206/94.5635452 secs/batch = 0.2958s, grad.norm=13.06065083
  7455: 5 [  820/ 1327], train_loss/perplexity = 4.33534431/76.3512421 secs/batch = 0.3009s, grad.norm=13.87590981
  7460: 5 [  825/ 1327], train_loss/perplexity = 4.49599934/89.6577225 secs/batch = 0.2957s, grad.norm=13.79824257
  7465: 5 [  830/ 1327], train_loss/perplexity = 4.29826784/73.5722427 secs/batch = 0.2924s, grad.norm=14.54384422
  7470: 5 [  835/ 1327], train_loss/perplexity = 4.59031916/98.5258713 secs/batch = 0.2994s, grad.norm=14.13971519
  7475: 5 [  840/ 1327], train_loss/perplexity = 4.62994576/102.5085068 secs/batch = 0.2952s, grad.norm=13.71486950
  7480: 5 [  845/ 1327], train_loss/perplexity = 4.41379309/82.5821075 secs/batch = 0.2945s, grad.norm=14.18835640
  7485: 5 [  850/ 1327], train_loss/perplexity = 4.54875088/94.5142746 secs/batch = 0.3002s, grad.norm=13.71053696
  7490: 5 [  855/ 1327], train_loss/perplexity = 4.66308308/105.9622726 secs/batch = 0.2985s, grad.norm=14.20621204
  7495: 5 [  860/ 1327], train_loss/perplexity = 4.26069498/70.8592148 secs/batch = 0.2970s, grad.norm=13.06702805
  7500: 5 [  865/ 1327], train_loss/perplexity = 4.78779268/120.0361176 secs/batch = 0.2949s, grad.norm=13.87506390
  7505: 5 [  870/ 1327], train_loss/perplexity = 4.68752337/108.5839233 secs/batch = 0.2948s, grad.norm=13.84150887
  7510: 5 [  875/ 1327], train_loss/perplexity = 4.19641209/66.4474945 secs/batch = 0.2989s, grad.norm=13.68022919
  7515: 5 [  880/ 1327], train_loss/perplexity = 4.38050365/79.8782578 secs/batch = 0.2950s, grad.norm=12.79904461
  7520: 5 [  885/ 1327], train_loss/perplexity = 4.59843349/99.3285980 secs/batch = 0.3018s, grad.norm=13.27802944
  7525: 5 [  890/ 1327], train_loss/perplexity = 4.78331947/119.5003738 secs/batch = 0.2941s, grad.norm=14.03634357
  7530: 5 [  895/ 1327], train_loss/perplexity = 4.74055433/114.4976501 secs/batch = 0.2955s, grad.norm=12.79713821
  7535: 5 [  900/ 1327], train_loss/perplexity = 4.58790016/98.2878265 secs/batch = 0.2927s, grad.norm=14.61682034
  7540: 5 [  905/ 1327], train_loss/perplexity = 4.45636177/86.1734161 secs/batch = 0.2950s, grad.norm=13.34429264
  7545: 5 [  910/ 1327], train_loss/perplexity = 4.41410160/82.6075897 secs/batch = 0.3006s, grad.norm=13.69286251
  7550: 5 [  915/ 1327], train_loss/perplexity = 4.68749809/108.5811768 secs/batch = 0.2939s, grad.norm=13.12688160
  7555: 5 [  920/ 1327], train_loss/perplexity = 4.89366531/133.4417877 secs/batch = 0.2953s, grad.norm=13.19726849
  7560: 5 [  925/ 1327], train_loss/perplexity = 4.70864534/110.9018250 secs/batch = 0.2972s, grad.norm=13.79327011
  7565: 5 [  930/ 1327], train_loss/perplexity = 4.69684410/109.6007385 secs/batch = 0.2951s, grad.norm=13.23903942
  7570: 5 [  935/ 1327], train_loss/perplexity = 4.75154877/115.7634354 secs/batch = 0.2921s, grad.norm=13.82350349
  7575: 5 [  940/ 1327], train_loss/perplexity = 4.74776793/115.3265762 secs/batch = 0.2955s, grad.norm=13.80055809
  7580: 5 [  945/ 1327], train_loss/perplexity = 4.94708538/140.7640991 secs/batch = 0.2949s, grad.norm=13.75486088
  7585: 5 [  950/ 1327], train_loss/perplexity = 4.65357924/104.9599915 secs/batch = 0.2938s, grad.norm=13.72321701
  7590: 5 [  955/ 1327], train_loss/perplexity = 4.70599079/110.6078186 secs/batch = 0.2946s, grad.norm=13.86737347
  7595: 5 [  960/ 1327], train_loss/perplexity = 4.91047668/135.7040863 secs/batch = 0.2928s, grad.norm=13.60194016
  7600: 5 [  965/ 1327], train_loss/perplexity = 4.78137398/119.2681122 secs/batch = 0.2934s, grad.norm=13.87868023
  7605: 5 [  970/ 1327], train_loss/perplexity = 4.89905930/134.1635132 secs/batch = 0.3004s, grad.norm=13.51230335
  7610: 5 [  975/ 1327], train_loss/perplexity = 4.66117764/105.7605591 secs/batch = 0.2948s, grad.norm=14.41089725
  7615: 5 [  980/ 1327], train_loss/perplexity = 4.42841625/83.7985992 secs/batch = 0.2975s, grad.norm=13.46892452
  7620: 5 [  985/ 1327], train_loss/perplexity = 4.64162731/103.7129822 secs/batch = 0.2942s, grad.norm=14.37388706
  7625: 5 [  990/ 1327], train_loss/perplexity = 4.84244156/126.7785110 secs/batch = 0.2995s, grad.norm=13.72506142
  7630: 5 [  995/ 1327], train_loss/perplexity = 4.78790379/120.0494537 secs/batch = 0.2984s, grad.norm=13.09896183
  7635: 5 [ 1000/ 1327], train_loss/perplexity = 4.32968712/75.9205322 secs/batch = 0.2924s, grad.norm=13.12679863
  7640: 5 [ 1005/ 1327], train_loss/perplexity = 4.77336979/118.3172760 secs/batch = 0.2944s, grad.norm=13.66712379
  7645: 5 [ 1010/ 1327], train_loss/perplexity = 4.32370758/75.4679108 secs/batch = 0.2944s, grad.norm=12.72593784
  7650: 5 [ 1015/ 1327], train_loss/perplexity = 4.87600517/131.1058655 secs/batch = 0.2948s, grad.norm=14.15329552
  7655: 5 [ 1020/ 1327], train_loss/perplexity = 4.99000359/146.9369507 secs/batch = 0.2954s, grad.norm=12.94313908
  7660: 5 [ 1025/ 1327], train_loss/perplexity = 4.82565403/124.6679764 secs/batch = 0.2940s, grad.norm=13.08362865
  7665: 5 [ 1030/ 1327], train_loss/perplexity = 4.56037140/95.6189880 secs/batch = 0.2960s, grad.norm=13.05347538
  7670: 5 [ 1035/ 1327], train_loss/perplexity = 4.55349445/94.9636765 secs/batch = 0.2926s, grad.norm=13.21073437
  7675: 5 [ 1040/ 1327], train_loss/perplexity = 4.75881433/116.6075897 secs/batch = 0.2938s, grad.norm=14.06428623
  7680: 5 [ 1045/ 1327], train_loss/perplexity = 4.38969517/80.6158371 secs/batch = 0.2962s, grad.norm=12.98962021
  7685: 5 [ 1050/ 1327], train_loss/perplexity = 4.47904444/88.1503983 secs/batch = 0.2987s, grad.norm=14.24632645
  7690: 5 [ 1055/ 1327], train_loss/perplexity = 4.50585508/90.5457382 secs/batch = 0.2951s, grad.norm=13.98546696
  7695: 5 [ 1060/ 1327], train_loss/perplexity = 4.18750620/65.8583450 secs/batch = 0.2998s, grad.norm=15.00549030
  7700: 5 [ 1065/ 1327], train_loss/perplexity = 4.35589886/77.9368515 secs/batch = 0.2927s, grad.norm=13.28100395
  7705: 5 [ 1070/ 1327], train_loss/perplexity = 4.68469906/108.2776794 secs/batch = 0.2983s, grad.norm=13.67339134
  7710: 5 [ 1075/ 1327], train_loss/perplexity = 4.41190195/82.4260864 secs/batch = 0.2949s, grad.norm=13.92567348
  7715: 5 [ 1080/ 1327], train_loss/perplexity = 4.31476450/74.7960052 secs/batch = 0.2958s, grad.norm=13.75609112
  7720: 5 [ 1085/ 1327], train_loss/perplexity = 4.21722364/67.8448639 secs/batch = 0.2975s, grad.norm=13.94864845
  7725: 5 [ 1090/ 1327], train_loss/perplexity = 4.49623823/89.6791458 secs/batch = 0.2954s, grad.norm=13.83894157
  7730: 5 [ 1095/ 1327], train_loss/perplexity = 4.59586811/99.0741043 secs/batch = 0.2997s, grad.norm=14.47995853
  7735: 5 [ 1100/ 1327], train_loss/perplexity = 4.36982489/79.0297928 secs/batch = 0.2943s, grad.norm=16.38989830
  7740: 5 [ 1105/ 1327], train_loss/perplexity = 4.28557110/72.6440201 secs/batch = 0.2925s, grad.norm=13.71041012
  7745: 5 [ 1110/ 1327], train_loss/perplexity = 4.63529253/103.0580597 secs/batch = 0.2979s, grad.norm=14.15008926
  7750: 5 [ 1115/ 1327], train_loss/perplexity = 4.40237236/81.6443253 secs/batch = 0.2996s, grad.norm=13.96780491
  7755: 5 [ 1120/ 1327], train_loss/perplexity = 4.62822199/102.3319550 secs/batch = 0.3000s, grad.norm=13.40394020
  7760: 5 [ 1125/ 1327], train_loss/perplexity = 4.83123732/125.3659821 secs/batch = 0.2938s, grad.norm=14.28195763
  7765: 5 [ 1130/ 1327], train_loss/perplexity = 4.53251791/92.9924164 secs/batch = 0.2948s, grad.norm=14.53613377
  7770: 5 [ 1135/ 1327], train_loss/perplexity = 4.52236891/92.0534058 secs/batch = 0.2956s, grad.norm=14.05028248
  7775: 5 [ 1140/ 1327], train_loss/perplexity = 4.78682899/119.9204941 secs/batch = 0.2982s, grad.norm=14.31267834
  7780: 5 [ 1145/ 1327], train_loss/perplexity = 4.58904076/98.3999939 secs/batch = 0.2953s, grad.norm=14.38572025
  7785: 5 [ 1150/ 1327], train_loss/perplexity = 4.53395700/93.1263351 secs/batch = 0.2928s, grad.norm=13.86452198
  7790: 5 [ 1155/ 1327], train_loss/perplexity = 4.60039043/99.5231628 secs/batch = 0.2940s, grad.norm=14.36612988
  7795: 5 [ 1160/ 1327], train_loss/perplexity = 4.60252142/99.7354736 secs/batch = 0.3005s, grad.norm=13.98533249
  7800: 5 [ 1165/ 1327], train_loss/perplexity = 4.61391783/100.8786011 secs/batch = 0.3011s, grad.norm=13.49953461
  7805: 5 [ 1170/ 1327], train_loss/perplexity = 4.49477911/89.5483856 secs/batch = 0.2932s, grad.norm=13.33003330
  7810: 5 [ 1175/ 1327], train_loss/perplexity = 4.20388603/66.9459839 secs/batch = 0.2993s, grad.norm=13.57932854
  7815: 5 [ 1180/ 1327], train_loss/perplexity = 4.30513239/74.0790253 secs/batch = 0.2943s, grad.norm=13.57616615
  7820: 5 [ 1185/ 1327], train_loss/perplexity = 4.47325468/87.6415024 secs/batch = 0.2935s, grad.norm=13.50424004
  7825: 5 [ 1190/ 1327], train_loss/perplexity = 4.54093790/93.7787170 secs/batch = 0.2997s, grad.norm=13.63096905
  7830: 5 [ 1195/ 1327], train_loss/perplexity = 4.38489580/80.2298660 secs/batch = 0.2949s, grad.norm=13.09450436
  7835: 5 [ 1200/ 1327], train_loss/perplexity = 4.33961248/76.6778183 secs/batch = 0.2950s, grad.norm=13.79552269
  7840: 5 [ 1205/ 1327], train_loss/perplexity = 4.35076904/77.5380707 secs/batch = 0.2984s, grad.norm=13.69642830
  7845: 5 [ 1210/ 1327], train_loss/perplexity = 3.99208736/54.1678391 secs/batch = 0.2954s, grad.norm=13.84538460
  7850: 5 [ 1215/ 1327], train_loss/perplexity = 4.23063898/68.7611542 secs/batch = 0.2993s, grad.norm=13.69034481
  7855: 5 [ 1220/ 1327], train_loss/perplexity = 4.40971041/82.2456436 secs/batch = 0.2928s, grad.norm=14.46562958
  7860: 5 [ 1225/ 1327], train_loss/perplexity = 4.14237118/62.9519157 secs/batch = 0.2939s, grad.norm=15.10049057
  7865: 5 [ 1230/ 1327], train_loss/perplexity = 4.39904451/81.3730774 secs/batch = 0.2954s, grad.norm=13.78349209
  7870: 5 [ 1235/ 1327], train_loss/perplexity = 4.37702179/79.6006165 secs/batch = 0.3013s, grad.norm=13.78229523
  7875: 5 [ 1240/ 1327], train_loss/perplexity = 4.54568100/94.2245712 secs/batch = 0.2941s, grad.norm=14.41236210
  7880: 5 [ 1245/ 1327], train_loss/perplexity = 4.48877382/89.0122375 secs/batch = 0.2925s, grad.norm=13.14821625
  7885: 5 [ 1250/ 1327], train_loss/perplexity = 4.60305071/99.7882767 secs/batch = 0.2984s, grad.norm=12.87889957
  7890: 5 [ 1255/ 1327], train_loss/perplexity = 4.61914158/101.4069443 secs/batch = 0.2959s, grad.norm=13.27630615
  7895: 5 [ 1260/ 1327], train_loss/perplexity = 4.47259712/87.5838928 secs/batch = 0.2937s, grad.norm=14.04977798
  7900: 5 [ 1265/ 1327], train_loss/perplexity = 4.70469236/110.4642944 secs/batch = 0.2945s, grad.norm=14.73361969
  7905: 5 [ 1270/ 1327], train_loss/perplexity = 4.35360193/77.7580414 secs/batch = 0.2937s, grad.norm=13.88197422
  7910: 5 [ 1275/ 1327], train_loss/perplexity = 4.56593847/96.1527863 secs/batch = 0.2962s, grad.norm=14.58504677
  7915: 5 [ 1280/ 1327], train_loss/perplexity = 4.44544601/85.2378845 secs/batch = 0.2992s, grad.norm=15.33958054
  7920: 5 [ 1285/ 1327], train_loss/perplexity = 4.39308739/80.8897705 secs/batch = 0.2963s, grad.norm=14.18230915
  7925: 5 [ 1290/ 1327], train_loss/perplexity = 4.52467918/92.2663193 secs/batch = 0.2938s, grad.norm=13.36640644
  7930: 5 [ 1295/ 1327], train_loss/perplexity = 4.58233261/97.7421265 secs/batch = 0.2950s, grad.norm=13.85009766
  7935: 5 [ 1300/ 1327], train_loss/perplexity = 4.73499203/113.8625488 secs/batch = 0.2953s, grad.norm=13.75212193
  7940: 5 [ 1305/ 1327], train_loss/perplexity = 4.83406067/125.7204361 secs/batch = 0.2925s, grad.norm=14.89484024
  7945: 5 [ 1310/ 1327], train_loss/perplexity = 5.04981375/155.9934082 secs/batch = 0.2949s, grad.norm=13.77844810
  7950: 5 [ 1315/ 1327], train_loss/perplexity = 4.85084963/127.8489685 secs/batch = 0.2949s, grad.norm=13.59574699
  7955: 5 [ 1320/ 1327], train_loss/perplexity = 4.82703781/124.8406143 secs/batch = 0.2941s, grad.norm=13.66687679
  7960: 5 [ 1325/ 1327], train_loss/perplexity = 4.78647232/119.8777313 secs/batch = 0.2930s, grad.norm=14.12320042
Epoch training time: 393.37480449676514
	> validation loss = 4.87962532, perplexity = 131.58135986
	> validation loss = 4.75776577, perplexity = 116.48538208
	> validation loss = 4.73117590, perplexity = 113.42886353
	> validation loss = 4.72543526, perplexity = 112.77957916
	> validation loss = 4.94077063, perplexity = 139.87800598
	> validation loss = 4.79451752, perplexity = 120.84606171
	> validation loss = 4.80881214, perplexity = 122.58591461
	> validation loss = 4.65580750, perplexity = 105.19412994
	> validation loss = 4.46301031, perplexity = 86.74825287
	> validation loss = 4.53965664, perplexity = 93.65863800
	> validation loss = 4.66722870, perplexity = 106.40245819
	> validation loss = 4.77485514, perplexity = 118.49314880
	> validation loss = 4.65660191, perplexity = 105.27773285
	> validation loss = 4.51136303, perplexity = 91.04582977
	> validation loss = 4.38194323, perplexity = 79.99332428
	> validation loss = 4.42721558, perplexity = 83.69804382
	> validation loss = 4.87128925, perplexity = 130.48904419
	> validation loss = 4.42330265, perplexity = 83.37117767
	> validation loss = 4.83192682, perplexity = 125.45245361
	> validation loss = 4.68955564, perplexity = 108.80482483
	> validation loss = 4.53223515, perplexity = 92.96612549
at the end of epoch: 5
train loss = 4.69216613, perplexity = 109.08922580
validation loss = 4.66759695, perplexity = 106.44165059
Saved model cv/epoch005_4.6676.model
  7967: 6 [    5/ 1327], train_loss/perplexity = 4.78875971/120.1522522 secs/batch = 0.2992s, grad.norm=13.65207005
  7972: 6 [   10/ 1327], train_loss/perplexity = 4.38640928/80.3513794 secs/batch = 0.2963s, grad.norm=14.11778831
  7977: 6 [   15/ 1327], train_loss/perplexity = 4.62657595/102.1636505 secs/batch = 0.3002s, grad.norm=12.78016090
  7982: 6 [   20/ 1327], train_loss/perplexity = 4.83210611/125.4749451 secs/batch = 0.2986s, grad.norm=13.49668217
  7987: 6 [   25/ 1327], train_loss/perplexity = 4.69767809/109.6921844 secs/batch = 0.2928s, grad.norm=14.11866474
  7992: 6 [   30/ 1327], train_loss/perplexity = 4.76130342/116.8981934 secs/batch = 0.2942s, grad.norm=13.70922375
  7997: 6 [   35/ 1327], train_loss/perplexity = 4.57231426/96.7677994 secs/batch = 0.2986s, grad.norm=13.41638184
  8002: 6 [   40/ 1327], train_loss/perplexity = 4.53208828/92.9524689 secs/batch = 0.2933s, grad.norm=13.72462082
  8007: 6 [   45/ 1327], train_loss/perplexity = 4.36873674/78.9438400 secs/batch = 0.2950s, grad.norm=12.98179913
  8012: 6 [   50/ 1327], train_loss/perplexity = 4.53803205/93.5065994 secs/batch = 0.2957s, grad.norm=13.59481525
  8017: 6 [   55/ 1327], train_loss/perplexity = 4.44894838/85.5369415 secs/batch = 0.3000s, grad.norm=14.41013908
  8022: 6 [   60/ 1327], train_loss/perplexity = 4.80393744/121.9897995 secs/batch = 0.2926s, grad.norm=13.79633331
  8027: 6 [   65/ 1327], train_loss/perplexity = 4.33653069/76.4418793 secs/batch = 0.2984s, grad.norm=13.19567108
  8032: 6 [   70/ 1327], train_loss/perplexity = 4.17132425/64.8012085 secs/batch = 0.2952s, grad.norm=14.16795349
  8037: 6 [   75/ 1327], train_loss/perplexity = 4.01998854/55.7004662 secs/batch = 0.2947s, grad.norm=13.50397491
  8042: 6 [   80/ 1327], train_loss/perplexity = 4.45788097/86.3044357 secs/batch = 0.2954s, grad.norm=13.94861126
  8047: 6 [   85/ 1327], train_loss/perplexity = 4.46610355/87.0170059 secs/batch = 0.2989s, grad.norm=13.99741268
  8052: 6 [   90/ 1327], train_loss/perplexity = 4.51283455/91.1799088 secs/batch = 0.2949s, grad.norm=14.18927193
  8057: 6 [   95/ 1327], train_loss/perplexity = 4.41011143/82.2786331 secs/batch = 0.2947s, grad.norm=14.48523712
  8062: 6 [  100/ 1327], train_loss/perplexity = 4.68737030/108.5673065 secs/batch = 0.2919s, grad.norm=14.26523018
  8067: 6 [  105/ 1327], train_loss/perplexity = 4.49500751/89.5688400 secs/batch = 0.2965s, grad.norm=14.87728596
  8072: 6 [  110/ 1327], train_loss/perplexity = 4.40909481/82.1950302 secs/batch = 0.2944s, grad.norm=13.40368462
  8077: 6 [  115/ 1327], train_loss/perplexity = 4.32968998/75.9207458 secs/batch = 0.2941s, grad.norm=14.88325977
  8082: 6 [  120/ 1327], train_loss/perplexity = 4.43980122/84.7580948 secs/batch = 0.2994s, grad.norm=14.98639679
  8087: 6 [  125/ 1327], train_loss/perplexity = 4.60667706/100.1508026 secs/batch = 0.2962s, grad.norm=14.66822815
  8092: 6 [  130/ 1327], train_loss/perplexity = 4.48930454/89.0594864 secs/batch = 0.2991s, grad.norm=14.39344597
  8097: 6 [  135/ 1327], train_loss/perplexity = 4.50287008/90.2758560 secs/batch = 0.2954s, grad.norm=13.12869263
  8102: 6 [  140/ 1327], train_loss/perplexity = 4.83417797/125.7351837 secs/batch = 0.2933s, grad.norm=14.54318428
  8107: 6 [  145/ 1327], train_loss/perplexity = 4.66269779/105.9214478 secs/batch = 0.2946s, grad.norm=15.44960785
  8112: 6 [  150/ 1327], train_loss/perplexity = 4.66895151/106.5859299 secs/batch = 0.2935s, grad.norm=14.06589508
  8117: 6 [  155/ 1327], train_loss/perplexity = 4.90520763/134.9909363 secs/batch = 0.2956s, grad.norm=13.88810253
  8122: 6 [  160/ 1327], train_loss/perplexity = 4.53156424/92.9037704 secs/batch = 0.2933s, grad.norm=13.00078678
  8127: 6 [  165/ 1327], train_loss/perplexity = 4.77323771/118.3016510 secs/batch = 0.2948s, grad.norm=13.72093582
  8132: 6 [  170/ 1327], train_loss/perplexity = 4.48718166/88.8706284 secs/batch = 0.3008s, grad.norm=13.37630939
  8137: 6 [  175/ 1327], train_loss/perplexity = 4.79386139/120.7667999 secs/batch = 0.2947s, grad.norm=13.94361115
  8142: 6 [  180/ 1327], train_loss/perplexity = 4.66125679/105.7689285 secs/batch = 0.2936s, grad.norm=14.49716187
  8147: 6 [  185/ 1327], train_loss/perplexity = 4.91113043/135.7928314 secs/batch = 0.2998s, grad.norm=14.04903030
  8152: 6 [  190/ 1327], train_loss/perplexity = 4.44397402/85.1125107 secs/batch = 0.2959s, grad.norm=12.80542564
  8157: 6 [  195/ 1327], train_loss/perplexity = 4.77040768/117.9673233 secs/batch = 0.3007s, grad.norm=12.56176376
  8162: 6 [  200/ 1327], train_loss/perplexity = 4.68195295/107.9807510 secs/batch = 0.2942s, grad.norm=14.83431911
  8167: 6 [  205/ 1327], train_loss/perplexity = 4.83865070/126.2988205 secs/batch = 0.3004s, grad.norm=14.11947727
  8172: 6 [  210/ 1327], train_loss/perplexity = 4.65875196/105.5043259 secs/batch = 0.2934s, grad.norm=13.04426956
  8177: 6 [  215/ 1327], train_loss/perplexity = 4.79577875/120.9985733 secs/batch = 0.3015s, grad.norm=13.63869381
  8182: 6 [  220/ 1327], train_loss/perplexity = 4.75330925/115.9674149 secs/batch = 0.2987s, grad.norm=13.84606743
  8187: 6 [  225/ 1327], train_loss/perplexity = 4.86011600/129.0391693 secs/batch = 0.2951s, grad.norm=13.73495865
  8192: 6 [  230/ 1327], train_loss/perplexity = 4.76687431/117.5512390 secs/batch = 0.2949s, grad.norm=14.07968235
  8197: 6 [  235/ 1327], train_loss/perplexity = 4.62922955/102.4351120 secs/batch = 0.2952s, grad.norm=13.69941616
  8202: 6 [  240/ 1327], train_loss/perplexity = 4.40194225/81.6092224 secs/batch = 0.2945s, grad.norm=14.18570805
  8207: 6 [  245/ 1327], train_loss/perplexity = 4.69186497/109.0563736 secs/batch = 0.2983s, grad.norm=13.57669640
  8212: 6 [  250/ 1327], train_loss/perplexity = 4.49086380/89.1984634 secs/batch = 0.2989s, grad.norm=13.86598396
  8217: 6 [  255/ 1327], train_loss/perplexity = 4.48788929/88.9335327 secs/batch = 0.2952s, grad.norm=14.32840729
  8222: 6 [  260/ 1327], train_loss/perplexity = 4.82329512/124.3742447 secs/batch = 0.2950s, grad.norm=15.52836800
  8227: 6 [  265/ 1327], train_loss/perplexity = 4.83783197/126.1954575 secs/batch = 0.2939s, grad.norm=12.82429409
  8232: 6 [  270/ 1327], train_loss/perplexity = 4.95977974/142.5623932 secs/batch = 0.2956s, grad.norm=13.85766697
  8237: 6 [  275/ 1327], train_loss/perplexity = 4.97416306/144.6277313 secs/batch = 0.2940s, grad.norm=13.85862732
  8242: 6 [  280/ 1327], train_loss/perplexity = 4.70058298/110.0112915 secs/batch = 0.2965s, grad.norm=13.44937325
  8247: 6 [  285/ 1327], train_loss/perplexity = 4.95839262/142.3647766 secs/batch = 0.2953s, grad.norm=14.10508347
  8252: 6 [  290/ 1327], train_loss/perplexity = 4.68543911/108.3578415 secs/batch = 0.2941s, grad.norm=14.12140846
  8257: 6 [  295/ 1327], train_loss/perplexity = 4.51132679/91.0425339 secs/batch = 0.2995s, grad.norm=13.52775574
  8262: 6 [  300/ 1327], train_loss/perplexity = 4.11336851/61.1523628 secs/batch = 0.3006s, grad.norm=13.71159554
  8267: 6 [  305/ 1327], train_loss/perplexity = 4.54395771/94.0623322 secs/batch = 0.2940s, grad.norm=14.27313995
  8272: 6 [  310/ 1327], train_loss/perplexity = 4.53805351/93.5086060 secs/batch = 0.2947s, grad.norm=13.86252022
  8277: 6 [  315/ 1327], train_loss/perplexity = 4.16082382/64.1243286 secs/batch = 0.3015s, grad.norm=13.55906773
  8282: 6 [  320/ 1327], train_loss/perplexity = 4.08875608/59.6656265 secs/batch = 0.2944s, grad.norm=15.17057323
  8287: 6 [  325/ 1327], train_loss/perplexity = 4.11530161/61.2706909 secs/batch = 0.2955s, grad.norm=13.25169849
  8292: 6 [  330/ 1327], train_loss/perplexity = 4.61477375/100.9649811 secs/batch = 0.3019s, grad.norm=14.34611320
  8297: 6 [  335/ 1327], train_loss/perplexity = 4.05901432/57.9171944 secs/batch = 0.2965s, grad.norm=13.70893383
  8302: 6 [  340/ 1327], train_loss/perplexity = 4.73039818/113.3406830 secs/batch = 0.2945s, grad.norm=13.41388988
  8307: 6 [  345/ 1327], train_loss/perplexity = 4.62011671/101.5058746 secs/batch = 0.2957s, grad.norm=13.10521603
  8312: 6 [  350/ 1327], train_loss/perplexity = 4.65290833/104.8895950 secs/batch = 0.2946s, grad.norm=14.15272903
  8317: 6 [  355/ 1327], train_loss/perplexity = 4.70025063/109.9747314 secs/batch = 0.2945s, grad.norm=14.27998734
  8322: 6 [  360/ 1327], train_loss/perplexity = 4.85258341/128.0708160 secs/batch = 0.2938s, grad.norm=14.64529228
  8327: 6 [  365/ 1327], train_loss/perplexity = 4.70478010/110.4739914 secs/batch = 0.2986s, grad.norm=13.19907856
  8332: 6 [  370/ 1327], train_loss/perplexity = 4.84317970/126.8721237 secs/batch = 0.2952s, grad.norm=14.05872059
  8337: 6 [  375/ 1327], train_loss/perplexity = 4.12525272/61.8834457 secs/batch = 0.3009s, grad.norm=14.12469196
  8342: 6 [  380/ 1327], train_loss/perplexity = 4.26461267/71.1373596 secs/batch = 0.2942s, grad.norm=14.57275486
  8347: 6 [  385/ 1327], train_loss/perplexity = 4.46749496/87.1381683 secs/batch = 0.2959s, grad.norm=14.84271908
  8352: 6 [  390/ 1327], train_loss/perplexity = 4.57389021/96.9204178 secs/batch = 0.2953s, grad.norm=14.17299938
  8357: 6 [  395/ 1327], train_loss/perplexity = 4.75089502/115.6877823 secs/batch = 0.2949s, grad.norm=13.98107433
  8362: 6 [  400/ 1327], train_loss/perplexity = 4.55049133/94.6789169 secs/batch = 0.2955s, grad.norm=13.31921005
  8367: 6 [  405/ 1327], train_loss/perplexity = 4.93814087/139.5106354 secs/batch = 0.2949s, grad.norm=14.33951378
  8372: 6 [  410/ 1327], train_loss/perplexity = 4.52740192/92.5178833 secs/batch = 0.3011s, grad.norm=13.56784725
  8377: 6 [  415/ 1327], train_loss/perplexity = 4.44077969/84.8410645 secs/batch = 0.2953s, grad.norm=13.67232037
  8382: 6 [  420/ 1327], train_loss/perplexity = 4.13636017/62.5746460 secs/batch = 0.2970s, grad.norm=14.09751320
  8387: 6 [  425/ 1327], train_loss/perplexity = 4.45672035/86.2043228 secs/batch = 0.3021s, grad.norm=14.63989925
  8392: 6 [  430/ 1327], train_loss/perplexity = 4.67857981/107.6171265 secs/batch = 0.2950s, grad.norm=14.95890808
  8397: 6 [  435/ 1327], train_loss/perplexity = 4.76700687/117.5668259 secs/batch = 0.2944s, grad.norm=13.83897305
  8402: 6 [  440/ 1327], train_loss/perplexity = 4.33705616/76.4820557 secs/batch = 0.2958s, grad.norm=14.81094360
  8407: 6 [  445/ 1327], train_loss/perplexity = 4.67276192/106.9928436 secs/batch = 0.2967s, grad.norm=15.18921757
  8412: 6 [  450/ 1327], train_loss/perplexity = 4.52578831/92.3687134 secs/batch = 0.2951s, grad.norm=13.84092999
  8417: 6 [  455/ 1327], train_loss/perplexity = 4.46739197/87.1291885 secs/batch = 0.3009s, grad.norm=13.35440540
  8422: 6 [  460/ 1327], train_loss/perplexity = 4.50813484/90.7523956 secs/batch = 0.2959s, grad.norm=14.63452244
  8427: 6 [  465/ 1327], train_loss/perplexity = 4.24516869/69.7675247 secs/batch = 0.2936s, grad.norm=15.81559849
  8432: 6 [  470/ 1327], train_loss/perplexity = 4.92151928/137.2109222 secs/batch = 0.2951s, grad.norm=13.60566998
  8437: 6 [  475/ 1327], train_loss/perplexity = 4.43718386/84.5365372 secs/batch = 0.2988s, grad.norm=14.38118458
  8442: 6 [  480/ 1327], train_loss/perplexity = 4.53068590/92.8222046 secs/batch = 0.2965s, grad.norm=14.95981503
  8447: 6 [  485/ 1327], train_loss/perplexity = 4.49976349/89.9958420 secs/batch = 0.2955s, grad.norm=13.91167545
  8452: 6 [  490/ 1327], train_loss/perplexity = 4.35160875/77.6032104 secs/batch = 0.2984s, grad.norm=14.71248817
  8457: 6 [  495/ 1327], train_loss/perplexity = 4.44842815/85.4924545 secs/batch = 0.2959s, grad.norm=14.00036430
  8462: 6 [  500/ 1327], train_loss/perplexity = 4.64243412/103.7966919 secs/batch = 0.2952s, grad.norm=14.22192001
  8467: 6 [  505/ 1327], train_loss/perplexity = 4.68434000/108.2388077 secs/batch = 0.2970s, grad.norm=13.26914120
  8472: 6 [  510/ 1327], train_loss/perplexity = 5.09507561/163.2161865 secs/batch = 0.2955s, grad.norm=13.55091381
  8477: 6 [  515/ 1327], train_loss/perplexity = 4.66562843/106.2323227 secs/batch = 0.2971s, grad.norm=13.33256531
  8482: 6 [  520/ 1327], train_loss/perplexity = 4.89008522/132.9649048 secs/batch = 0.3011s, grad.norm=13.76872063
  8487: 6 [  525/ 1327], train_loss/perplexity = 4.42323256/83.3653336 secs/batch = 0.2950s, grad.norm=13.51848125
  8492: 6 [  530/ 1327], train_loss/perplexity = 4.45838118/86.3476181 secs/batch = 0.2960s, grad.norm=14.47233105
  8497: 6 [  535/ 1327], train_loss/perplexity = 4.63668537/103.2017059 secs/batch = 0.2929s, grad.norm=13.87834263
  8502: 6 [  540/ 1327], train_loss/perplexity = 4.70747519/110.7721252 secs/batch = 0.2949s, grad.norm=13.12376595
  8507: 6 [  545/ 1327], train_loss/perplexity = 4.68262529/108.0533752 secs/batch = 0.2965s, grad.norm=14.20756912
  8512: 6 [  550/ 1327], train_loss/perplexity = 4.60996675/100.4808121 secs/batch = 0.3007s, grad.norm=14.27374363
  8517: 6 [  555/ 1327], train_loss/perplexity = 4.41259670/82.4833679 secs/batch = 0.3015s, grad.norm=13.02318192
  8522: 6 [  560/ 1327], train_loss/perplexity = 4.59217310/98.7087021 secs/batch = 0.2952s, grad.norm=16.15295410
  8527: 6 [  565/ 1327], train_loss/perplexity = 4.51793480/91.6461334 secs/batch = 0.2970s, grad.norm=14.24061108
  8532: 6 [  570/ 1327], train_loss/perplexity = 4.51461983/91.3428345 secs/batch = 0.2953s, grad.norm=14.65781879
  8537: 6 [  575/ 1327], train_loss/perplexity = 4.35490894/77.8597336 secs/batch = 0.2956s, grad.norm=15.04497337
  8542: 6 [  580/ 1327], train_loss/perplexity = 4.71180439/111.2527237 secs/batch = 0.2967s, grad.norm=13.66445827
  8547: 6 [  585/ 1327], train_loss/perplexity = 4.16422653/64.3428955 secs/batch = 0.2955s, grad.norm=14.07404518
  8552: 6 [  590/ 1327], train_loss/perplexity = 4.67296410/107.0144730 secs/batch = 0.3012s, grad.norm=14.16657639
  8557: 6 [  595/ 1327], train_loss/perplexity = 4.63852978/103.3922272 secs/batch = 0.2958s, grad.norm=15.40055180
  8562: 6 [  600/ 1327], train_loss/perplexity = 4.76447010/117.2689590 secs/batch = 0.3017s, grad.norm=13.73491287
  8567: 6 [  605/ 1327], train_loss/perplexity = 4.69021606/108.8767014 secs/batch = 0.3005s, grad.norm=14.60470104
  8572: 6 [  610/ 1327], train_loss/perplexity = 4.81008339/122.7418518 secs/batch = 0.2945s, grad.norm=14.79810143
  8577: 6 [  615/ 1327], train_loss/perplexity = 4.39240456/80.8345566 secs/batch = 0.2946s, grad.norm=13.98771000
  8582: 6 [  620/ 1327], train_loss/perplexity = 4.72579145/112.8197556 secs/batch = 0.2944s, grad.norm=13.89179420
  8587: 6 [  625/ 1327], train_loss/perplexity = 4.79391003/120.7726746 secs/batch = 0.2926s, grad.norm=15.13588715
  8592: 6 [  630/ 1327], train_loss/perplexity = 4.87383413/130.8215485 secs/batch = 0.3012s, grad.norm=13.74793816
  8597: 6 [  635/ 1327], train_loss/perplexity = 4.59655714/99.1423950 secs/batch = 0.2951s, grad.norm=14.70215130
  8602: 6 [  640/ 1327], train_loss/perplexity = 4.54099512/93.7840805 secs/batch = 0.2985s, grad.norm=14.17770386
  8607: 6 [  645/ 1327], train_loss/perplexity = 4.88891220/132.8090210 secs/batch = 0.3019s, grad.norm=15.84594917
  8612: 6 [  650/ 1327], train_loss/perplexity = 4.35486221/77.8560944 secs/batch = 0.2968s, grad.norm=15.05973625
  8617: 6 [  655/ 1327], train_loss/perplexity = 4.57427597/96.9578171 secs/batch = 0.2997s, grad.norm=14.75938320
  8622: 6 [  660/ 1327], train_loss/perplexity = 4.43781662/84.5900497 secs/batch = 0.2986s, grad.norm=14.04453659
  8627: 6 [  665/ 1327], train_loss/perplexity = 4.63678312/103.2117920 secs/batch = 0.2978s, grad.norm=14.03613186
  8632: 6 [  670/ 1327], train_loss/perplexity = 4.52201271/92.0206223 secs/batch = 0.2952s, grad.norm=14.63195133
  8637: 6 [  675/ 1327], train_loss/perplexity = 4.30019379/73.7140808 secs/batch = 0.2951s, grad.norm=13.66629791
  8642: 6 [  680/ 1327], train_loss/perplexity = 4.57450438/96.9799652 secs/batch = 0.2956s, grad.norm=15.23957062
  8647: 6 [  685/ 1327], train_loss/perplexity = 4.40597391/81.9389038 secs/batch = 0.2947s, grad.norm=14.65731335
  8652: 6 [  690/ 1327], train_loss/perplexity = 4.84004498/126.4750366 secs/batch = 0.3016s, grad.norm=15.11212540
  8657: 6 [  695/ 1327], train_loss/perplexity = 4.50162363/90.1634064 secs/batch = 0.2941s, grad.norm=14.38878727
  8662: 6 [  700/ 1327], train_loss/perplexity = 4.81688547/123.5795975 secs/batch = 0.3008s, grad.norm=13.97980595
  8667: 6 [  705/ 1327], train_loss/perplexity = 4.52108383/91.9351883 secs/batch = 0.2933s, grad.norm=13.48287296
  8672: 6 [  710/ 1327], train_loss/perplexity = 4.46827698/87.2063370 secs/batch = 0.2951s, grad.norm=14.23191166
  8677: 6 [  715/ 1327], train_loss/perplexity = 4.45386744/85.9587402 secs/batch = 0.2951s, grad.norm=14.82051277
  8682: 6 [  720/ 1327], train_loss/perplexity = 4.40083456/81.5188751 secs/batch = 0.2954s, grad.norm=14.48804188
  8687: 6 [  725/ 1327], train_loss/perplexity = 4.39187002/80.7913589 secs/batch = 0.2946s, grad.norm=13.58312416
  8692: 6 [  730/ 1327], train_loss/perplexity = 4.56903744/96.4512253 secs/batch = 0.2963s, grad.norm=13.63950634
  8697: 6 [  735/ 1327], train_loss/perplexity = 4.65841484/105.4687653 secs/batch = 0.2987s, grad.norm=14.32406998
  8702: 6 [  740/ 1327], train_loss/perplexity = 4.06522512/58.2780266 secs/batch = 0.2999s, grad.norm=13.65024376
  8707: 6 [  745/ 1327], train_loss/perplexity = 4.56036568/95.6184387 secs/batch = 0.2946s, grad.norm=13.84836292
  8712: 6 [  750/ 1327], train_loss/perplexity = 4.43237400/84.1309052 secs/batch = 0.2936s, grad.norm=14.33687496
  8717: 6 [  755/ 1327], train_loss/perplexity = 4.32400513/75.4903717 secs/batch = 0.2992s, grad.norm=14.07907963
  8722: 6 [  760/ 1327], train_loss/perplexity = 4.30301905/73.9226303 secs/batch = 0.2942s, grad.norm=14.12498951
  8727: 6 [  765/ 1327], train_loss/perplexity = 4.31347275/74.6994476 secs/batch = 0.3006s, grad.norm=14.44252777
  8732: 6 [  770/ 1327], train_loss/perplexity = 4.26920319/71.4646683 secs/batch = 0.2941s, grad.norm=14.84918499
  8737: 6 [  775/ 1327], train_loss/perplexity = 4.39530182/81.0690918 secs/batch = 0.2986s, grad.norm=14.92873669
  8742: 6 [  780/ 1327], train_loss/perplexity = 4.79575872/120.9961472 secs/batch = 0.2942s, grad.norm=14.16056156
  8747: 6 [  785/ 1327], train_loss/perplexity = 4.63117552/102.6346436 secs/batch = 0.2947s, grad.norm=14.43022346
  8752: 6 [  790/ 1327], train_loss/perplexity = 4.36719370/78.8221207 secs/batch = 0.2941s, grad.norm=13.90045547
  8757: 6 [  795/ 1327], train_loss/perplexity = 4.70456505/110.4502335 secs/batch = 0.3005s, grad.norm=15.02735710
  8762: 6 [  800/ 1327], train_loss/perplexity = 4.60990429/100.4745331 secs/batch = 0.2987s, grad.norm=15.90169907
  8767: 6 [  805/ 1327], train_loss/perplexity = 4.91909075/136.8780975 secs/batch = 0.3017s, grad.norm=13.04314327
  8772: 6 [  810/ 1327], train_loss/perplexity = 4.55240631/94.8603973 secs/batch = 0.2948s, grad.norm=14.27855015
  8777: 6 [  815/ 1327], train_loss/perplexity = 4.46826315/87.2051315 secs/batch = 0.2952s, grad.norm=13.52059460
  8782: 6 [  820/ 1327], train_loss/perplexity = 4.27079010/71.5781708 secs/batch = 0.2947s, grad.norm=13.69008064
  8787: 6 [  825/ 1327], train_loss/perplexity = 4.52880478/92.6477585 secs/batch = 0.2950s, grad.norm=14.23144245
  8792: 6 [  830/ 1327], train_loss/perplexity = 4.27710724/72.0317688 secs/batch = 0.2998s, grad.norm=14.15585518
  8797: 6 [  835/ 1327], train_loss/perplexity = 4.52798223/92.5715866 secs/batch = 0.2944s, grad.norm=14.44495010
  8802: 6 [  840/ 1327], train_loss/perplexity = 4.63161659/102.6799240 secs/batch = 0.3009s, grad.norm=14.25419617
  8807: 6 [  845/ 1327], train_loss/perplexity = 4.41646862/82.8033600 secs/batch = 0.2954s, grad.norm=13.90234470
  8812: 6 [  850/ 1327], train_loss/perplexity = 4.52768707/92.5442657 secs/batch = 0.2998s, grad.norm=13.32050323
  8817: 6 [  855/ 1327], train_loss/perplexity = 4.54705524/94.3541489 secs/batch = 0.2995s, grad.norm=15.19078732
  8822: 6 [  860/ 1327], train_loss/perplexity = 4.22822809/68.5955811 secs/batch = 0.2956s, grad.norm=13.97195911
  8827: 6 [  865/ 1327], train_loss/perplexity = 4.74619007/115.1447525 secs/batch = 0.3002s, grad.norm=13.98911858
  8832: 6 [  870/ 1327], train_loss/perplexity = 4.64121675/103.6704102 secs/batch = 0.3004s, grad.norm=14.46564579
  8837: 6 [  875/ 1327], train_loss/perplexity = 4.17086887/64.7717056 secs/batch = 0.2990s, grad.norm=14.36197758
  8842: 6 [  880/ 1327], train_loss/perplexity = 4.36755419/78.8505402 secs/batch = 0.3014s, grad.norm=13.02232838
  8847: 6 [  885/ 1327], train_loss/perplexity = 4.55336523/94.9514084 secs/batch = 0.2939s, grad.norm=13.56472492
  8852: 6 [  890/ 1327], train_loss/perplexity = 4.65956593/105.5902405 secs/batch = 0.2961s, grad.norm=13.89825630
  8857: 6 [  895/ 1327], train_loss/perplexity = 4.71045732/111.1029587 secs/batch = 0.2979s, grad.norm=14.37924004
  8862: 6 [  900/ 1327], train_loss/perplexity = 4.48810482/88.9527054 secs/batch = 0.2954s, grad.norm=14.27800369
  8867: 6 [  905/ 1327], train_loss/perplexity = 4.40704155/82.0264359 secs/batch = 0.2951s, grad.norm=14.31689262
  8872: 6 [  910/ 1327], train_loss/perplexity = 4.37106085/79.1275330 secs/batch = 0.2947s, grad.norm=13.42474937
  8877: 6 [  915/ 1327], train_loss/perplexity = 4.65850258/105.4780197 secs/batch = 0.2943s, grad.norm=13.69811630
  8882: 6 [  920/ 1327], train_loss/perplexity = 4.78839636/120.1086044 secs/batch = 0.2954s, grad.norm=14.60917568
  8887: 6 [  925/ 1327], train_loss/perplexity = 4.67090321/106.7941589 secs/batch = 0.2952s, grad.norm=13.60555744
  8892: 6 [  930/ 1327], train_loss/perplexity = 4.61139011/100.6239319 secs/batch = 0.2947s, grad.norm=13.95299911
  8897: 6 [  935/ 1327], train_loss/perplexity = 4.61944056/101.4372711 secs/batch = 0.2966s, grad.norm=12.96253872
  8902: 6 [  940/ 1327], train_loss/perplexity = 4.63100672/102.6173172 secs/batch = 0.3001s, grad.norm=13.64753532
  8907: 6 [  945/ 1327], train_loss/perplexity = 4.85235691/128.0418243 secs/batch = 0.2960s, grad.norm=13.54285049
  8912: 6 [  950/ 1327], train_loss/perplexity = 4.56340456/95.9094543 secs/batch = 0.2953s, grad.norm=13.57078075
  8917: 6 [  955/ 1327], train_loss/perplexity = 4.62169075/101.6657791 secs/batch = 0.2998s, grad.norm=13.48548031
  8922: 6 [  960/ 1327], train_loss/perplexity = 4.90174913/134.5248718 secs/batch = 0.3002s, grad.norm=13.63935566
  8927: 6 [  965/ 1327], train_loss/perplexity = 4.70344496/110.3265915 secs/batch = 0.2939s, grad.norm=13.84088516
  8932: 6 [  970/ 1327], train_loss/perplexity = 4.89294004/133.3450317 secs/batch = 0.2981s, grad.norm=13.62277603
  8937: 6 [  975/ 1327], train_loss/perplexity = 4.56558275/96.1185913 secs/batch = 0.2973s, grad.norm=14.64059258
  8942: 6 [  980/ 1327], train_loss/perplexity = 4.32799959/75.7925186 secs/batch = 0.2945s, grad.norm=13.75843811
  8947: 6 [  985/ 1327], train_loss/perplexity = 4.58120203/97.6316833 secs/batch = 0.2964s, grad.norm=15.26152325
  8952: 6 [  990/ 1327], train_loss/perplexity = 4.78375626/119.5525742 secs/batch = 0.2950s, grad.norm=14.23575020
  8957: 6 [  995/ 1327], train_loss/perplexity = 4.75044775/115.6360474 secs/batch = 0.2953s, grad.norm=14.16880322
  8962: 6 [ 1000/ 1327], train_loss/perplexity = 4.23245907/68.8864212 secs/batch = 0.3015s, grad.norm=13.23353195
  8967: 6 [ 1005/ 1327], train_loss/perplexity = 4.75328398/115.9644852 secs/batch = 0.2955s, grad.norm=14.17107010
  8972: 6 [ 1010/ 1327], train_loss/perplexity = 4.31441021/74.7695084 secs/batch = 0.2926s, grad.norm=13.40157604
  8977: 6 [ 1015/ 1327], train_loss/perplexity = 4.84058142/126.5429077 secs/batch = 0.2998s, grad.norm=13.75753784
  8982: 6 [ 1020/ 1327], train_loss/perplexity = 4.91242695/135.9690094 secs/batch = 0.2958s, grad.norm=13.56205082
  8987: 6 [ 1025/ 1327], train_loss/perplexity = 4.76313305/117.1122665 secs/batch = 0.2938s, grad.norm=13.46681309
  8992: 6 [ 1030/ 1327], train_loss/perplexity = 4.53159094/92.9062500 secs/batch = 0.2976s, grad.norm=13.57771683
  8997: 6 [ 1035/ 1327], train_loss/perplexity = 4.50190926/90.1891632 secs/batch = 0.2944s, grad.norm=13.46179771
  9002: 6 [ 1040/ 1327], train_loss/perplexity = 4.75511932/116.1775131 secs/batch = 0.2944s, grad.norm=13.99569511
  9007: 6 [ 1045/ 1327], train_loss/perplexity = 4.29031515/72.9894638 secs/batch = 0.2953s, grad.norm=13.62612247
  9012: 6 [ 1050/ 1327], train_loss/perplexity = 4.40557289/81.9060516 secs/batch = 0.2959s, grad.norm=13.73896217
  9017: 6 [ 1055/ 1327], train_loss/perplexity = 4.51881218/91.7265778 secs/batch = 0.3015s, grad.norm=14.91803837
  9022: 6 [ 1060/ 1327], train_loss/perplexity = 4.13136864/62.2630806 secs/batch = 0.2935s, grad.norm=15.35188484
  9027: 6 [ 1065/ 1327], train_loss/perplexity = 4.23433924/69.0160599 secs/batch = 0.3012s, grad.norm=13.71373940
  9032: 6 [ 1070/ 1327], train_loss/perplexity = 4.67220306/106.9330597 secs/batch = 0.2922s, grad.norm=14.37375164
  9037: 6 [ 1075/ 1327], train_loss/perplexity = 4.30219364/73.8616409 secs/batch = 0.2948s, grad.norm=13.78226566
  9042: 6 [ 1080/ 1327], train_loss/perplexity = 4.21443176/67.6557083 secs/batch = 0.3012s, grad.norm=13.51686668
  9047: 6 [ 1085/ 1327], train_loss/perplexity = 4.16287613/64.2560654 secs/batch = 0.2960s, grad.norm=13.82848835
  9052: 6 [ 1090/ 1327], train_loss/perplexity = 4.35784245/78.0884705 secs/batch = 0.3011s, grad.norm=14.71775246
  9057: 6 [ 1095/ 1327], train_loss/perplexity = 4.45569944/86.1163635 secs/batch = 0.2920s, grad.norm=15.06670761
  9062: 6 [ 1100/ 1327], train_loss/perplexity = 4.34636021/77.1969681 secs/batch = 0.2967s, grad.norm=17.15721130
  9067: 6 [ 1105/ 1327], train_loss/perplexity = 4.24583578/69.8140869 secs/batch = 0.2947s, grad.norm=14.34136486
  9072: 6 [ 1110/ 1327], train_loss/perplexity = 4.59492493/98.9807053 secs/batch = 0.3008s, grad.norm=14.26250648
  9077: 6 [ 1115/ 1327], train_loss/perplexity = 4.36648655/78.7664032 secs/batch = 0.2958s, grad.norm=14.56443501
  9082: 6 [ 1120/ 1327], train_loss/perplexity = 4.52167463/91.9895172 secs/batch = 0.2984s, grad.norm=13.98186016
  9087: 6 [ 1125/ 1327], train_loss/perplexity = 4.79990101/121.4983902 secs/batch = 0.2941s, grad.norm=14.47317314
  9092: 6 [ 1130/ 1327], train_loss/perplexity = 4.36811161/78.8945084 secs/batch = 0.2947s, grad.norm=14.40860748
  9097: 6 [ 1135/ 1327], train_loss/perplexity = 4.40626431/81.9626999 secs/batch = 0.2932s, grad.norm=14.33472061
  9102: 6 [ 1140/ 1327], train_loss/perplexity = 4.71457577/111.5614700 secs/batch = 0.2991s, grad.norm=14.78512287
  9107: 6 [ 1145/ 1327], train_loss/perplexity = 4.46668911/87.0679703 secs/batch = 0.2965s, grad.norm=13.56411457
  9112: 6 [ 1150/ 1327], train_loss/perplexity = 4.48754263/88.9027100 secs/batch = 0.2990s, grad.norm=13.68165493
  9117: 6 [ 1155/ 1327], train_loss/perplexity = 4.55420065/95.0307617 secs/batch = 0.3002s, grad.norm=14.09770012
  9122: 6 [ 1160/ 1327], train_loss/perplexity = 4.46829891/87.2082443 secs/batch = 0.2955s, grad.norm=14.13431072
  9127: 6 [ 1165/ 1327], train_loss/perplexity = 4.56382656/95.9499359 secs/batch = 0.2955s, grad.norm=13.98663521
  9132: 6 [ 1170/ 1327], train_loss/perplexity = 4.41752195/82.8906250 secs/batch = 0.2953s, grad.norm=14.39816570
  9137: 6 [ 1175/ 1327], train_loss/perplexity = 4.17972469/65.3478622 secs/batch = 0.2940s, grad.norm=14.83283043
  9142: 6 [ 1180/ 1327], train_loss/perplexity = 4.22824621/68.5968246 secs/batch = 0.2990s, grad.norm=14.72401047
  9147: 6 [ 1185/ 1327], train_loss/perplexity = 4.41474485/82.6607513 secs/batch = 0.2950s, grad.norm=14.46950817
  9152: 6 [ 1190/ 1327], train_loss/perplexity = 4.54507875/94.1678467 secs/batch = 0.3004s, grad.norm=14.32766438
  9157: 6 [ 1195/ 1327], train_loss/perplexity = 4.30066586/73.7488861 secs/batch = 0.2938s, grad.norm=14.26010704
  9162: 6 [ 1200/ 1327], train_loss/perplexity = 4.26697493/71.3056030 secs/batch = 0.3003s, grad.norm=13.93084621
  9167: 6 [ 1205/ 1327], train_loss/perplexity = 4.31146908/74.5499268 secs/batch = 0.2919s, grad.norm=14.35055923
  9172: 6 [ 1210/ 1327], train_loss/perplexity = 3.95078754/51.9762840 secs/batch = 0.3012s, grad.norm=14.78662109
  9177: 6 [ 1215/ 1327], train_loss/perplexity = 4.18580151/65.7461777 secs/batch = 0.2999s, grad.norm=13.62450314
  9182: 6 [ 1220/ 1327], train_loss/perplexity = 4.42259169/83.3119278 secs/batch = 0.2947s, grad.norm=14.46829796
  9187: 6 [ 1225/ 1327], train_loss/perplexity = 4.06027079/57.9900131 secs/batch = 0.3011s, grad.norm=15.09208965
  9192: 6 [ 1230/ 1327], train_loss/perplexity = 4.31968403/75.1648712 secs/batch = 0.2952s, grad.norm=13.78384399
  9197: 6 [ 1235/ 1327], train_loss/perplexity = 4.27310324/71.7439270 secs/batch = 0.2951s, grad.norm=13.82233620
  9202: 6 [ 1240/ 1327], train_loss/perplexity = 4.51937771/91.7784653 secs/batch = 0.3009s, grad.norm=15.89445019
  9207: 6 [ 1245/ 1327], train_loss/perplexity = 4.45428467/85.9946136 secs/batch = 0.2978s, grad.norm=13.85583305
  9212: 6 [ 1250/ 1327], train_loss/perplexity = 4.51010656/90.9315109 secs/batch = 0.2944s, grad.norm=13.91119671
  9217: 6 [ 1255/ 1327], train_loss/perplexity = 4.59814358/99.2998047 secs/batch = 0.2940s, grad.norm=13.15869713
  9222: 6 [ 1260/ 1327], train_loss/perplexity = 4.37380648/79.3450851 secs/batch = 0.2949s, grad.norm=14.26926613
  9227: 6 [ 1265/ 1327], train_loss/perplexity = 4.58177710/97.6878433 secs/batch = 0.2937s, grad.norm=13.83785057
  9232: 6 [ 1270/ 1327], train_loss/perplexity = 4.34814072/77.3345413 secs/batch = 0.2940s, grad.norm=13.95180798
  9237: 6 [ 1275/ 1327], train_loss/perplexity = 4.55888462/95.4769287 secs/batch = 0.3006s, grad.norm=15.28866863
  9242: 6 [ 1280/ 1327], train_loss/perplexity = 4.38372231/80.1357651 secs/batch = 0.2949s, grad.norm=14.36730099
  9247: 6 [ 1285/ 1327], train_loss/perplexity = 4.31062508/74.4870377 secs/batch = 0.2940s, grad.norm=14.41347408
  9252: 6 [ 1290/ 1327], train_loss/perplexity = 4.47929478/88.1724701 secs/batch = 0.2934s, grad.norm=13.95182896
  9257: 6 [ 1295/ 1327], train_loss/perplexity = 4.49055767/89.1711578 secs/batch = 0.2962s, grad.norm=14.02095413
  9262: 6 [ 1300/ 1327], train_loss/perplexity = 4.67360115/107.0826721 secs/batch = 0.2955s, grad.norm=14.23200035
  9267: 6 [ 1305/ 1327], train_loss/perplexity = 4.76685238/117.5486603 secs/batch = 0.2986s, grad.norm=15.05524731
  9272: 6 [ 1310/ 1327], train_loss/perplexity = 4.98508549/146.2160797 secs/batch = 0.2948s, grad.norm=14.46127701
  9277: 6 [ 1315/ 1327], train_loss/perplexity = 4.83386326/125.6956177 secs/batch = 0.2999s, grad.norm=13.94771862
  9282: 6 [ 1320/ 1327], train_loss/perplexity = 4.77558279/118.5793991 secs/batch = 0.2949s, grad.norm=13.86969852
  9287: 6 [ 1325/ 1327], train_loss/perplexity = 4.70723867/110.7459335 secs/batch = 0.3010s, grad.norm=14.29362774
Epoch training time: 393.5220031738281
	> validation loss = 4.86523056, perplexity = 129.70083618
	> validation loss = 4.72206020, perplexity = 112.39958191
	> validation loss = 4.67962837, perplexity = 107.73002625
	> validation loss = 4.70182943, perplexity = 110.14849854
	> validation loss = 4.88766861, perplexity = 132.64396667
	> validation loss = 4.75126219, perplexity = 115.73026276
	> validation loss = 4.75964642, perplexity = 116.70465088
	> validation loss = 4.61473656, perplexity = 100.96122742
	> validation loss = 4.45662642, perplexity = 86.19622803
	> validation loss = 4.50357771, perplexity = 90.33975983
	> validation loss = 4.64924622, perplexity = 104.50617981
	> validation loss = 4.71804714, perplexity = 111.94941711
	> validation loss = 4.64638424, perplexity = 104.20751190
	> validation loss = 4.48318911, perplexity = 88.51651001
	> validation loss = 4.34155226, perplexity = 76.82669830
	> validation loss = 4.38999224, perplexity = 80.63979340
	> validation loss = 4.79062033, perplexity = 120.37601471
	> validation loss = 4.38650227, perplexity = 80.35885620
	> validation loss = 4.80856657, perplexity = 122.55581665
	> validation loss = 4.65475273, perplexity = 105.08323669
	> validation loss = 4.44139576, perplexity = 84.89334869
at the end of epoch: 6
train loss = 4.61844285, perplexity = 101.33611374
validation loss = 4.63467710, perplexity = 102.99465579
Saved model cv/epoch006_4.6347.model
  9294: 7 [    5/ 1327], train_loss/perplexity = 4.67307854/107.0267181 secs/batch = 0.2926s, grad.norm=14.81152916
  9299: 7 [   10/ 1327], train_loss/perplexity = 4.20527649/67.0391312 secs/batch = 0.2941s, grad.norm=14.19181347
  9304: 7 [   15/ 1327], train_loss/perplexity = 4.62630177/102.1356430 secs/batch = 0.2952s, grad.norm=13.36369991
  9309: 7 [   20/ 1327], train_loss/perplexity = 4.79380178/120.7595978 secs/batch = 0.2932s, grad.norm=14.05560780
  9314: 7 [   25/ 1327], train_loss/perplexity = 4.62729359/102.2369919 secs/batch = 0.2920s, grad.norm=14.28248119
  9319: 7 [   30/ 1327], train_loss/perplexity = 4.65678596/105.2971115 secs/batch = 0.2989s, grad.norm=13.58142948
  9324: 7 [   35/ 1327], train_loss/perplexity = 4.50324821/90.3100052 secs/batch = 0.2992s, grad.norm=13.86323738
  9329: 7 [   40/ 1327], train_loss/perplexity = 4.46819210/87.1989365 secs/batch = 0.2995s, grad.norm=13.95723152
  9334: 7 [   45/ 1327], train_loss/perplexity = 4.27046490/71.5548935 secs/batch = 0.2966s, grad.norm=13.48289585
  9339: 7 [   50/ 1327], train_loss/perplexity = 4.48319912/88.5173950 secs/batch = 0.2958s, grad.norm=13.43832302
  9344: 7 [   55/ 1327], train_loss/perplexity = 4.40513182/81.8699341 secs/batch = 0.3004s, grad.norm=14.10492229
  9349: 7 [   60/ 1327], train_loss/perplexity = 4.70400810/110.3887329 secs/batch = 0.2981s, grad.norm=14.22851849
  9354: 7 [   65/ 1327], train_loss/perplexity = 4.28870916/72.8723450 secs/batch = 0.3008s, grad.norm=13.74175262
  9359: 7 [   70/ 1327], train_loss/perplexity = 4.14211941/62.9360657 secs/batch = 0.2945s, grad.norm=14.33844662
  9364: 7 [   75/ 1327], train_loss/perplexity = 3.95438290/52.1634941 secs/batch = 0.2944s, grad.norm=13.62038708
  9369: 7 [   80/ 1327], train_loss/perplexity = 4.50369310/90.3501892 secs/batch = 0.2932s, grad.norm=16.34284973
  9374: 7 [   85/ 1327], train_loss/perplexity = 4.46688890/87.0853729 secs/batch = 0.2997s, grad.norm=14.09462166
  9379: 7 [   90/ 1327], train_loss/perplexity = 4.47553921/87.8419495 secs/batch = 0.2961s, grad.norm=14.21744728
  9384: 7 [   95/ 1327], train_loss/perplexity = 4.36336327/78.5207748 secs/batch = 0.3001s, grad.norm=13.89008427
  9389: 7 [  100/ 1327], train_loss/perplexity = 4.60824537/100.3079910 secs/batch = 0.2957s, grad.norm=14.14783096
  9394: 7 [  105/ 1327], train_loss/perplexity = 4.53774452/93.4797211 secs/batch = 0.3008s, grad.norm=15.51384449
  9399: 7 [  110/ 1327], train_loss/perplexity = 4.36776495/78.8671646 secs/batch = 0.2944s, grad.norm=13.64144993
  9404: 7 [  115/ 1327], train_loss/perplexity = 4.24319553/69.6300049 secs/batch = 0.2961s, grad.norm=14.66829681
  9409: 7 [  120/ 1327], train_loss/perplexity = 4.40206766/81.6194534 secs/batch = 0.2944s, grad.norm=14.61016941
  9414: 7 [  125/ 1327], train_loss/perplexity = 4.48505592/88.6819077 secs/batch = 0.3002s, grad.norm=15.03239632
  9419: 7 [  130/ 1327], train_loss/perplexity = 4.38966560/80.6134567 secs/batch = 0.2963s, grad.norm=14.45250893
  9424: 7 [  135/ 1327], train_loss/perplexity = 4.41586304/82.7532272 secs/batch = 0.2952s, grad.norm=13.78676987
  9429: 7 [  140/ 1327], train_loss/perplexity = 4.69926405/109.8662872 secs/batch = 0.2993s, grad.norm=14.43559361
  9434: 7 [  145/ 1327], train_loss/perplexity = 4.58733511/98.2322998 secs/batch = 0.2966s, grad.norm=15.77645302
  9439: 7 [  150/ 1327], train_loss/perplexity = 4.56482697/96.0459747 secs/batch = 0.2951s, grad.norm=14.77065659
  9444: 7 [  155/ 1327], train_loss/perplexity = 4.88888550/132.8054810 secs/batch = 0.2966s, grad.norm=13.84642029
  9449: 7 [  160/ 1327], train_loss/perplexity = 4.49261713/89.3549957 secs/batch = 0.2954s, grad.norm=13.23567009
  9454: 7 [  165/ 1327], train_loss/perplexity = 4.62320566/101.8199081 secs/batch = 0.2939s, grad.norm=13.62240601
  9459: 7 [  170/ 1327], train_loss/perplexity = 4.46467638/86.8929062 secs/batch = 0.2969s, grad.norm=13.90289497
  9464: 7 [  175/ 1327], train_loss/perplexity = 4.76462412/117.2870255 secs/batch = 0.2942s, grad.norm=14.27462387
  9469: 7 [  180/ 1327], train_loss/perplexity = 4.54654312/94.3058395 secs/batch = 0.2950s, grad.norm=14.66178799
  9474: 7 [  185/ 1327], train_loss/perplexity = 4.85967445/128.9822083 secs/batch = 0.2940s, grad.norm=14.63616276
  9479: 7 [  190/ 1327], train_loss/perplexity = 4.36509323/78.6567307 secs/batch = 0.2960s, grad.norm=13.74305058
  9484: 7 [  195/ 1327], train_loss/perplexity = 4.75175190/115.7869568 secs/batch = 0.2946s, grad.norm=14.15371799
  9489: 7 [  200/ 1327], train_loss/perplexity = 4.58449602/97.9538116 secs/batch = 0.2938s, grad.norm=14.35913372
  9494: 7 [  205/ 1327], train_loss/perplexity = 4.78220463/119.3672180 secs/batch = 0.3000s, grad.norm=14.07979488
  9499: 7 [  210/ 1327], train_loss/perplexity = 4.62820864/102.3305893 secs/batch = 0.3013s, grad.norm=13.91691494
  9504: 7 [  215/ 1327], train_loss/perplexity = 4.77765894/118.8258438 secs/batch = 0.2947s, grad.norm=13.94940186
  9509: 7 [  220/ 1327], train_loss/perplexity = 4.71166325/111.2370224 secs/batch = 0.2946s, grad.norm=13.90643883
  9514: 7 [  225/ 1327], train_loss/perplexity = 4.89773369/133.9857788 secs/batch = 0.2991s, grad.norm=13.48542023
  9519: 7 [  230/ 1327], train_loss/perplexity = 4.71319151/111.4071503 secs/batch = 0.2951s, grad.norm=14.47492409
  9524: 7 [  235/ 1327], train_loss/perplexity = 4.50994158/90.9165039 secs/batch = 0.2948s, grad.norm=13.31732655
  9529: 7 [  240/ 1327], train_loss/perplexity = 4.36553144/78.6912079 secs/batch = 0.2935s, grad.norm=14.45473671
  9534: 7 [  245/ 1327], train_loss/perplexity = 4.63760805/103.2969742 secs/batch = 0.3012s, grad.norm=13.93723392
  9539: 7 [  250/ 1327], train_loss/perplexity = 4.41797161/82.9279022 secs/batch = 0.2984s, grad.norm=13.64809418
  9544: 7 [  255/ 1327], train_loss/perplexity = 4.45422459/85.9894485 secs/batch = 0.2952s, grad.norm=14.52913761
  9549: 7 [  260/ 1327], train_loss/perplexity = 4.71625614/111.7490921 secs/batch = 0.2941s, grad.norm=14.54280853
  9554: 7 [  265/ 1327], train_loss/perplexity = 4.83700609/126.0912781 secs/batch = 0.2941s, grad.norm=13.56589603
  9559: 7 [  270/ 1327], train_loss/perplexity = 4.91042519/135.6970978 secs/batch = 0.2953s, grad.norm=13.27160931
  9564: 7 [  275/ 1327], train_loss/perplexity = 4.93200254/138.6569061 secs/batch = 0.2947s, grad.norm=14.91834450
  9569: 7 [  280/ 1327], train_loss/perplexity = 4.66974115/106.6701279 secs/batch = 0.2946s, grad.norm=14.22205067
  9574: 7 [  285/ 1327], train_loss/perplexity = 4.91508484/136.3308716 secs/batch = 0.2954s, grad.norm=13.56527424
  9579: 7 [  290/ 1327], train_loss/perplexity = 4.59546804/99.0344772 secs/batch = 0.3012s, grad.norm=14.49200344
  9584: 7 [  295/ 1327], train_loss/perplexity = 4.39212465/80.8119354 secs/batch = 0.2966s, grad.norm=14.27238560
  9589: 7 [  300/ 1327], train_loss/perplexity = 3.96832752/52.8959885 secs/batch = 0.2924s, grad.norm=13.86466408
  9594: 7 [  305/ 1327], train_loss/perplexity = 4.45875454/86.3798599 secs/batch = 0.2998s, grad.norm=14.31095505
  9599: 7 [  310/ 1327], train_loss/perplexity = 4.46966267/87.3272629 secs/batch = 0.2966s, grad.norm=14.09008026
  9604: 7 [  315/ 1327], train_loss/perplexity = 4.05976343/57.9605980 secs/batch = 0.3013s, grad.norm=13.78820801
  9609: 7 [  320/ 1327], train_loss/perplexity = 4.00659752/54.9595528 secs/batch = 0.2950s, grad.norm=15.75358009
  9614: 7 [  325/ 1327], train_loss/perplexity = 4.05999517/57.9740295 secs/batch = 0.2935s, grad.norm=14.08319664
  9619: 7 [  330/ 1327], train_loss/perplexity = 4.56605768/96.1642532 secs/batch = 0.3014s, grad.norm=14.69504833
  9624: 7 [  335/ 1327], train_loss/perplexity = 3.96768856/52.8622017 secs/batch = 0.2959s, grad.norm=13.72258472
  9629: 7 [  340/ 1327], train_loss/perplexity = 4.73832464/114.2426453 secs/batch = 0.3001s, grad.norm=13.65581608
  9634: 7 [  345/ 1327], train_loss/perplexity = 4.57323933/96.8573532 secs/batch = 0.2956s, grad.norm=13.48335648
  9639: 7 [  350/ 1327], train_loss/perplexity = 4.55318689/94.9344711 secs/batch = 0.2975s, grad.norm=14.14399242
  9644: 7 [  355/ 1327], train_loss/perplexity = 4.62092686/101.5881424 secs/batch = 0.2951s, grad.norm=14.50022507
  9649: 7 [  360/ 1327], train_loss/perplexity = 4.71375847/111.4703293 secs/batch = 0.3019s, grad.norm=15.46040726
  9654: 7 [  365/ 1327], train_loss/perplexity = 4.67479849/107.2109604 secs/batch = 0.2993s, grad.norm=13.98572254
  9659: 7 [  370/ 1327], train_loss/perplexity = 4.74561787/115.0788879 secs/batch = 0.2945s, grad.norm=14.41004753
  9664: 7 [  375/ 1327], train_loss/perplexity = 4.09213448/59.8675423 secs/batch = 0.2962s, grad.norm=14.27480888
  9669: 7 [  380/ 1327], train_loss/perplexity = 4.22178316/68.1549072 secs/batch = 0.2955s, grad.norm=14.69449806
  9674: 7 [  385/ 1327], train_loss/perplexity = 4.43992424/84.7685165 secs/batch = 0.2963s, grad.norm=14.96566010
  9679: 7 [  390/ 1327], train_loss/perplexity = 4.50632906/90.5886612 secs/batch = 0.2949s, grad.norm=13.66243458
  9684: 7 [  395/ 1327], train_loss/perplexity = 4.57935619/97.4516373 secs/batch = 0.2982s, grad.norm=13.97776985
  9689: 7 [  400/ 1327], train_loss/perplexity = 4.48606634/88.7715607 secs/batch = 0.2932s, grad.norm=14.05614662
  9694: 7 [  405/ 1327], train_loss/perplexity = 4.78128576/119.2575912 secs/batch = 0.2961s, grad.norm=14.42983818
  9699: 7 [  410/ 1327], train_loss/perplexity = 4.45927715/86.4250107 secs/batch = 0.2960s, grad.norm=13.79688072
  9704: 7 [  415/ 1327], train_loss/perplexity = 4.40761375/82.0733795 secs/batch = 0.2943s, grad.norm=14.43592739
  9709: 7 [  420/ 1327], train_loss/perplexity = 4.01742983/55.5581284 secs/batch = 0.2926s, grad.norm=14.48243141
  9714: 7 [  425/ 1327], train_loss/perplexity = 4.41243982/82.4704285 secs/batch = 0.2965s, grad.norm=14.97736073
  9719: 7 [  430/ 1327], train_loss/perplexity = 4.56388998/95.9560242 secs/batch = 0.2961s, grad.norm=14.51965809
  9724: 7 [  435/ 1327], train_loss/perplexity = 4.61772299/101.2631912 secs/batch = 0.2987s, grad.norm=13.82323933
  9729: 7 [  440/ 1327], train_loss/perplexity = 4.27484322/71.8688736 secs/batch = 0.2941s, grad.norm=15.03594780
  9734: 7 [  445/ 1327], train_loss/perplexity = 4.55868340/95.4577179 secs/batch = 0.2954s, grad.norm=14.97611237
  9739: 7 [  450/ 1327], train_loss/perplexity = 4.50130892/90.1350327 secs/batch = 0.2934s, grad.norm=14.40550041
  9744: 7 [  455/ 1327], train_loss/perplexity = 4.35577679/77.9273376 secs/batch = 0.3016s, grad.norm=13.49389648
  9749: 7 [  460/ 1327], train_loss/perplexity = 4.42316628/83.3598099 secs/batch = 0.2933s, grad.norm=14.98205948
  9754: 7 [  465/ 1327], train_loss/perplexity = 4.10673761/60.7482109 secs/batch = 0.3014s, grad.norm=15.57017899
  9759: 7 [  470/ 1327], train_loss/perplexity = 4.76617479/117.4690399 secs/batch = 0.2960s, grad.norm=13.12954617
  9764: 7 [  475/ 1327], train_loss/perplexity = 4.32318783/75.4286957 secs/batch = 0.3003s, grad.norm=14.61251831
  9769: 7 [  480/ 1327], train_loss/perplexity = 4.43807077/84.6115494 secs/batch = 0.3010s, grad.norm=13.88846397
  9774: 7 [  485/ 1327], train_loss/perplexity = 4.46412945/86.8453903 secs/batch = 0.2939s, grad.norm=15.07370663
  9779: 7 [  490/ 1327], train_loss/perplexity = 4.25307608/70.3213959 secs/batch = 0.2945s, grad.norm=15.25836945
  9784: 7 [  495/ 1327], train_loss/perplexity = 4.33678055/76.4609833 secs/batch = 0.2968s, grad.norm=14.41775799
  9789: 7 [  500/ 1327], train_loss/perplexity = 4.56501245/96.0637894 secs/batch = 0.2951s, grad.norm=14.98044014
  9794: 7 [  505/ 1327], train_loss/perplexity = 4.65219688/104.8150024 secs/batch = 0.2949s, grad.norm=13.48234940
  9799: 7 [  510/ 1327], train_loss/perplexity = 4.96959829/143.9690399 secs/batch = 0.2923s, grad.norm=13.75952911
  9804: 7 [  515/ 1327], train_loss/perplexity = 4.58526707/98.0293655 secs/batch = 0.2957s, grad.norm=13.67587185
  9809: 7 [  520/ 1327], train_loss/perplexity = 4.83492756/125.8294678 secs/batch = 0.3015s, grad.norm=14.56175518
  9814: 7 [  525/ 1327], train_loss/perplexity = 4.35223913/77.6521454 secs/batch = 0.3000s, grad.norm=14.17546463
  9819: 7 [  530/ 1327], train_loss/perplexity = 4.42464733/83.4833603 secs/batch = 0.2927s, grad.norm=15.54993725
  9824: 7 [  535/ 1327], train_loss/perplexity = 4.51699162/91.5597382 secs/batch = 0.2985s, grad.norm=13.91608429
  9829: 7 [  540/ 1327], train_loss/perplexity = 4.62191248/101.6883240 secs/batch = 0.2953s, grad.norm=14.03238010
  9834: 7 [  545/ 1327], train_loss/perplexity = 4.62916851/102.4288635 secs/batch = 0.3003s, grad.norm=14.83496380
  9839: 7 [  550/ 1327], train_loss/perplexity = 4.60792780/100.2761383 secs/batch = 0.2939s, grad.norm=14.43045807
  9844: 7 [  555/ 1327], train_loss/perplexity = 4.44202280/84.9465942 secs/batch = 0.2959s, grad.norm=13.78938293
  9849: 7 [  560/ 1327], train_loss/perplexity = 4.54100323/93.7848434 secs/batch = 0.2955s, grad.norm=15.85127354
  9854: 7 [  565/ 1327], train_loss/perplexity = 4.50685167/90.6360168 secs/batch = 0.2950s, grad.norm=15.73557949
  9859: 7 [  570/ 1327], train_loss/perplexity = 4.41506672/82.6873550 secs/batch = 0.2949s, grad.norm=16.29969597
  9864: 7 [  575/ 1327], train_loss/perplexity = 4.20940495/67.3164673 secs/batch = 0.2989s, grad.norm=15.29549980
  9869: 7 [  580/ 1327], train_loss/perplexity = 4.59126043/98.6186523 secs/batch = 0.2947s, grad.norm=14.14175510
  9874: 7 [  585/ 1327], train_loss/perplexity = 4.10612392/60.7109413 secs/batch = 0.2956s, grad.norm=14.30339050
  9879: 7 [  590/ 1327], train_loss/perplexity = 4.50241661/90.2349319 secs/batch = 0.2958s, grad.norm=14.06621170
  9884: 7 [  595/ 1327], train_loss/perplexity = 4.53242159/92.9834595 secs/batch = 0.2937s, grad.norm=14.96414852
  9889: 7 [  600/ 1327], train_loss/perplexity = 4.76573515/117.4174042 secs/batch = 0.2952s, grad.norm=13.75605202
  9894: 7 [  605/ 1327], train_loss/perplexity = 4.58441877/97.9462433 secs/batch = 0.2962s, grad.norm=14.34724426
  9899: 7 [  610/ 1327], train_loss/perplexity = 4.79769611/121.2307968 secs/batch = 0.2966s, grad.norm=14.72787476
  9904: 7 [  615/ 1327], train_loss/perplexity = 4.37589264/79.5107803 secs/batch = 0.2934s, grad.norm=13.99366093
  9909: 7 [  620/ 1327], train_loss/perplexity = 4.67067575/106.7698669 secs/batch = 0.2986s, grad.norm=14.89402008
  9914: 7 [  625/ 1327], train_loss/perplexity = 4.64808989/104.3854065 secs/batch = 0.2967s, grad.norm=13.73740673
  9919: 7 [  630/ 1327], train_loss/perplexity = 4.80019855/121.5345459 secs/batch = 0.2946s, grad.norm=14.01667023
  9924: 7 [  635/ 1327], train_loss/perplexity = 4.57475710/97.0044708 secs/batch = 0.2951s, grad.norm=14.62331104
  9929: 7 [  640/ 1327], train_loss/perplexity = 4.53697634/93.4079361 secs/batch = 0.2998s, grad.norm=14.15727139
  9934: 7 [  645/ 1327], train_loss/perplexity = 4.75999403/116.7452316 secs/batch = 0.2952s, grad.norm=15.47777843
  9939: 7 [  650/ 1327], train_loss/perplexity = 4.30213261/73.8571320 secs/batch = 0.3014s, grad.norm=14.96212101
  9944: 7 [  655/ 1327], train_loss/perplexity = 4.47087622/87.4332962 secs/batch = 0.2949s, grad.norm=14.75388622
  9949: 7 [  660/ 1327], train_loss/perplexity = 4.40601063/81.9419174 secs/batch = 0.2935s, grad.norm=14.40021324
  9954: 7 [  665/ 1327], train_loss/perplexity = 4.58412743/97.9177094 secs/batch = 0.2972s, grad.norm=14.54987240
  9959: 7 [  670/ 1327], train_loss/perplexity = 4.41831398/82.9562988 secs/batch = 0.2960s, grad.norm=14.28543091
  9964: 7 [  675/ 1327], train_loss/perplexity = 4.26831770/71.4014130 secs/batch = 0.2940s, grad.norm=14.67283535
  9969: 7 [  680/ 1327], train_loss/perplexity = 4.48596430/88.7625046 secs/batch = 0.3003s, grad.norm=15.37925529
  9974: 7 [  685/ 1327], train_loss/perplexity = 4.35067606/77.5308609 secs/batch = 0.2962s, grad.norm=14.51667595
  9979: 7 [  690/ 1327], train_loss/perplexity = 4.72019053/112.1896286 secs/batch = 0.2991s, grad.norm=14.08028030
  9984: 7 [  695/ 1327], train_loss/perplexity = 4.51570702/91.4421921 secs/batch = 0.2962s, grad.norm=14.44579411
  9989: 7 [  700/ 1327], train_loss/perplexity = 4.76001072/116.7471771 secs/batch = 0.2948s, grad.norm=14.77540016
  9994: 7 [  705/ 1327], train_loss/perplexity = 4.50765610/90.7089539 secs/batch = 0.2997s, grad.norm=13.39958858
  9999: 7 [  710/ 1327], train_loss/perplexity = 4.41471243/82.6580658 secs/batch = 0.2940s, grad.norm=14.46337223
 10004: 7 [  715/ 1327], train_loss/perplexity = 4.32065344/75.2377777 secs/batch = 0.2948s, grad.norm=14.09464836
 10009: 7 [  720/ 1327], train_loss/perplexity = 4.33176804/76.0786743 secs/batch = 0.3025s, grad.norm=15.09664726
 10014: 7 [  725/ 1327], train_loss/perplexity = 4.32936764/75.8962784 secs/batch = 0.2945s, grad.norm=14.71300220
 10019: 7 [  730/ 1327], train_loss/perplexity = 4.52306128/92.1171646 secs/batch = 0.2933s, grad.norm=14.91620064
 10024: 7 [  735/ 1327], train_loss/perplexity = 4.55372095/94.9851837 secs/batch = 0.2939s, grad.norm=14.82314396
 10029: 7 [  740/ 1327], train_loss/perplexity = 4.00248051/54.7337494 secs/batch = 0.2938s, grad.norm=13.43223572
 10034: 7 [  745/ 1327], train_loss/perplexity = 4.47583342/87.8677979 secs/batch = 0.2943s, grad.norm=14.08053684
 10039: 7 [  750/ 1327], train_loss/perplexity = 4.38009167/79.8453522 secs/batch = 0.2953s, grad.norm=14.43447399
 10044: 7 [  755/ 1327], train_loss/perplexity = 4.26031733/70.8324585 secs/batch = 0.2988s, grad.norm=14.91560555
 10049: 7 [  760/ 1327], train_loss/perplexity = 4.18358660/65.6007156 secs/batch = 0.2935s, grad.norm=13.79203892
 10054: 7 [  765/ 1327], train_loss/perplexity = 4.24681091/69.8821945 secs/batch = 0.2922s, grad.norm=14.17940426
 10059: 7 [  770/ 1327], train_loss/perplexity = 4.20569849/67.0674286 secs/batch = 0.2969s, grad.norm=15.11690807
 10064: 7 [  775/ 1327], train_loss/perplexity = 4.41474199/82.6605148 secs/batch = 0.2984s, grad.norm=14.72157097
 10069: 7 [  780/ 1327], train_loss/perplexity = 4.74769211/115.3178329 secs/batch = 0.3010s, grad.norm=13.78048611
 10074: 7 [  785/ 1327], train_loss/perplexity = 4.50161600/90.1627197 secs/batch = 0.2946s, grad.norm=14.23163414
 10079: 7 [  790/ 1327], train_loss/perplexity = 4.31120634/74.5303421 secs/batch = 0.2933s, grad.norm=14.59282112
 10084: 7 [  795/ 1327], train_loss/perplexity = 4.66284561/105.9371109 secs/batch = 0.2953s, grad.norm=14.64311981
 10089: 7 [  800/ 1327], train_loss/perplexity = 4.59130669/98.6232147 secs/batch = 0.2998s, grad.norm=15.53959084
 10094: 7 [  805/ 1327], train_loss/perplexity = 4.85935116/128.9405060 secs/batch = 0.2973s, grad.norm=13.58596802
 10099: 7 [  810/ 1327], train_loss/perplexity = 4.50264263/90.2553253 secs/batch = 0.2915s, grad.norm=14.30197239
 10104: 7 [  815/ 1327], train_loss/perplexity = 4.36880255/78.9490356 secs/batch = 0.2951s, grad.norm=13.99338627
 10109: 7 [  820/ 1327], train_loss/perplexity = 4.18147373/65.4622574 secs/batch = 0.2959s, grad.norm=13.75923920
 10114: 7 [  825/ 1327], train_loss/perplexity = 4.46295834/86.7437439 secs/batch = 0.2943s, grad.norm=14.21501541
 10119: 7 [  830/ 1327], train_loss/perplexity = 4.17486954/65.0313568 secs/batch = 0.2927s, grad.norm=14.53972721
 10124: 7 [  835/ 1327], train_loss/perplexity = 4.52093697/91.9216843 secs/batch = 0.2984s, grad.norm=14.90026665
 10129: 7 [  840/ 1327], train_loss/perplexity = 4.54173756/93.8537369 secs/batch = 0.2935s, grad.norm=14.55277920
 10134: 7 [  845/ 1327], train_loss/perplexity = 4.33509731/76.3323898 secs/batch = 0.2984s, grad.norm=14.48875713
 10139: 7 [  850/ 1327], train_loss/perplexity = 4.44682693/85.3556747 secs/batch = 0.3000s, grad.norm=14.23023701
 10144: 7 [  855/ 1327], train_loss/perplexity = 4.49127579/89.2352219 secs/batch = 0.2943s, grad.norm=15.50959969
 10149: 7 [  860/ 1327], train_loss/perplexity = 4.15703154/63.8816109 secs/batch = 0.2965s, grad.norm=14.30757141
 10154: 7 [  865/ 1327], train_loss/perplexity = 4.62145329/101.6416397 secs/batch = 0.3004s, grad.norm=14.40592861
 10159: 7 [  870/ 1327], train_loss/perplexity = 4.55236435/94.8564148 secs/batch = 0.2940s, grad.norm=14.52108192
 10164: 7 [  875/ 1327], train_loss/perplexity = 4.07750463/58.9980659 secs/batch = 0.2935s, grad.norm=13.97390175
 10169: 7 [  880/ 1327], train_loss/perplexity = 4.26157665/70.9217148 secs/batch = 0.2975s, grad.norm=13.29267883
 10174: 7 [  885/ 1327], train_loss/perplexity = 4.56481934/96.0452423 secs/batch = 0.2944s, grad.norm=17.49641418
 10179: 7 [  890/ 1327], train_loss/perplexity = 4.64577818/104.1443787 secs/batch = 0.2996s, grad.norm=14.00505733
 10184: 7 [  895/ 1327], train_loss/perplexity = 4.59879780/99.3647842 secs/batch = 0.2933s, grad.norm=13.42994499
 10189: 7 [  900/ 1327], train_loss/perplexity = 4.42621136/83.6140289 secs/batch = 0.2960s, grad.norm=14.80883312
 10194: 7 [  905/ 1327], train_loss/perplexity = 4.25885725/70.7291107 secs/batch = 0.2959s, grad.norm=15.05801105
 10199: 7 [  910/ 1327], train_loss/perplexity = 4.27798796/72.0952377 secs/batch = 0.2951s, grad.norm=14.00061131
 10204: 7 [  915/ 1327], train_loss/perplexity = 4.61941051/101.4342194 secs/batch = 0.2956s, grad.norm=14.41446209
 10209: 7 [  920/ 1327], train_loss/perplexity = 4.77345514/118.3273773 secs/batch = 0.2958s, grad.norm=15.19217396
 10214: 7 [  925/ 1327], train_loss/perplexity = 4.55898666/95.4866714 secs/batch = 0.2939s, grad.norm=13.79040527
 10219: 7 [  930/ 1327], train_loss/perplexity = 4.56899929/96.4475479 secs/batch = 0.3014s, grad.norm=14.16726303
 10224: 7 [  935/ 1327], train_loss/perplexity = 4.58081532/97.5939331 secs/batch = 0.2988s, grad.norm=13.79577351
 10229: 7 [  940/ 1327], train_loss/perplexity = 4.57007122/96.5509872 secs/batch = 0.2942s, grad.norm=13.62510872
 10234: 7 [  945/ 1327], train_loss/perplexity = 4.78918600/120.2034836 secs/batch = 0.2967s, grad.norm=13.55630016
 10239: 7 [  950/ 1327], train_loss/perplexity = 4.51556587/91.4292908 secs/batch = 0.2994s, grad.norm=13.88802338
 10244: 7 [  955/ 1327], train_loss/perplexity = 4.53991032/93.6823959 secs/batch = 0.2957s, grad.norm=14.26686382
 10249: 7 [  960/ 1327], train_loss/perplexity = 4.83115578/125.3557587 secs/batch = 0.2959s, grad.norm=14.63681793
 10254: 7 [  965/ 1327], train_loss/perplexity = 4.60208225/99.6916809 secs/batch = 0.3003s, grad.norm=13.75900650
 10259: 7 [  970/ 1327], train_loss/perplexity = 4.81685925/123.5763550 secs/batch = 0.2974s, grad.norm=14.54761982
 10264: 7 [  975/ 1327], train_loss/perplexity = 4.50912285/90.8421021 secs/batch = 0.2925s, grad.norm=14.92453480
 10269: 7 [  980/ 1327], train_loss/perplexity = 4.32683039/75.7039566 secs/batch = 0.2963s, grad.norm=13.66564655
 10274: 7 [  985/ 1327], train_loss/perplexity = 4.50287771/90.2765503 secs/batch = 0.2934s, grad.norm=14.43111706
 10279: 7 [  990/ 1327], train_loss/perplexity = 4.69982767/109.9282303 secs/batch = 0.2949s, grad.norm=15.00832462
 10284: 7 [  995/ 1327], train_loss/perplexity = 4.65236759/104.8328934 secs/batch = 0.2952s, grad.norm=13.86694336
 10289: 7 [ 1000/ 1327], train_loss/perplexity = 4.26305771/71.0268326 secs/batch = 0.2949s, grad.norm=13.52737999
 10294: 7 [ 1005/ 1327], train_loss/perplexity = 4.68712044/108.5401840 secs/batch = 0.2946s, grad.norm=14.07695198
 10299: 7 [ 1010/ 1327], train_loss/perplexity = 4.24200726/69.5473099 secs/batch = 0.2937s, grad.norm=12.99168873
 10304: 7 [ 1015/ 1327], train_loss/perplexity = 4.81355762/123.1690292 secs/batch = 0.2963s, grad.norm=14.03117466
 10309: 7 [ 1020/ 1327], train_loss/perplexity = 4.83556414/125.9095917 secs/batch = 0.2956s, grad.norm=13.70728111
 10314: 7 [ 1025/ 1327], train_loss/perplexity = 4.70195103/110.1618881 secs/batch = 0.2949s, grad.norm=13.94885159
 10319: 7 [ 1030/ 1327], train_loss/perplexity = 4.50144911/90.1476669 secs/batch = 0.2960s, grad.norm=13.51415730
 10324: 7 [ 1035/ 1327], train_loss/perplexity = 4.41679049/82.8300171 secs/batch = 0.2937s, grad.norm=13.61880589
 10329: 7 [ 1040/ 1327], train_loss/perplexity = 4.60144615/99.6282883 secs/batch = 0.2993s, grad.norm=14.24561024
 10334: 7 [ 1045/ 1327], train_loss/perplexity = 4.19132471/66.1103134 secs/batch = 0.2939s, grad.norm=13.49416065
 10339: 7 [ 1050/ 1327], train_loss/perplexity = 4.32225084/75.3580551 secs/batch = 0.2935s, grad.norm=15.15816975
 10344: 7 [ 1055/ 1327], train_loss/perplexity = 4.41299343/82.5160980 secs/batch = 0.2941s, grad.norm=15.09584045
 10349: 7 [ 1060/ 1327], train_loss/perplexity = 4.02588749/56.0300140 secs/batch = 0.2954s, grad.norm=15.14982319
 10354: 7 [ 1065/ 1327], train_loss/perplexity = 4.17649364/65.1370621 secs/batch = 0.2958s, grad.norm=14.33887482
 10359: 7 [ 1070/ 1327], train_loss/perplexity = 4.54167843/93.8481827 secs/batch = 0.2968s, grad.norm=14.48165035
 10364: 7 [ 1075/ 1327], train_loss/perplexity = 4.29226828/73.1321640 secs/batch = 0.2938s, grad.norm=14.05180740
 10369: 7 [ 1080/ 1327], train_loss/perplexity = 4.22938967/68.6753082 secs/batch = 0.3015s, grad.norm=14.22448540
 10374: 7 [ 1085/ 1327], train_loss/perplexity = 4.08342934/59.3486481 secs/batch = 0.2982s, grad.norm=14.08048725
 10379: 7 [ 1090/ 1327], train_loss/perplexity = 4.31333971/74.6895142 secs/batch = 0.3008s, grad.norm=15.10140419
 10384: 7 [ 1095/ 1327], train_loss/perplexity = 4.41480446/82.6656723 secs/batch = 0.2946s, grad.norm=14.05297279
 10389: 7 [ 1100/ 1327], train_loss/perplexity = 4.18455410/65.6642151 secs/batch = 0.2918s, grad.norm=16.40756226
 10394: 7 [ 1105/ 1327], train_loss/perplexity = 4.16578150/64.4430237 secs/batch = 0.2956s, grad.norm=14.22085285
 10399: 7 [ 1110/ 1327], train_loss/perplexity = 4.55808067/95.4001999 secs/batch = 0.2941s, grad.norm=15.48421383
 10404: 7 [ 1115/ 1327], train_loss/perplexity = 4.23138380/68.8123856 secs/batch = 0.2978s, grad.norm=14.25875282
 10409: 7 [ 1120/ 1327], train_loss/perplexity = 4.48038149/88.2683411 secs/batch = 0.2933s, grad.norm=14.24770164
 10414: 7 [ 1125/ 1327], train_loss/perplexity = 4.66985226/106.6819763 secs/batch = 0.2998s, grad.norm=15.14347553
 10419: 7 [ 1130/ 1327], train_loss/perplexity = 4.39874506/81.3487167 secs/batch = 0.2944s, grad.norm=16.35465622
 10424: 7 [ 1135/ 1327], train_loss/perplexity = 4.34195805/76.8578796 secs/batch = 0.2987s, grad.norm=14.07437134
 10429: 7 [ 1140/ 1327], train_loss/perplexity = 4.59604645/99.0917740 secs/batch = 0.2939s, grad.norm=15.66529465
 10434: 7 [ 1145/ 1327], train_loss/perplexity = 4.36986637/79.0330734 secs/batch = 0.2986s, grad.norm=14.28535271
 10439: 7 [ 1150/ 1327], train_loss/perplexity = 4.47155523/87.4926910 secs/batch = 0.2959s, grad.norm=14.82973480
 10444: 7 [ 1155/ 1327], train_loss/perplexity = 4.50314236/90.3004456 secs/batch = 0.2931s, grad.norm=14.91918087
 10449: 7 [ 1160/ 1327], train_loss/perplexity = 4.47089958/87.4353409 secs/batch = 0.2949s, grad.norm=14.77555180
 10454: 7 [ 1165/ 1327], train_loss/perplexity = 4.55589867/95.1922607 secs/batch = 0.2925s, grad.norm=14.36019421
 10459: 7 [ 1170/ 1327], train_loss/perplexity = 4.33721542/76.4942398 secs/batch = 0.3008s, grad.norm=14.95992374
 10464: 7 [ 1175/ 1327], train_loss/perplexity = 4.13804293/62.6800308 secs/batch = 0.2934s, grad.norm=15.46064281
 10469: 7 [ 1180/ 1327], train_loss/perplexity = 4.14455605/63.0896072 secs/batch = 0.2941s, grad.norm=14.47562313
 10474: 7 [ 1185/ 1327], train_loss/perplexity = 4.38312435/80.0878677 secs/batch = 0.2927s, grad.norm=14.89617443
 10479: 7 [ 1190/ 1327], train_loss/perplexity = 4.55010891/94.6427155 secs/batch = 0.2948s, grad.norm=15.21753597
 10484: 7 [ 1195/ 1327], train_loss/perplexity = 4.20421505/66.9680099 secs/batch = 0.2996s, grad.norm=14.08783722
 10489: 7 [ 1200/ 1327], train_loss/perplexity = 4.20683622/67.1437759 secs/batch = 0.2941s, grad.norm=13.77921104
 10494: 7 [ 1205/ 1327], train_loss/perplexity = 4.26070786/70.8601227 secs/batch = 0.2944s, grad.norm=14.90237617
 10499: 7 [ 1210/ 1327], train_loss/perplexity = 3.84027529/46.5382843 secs/batch = 0.2956s, grad.norm=14.93172455
 10504: 7 [ 1215/ 1327], train_loss/perplexity = 4.16840410/64.6122589 secs/batch = 0.2947s, grad.norm=14.50127125
 10509: 7 [ 1220/ 1327], train_loss/perplexity = 4.24217749/69.5591507 secs/batch = 0.2953s, grad.norm=14.88627529
 10514: 7 [ 1225/ 1327], train_loss/perplexity = 3.99169970/54.1468430 secs/batch = 0.2945s, grad.norm=15.36440754
 10519: 7 [ 1230/ 1327], train_loss/perplexity = 4.26588249/71.2277527 secs/batch = 0.2949s, grad.norm=14.24230385
 10524: 7 [ 1235/ 1327], train_loss/perplexity = 4.23323059/68.9395905 secs/batch = 0.2936s, grad.norm=14.11191273
 10529: 7 [ 1240/ 1327], train_loss/perplexity = 4.43441868/84.3031006 secs/batch = 0.3002s, grad.norm=14.66986179
 10534: 7 [ 1245/ 1327], train_loss/perplexity = 4.40373421/81.7555923 secs/batch = 0.2991s, grad.norm=13.69259739
 10539: 7 [ 1250/ 1327], train_loss/perplexity = 4.52276611/92.0899811 secs/batch = 0.3009s, grad.norm=14.14680672
 10544: 7 [ 1255/ 1327], train_loss/perplexity = 4.50720453/90.6680069 secs/batch = 0.2955s, grad.norm=13.45941925
 10549: 7 [ 1260/ 1327], train_loss/perplexity = 4.34814262/77.3346863 secs/batch = 0.2948s, grad.norm=15.54246998
 10554: 7 [ 1265/ 1327], train_loss/perplexity = 4.55556536/95.1605377 secs/batch = 0.2964s, grad.norm=14.95712185
 10559: 7 [ 1270/ 1327], train_loss/perplexity = 4.32319927/75.4295654 secs/batch = 0.2940s, grad.norm=15.08626080
 10564: 7 [ 1275/ 1327], train_loss/perplexity = 4.42422390/83.4480209 secs/batch = 0.2988s, grad.norm=14.49556828
 10569: 7 [ 1280/ 1327], train_loss/perplexity = 4.26484966/71.1542206 secs/batch = 0.2944s, grad.norm=14.54991341
 10574: 7 [ 1285/ 1327], train_loss/perplexity = 4.21144962/67.4542542 secs/batch = 0.2930s, grad.norm=14.63653469
 10579: 7 [ 1290/ 1327], train_loss/perplexity = 4.37160254/79.1704025 secs/batch = 0.2942s, grad.norm=14.01848316
 10584: 7 [ 1295/ 1327], train_loss/perplexity = 4.43927145/84.7132034 secs/batch = 0.2987s, grad.norm=14.65821552
 10589: 7 [ 1300/ 1327], train_loss/perplexity = 4.62483454/101.9858932 secs/batch = 0.2986s, grad.norm=13.95392513
 10594: 7 [ 1305/ 1327], train_loss/perplexity = 4.69338512/109.2222824 secs/batch = 0.2937s, grad.norm=15.22968388
 10599: 7 [ 1310/ 1327], train_loss/perplexity = 4.94182539/140.0256195 secs/batch = 0.2925s, grad.norm=14.34147453
 10604: 7 [ 1315/ 1327], train_loss/perplexity = 4.77592134/118.6195526 secs/batch = 0.2947s, grad.norm=14.28022861
 10609: 7 [ 1320/ 1327], train_loss/perplexity = 4.74915171/115.4862747 secs/batch = 0.2943s, grad.norm=14.41071415
 10614: 7 [ 1325/ 1327], train_loss/perplexity = 4.57994080/97.5086212 secs/batch = 0.2933s, grad.norm=14.14313507
Epoch training time: 393.3032867908478
	> validation loss = 4.82270193, perplexity = 124.30049133
	> validation loss = 4.68375015, perplexity = 108.17498779
	> validation loss = 4.65575647, perplexity = 105.18875885
	> validation loss = 4.66920614, perplexity = 106.61307526
	> validation loss = 4.84684658, perplexity = 127.33820343
	> validation loss = 4.74379253, perplexity = 114.86901855
	> validation loss = 4.72252846, perplexity = 112.45222473
	> validation loss = 4.57983780, perplexity = 97.49858093
	> validation loss = 4.36994743, perplexity = 79.03947449
	> validation loss = 4.47017050, perplexity = 87.37162018
	> validation loss = 4.63783836, perplexity = 103.32076263
	> validation loss = 4.69042015, perplexity = 108.89892578
	> validation loss = 4.56128931, perplexity = 95.70679474
	> validation loss = 4.42222404, perplexity = 83.28130341
	> validation loss = 4.35208035, perplexity = 77.63981628
	> validation loss = 4.32191706, perplexity = 75.33290863
	> validation loss = 4.76369381, perplexity = 117.17796326
	> validation loss = 4.32263517, perplexity = 75.38702393
	> validation loss = 4.74213362, perplexity = 114.67861938
	> validation loss = 4.62953806, perplexity = 102.46672058
	> validation loss = 4.43446732, perplexity = 84.30720520
at the end of epoch: 7
train loss = 4.55926686, perplexity = 95.51342989
validation loss = 4.59203821, perplexity = 98.69538713
Saved model cv/epoch007_4.5920.model
 10621: 8 [    5/ 1327], train_loss/perplexity = 4.70983553/111.0338974 secs/batch = 0.2948s, grad.norm=14.43259048
 10626: 8 [   10/ 1327], train_loss/perplexity = 4.22472143/68.3554611 secs/batch = 0.2940s, grad.norm=13.60420513
 10631: 8 [   15/ 1327], train_loss/perplexity = 4.57460451/96.9896698 secs/batch = 0.2944s, grad.norm=12.96269798
 10636: 8 [   20/ 1327], train_loss/perplexity = 4.70543909/110.5468140 secs/batch = 0.2959s, grad.norm=13.74523830
 10641: 8 [   25/ 1327], train_loss/perplexity = 4.58384037/97.8896027 secs/batch = 0.2959s, grad.norm=14.68286800
 10646: 8 [   30/ 1327], train_loss/perplexity = 4.63884830/103.4251633 secs/batch = 0.2943s, grad.norm=14.03513241
 10651: 8 [   35/ 1327], train_loss/perplexity = 4.41321516/82.5344009 secs/batch = 0.2994s, grad.norm=14.29037666
 10656: 8 [   40/ 1327], train_loss/perplexity = 4.37525749/79.4602966 secs/batch = 0.3014s, grad.norm=14.15499687
 10661: 8 [   45/ 1327], train_loss/perplexity = 4.19775915/66.5370636 secs/batch = 0.2943s, grad.norm=13.50201988
 10666: 8 [   50/ 1327], train_loss/perplexity = 4.41007996/82.2760391 secs/batch = 0.3006s, grad.norm=14.06187630
 10671: 8 [   55/ 1327], train_loss/perplexity = 4.38097858/79.9161987 secs/batch = 0.2944s, grad.norm=14.32266521
 10676: 8 [   60/ 1327], train_loss/perplexity = 4.69341278/109.2253036 secs/batch = 0.2992s, grad.norm=15.03293800
 10681: 8 [   65/ 1327], train_loss/perplexity = 4.21682215/67.8176270 secs/batch = 0.2943s, grad.norm=13.39267349
 10686: 8 [   70/ 1327], train_loss/perplexity = 4.08434820/59.4032059 secs/batch = 0.2951s, grad.norm=14.60383034
 10691: 8 [   75/ 1327], train_loss/perplexity = 3.92875314/50.8435440 secs/batch = 0.2944s, grad.norm=13.86067200
 10696: 8 [   80/ 1327], train_loss/perplexity = 4.37148190/79.1608505 secs/batch = 0.2952s, grad.norm=14.93636513
 10701: 8 [   85/ 1327], train_loss/perplexity = 4.37676144/79.5798874 secs/batch = 0.2950s, grad.norm=14.52816105
 10706: 8 [   90/ 1327], train_loss/perplexity = 4.44371176/85.0901871 secs/batch = 0.3010s, grad.norm=14.97508621
 10711: 8 [   95/ 1327], train_loss/perplexity = 4.28692627/72.7425308 secs/batch = 0.2962s, grad.norm=14.84729195
 10716: 8 [  100/ 1327], train_loss/perplexity = 4.58293629/97.8011475 secs/batch = 0.2949s, grad.norm=15.37567806
 10721: 8 [  105/ 1327], train_loss/perplexity = 4.40259933/81.6628647 secs/batch = 0.2956s, grad.norm=15.02801609
 10726: 8 [  110/ 1327], train_loss/perplexity = 4.37200451/79.2022324 secs/batch = 0.3008s, grad.norm=14.68516827
 10731: 8 [  115/ 1327], train_loss/perplexity = 4.22026253/68.0513458 secs/batch = 0.2951s, grad.norm=15.02362633
 10736: 8 [  120/ 1327], train_loss/perplexity = 4.37455034/79.4041290 secs/batch = 0.2994s, grad.norm=14.55173588
 10741: 8 [  125/ 1327], train_loss/perplexity = 4.44959593/85.5923538 secs/batch = 0.2915s, grad.norm=15.58037186
 10746: 8 [  130/ 1327], train_loss/perplexity = 4.37805223/79.6826782 secs/batch = 0.2933s, grad.norm=14.99466991
 10751: 8 [  135/ 1327], train_loss/perplexity = 4.30951405/74.4043198 secs/batch = 0.2950s, grad.norm=13.71502018
 10756: 8 [  140/ 1327], train_loss/perplexity = 4.64623594/104.1920624 secs/batch = 0.2981s, grad.norm=14.37548637
 10761: 8 [  145/ 1327], train_loss/perplexity = 4.46008062/86.4944839 secs/batch = 0.2937s, grad.norm=15.43995762
 10766: 8 [  150/ 1327], train_loss/perplexity = 4.52262163/92.0766754 secs/batch = 0.2926s, grad.norm=14.53861332
 10771: 8 [  155/ 1327], train_loss/perplexity = 4.84617186/127.2523193 secs/batch = 0.3028s, grad.norm=13.90569019
 10776: 8 [  160/ 1327], train_loss/perplexity = 4.49729395/89.7738724 secs/batch = 0.2967s, grad.norm=14.17118168
 10781: 8 [  165/ 1327], train_loss/perplexity = 4.60414314/99.8973465 secs/batch = 0.2937s, grad.norm=14.23878956
 10786: 8 [  170/ 1327], train_loss/perplexity = 4.38315344/80.0901947 secs/batch = 0.2932s, grad.norm=13.83511353
 10791: 8 [  175/ 1327], train_loss/perplexity = 4.71041012/111.0977173 secs/batch = 0.3011s, grad.norm=14.62021065
 10796: 8 [  180/ 1327], train_loss/perplexity = 4.51888514/91.7332687 secs/batch = 0.2938s, grad.norm=14.68806458
 10801: 8 [  185/ 1327], train_loss/perplexity = 4.84987497/127.7244186 secs/batch = 0.2989s, grad.norm=14.48252773
 10806: 8 [  190/ 1327], train_loss/perplexity = 4.33356428/76.2154541 secs/batch = 0.2943s, grad.norm=13.17004299
 10811: 8 [  195/ 1327], train_loss/perplexity = 4.66265678/105.9171066 secs/batch = 0.2997s, grad.norm=13.82236290
 10816: 8 [  200/ 1327], train_loss/perplexity = 4.56515360/96.0773468 secs/batch = 0.2983s, grad.norm=14.41179466
 10821: 8 [  205/ 1327], train_loss/perplexity = 4.74604940/115.1285553 secs/batch = 0.2987s, grad.norm=14.58410263
 10826: 8 [  210/ 1327], train_loss/perplexity = 4.55946398/95.5322571 secs/batch = 0.2955s, grad.norm=13.35895157
 10831: 8 [  215/ 1327], train_loss/perplexity = 4.74995089/115.5786057 secs/batch = 0.2948s, grad.norm=13.54279709
 10836: 8 [  220/ 1327], train_loss/perplexity = 4.62046766/101.5415115 secs/batch = 0.2948s, grad.norm=13.90000916
 10841: 8 [  225/ 1327], train_loss/perplexity = 4.79645395/121.0802994 secs/batch = 0.2942s, grad.norm=14.02064705
 10846: 8 [  230/ 1327], train_loss/perplexity = 4.71411228/111.5097809 secs/batch = 0.2923s, grad.norm=15.01977348
 10851: 8 [  235/ 1327], train_loss/perplexity = 4.48778009/88.9238205 secs/batch = 0.2952s, grad.norm=13.40046120
 10856: 8 [  240/ 1327], train_loss/perplexity = 4.26202106/70.9532394 secs/batch = 0.2944s, grad.norm=14.57997608
 10861: 8 [  245/ 1327], train_loss/perplexity = 4.50834513/90.7714767 secs/batch = 0.2942s, grad.norm=13.71765423
 10866: 8 [  250/ 1327], train_loss/perplexity = 4.34724998/77.2656860 secs/batch = 0.2940s, grad.norm=13.67837334
 10871: 8 [  255/ 1327], train_loss/perplexity = 4.28661251/72.7197113 secs/batch = 0.2954s, grad.norm=14.32262516
 10876: 8 [  260/ 1327], train_loss/perplexity = 4.68215132/108.0021667 secs/batch = 0.2942s, grad.norm=15.28166771
 10881: 8 [  265/ 1327], train_loss/perplexity = 4.72649384/112.8990250 secs/batch = 0.2929s, grad.norm=13.75025558
 10886: 8 [  270/ 1327], train_loss/perplexity = 4.83698082/126.0880966 secs/batch = 0.2961s, grad.norm=14.16061783
 10891: 8 [  275/ 1327], train_loss/perplexity = 4.83035564/125.2555008 secs/batch = 0.2934s, grad.norm=14.46249008
 10896: 8 [  280/ 1327], train_loss/perplexity = 4.56260395/95.8326950 secs/batch = 0.2949s, grad.norm=14.77841663
 10901: 8 [  285/ 1327], train_loss/perplexity = 4.83051062/125.2749100 secs/batch = 0.2924s, grad.norm=13.97023296
 10906: 8 [  290/ 1327], train_loss/perplexity = 4.54721785/94.3694916 secs/batch = 0.2947s, grad.norm=14.92243385
 10911: 8 [  295/ 1327], train_loss/perplexity = 4.33112717/76.0299377 secs/batch = 0.2946s, grad.norm=15.09537029
 10916: 8 [  300/ 1327], train_loss/perplexity = 3.91434860/50.1164169 secs/batch = 0.2918s, grad.norm=14.17567539
 10921: 8 [  305/ 1327], train_loss/perplexity = 4.43671942/84.4972839 secs/batch = 0.2967s, grad.norm=14.42420959
 10926: 8 [  310/ 1327], train_loss/perplexity = 4.44173241/84.9219360 secs/batch = 0.2986s, grad.norm=14.16920090
 10931: 8 [  315/ 1327], train_loss/perplexity = 3.93804526/51.3181915 secs/batch = 0.2938s, grad.norm=14.01597881
 10936: 8 [  320/ 1327], train_loss/perplexity = 3.95308423/52.0957947 secs/batch = 0.2940s, grad.norm=15.13844776
 10941: 8 [  325/ 1327], train_loss/perplexity = 3.93671656/51.2500496 secs/batch = 0.2947s, grad.norm=13.63564205
 10946: 8 [  330/ 1327], train_loss/perplexity = 4.55380058/94.9927521 secs/batch = 0.2951s, grad.norm=15.30051804
 10951: 8 [  335/ 1327], train_loss/perplexity = 3.97181129/53.0805893 secs/batch = 0.2981s, grad.norm=13.80104351
 10956: 8 [  340/ 1327], train_loss/perplexity = 4.63686609/103.2203598 secs/batch = 0.2939s, grad.norm=13.81159401
 10961: 8 [  345/ 1327], train_loss/perplexity = 4.47544718/87.8338699 secs/batch = 0.2990s, grad.norm=13.88248444
 10966: 8 [  350/ 1327], train_loss/perplexity = 4.48962498/89.0880280 secs/batch = 0.2958s, grad.norm=14.89025784
 10971: 8 [  355/ 1327], train_loss/perplexity = 4.57736397/97.2576828 secs/batch = 0.2981s, grad.norm=14.91433239
 10976: 8 [  360/ 1327], train_loss/perplexity = 4.61576605/101.0652161 secs/batch = 0.2943s, grad.norm=15.16628647
 10981: 8 [  365/ 1327], train_loss/perplexity = 4.62659264/102.1653595 secs/batch = 0.2945s, grad.norm=14.83689785
 10986: 8 [  370/ 1327], train_loss/perplexity = 4.63252163/102.7728958 secs/batch = 0.2980s, grad.norm=14.08064079
 10991: 8 [  375/ 1327], train_loss/perplexity = 4.04815197/57.2914810 secs/batch = 0.3006s, grad.norm=14.65550804
 10996: 8 [  380/ 1327], train_loss/perplexity = 4.14006424/62.8068542 secs/batch = 0.2937s, grad.norm=14.34376621
 11001: 8 [  385/ 1327], train_loss/perplexity = 4.38560820/80.2870407 secs/batch = 0.2987s, grad.norm=15.58464050
 11006: 8 [  390/ 1327], train_loss/perplexity = 4.51131010/91.0410156 secs/batch = 0.2944s, grad.norm=14.47073174
 11011: 8 [  395/ 1327], train_loss/perplexity = 4.61850309/101.3422165 secs/batch = 0.2936s, grad.norm=14.60024166
 11016: 8 [  400/ 1327], train_loss/perplexity = 4.43037128/83.9625854 secs/batch = 0.2985s, grad.norm=13.48329067
 11021: 8 [  405/ 1327], train_loss/perplexity = 4.78672743/119.9083176 secs/batch = 0.2943s, grad.norm=14.43558979
 11026: 8 [  410/ 1327], train_loss/perplexity = 4.41600227/82.7647552 secs/batch = 0.2933s, grad.norm=14.41045094
 11031: 8 [  415/ 1327], train_loss/perplexity = 4.35342789/77.7445068 secs/batch = 0.2947s, grad.norm=14.40458488
 11036: 8 [  420/ 1327], train_loss/perplexity = 3.98146629/53.5955620 secs/batch = 0.2950s, grad.norm=14.49156761
 11041: 8 [  425/ 1327], train_loss/perplexity = 4.37283659/79.2681656 secs/batch = 0.2944s, grad.norm=15.75422287
 11046: 8 [  430/ 1327], train_loss/perplexity = 4.48254299/88.4593353 secs/batch = 0.2929s, grad.norm=14.47286320
 11051: 8 [  435/ 1327], train_loss/perplexity = 4.59426403/98.9153137 secs/batch = 0.2916s, grad.norm=14.65792942
 11056: 8 [  440/ 1327], train_loss/perplexity = 4.16910505/64.6575623 secs/batch = 0.2950s, grad.norm=15.00258255
 11061: 8 [  445/ 1327], train_loss/perplexity = 4.56493092/96.0559616 secs/batch = 0.2927s, grad.norm=15.64606953
 11066: 8 [  450/ 1327], train_loss/perplexity = 4.46538353/86.9543762 secs/batch = 0.2941s, grad.norm=15.04724407
 11071: 8 [  455/ 1327], train_loss/perplexity = 4.29581594/73.3920746 secs/batch = 0.2950s, grad.norm=14.06996918
 11076: 8 [  460/ 1327], train_loss/perplexity = 4.45087528/85.7019272 secs/batch = 0.2954s, grad.norm=15.31564426
 11081: 8 [  465/ 1327], train_loss/perplexity = 4.09078741/59.7869492 secs/batch = 0.2966s, grad.norm=15.88852692
 11086: 8 [  470/ 1327], train_loss/perplexity = 4.78878069/120.1547699 secs/batch = 0.2935s, grad.norm=14.13838863
 11091: 8 [  475/ 1327], train_loss/perplexity = 4.22283697/68.2267685 secs/batch = 0.2940s, grad.norm=15.02556419
 11096: 8 [  480/ 1327], train_loss/perplexity = 4.41335392/82.5458527 secs/batch = 0.2935s, grad.norm=14.61545849
 11101: 8 [  485/ 1327], train_loss/perplexity = 4.37748766/79.6377029 secs/batch = 0.2973s, grad.norm=14.36125088
 11106: 8 [  490/ 1327], train_loss/perplexity = 4.24481392/69.7427826 secs/batch = 0.2999s, grad.norm=15.62499428
 11111: 8 [  495/ 1327], train_loss/perplexity = 4.23053598/68.7540741 secs/batch = 0.2951s, grad.norm=14.03087330
 11116: 8 [  500/ 1327], train_loss/perplexity = 4.58312941/97.8200378 secs/batch = 0.2916s, grad.norm=15.41701031
 11121: 8 [  505/ 1327], train_loss/perplexity = 4.60578060/100.0610580 secs/batch = 0.2948s, grad.norm=13.28658295
 11126: 8 [  510/ 1327], train_loss/perplexity = 4.91935062/136.9136810 secs/batch = 0.2970s, grad.norm=13.89292431
 11131: 8 [  515/ 1327], train_loss/perplexity = 4.59222603/98.7139282 secs/batch = 0.2986s, grad.norm=14.70032310
 11136: 8 [  520/ 1327], train_loss/perplexity = 4.77228308/118.1887665 secs/batch = 0.2945s, grad.norm=13.84953785
 11141: 8 [  525/ 1327], train_loss/perplexity = 4.37867212/79.7320862 secs/batch = 0.2974s, grad.norm=14.26275730
 11146: 8 [  530/ 1327], train_loss/perplexity = 4.37595892/79.5160522 secs/batch = 0.2998s, grad.norm=15.11697197
 11151: 8 [  535/ 1327], train_loss/perplexity = 4.45768738/86.2877274 secs/batch = 0.2919s, grad.norm=14.41552353
 11156: 8 [  540/ 1327], train_loss/perplexity = 4.57992268/97.5068512 secs/batch = 0.2940s, grad.norm=14.11288452
 11161: 8 [  545/ 1327], train_loss/perplexity = 4.57514143/97.0417633 secs/batch = 0.2991s, grad.norm=15.00817204
 11166: 8 [  550/ 1327], train_loss/perplexity = 4.50170994/90.1711884 secs/batch = 0.2928s, grad.norm=14.03837967
 11171: 8 [  555/ 1327], train_loss/perplexity = 4.31446457/74.7735748 secs/batch = 0.2975s, grad.norm=13.96122265
 11176: 8 [  560/ 1327], train_loss/perplexity = 4.53301430/93.0385818 secs/batch = 0.2942s, grad.norm=16.45975876
 11181: 8 [  565/ 1327], train_loss/perplexity = 4.37391424/79.3536377 secs/batch = 0.2945s, grad.norm=15.16332436
 11186: 8 [  570/ 1327], train_loss/perplexity = 4.32136011/75.2909622 secs/batch = 0.2994s, grad.norm=15.82570171
 11191: 8 [  575/ 1327], train_loss/perplexity = 4.19994736/66.6828232 secs/batch = 0.2932s, grad.norm=15.22941494
 11196: 8 [  580/ 1327], train_loss/perplexity = 4.51477432/91.3569489 secs/batch = 0.2942s, grad.norm=14.52134895
 11201: 8 [  585/ 1327], train_loss/perplexity = 4.13388872/62.4201851 secs/batch = 0.2986s, grad.norm=14.88997936
 11206: 8 [  590/ 1327], train_loss/perplexity = 4.46587086/86.9967575 secs/batch = 0.2909s, grad.norm=14.65017509
 11211: 8 [  595/ 1327], train_loss/perplexity = 4.51309013/91.2032166 secs/batch = 0.2947s, grad.norm=15.64346886
 11216: 8 [  600/ 1327], train_loss/perplexity = 4.67699051/107.4462280 secs/batch = 0.2944s, grad.norm=13.68295097
 11221: 8 [  605/ 1327], train_loss/perplexity = 4.53301239/93.0384064 secs/batch = 0.2932s, grad.norm=14.61798000
 11226: 8 [  610/ 1327], train_loss/perplexity = 4.77122068/118.0632706 secs/batch = 0.2943s, grad.norm=15.17486763
 11231: 8 [  615/ 1327], train_loss/perplexity = 4.23781490/69.2563553 secs/batch = 0.3008s, grad.norm=14.17301750
 11236: 8 [  620/ 1327], train_loss/perplexity = 4.69537163/109.4394684 secs/batch = 0.2919s, grad.norm=14.45662308
 11241: 8 [  625/ 1327], train_loss/perplexity = 4.68912792/108.7582932 secs/batch = 0.2907s, grad.norm=14.36381054
 11246: 8 [  630/ 1327], train_loss/perplexity = 4.73999548/114.4336853 secs/batch = 0.2950s, grad.norm=14.20018768
 11251: 8 [  635/ 1327], train_loss/perplexity = 4.54369068/94.0372238 secs/batch = 0.2924s, grad.norm=15.05645657
 11256: 8 [  640/ 1327], train_loss/perplexity = 4.46425104/86.8559570 secs/batch = 0.2922s, grad.norm=14.74142265
 11261: 8 [  645/ 1327], train_loss/perplexity = 4.76872349/117.7688141 secs/batch = 0.2933s, grad.norm=15.85214138
 11266: 8 [  650/ 1327], train_loss/perplexity = 4.19991302/66.6805344 secs/batch = 0.2921s, grad.norm=14.60150623
 11271: 8 [  655/ 1327], train_loss/perplexity = 4.40361595/81.7459259 secs/batch = 0.2910s, grad.norm=14.61966133
 11276: 8 [  660/ 1327], train_loss/perplexity = 4.30893564/74.3612976 secs/batch = 0.3012s, grad.norm=14.37095928
 11281: 8 [  665/ 1327], train_loss/perplexity = 4.50477505/90.4479980 secs/batch = 0.2926s, grad.norm=14.82622719
 11286: 8 [  670/ 1327], train_loss/perplexity = 4.48714924/88.8677444 secs/batch = 0.2924s, grad.norm=15.47499371
 11291: 8 [  675/ 1327], train_loss/perplexity = 4.19210911/66.1621857 secs/batch = 0.2929s, grad.norm=14.62875271
 11296: 8 [  680/ 1327], train_loss/perplexity = 4.45148134/85.7538834 secs/batch = 0.2942s, grad.norm=15.26123714
 11301: 8 [  685/ 1327], train_loss/perplexity = 4.22184849/68.1593628 secs/batch = 0.2939s, grad.norm=13.81235981
 11306: 8 [  690/ 1327], train_loss/perplexity = 4.64864540/104.4434128 secs/batch = 0.2955s, grad.norm=15.18928242
 11311: 8 [  695/ 1327], train_loss/perplexity = 4.46128559/86.5987701 secs/batch = 0.2944s, grad.norm=14.35525131
 11316: 8 [  700/ 1327], train_loss/perplexity = 4.66564846/106.2344513 secs/batch = 0.2990s, grad.norm=14.64687347
 11321: 8 [  705/ 1327], train_loss/perplexity = 4.46184063/86.6468506 secs/batch = 0.2926s, grad.norm=13.90277481
 11326: 8 [  710/ 1327], train_loss/perplexity = 4.32681417/75.7027283 secs/batch = 0.2985s, grad.norm=15.32859325
 11331: 8 [  715/ 1327], train_loss/perplexity = 4.23088360/68.7779770 secs/batch = 0.3005s, grad.norm=14.51210690
 11336: 8 [  720/ 1327], train_loss/perplexity = 4.26335144/71.0476990 secs/batch = 0.2972s, grad.norm=15.00893211
 11341: 8 [  725/ 1327], train_loss/perplexity = 4.28689051/72.7399292 secs/batch = 0.2921s, grad.norm=14.76854038
 11346: 8 [  730/ 1327], train_loss/perplexity = 4.49791193/89.8293686 secs/batch = 0.2977s, grad.norm=15.03986549
 11351: 8 [  735/ 1327], train_loss/perplexity = 4.52977085/92.7373047 secs/batch = 0.2956s, grad.norm=15.00859451
 11356: 8 [  740/ 1327], train_loss/perplexity = 3.97848582/53.4360619 secs/batch = 0.2991s, grad.norm=13.88019562
 11361: 8 [  745/ 1327], train_loss/perplexity = 4.45992088/86.4806671 secs/batch = 0.2943s, grad.norm=14.00183010
 11366: 8 [  750/ 1327], train_loss/perplexity = 4.22788858/68.5722961 secs/batch = 0.2974s, grad.norm=14.46722984
 11371: 8 [  755/ 1327], train_loss/perplexity = 4.17147827/64.8111877 secs/batch = 0.2948s, grad.norm=15.57893276
 11376: 8 [  760/ 1327], train_loss/perplexity = 4.13840628/62.7028122 secs/batch = 0.2940s, grad.norm=15.41142082
 11381: 8 [  765/ 1327], train_loss/perplexity = 4.20420456/66.9673080 secs/batch = 0.2913s, grad.norm=14.39856434
 11386: 8 [  770/ 1327], train_loss/perplexity = 4.17588043/65.0971298 secs/batch = 0.2953s, grad.norm=15.68377686
 11391: 8 [  775/ 1327], train_loss/perplexity = 4.24978256/70.0901718 secs/batch = 0.2937s, grad.norm=14.98161697
 11396: 8 [  780/ 1327], train_loss/perplexity = 4.65412474/105.0172653 secs/batch = 0.2928s, grad.norm=14.87091446
 11401: 8 [  785/ 1327], train_loss/perplexity = 4.47793245/88.0524292 secs/batch = 0.2931s, grad.norm=15.09361172
 11406: 8 [  790/ 1327], train_loss/perplexity = 4.20438194/66.9791870 secs/batch = 0.2932s, grad.norm=14.99996471
 11411: 8 [  795/ 1327], train_loss/perplexity = 4.59731436/99.2174911 secs/batch = 0.3004s, grad.norm=14.75121307
 11416: 8 [  800/ 1327], train_loss/perplexity = 4.47339249/87.6535797 secs/batch = 0.2952s, grad.norm=14.42594719
 11421: 8 [  805/ 1327], train_loss/perplexity = 4.85490751/128.3688202 secs/batch = 0.2938s, grad.norm=14.33300495
 11426: 8 [  810/ 1327], train_loss/perplexity = 4.46575928/86.9870529 secs/batch = 0.2922s, grad.norm=14.33340168
 11431: 8 [  815/ 1327], train_loss/perplexity = 4.28873396/72.8741455 secs/batch = 0.2927s, grad.norm=14.14590740
 11436: 8 [  820/ 1327], train_loss/perplexity = 4.12121391/61.6340141 secs/batch = 0.2940s, grad.norm=13.57013702
 11441: 8 [  825/ 1327], train_loss/perplexity = 4.41147327/82.3907547 secs/batch = 0.2919s, grad.norm=14.21344948
 11446: 8 [  830/ 1327], train_loss/perplexity = 4.15864086/63.9845009 secs/batch = 0.2987s, grad.norm=14.54573154
 11451: 8 [  835/ 1327], train_loss/perplexity = 4.41358900/82.5652618 secs/batch = 0.2936s, grad.norm=15.39398766
 11456: 8 [  840/ 1327], train_loss/perplexity = 4.49203682/89.3031540 secs/batch = 0.2932s, grad.norm=15.09301949
 11461: 8 [  845/ 1327], train_loss/perplexity = 4.29469681/73.3099823 secs/batch = 0.2941s, grad.norm=15.13394642
 11466: 8 [  850/ 1327], train_loss/perplexity = 4.33629608/76.4239502 secs/batch = 0.2984s, grad.norm=13.93941498
 11471: 8 [  855/ 1327], train_loss/perplexity = 4.45177555/85.7791138 secs/batch = 0.2944s, grad.norm=15.51598358
 11476: 8 [  860/ 1327], train_loss/perplexity = 4.11794710/61.4329987 secs/batch = 0.2934s, grad.norm=14.82171440
 11481: 8 [  865/ 1327], train_loss/perplexity = 4.58753204/98.2516479 secs/batch = 0.2942s, grad.norm=14.73727703
 11486: 8 [  870/ 1327], train_loss/perplexity = 4.54046154/93.7340546 secs/batch = 0.2946s, grad.norm=14.98971367
 11491: 8 [  875/ 1327], train_loss/perplexity = 4.06349707/58.1774063 secs/batch = 0.2934s, grad.norm=14.33258820
 11496: 8 [  880/ 1327], train_loss/perplexity = 4.30257082/73.8895035 secs/batch = 0.2926s, grad.norm=14.34810448
 11501: 8 [  885/ 1327], train_loss/perplexity = 4.51708841/91.5685959 secs/batch = 0.2933s, grad.norm=14.72135544
 11506: 8 [  890/ 1327], train_loss/perplexity = 4.60835743/100.3192291 secs/batch = 0.2979s, grad.norm=14.43165398
 11511: 8 [  895/ 1327], train_loss/perplexity = 4.55649233/95.2487946 secs/batch = 0.2909s, grad.norm=15.65149021
 11516: 8 [  900/ 1327], train_loss/perplexity = 4.38260841/80.0465546 secs/batch = 0.2929s, grad.norm=15.31802940
 11521: 8 [  905/ 1327], train_loss/perplexity = 4.29936981/73.6533661 secs/batch = 0.2980s, grad.norm=14.22924519
 11526: 8 [  910/ 1327], train_loss/perplexity = 4.28738308/72.7757721 secs/batch = 0.2919s, grad.norm=13.80527782
 11531: 8 [  915/ 1327], train_loss/perplexity = 4.47446823/87.7479248 secs/batch = 0.2983s, grad.norm=14.45682621
 11536: 8 [  920/ 1327], train_loss/perplexity = 4.69064474/108.9233856 secs/batch = 0.2940s, grad.norm=14.15702343
 11541: 8 [  925/ 1327], train_loss/perplexity = 4.54708099/94.3565750 secs/batch = 0.2971s, grad.norm=14.01730824
 11546: 8 [  930/ 1327], train_loss/perplexity = 4.52451277/92.2509689 secs/batch = 0.2928s, grad.norm=13.97767353
 11551: 8 [  935/ 1327], train_loss/perplexity = 4.56111050/95.6896820 secs/batch = 0.2944s, grad.norm=14.15395832
 11556: 8 [  940/ 1327], train_loss/perplexity = 4.57292461/96.8268738 secs/batch = 0.2932s, grad.norm=13.70506287
 11561: 8 [  945/ 1327], train_loss/perplexity = 4.72341347/112.5517883 secs/batch = 0.2938s, grad.norm=13.79095173
 11566: 8 [  950/ 1327], train_loss/perplexity = 4.44073486/84.8372650 secs/batch = 0.2946s, grad.norm=14.13120270
 11571: 8 [  955/ 1327], train_loss/perplexity = 4.49538040/89.6022491 secs/batch = 0.2938s, grad.norm=14.60637569
 11576: 8 [  960/ 1327], train_loss/perplexity = 4.75199318/115.8148956 secs/batch = 0.2939s, grad.norm=14.88114071
 11581: 8 [  965/ 1327], train_loss/perplexity = 4.53929663/93.6249237 secs/batch = 0.2948s, grad.norm=14.50017166
 11586: 8 [  970/ 1327], train_loss/perplexity = 4.80816221/122.5062714 secs/batch = 0.2935s, grad.norm=14.54891682
 11591: 8 [  975/ 1327], train_loss/perplexity = 4.49872971/89.9028549 secs/batch = 0.2991s, grad.norm=15.58848095
 11596: 8 [  980/ 1327], train_loss/perplexity = 4.24854469/70.0034637 secs/batch = 0.2983s, grad.norm=13.65960503
 11601: 8 [  985/ 1327], train_loss/perplexity = 4.45906115/86.4063492 secs/batch = 0.2989s, grad.norm=14.79948521
 11606: 8 [  990/ 1327], train_loss/perplexity = 4.63703394/103.2376862 secs/batch = 0.2991s, grad.norm=14.54328442
 11611: 8 [  995/ 1327], train_loss/perplexity = 4.58717060/98.2161407 secs/batch = 0.2935s, grad.norm=14.11634159
 11616: 8 [ 1000/ 1327], train_loss/perplexity = 4.12452602/61.8384933 secs/batch = 0.2945s, grad.norm=13.16639519
 11621: 8 [ 1005/ 1327], train_loss/perplexity = 4.66361427/106.0185699 secs/batch = 0.2942s, grad.norm=14.46339607
 11626: 8 [ 1010/ 1327], train_loss/perplexity = 4.23687792/69.1914902 secs/batch = 0.2919s, grad.norm=14.01286411
 11631: 8 [ 1015/ 1327], train_loss/perplexity = 4.71698284/111.8303299 secs/batch = 0.2978s, grad.norm=14.36522770
 11636: 8 [ 1020/ 1327], train_loss/perplexity = 4.82415581/124.4813385 secs/batch = 0.2942s, grad.norm=13.94108582
 11641: 8 [ 1025/ 1327], train_loss/perplexity = 4.64424992/103.9853363 secs/batch = 0.2924s, grad.norm=13.73065090
 11646: 8 [ 1030/ 1327], train_loss/perplexity = 4.46684551/87.0815887 secs/batch = 0.2955s, grad.norm=13.89909744
 11651: 8 [ 1035/ 1327], train_loss/perplexity = 4.37145853/79.1590042 secs/batch = 0.2987s, grad.norm=13.06759834
 11656: 8 [ 1040/ 1327], train_loss/perplexity = 4.68008471/107.7792053 secs/batch = 0.2930s, grad.norm=14.43255615
 11661: 8 [ 1045/ 1327], train_loss/perplexity = 4.17070007/64.7607727 secs/batch = 0.2938s, grad.norm=14.60025883
 11666: 8 [ 1050/ 1327], train_loss/perplexity = 4.33794212/76.5498505 secs/batch = 0.2922s, grad.norm=14.75637054
 11671: 8 [ 1055/ 1327], train_loss/perplexity = 4.32032013/75.2126999 secs/batch = 0.2925s, grad.norm=14.73521614
 11676: 8 [ 1060/ 1327], train_loss/perplexity = 4.02652740/56.0658798 secs/batch = 0.2935s, grad.norm=15.71949387
 11681: 8 [ 1065/ 1327], train_loss/perplexity = 4.11835194/61.4578705 secs/batch = 0.2938s, grad.norm=14.91812611
 11686: 8 [ 1070/ 1327], train_loss/perplexity = 4.43009424/83.9393234 secs/batch = 0.2937s, grad.norm=14.80339050
 11691: 8 [ 1075/ 1327], train_loss/perplexity = 4.13301563/62.3657112 secs/batch = 0.2937s, grad.norm=14.29016113
 11696: 8 [ 1080/ 1327], train_loss/perplexity = 4.13669062/62.5953255 secs/batch = 0.2921s, grad.norm=13.98000908
 11701: 8 [ 1085/ 1327], train_loss/perplexity = 3.99810672/54.4948769 secs/batch = 0.2960s, grad.norm=14.45578480
 11706: 8 [ 1090/ 1327], train_loss/perplexity = 4.29627180/73.4255371 secs/batch = 0.2952s, grad.norm=14.89113617
 11711: 8 [ 1095/ 1327], train_loss/perplexity = 4.34719706/77.2615967 secs/batch = 0.2931s, grad.norm=14.26728916
 11716: 8 [ 1100/ 1327], train_loss/perplexity = 4.07514477/58.8590012 secs/batch = 0.2943s, grad.norm=15.64110279
 11721: 8 [ 1105/ 1327], train_loss/perplexity = 4.12733316/62.0123253 secs/batch = 0.2951s, grad.norm=14.98317719
 11726: 8 [ 1110/ 1327], train_loss/perplexity = 4.44411278/85.1243210 secs/batch = 0.2945s, grad.norm=15.32858467
 11731: 8 [ 1115/ 1327], train_loss/perplexity = 4.25463390/70.4310303 secs/batch = 0.2929s, grad.norm=13.82580090
 11736: 8 [ 1120/ 1327], train_loss/perplexity = 4.46100235/86.5742416 secs/batch = 0.2994s, grad.norm=14.20139122
 11741: 8 [ 1125/ 1327], train_loss/perplexity = 4.74426699/114.9235306 secs/batch = 0.3018s, grad.norm=15.40517712
 11746: 8 [ 1130/ 1327], train_loss/perplexity = 4.37877655/79.7404175 secs/batch = 0.2937s, grad.norm=15.03954315
 11751: 8 [ 1135/ 1327], train_loss/perplexity = 4.28406048/72.5343704 secs/batch = 0.2971s, grad.norm=14.00842381
 11756: 8 [ 1140/ 1327], train_loss/perplexity = 4.60241222/99.7245865 secs/batch = 0.2962s, grad.norm=14.95696640
 11761: 8 [ 1145/ 1327], train_loss/perplexity = 4.35093927/77.5512695 secs/batch = 0.2940s, grad.norm=14.73359871
 11766: 8 [ 1150/ 1327], train_loss/perplexity = 4.33549881/76.3630371 secs/batch = 0.2987s, grad.norm=14.06694221
 11771: 8 [ 1155/ 1327], train_loss/perplexity = 4.44470787/85.1749954 secs/batch = 0.2954s, grad.norm=14.74033070
 11776: 8 [ 1160/ 1327], train_loss/perplexity = 4.40035248/81.4795837 secs/batch = 0.3003s, grad.norm=15.50809002
 11781: 8 [ 1165/ 1327], train_loss/perplexity = 4.46190643/86.6525497 secs/batch = 0.2988s, grad.norm=14.50598717
 11786: 8 [ 1170/ 1327], train_loss/perplexity = 4.35414076/77.7999496 secs/batch = 0.2949s, grad.norm=14.52029610
 11791: 8 [ 1175/ 1327], train_loss/perplexity = 4.06746435/58.4086723 secs/batch = 0.2927s, grad.norm=14.13011074
 11796: 8 [ 1180/ 1327], train_loss/perplexity = 4.09270668/59.9018059 secs/batch = 0.2933s, grad.norm=14.54373741
 11801: 8 [ 1185/ 1327], train_loss/perplexity = 4.22613049/68.4518433 secs/batch = 0.2975s, grad.norm=13.82270622
 11806: 8 [ 1190/ 1327], train_loss/perplexity = 4.42922211/83.8661499 secs/batch = 0.2916s, grad.norm=14.80468369
 11811: 8 [ 1195/ 1327], train_loss/perplexity = 4.16322088/64.2782211 secs/batch = 0.2951s, grad.norm=13.70234489
 11816: 8 [ 1200/ 1327], train_loss/perplexity = 4.19468689/66.3329620 secs/batch = 0.2988s, grad.norm=14.62678528
 11821: 8 [ 1205/ 1327], train_loss/perplexity = 4.16963100/64.6915741 secs/batch = 0.2936s, grad.norm=14.91828632
 11826: 8 [ 1210/ 1327], train_loss/perplexity = 3.84980679/46.9839859 secs/batch = 0.2924s, grad.norm=14.58084583
 11831: 8 [ 1215/ 1327], train_loss/perplexity = 4.07008410/58.5618858 secs/batch = 0.2948s, grad.norm=14.17523289
 11836: 8 [ 1220/ 1327], train_loss/perplexity = 4.27771854/72.0758133 secs/batch = 0.2953s, grad.norm=15.20272732
 11841: 8 [ 1225/ 1327], train_loss/perplexity = 3.94290042/51.5679550 secs/batch = 0.2939s, grad.norm=15.51114941
 11846: 8 [ 1230/ 1327], train_loss/perplexity = 4.20544004/67.0500946 secs/batch = 0.2935s, grad.norm=14.47151184
 11851: 8 [ 1235/ 1327], train_loss/perplexity = 4.18738699/65.8504944 secs/batch = 0.2930s, grad.norm=14.58838940
 11856: 8 [ 1240/ 1327], train_loss/perplexity = 4.42434692/83.4582825 secs/batch = 0.2926s, grad.norm=15.15077400
 11861: 8 [ 1245/ 1327], train_loss/perplexity = 4.32527113/75.5860062 secs/batch = 0.2991s, grad.norm=13.86124039
 11866: 8 [ 1250/ 1327], train_loss/perplexity = 4.39194250/80.7972183 secs/batch = 0.2951s, grad.norm=13.89577866
 11871: 8 [ 1255/ 1327], train_loss/perplexity = 4.50617599/90.5747986 secs/batch = 0.2998s, grad.norm=13.78444862
 11876: 8 [ 1260/ 1327], train_loss/perplexity = 4.31016397/74.4526978 secs/batch = 0.2952s, grad.norm=15.59823513
 11881: 8 [ 1265/ 1327], train_loss/perplexity = 4.50320816/90.3063812 secs/batch = 0.2955s, grad.norm=15.45239544
 11886: 8 [ 1270/ 1327], train_loss/perplexity = 4.17548704/65.0715256 secs/batch = 0.2978s, grad.norm=15.09336472
 11891: 8 [ 1275/ 1327], train_loss/perplexity = 4.46453571/86.8806839 secs/batch = 0.2919s, grad.norm=14.96226501
 11896: 8 [ 1280/ 1327], train_loss/perplexity = 4.28709793/72.7550201 secs/batch = 0.2924s, grad.norm=14.44914913
 11901: 8 [ 1285/ 1327], train_loss/perplexity = 4.13330555/62.3837967 secs/batch = 0.2936s, grad.norm=14.97595406
 11906: 8 [ 1290/ 1327], train_loss/perplexity = 4.27729273/72.0451279 secs/batch = 0.2923s, grad.norm=14.07709026
 11911: 8 [ 1295/ 1327], train_loss/perplexity = 4.39134789/80.7491837 secs/batch = 0.2977s, grad.norm=14.59721661
 11916: 8 [ 1300/ 1327], train_loss/perplexity = 4.51701927/91.5622711 secs/batch = 0.2938s, grad.norm=14.11764240
 11921: 8 [ 1305/ 1327], train_loss/perplexity = 4.70555687/110.5598373 secs/batch = 0.2937s, grad.norm=15.26436806
 11926: 8 [ 1310/ 1327], train_loss/perplexity = 4.88397264/132.1546326 secs/batch = 0.2944s, grad.norm=14.64296818
 11931: 8 [ 1315/ 1327], train_loss/perplexity = 4.69299936/109.1801605 secs/batch = 0.2940s, grad.norm=14.84515762
 11936: 8 [ 1320/ 1327], train_loss/perplexity = 4.68043041/107.8164673 secs/batch = 0.2926s, grad.norm=14.65122604
 11941: 8 [ 1325/ 1327], train_loss/perplexity = 4.59907722/99.3925552 secs/batch = 0.2999s, grad.norm=14.48422432
Epoch training time: 392.0228524208069
	> validation loss = 4.77556515, perplexity = 118.57730865
	> validation loss = 4.63832664, perplexity = 103.37122345
	> validation loss = 4.66706944, perplexity = 106.38551331
	> validation loss = 4.63864470, perplexity = 103.40410614
	> validation loss = 4.84065437, perplexity = 126.55213928
	> validation loss = 4.75618744, perplexity = 116.30167389
	> validation loss = 4.69069719, perplexity = 108.92910004
	> validation loss = 4.56333590, perplexity = 95.90287018
	> validation loss = 4.33104849, perplexity = 76.02395630
	> validation loss = 4.45382977, perplexity = 85.95550537
	> validation loss = 4.59344053, perplexity = 98.83388519
	> validation loss = 4.66217709, perplexity = 105.86631012
	> validation loss = 4.55959463, perplexity = 95.54473877
	> validation loss = 4.39039421, perplexity = 80.67221832
	> validation loss = 4.30241489, perplexity = 73.87798309
	> validation loss = 4.31363201, perplexity = 74.71134949
	> validation loss = 4.77303219, perplexity = 118.27733612
	> validation loss = 4.32497597, perplexity = 75.56369781
	> validation loss = 4.70097399, perplexity = 110.05431366
	> validation loss = 4.59618711, perplexity = 99.10571289
	> validation loss = 4.39953804, perplexity = 81.41325378
at the end of epoch: 8
train loss = 4.51823217, perplexity = 91.67339151
validation loss = 4.56955906, perplexity = 96.50154909
Saved model cv/epoch008_4.5696.model
 11948: 9 [    5/ 1327], train_loss/perplexity = 4.57936287/97.4522858 secs/batch = 0.2943s, grad.norm=14.64481735
 11953: 9 [   10/ 1327], train_loss/perplexity = 4.14963341/63.4107513 secs/batch = 0.2987s, grad.norm=13.93920898
 11958: 9 [   15/ 1327], train_loss/perplexity = 4.45834255/86.3442764 secs/batch = 0.2930s, grad.norm=13.45036602
 11963: 9 [   20/ 1327], train_loss/perplexity = 4.70031595/109.9819183 secs/batch = 0.2931s, grad.norm=14.55069542
 11968: 9 [   25/ 1327], train_loss/perplexity = 4.49418354/89.4950714 secs/batch = 0.2941s, grad.norm=14.86254883
 11973: 9 [   30/ 1327], train_loss/perplexity = 4.55773211/95.3669510 secs/batch = 0.2938s, grad.norm=14.56968880
 11978: 9 [   35/ 1327], train_loss/perplexity = 4.33943176/76.6639633 secs/batch = 0.2935s, grad.norm=13.88459206
 11983: 9 [   40/ 1327], train_loss/perplexity = 4.33844090/76.5880356 secs/batch = 0.2990s, grad.norm=14.22670269
 11988: 9 [   45/ 1327], train_loss/perplexity = 4.12758255/62.0277939 secs/batch = 0.2918s, grad.norm=14.11995125
 11993: 9 [   50/ 1327], train_loss/perplexity = 4.39349270/80.9225616 secs/batch = 0.2942s, grad.norm=14.55479622
 11998: 9 [   55/ 1327], train_loss/perplexity = 4.31461287/74.7846680 secs/batch = 0.3001s, grad.norm=14.75225639
 12003: 9 [   60/ 1327], train_loss/perplexity = 4.59614468/99.1015091 secs/batch = 0.2933s, grad.norm=14.82059860
 12008: 9 [   65/ 1327], train_loss/perplexity = 4.21644449/67.7920227 secs/batch = 0.2950s, grad.norm=14.24258041
 12013: 9 [   70/ 1327], train_loss/perplexity = 4.00523472/54.8847046 secs/batch = 0.2937s, grad.norm=14.40186214
 12018: 9 [   75/ 1327], train_loss/perplexity = 3.85124826/47.0517578 secs/batch = 0.2933s, grad.norm=14.57474995
 12023: 9 [   80/ 1327], train_loss/perplexity = 4.33693504/76.4727936 secs/batch = 0.2949s, grad.norm=15.04981422
 12028: 9 [   85/ 1327], train_loss/perplexity = 4.38464165/80.2094727 secs/batch = 0.2929s, grad.norm=14.48716736
 12033: 9 [   90/ 1327], train_loss/perplexity = 4.41212606/82.4445572 secs/batch = 0.2944s, grad.norm=15.11750031
 12038: 9 [   95/ 1327], train_loss/perplexity = 4.22781324/68.5671310 secs/batch = 0.2920s, grad.norm=14.70907021
 12043: 9 [  100/ 1327], train_loss/perplexity = 4.55915642/95.5028839 secs/batch = 0.2930s, grad.norm=15.25360680
 12048: 9 [  105/ 1327], train_loss/perplexity = 4.35176897/77.6156387 secs/batch = 0.3005s, grad.norm=15.06472206
 12053: 9 [  110/ 1327], train_loss/perplexity = 4.25818205/70.6813736 secs/batch = 0.2939s, grad.norm=14.46097851
 12058: 9 [  115/ 1327], train_loss/perplexity = 4.26949310/71.4853897 secs/batch = 0.2956s, grad.norm=15.67957973
 12063: 9 [  120/ 1327], train_loss/perplexity = 4.27437162/71.8349838 secs/batch = 0.2981s, grad.norm=15.58248520
 12068: 9 [  125/ 1327], train_loss/perplexity = 4.32312155/75.4236984 secs/batch = 0.2953s, grad.norm=14.49023247
 12073: 9 [  130/ 1327], train_loss/perplexity = 4.30393076/73.9900589 secs/batch = 0.2947s, grad.norm=15.23722935
 12078: 9 [  135/ 1327], train_loss/perplexity = 4.26360321/71.0655899 secs/batch = 0.2955s, grad.norm=14.29572773
 12083: 9 [  140/ 1327], train_loss/perplexity = 4.65592813/105.2068176 secs/batch = 0.2946s, grad.norm=15.19514465
 12088: 9 [  145/ 1327], train_loss/perplexity = 4.49347162/89.4313812 secs/batch = 0.2957s, grad.norm=16.04459953
 12093: 9 [  150/ 1327], train_loss/perplexity = 4.50946522/90.8732071 secs/batch = 0.2928s, grad.norm=14.58421993
 12098: 9 [  155/ 1327], train_loss/perplexity = 4.81593943/123.4627457 secs/batch = 0.2952s, grad.norm=14.30589294
 12103: 9 [  160/ 1327], train_loss/perplexity = 4.34849215/77.3617249 secs/batch = 0.2951s, grad.norm=13.47621727
 12108: 9 [  165/ 1327], train_loss/perplexity = 4.55476093/95.0840225 secs/batch = 0.2979s, grad.norm=14.13506222
 12113: 9 [  170/ 1327], train_loss/perplexity = 4.34114742/76.7956085 secs/batch = 0.2975s, grad.norm=13.71457863
 12118: 9 [  175/ 1327], train_loss/perplexity = 4.61125469/100.6103058 secs/batch = 0.2962s, grad.norm=15.36197472
 12123: 9 [  180/ 1327], train_loss/perplexity = 4.45120716/85.7303696 secs/batch = 0.2935s, grad.norm=15.59175777
 12128: 9 [  185/ 1327], train_loss/perplexity = 4.81978893/123.9389267 secs/batch = 0.2941s, grad.norm=14.90737343
 12133: 9 [  190/ 1327], train_loss/perplexity = 4.27005529/71.5255890 secs/batch = 0.2948s, grad.norm=13.35171986
 12138: 9 [  195/ 1327], train_loss/perplexity = 4.56811237/96.3620453 secs/batch = 0.2955s, grad.norm=14.15452766
 12143: 9 [  200/ 1327], train_loss/perplexity = 4.46239138/86.6945801 secs/batch = 0.2948s, grad.norm=14.41728973
 12148: 9 [  205/ 1327], train_loss/perplexity = 4.67974997/107.7431259 secs/batch = 0.2939s, grad.norm=14.33897305
 12153: 9 [  210/ 1327], train_loss/perplexity = 4.45234299/85.8278046 secs/batch = 0.3014s, grad.norm=13.81311607
 12158: 9 [  215/ 1327], train_loss/perplexity = 4.64357567/103.9152527 secs/batch = 0.2966s, grad.norm=13.66337204
 12163: 9 [  220/ 1327], train_loss/perplexity = 4.58396244/97.9015579 secs/batch = 0.2948s, grad.norm=13.77723789
 12168: 9 [  225/ 1327], train_loss/perplexity = 4.72793388/113.0617218 secs/batch = 0.2965s, grad.norm=15.58748341
 12173: 9 [  230/ 1327], train_loss/perplexity = 4.62940836/102.4534302 secs/batch = 0.2955s, grad.norm=14.45537663
 12178: 9 [  235/ 1327], train_loss/perplexity = 4.45615482/86.1555862 secs/batch = 0.2965s, grad.norm=13.97632790
 12183: 9 [  240/ 1327], train_loss/perplexity = 4.24570942/69.8052673 secs/batch = 0.2956s, grad.norm=14.66028500
 12188: 9 [  245/ 1327], train_loss/perplexity = 4.51835394/91.6845551 secs/batch = 0.2946s, grad.norm=14.59973907
 12193: 9 [  250/ 1327], train_loss/perplexity = 4.36608648/78.7349014 secs/batch = 0.2936s, grad.norm=13.26921749
 12198: 9 [  255/ 1327], train_loss/perplexity = 4.29939175/73.6549759 secs/batch = 0.2993s, grad.norm=14.12207794
 12203: 9 [  260/ 1327], train_loss/perplexity = 4.59111595/98.6044083 secs/batch = 0.2953s, grad.norm=15.07111549
 12208: 9 [  265/ 1327], train_loss/perplexity = 4.79562902/120.9804611 secs/batch = 0.2937s, grad.norm=14.11812210
 12213: 9 [  270/ 1327], train_loss/perplexity = 4.75260305/115.8855515 secs/batch = 0.2928s, grad.norm=14.67675591
 12218: 9 [  275/ 1327], train_loss/perplexity = 4.73992920/114.4261017 secs/batch = 0.2964s, grad.norm=14.40182114
 12223: 9 [  280/ 1327], train_loss/perplexity = 4.56145477/95.7226334 secs/batch = 0.2950s, grad.norm=14.25382710
 12228: 9 [  285/ 1327], train_loss/perplexity = 4.92281103/137.3882751 secs/batch = 0.2956s, grad.norm=14.92354870
 12233: 9 [  290/ 1327], train_loss/perplexity = 4.53589344/93.3068390 secs/batch = 0.2957s, grad.norm=15.43415070
 12238: 9 [  295/ 1327], train_loss/perplexity = 4.32721043/75.7327271 secs/batch = 0.2947s, grad.norm=14.58678150
 12243: 9 [  300/ 1327], train_loss/perplexity = 3.88059759/48.4531631 secs/batch = 0.3008s, grad.norm=13.86410427
 12248: 9 [  305/ 1327], train_loss/perplexity = 4.43047762/83.9715118 secs/batch = 0.2952s, grad.norm=14.85487366
 12253: 9 [  310/ 1327], train_loss/perplexity = 4.35654163/77.9869614 secs/batch = 0.2942s, grad.norm=14.35112381
 12258: 9 [  315/ 1327], train_loss/perplexity = 3.84790230/46.8945885 secs/batch = 0.2988s, grad.norm=13.80418587
 12263: 9 [  320/ 1327], train_loss/perplexity = 3.87330627/48.1011581 secs/batch = 0.2940s, grad.norm=15.88613987
 12268: 9 [  325/ 1327], train_loss/perplexity = 3.93920803/51.3778954 secs/batch = 0.3008s, grad.norm=13.69797993
 12273: 9 [  330/ 1327], train_loss/perplexity = 4.50339317/90.3230972 secs/batch = 0.3005s, grad.norm=14.89173317
 12278: 9 [  335/ 1327], train_loss/perplexity = 3.84131980/46.5869179 secs/batch = 0.2939s, grad.norm=14.10007668
 12283: 9 [  340/ 1327], train_loss/perplexity = 4.61058140/100.5425873 secs/batch = 0.2990s, grad.norm=14.36448288
 12288: 9 [  345/ 1327], train_loss/perplexity = 4.38652611/80.3607712 secs/batch = 0.2988s, grad.norm=13.75463486
 12293: 9 [  350/ 1327], train_loss/perplexity = 4.46939373/87.3037796 secs/batch = 0.2977s, grad.norm=14.98031712
 12298: 9 [  355/ 1327], train_loss/perplexity = 4.46573257/86.9847260 secs/batch = 0.2954s, grad.norm=14.77698612
 12303: 9 [  360/ 1327], train_loss/perplexity = 4.61441994/100.9292679 secs/batch = 0.2936s, grad.norm=15.69501114
 12308: 9 [  365/ 1327], train_loss/perplexity = 4.55926418/95.5131760 secs/batch = 0.2952s, grad.norm=15.29994011
 12313: 9 [  370/ 1327], train_loss/perplexity = 4.61744642/101.2351913 secs/batch = 0.2950s, grad.norm=14.77709007
 12318: 9 [  375/ 1327], train_loss/perplexity = 4.01574850/55.4647942 secs/batch = 0.2944s, grad.norm=14.58861923
 12323: 9 [  380/ 1327], train_loss/perplexity = 4.13382339/62.4161072 secs/batch = 0.2948s, grad.norm=15.46232796
 12328: 9 [  385/ 1327], train_loss/perplexity = 4.32420444/75.5054169 secs/batch = 0.2943s, grad.norm=14.67652416
 12333: 9 [  390/ 1327], train_loss/perplexity = 4.46468258/86.8934402 secs/batch = 0.2929s, grad.norm=14.69729996
 12338: 9 [  395/ 1327], train_loss/perplexity = 4.50468588/90.4399338 secs/batch = 0.2936s, grad.norm=14.90142536
 12343: 9 [  400/ 1327], train_loss/perplexity = 4.40608931/81.9483643 secs/batch = 0.2918s, grad.norm=14.21477413
 12348: 9 [  405/ 1327], train_loss/perplexity = 4.70176506/110.1414108 secs/batch = 0.3001s, grad.norm=15.02129745
 12353: 9 [  410/ 1327], train_loss/perplexity = 4.38048458/79.8767319 secs/batch = 0.2949s, grad.norm=14.35229492
 12358: 9 [  415/ 1327], train_loss/perplexity = 4.33782768/76.5410843 secs/batch = 0.2949s, grad.norm=14.30111504
 12363: 9 [  420/ 1327], train_loss/perplexity = 3.93117547/50.9668541 secs/batch = 0.2928s, grad.norm=14.57511711
 12368: 9 [  425/ 1327], train_loss/perplexity = 4.31982327/75.1753387 secs/batch = 0.2956s, grad.norm=15.55017757
 12373: 9 [  430/ 1327], train_loss/perplexity = 4.48661852/88.8205948 secs/batch = 0.2941s, grad.norm=15.30850410
 12378: 9 [  435/ 1327], train_loss/perplexity = 4.48566961/88.7363510 secs/batch = 0.2948s, grad.norm=15.38347244
 12383: 9 [  440/ 1327], train_loss/perplexity = 4.10918522/60.8970795 secs/batch = 0.2942s, grad.norm=15.39437389
 12388: 9 [  445/ 1327], train_loss/perplexity = 4.44768000/85.4285202 secs/batch = 0.2944s, grad.norm=15.35105896
 12393: 9 [  450/ 1327], train_loss/perplexity = 4.36678028/78.7895432 secs/batch = 0.2931s, grad.norm=14.74904823
 12398: 9 [  455/ 1327], train_loss/perplexity = 4.29580784/73.3914795 secs/batch = 0.2940s, grad.norm=13.81177998
 12403: 9 [  460/ 1327], train_loss/perplexity = 4.33506250/76.3297272 secs/batch = 0.2956s, grad.norm=16.06362534
 12408: 9 [  465/ 1327], train_loss/perplexity = 4.11587572/61.3058777 secs/batch = 0.2955s, grad.norm=16.43601608
 12413: 9 [  470/ 1327], train_loss/perplexity = 4.77144623/118.0899048 secs/batch = 0.2966s, grad.norm=14.37840462
 12418: 9 [  475/ 1327], train_loss/perplexity = 4.26310396/71.0301132 secs/batch = 0.2929s, grad.norm=14.49490261
 12423: 9 [  480/ 1327], train_loss/perplexity = 4.37227058/79.2233124 secs/batch = 0.2958s, grad.norm=14.69220734
 12428: 9 [  485/ 1327], train_loss/perplexity = 4.42767859/83.7368011 secs/batch = 0.3008s, grad.norm=14.97664165
 12433: 9 [  490/ 1327], train_loss/perplexity = 4.17112589/64.7883530 secs/batch = 0.2936s, grad.norm=16.20463753
 12438: 9 [  495/ 1327], train_loss/perplexity = 4.25081158/70.1623306 secs/batch = 0.2935s, grad.norm=14.04513359
 12443: 9 [  500/ 1327], train_loss/perplexity = 4.48158741/88.3748474 secs/batch = 0.2944s, grad.norm=15.40164661
 12448: 9 [  505/ 1327], train_loss/perplexity = 4.54544449/94.2022934 secs/batch = 0.2929s, grad.norm=13.47905636
 12453: 9 [  510/ 1327], train_loss/perplexity = 4.93010902/138.3945923 secs/batch = 0.2938s, grad.norm=14.00751209
 12458: 9 [  515/ 1327], train_loss/perplexity = 4.50161409/90.1625443 secs/batch = 0.2951s, grad.norm=14.03010941
 12463: 9 [  520/ 1327], train_loss/perplexity = 4.69319963/109.2020264 secs/batch = 0.2958s, grad.norm=14.51425648
 12468: 9 [  525/ 1327], train_loss/perplexity = 4.32355547/75.4564362 secs/batch = 0.2961s, grad.norm=14.75776386
 12473: 9 [  530/ 1327], train_loss/perplexity = 4.30090618/73.7666092 secs/batch = 0.2985s, grad.norm=15.46018219
 12478: 9 [  535/ 1327], train_loss/perplexity = 4.44762802/85.4240799 secs/batch = 0.2937s, grad.norm=13.92702961
 12483: 9 [  540/ 1327], train_loss/perplexity = 4.46973133/87.3332596 secs/batch = 0.3002s, grad.norm=14.27506351
 12488: 9 [  545/ 1327], train_loss/perplexity = 4.47755766/88.0194397 secs/batch = 0.2927s, grad.norm=15.40039062
 12493: 9 [  550/ 1327], train_loss/perplexity = 4.45952415/86.4463654 secs/batch = 0.2949s, grad.norm=15.04503441
 12498: 9 [  555/ 1327], train_loss/perplexity = 4.30392027/73.9892807 secs/batch = 0.2940s, grad.norm=13.85256577
 12503: 9 [  560/ 1327], train_loss/perplexity = 4.45547819/86.0973129 secs/batch = 0.2939s, grad.norm=16.93153381
 12508: 9 [  565/ 1327], train_loss/perplexity = 4.26591873/71.2303314 secs/batch = 0.2925s, grad.norm=15.20260048
 12513: 9 [  570/ 1327], train_loss/perplexity = 4.27667809/72.0008621 secs/batch = 0.3020s, grad.norm=15.59935665
 12518: 9 [  575/ 1327], train_loss/perplexity = 4.16132212/64.1562881 secs/batch = 0.2943s, grad.norm=15.96936321
 12523: 9 [  580/ 1327], train_loss/perplexity = 4.56030560/95.6126938 secs/batch = 0.2996s, grad.norm=15.98219967
 12528: 9 [  585/ 1327], train_loss/perplexity = 4.05883503/57.9068108 secs/batch = 0.2924s, grad.norm=14.61596680
 12533: 9 [  590/ 1327], train_loss/perplexity = 4.42705965/83.6849899 secs/batch = 0.2988s, grad.norm=14.60364723
 12538: 9 [  595/ 1327], train_loss/perplexity = 4.54494810/94.1555405 secs/batch = 0.2942s, grad.norm=15.10915470
 12543: 9 [  600/ 1327], train_loss/perplexity = 4.62361240/101.8613358 secs/batch = 0.2927s, grad.norm=14.24804211
 12548: 9 [  605/ 1327], train_loss/perplexity = 4.48791218/88.9355698 secs/batch = 0.2923s, grad.norm=14.18080807
 12553: 9 [  610/ 1327], train_loss/perplexity = 4.69180679/109.0500336 secs/batch = 0.2937s, grad.norm=14.88230896
 12558: 9 [  615/ 1327], train_loss/perplexity = 4.28176117/72.3677826 secs/batch = 0.3008s, grad.norm=14.23812485
 12563: 9 [  620/ 1327], train_loss/perplexity = 4.58521461/98.0242233 secs/batch = 0.3009s, grad.norm=15.32567406
 12568: 9 [  625/ 1327], train_loss/perplexity = 4.65289593/104.8882980 secs/batch = 0.2947s, grad.norm=14.90828037
 12573: 9 [  630/ 1327], train_loss/perplexity = 4.68617105/108.4371872 secs/batch = 0.2931s, grad.norm=14.55411625
 12578: 9 [  635/ 1327], train_loss/perplexity = 4.41490412/82.6739120 secs/batch = 0.2952s, grad.norm=15.08318615
 12583: 9 [  640/ 1327], train_loss/perplexity = 4.42039204/83.1288681 secs/batch = 0.3009s, grad.norm=14.76711845
 12588: 9 [  645/ 1327], train_loss/perplexity = 4.62858772/102.3693848 secs/batch = 0.2924s, grad.norm=15.85250187
 12593: 9 [  650/ 1327], train_loss/perplexity = 4.15938997/64.0324478 secs/batch = 0.2998s, grad.norm=14.59647942
 12598: 9 [  655/ 1327], train_loss/perplexity = 4.28390884/72.5233688 secs/batch = 0.2949s, grad.norm=15.19141960
 12603: 9 [  660/ 1327], train_loss/perplexity = 4.30873108/74.3460922 secs/batch = 0.2950s, grad.norm=14.84208965
 12608: 9 [  665/ 1327], train_loss/perplexity = 4.41435766/82.6287460 secs/batch = 0.2953s, grad.norm=14.93656635
 12613: 9 [  670/ 1327], train_loss/perplexity = 4.28716898/72.7601929 secs/batch = 0.2944s, grad.norm=14.89254856
 12618: 9 [  675/ 1327], train_loss/perplexity = 4.15163803/63.5379906 secs/batch = 0.2941s, grad.norm=15.47061253
 12623: 9 [  680/ 1327], train_loss/perplexity = 4.39158487/80.7683258 secs/batch = 0.2933s, grad.norm=16.42600632
 12628: 9 [  685/ 1327], train_loss/perplexity = 4.13649416/62.5830307 secs/batch = 0.2940s, grad.norm=14.95642090
 12633: 9 [  690/ 1327], train_loss/perplexity = 4.60795689/100.2790604 secs/batch = 0.2937s, grad.norm=14.70528412
 12638: 9 [  695/ 1327], train_loss/perplexity = 4.46780062/87.1648026 secs/batch = 0.2933s, grad.norm=16.16522217
 12643: 9 [  700/ 1327], train_loss/perplexity = 4.61984396/101.4781952 secs/batch = 0.2943s, grad.norm=15.49432373
 12648: 9 [  705/ 1327], train_loss/perplexity = 4.40713882/82.0344162 secs/batch = 0.2945s, grad.norm=14.32327938
 12653: 9 [  710/ 1327], train_loss/perplexity = 4.26306343/71.0272369 secs/batch = 0.2985s, grad.norm=14.83779907
 12658: 9 [  715/ 1327], train_loss/perplexity = 4.21677971/67.8147507 secs/batch = 0.2990s, grad.norm=14.57289124
 12663: 9 [  720/ 1327], train_loss/perplexity = 4.21859884/67.9382248 secs/batch = 0.2938s, grad.norm=15.13148403
 12668: 9 [  725/ 1327], train_loss/perplexity = 4.30023146/73.7168503 secs/batch = 0.3010s, grad.norm=14.85348797
 12673: 9 [  730/ 1327], train_loss/perplexity = 4.45376825/85.9502182 secs/batch = 0.2989s, grad.norm=14.85825634
 12678: 9 [  735/ 1327], train_loss/perplexity = 4.42562580/83.5650864 secs/batch = 0.2932s, grad.norm=15.08459663
 12683: 9 [  740/ 1327], train_loss/perplexity = 3.89399314/49.1065865 secs/batch = 0.2995s, grad.norm=14.62300110
 12688: 9 [  745/ 1327], train_loss/perplexity = 4.39087105/80.7106934 secs/batch = 0.2953s, grad.norm=17.00018883
 12693: 9 [  750/ 1327], train_loss/perplexity = 4.23100281/68.7861786 secs/batch = 0.2939s, grad.norm=14.83743858
 12698: 9 [  755/ 1327], train_loss/perplexity = 4.10537052/60.6652184 secs/batch = 0.2914s, grad.norm=15.31915569
 12703: 9 [  760/ 1327], train_loss/perplexity = 4.04877567/57.3272285 secs/batch = 0.2948s, grad.norm=14.60302639
 12708: 9 [  765/ 1327], train_loss/perplexity = 4.12411785/61.8132553 secs/batch = 0.2949s, grad.norm=14.95560741
 12713: 9 [  770/ 1327], train_loss/perplexity = 4.06129551/58.0494652 secs/batch = 0.3006s, grad.norm=14.03974152
 12718: 9 [  775/ 1327], train_loss/perplexity = 4.28528404/72.6231689 secs/batch = 0.2960s, grad.norm=15.62168121
 12723: 9 [  780/ 1327], train_loss/perplexity = 4.61062098/100.5465698 secs/batch = 0.2931s, grad.norm=14.75785542
 12728: 9 [  785/ 1327], train_loss/perplexity = 4.43687296/84.5102615 secs/batch = 0.2948s, grad.norm=16.01551247
 12733: 9 [  790/ 1327], train_loss/perplexity = 4.26062965/70.8545837 secs/batch = 0.3000s, grad.norm=15.62836170
 12738: 9 [  795/ 1327], train_loss/perplexity = 4.60209560/99.6930161 secs/batch = 0.2978s, grad.norm=15.21924496
 12743: 9 [  800/ 1327], train_loss/perplexity = 4.35518551/77.8812714 secs/batch = 0.2952s, grad.norm=14.70249748
 12748: 9 [  805/ 1327], train_loss/perplexity = 4.79366064/120.7425537 secs/batch = 0.2959s, grad.norm=14.55276966
 12753: 9 [  810/ 1327], train_loss/perplexity = 4.41898537/83.0120163 secs/batch = 0.2956s, grad.norm=14.59629631
 12758: 9 [  815/ 1327], train_loss/perplexity = 4.27492714/71.8749008 secs/batch = 0.3008s, grad.norm=15.33284473
 12763: 9 [  820/ 1327], train_loss/perplexity = 4.11703300/61.3768654 secs/batch = 0.2924s, grad.norm=13.85165215
 12768: 9 [  825/ 1327], train_loss/perplexity = 4.39938450/81.4007492 secs/batch = 0.2949s, grad.norm=14.55156231
 12773: 9 [  830/ 1327], train_loss/perplexity = 4.06147099/58.0596542 secs/batch = 0.2978s, grad.norm=14.68002033
 12778: 9 [  835/ 1327], train_loss/perplexity = 4.35296345/77.7084045 secs/batch = 0.2933s, grad.norm=14.58160400
 12783: 9 [  840/ 1327], train_loss/perplexity = 4.38798046/80.4777298 secs/batch = 0.2980s, grad.norm=14.48883247
 12788: 9 [  845/ 1327], train_loss/perplexity = 4.25146198/70.2079773 secs/batch = 0.3009s, grad.norm=14.75157070
 12793: 9 [  850/ 1327], train_loss/perplexity = 4.36395884/78.5675583 secs/batch = 0.2964s, grad.norm=14.84403133
 12798: 9 [  855/ 1327], train_loss/perplexity = 4.45637035/86.1741562 secs/batch = 0.2948s, grad.norm=15.31685925
 12803: 9 [  860/ 1327], train_loss/perplexity = 4.06972170/58.5406685 secs/batch = 0.2964s, grad.norm=14.10521984
 12808: 9 [  865/ 1327], train_loss/perplexity = 4.55593348/95.1955795 secs/batch = 0.3006s, grad.norm=14.63470554
 12813: 9 [  870/ 1327], train_loss/perplexity = 4.49076509/89.1896591 secs/batch = 0.2956s, grad.norm=15.30790901
 12818: 9 [  875/ 1327], train_loss/perplexity = 3.96343923/52.6380501 secs/batch = 0.3016s, grad.norm=14.20235443
 12823: 9 [  880/ 1327], train_loss/perplexity = 4.18428802/65.6467438 secs/batch = 0.2936s, grad.norm=13.76213932
 12828: 9 [  885/ 1327], train_loss/perplexity = 4.41623592/82.7840958 secs/batch = 0.2942s, grad.norm=14.53201962
 12833: 9 [  890/ 1327], train_loss/perplexity = 4.50377893/90.3579407 secs/batch = 0.2942s, grad.norm=14.47786617
 12838: 9 [  895/ 1327], train_loss/perplexity = 4.53098154/92.8496552 secs/batch = 0.2956s, grad.norm=14.44938278
 12843: 9 [  900/ 1327], train_loss/perplexity = 4.30050468/73.7369995 secs/batch = 0.3001s, grad.norm=15.13197041
 12848: 9 [  905/ 1327], train_loss/perplexity = 4.19359541/66.2605972 secs/batch = 0.2954s, grad.norm=14.52702427
 12853: 9 [  910/ 1327], train_loss/perplexity = 4.22600412/68.4431915 secs/batch = 0.2947s, grad.norm=14.17382431
 12858: 9 [  915/ 1327], train_loss/perplexity = 4.48250198/88.4557114 secs/batch = 0.2966s, grad.norm=14.44096756
 12863: 9 [  920/ 1327], train_loss/perplexity = 4.68418884/108.2224503 secs/batch = 0.2964s, grad.norm=14.25126076
 12868: 9 [  925/ 1327], train_loss/perplexity = 4.54267502/93.9417572 secs/batch = 0.3004s, grad.norm=16.42648125
 12873: 9 [  930/ 1327], train_loss/perplexity = 4.50524521/90.4905319 secs/batch = 0.2956s, grad.norm=15.10184765
 12878: 9 [  935/ 1327], train_loss/perplexity = 4.54055500/93.7428131 secs/batch = 0.2939s, grad.norm=14.23802280
 12883: 9 [  940/ 1327], train_loss/perplexity = 4.52664948/92.4482880 secs/batch = 0.2942s, grad.norm=13.77963924
 12888: 9 [  945/ 1327], train_loss/perplexity = 4.66625309/106.2987061 secs/batch = 0.3015s, grad.norm=14.07142830
 12893: 9 [  950/ 1327], train_loss/perplexity = 4.51356125/91.2461929 secs/batch = 0.2954s, grad.norm=14.30357838
 12898: 9 [  955/ 1327], train_loss/perplexity = 4.48880959/89.0154190 secs/batch = 0.2946s, grad.norm=14.82469177
 12903: 9 [  960/ 1327], train_loss/perplexity = 4.68380499/108.1809158 secs/batch = 0.3011s, grad.norm=13.96606159
 12908: 9 [  965/ 1327], train_loss/perplexity = 4.48778105/88.9239120 secs/batch = 0.2987s, grad.norm=14.20813847
 12913: 9 [  970/ 1327], train_loss/perplexity = 4.76239491/117.0258560 secs/batch = 0.3019s, grad.norm=14.27384663
 12918: 9 [  975/ 1327], train_loss/perplexity = 4.42565346/83.5673981 secs/batch = 0.2946s, grad.norm=14.97027683
 12923: 9 [  980/ 1327], train_loss/perplexity = 4.22486305/68.3651428 secs/batch = 0.2938s, grad.norm=14.40929222
 12928: 9 [  985/ 1327], train_loss/perplexity = 4.40319538/81.7115479 secs/batch = 0.2949s, grad.norm=15.07603359
 12933: 9 [  990/ 1327], train_loss/perplexity = 4.66053963/105.6931000 secs/batch = 0.2967s, grad.norm=14.97410202
 12938: 9 [  995/ 1327], train_loss/perplexity = 4.58555698/98.0577850 secs/batch = 0.2941s, grad.norm=14.45824146
 12943: 9 [ 1000/ 1327], train_loss/perplexity = 4.08884954/59.6712036 secs/batch = 0.2953s, grad.norm=14.38966084
 12948: 9 [ 1005/ 1327], train_loss/perplexity = 4.62798500/102.3077087 secs/batch = 0.2955s, grad.norm=14.74720383
 12953: 9 [ 1010/ 1327], train_loss/perplexity = 4.10937929/60.9088974 secs/batch = 0.2980s, grad.norm=13.24863243
 12958: 9 [ 1015/ 1327], train_loss/perplexity = 4.69965887/109.9096756 secs/batch = 0.2982s, grad.norm=14.52613354
 12963: 9 [ 1020/ 1327], train_loss/perplexity = 4.71249723/111.3298264 secs/batch = 0.2951s, grad.norm=13.87327480
 12968: 9 [ 1025/ 1327], train_loss/perplexity = 4.61506939/100.9948349 secs/batch = 0.2951s, grad.norm=14.20856857
 12973: 9 [ 1030/ 1327], train_loss/perplexity = 4.42764330/83.7338486 secs/batch = 0.2946s, grad.norm=13.99221706
 12978: 9 [ 1035/ 1327], train_loss/perplexity = 4.30768394/74.2682800 secs/batch = 0.2952s, grad.norm=14.20649433
 12983: 9 [ 1040/ 1327], train_loss/perplexity = 4.55457163/95.0660248 secs/batch = 0.3014s, grad.norm=14.93618870
 12988: 9 [ 1045/ 1327], train_loss/perplexity = 4.15483999/63.7417641 secs/batch = 0.2955s, grad.norm=13.73491859
 12993: 9 [ 1050/ 1327], train_loss/perplexity = 4.23713923/69.2095718 secs/batch = 0.3012s, grad.norm=15.15056229
 12998: 9 [ 1055/ 1327], train_loss/perplexity = 4.36574030/78.7076492 secs/batch = 0.2951s, grad.norm=15.38148785
 13003: 9 [ 1060/ 1327], train_loss/perplexity = 3.92554426/50.6806526 secs/batch = 0.2992s, grad.norm=15.46092987
 13008: 9 [ 1065/ 1327], train_loss/perplexity = 4.04445982/57.0803452 secs/batch = 0.2957s, grad.norm=15.07066631
 13013: 9 [ 1070/ 1327], train_loss/perplexity = 4.44394684/85.1101990 secs/batch = 0.2957s, grad.norm=15.17280388
 13018: 9 [ 1075/ 1327], train_loss/perplexity = 4.26750231/71.3432159 secs/batch = 0.2952s, grad.norm=14.94278622
 13023: 9 [ 1080/ 1327], train_loss/perplexity = 4.12879181/62.1028442 secs/batch = 0.2963s, grad.norm=14.46560574
 13028: 9 [ 1085/ 1327], train_loss/perplexity = 3.92221975/50.5124474 secs/batch = 0.3006s, grad.norm=14.91347313
 13033: 9 [ 1090/ 1327], train_loss/perplexity = 4.18132401/65.4524536 secs/batch = 0.2962s, grad.norm=15.08942413
 13038: 9 [ 1095/ 1327], train_loss/perplexity = 4.30142975/73.8052444 secs/batch = 0.2954s, grad.norm=14.76621151
 13043: 9 [ 1100/ 1327], train_loss/perplexity = 4.04032993/56.8450966 secs/batch = 0.2945s, grad.norm=15.66000080
 13048: 9 [ 1105/ 1327], train_loss/perplexity = 4.01309109/55.3175964 secs/batch = 0.2945s, grad.norm=14.28774548
 13053: 9 [ 1110/ 1327], train_loss/perplexity = 4.33899832/76.6307449 secs/batch = 0.2934s, grad.norm=15.16312408
 13058: 9 [ 1115/ 1327], train_loss/perplexity = 4.24155855/69.5161133 secs/batch = 0.2943s, grad.norm=15.53106785
 13063: 9 [ 1120/ 1327], train_loss/perplexity = 4.37718201/79.6133652 secs/batch = 0.2948s, grad.norm=15.04787445
 13068: 9 [ 1125/ 1327], train_loss/perplexity = 4.53320932/93.0567322 secs/batch = 0.2951s, grad.norm=15.03939915
 13073: 9 [ 1130/ 1327], train_loss/perplexity = 4.30224943/73.8657608 secs/batch = 0.3000s, grad.norm=14.93768024
 13078: 9 [ 1135/ 1327], train_loss/perplexity = 4.26719236/71.3211136 secs/batch = 0.2922s, grad.norm=14.89373875
 13083: 9 [ 1140/ 1327], train_loss/perplexity = 4.57278013/96.8128891 secs/batch = 0.2950s, grad.norm=15.94636536
 13088: 9 [ 1145/ 1327], train_loss/perplexity = 4.35079670/77.5402145 secs/batch = 0.2960s, grad.norm=14.77871990
 13093: 9 [ 1150/ 1327], train_loss/perplexity = 4.37074566/79.1025925 secs/batch = 0.2955s, grad.norm=15.22298527
 13098: 9 [ 1155/ 1327], train_loss/perplexity = 4.43954945/84.7367554 secs/batch = 0.3010s, grad.norm=15.79436493
 13103: 9 [ 1160/ 1327], train_loss/perplexity = 4.35372066/77.7672729 secs/batch = 0.2965s, grad.norm=15.42893028
 13108: 9 [ 1165/ 1327], train_loss/perplexity = 4.38984108/80.6276016 secs/batch = 0.2951s, grad.norm=15.35967731
 13113: 9 [ 1170/ 1327], train_loss/perplexity = 4.26021814/70.8254318 secs/batch = 0.2960s, grad.norm=14.31128311
 13118: 9 [ 1175/ 1327], train_loss/perplexity = 4.04017258/56.8361511 secs/batch = 0.3008s, grad.norm=15.06199932
 13123: 9 [ 1180/ 1327], train_loss/perplexity = 4.07739115/58.9913673 secs/batch = 0.3007s, grad.norm=15.10013676
 13128: 9 [ 1185/ 1327], train_loss/perplexity = 4.29438782/73.2873383 secs/batch = 0.2996s, grad.norm=15.29138660
 13133: 9 [ 1190/ 1327], train_loss/perplexity = 4.35787964/78.0913773 secs/batch = 0.2936s, grad.norm=14.60102177
 13138: 9 [ 1195/ 1327], train_loss/perplexity = 4.09283352/59.9094048 secs/batch = 0.2967s, grad.norm=13.95817947
 13143: 9 [ 1200/ 1327], train_loss/perplexity = 4.20527935/67.0393219 secs/batch = 0.2941s, grad.norm=14.92866421
 13148: 9 [ 1205/ 1327], train_loss/perplexity = 4.20403099/66.9556885 secs/batch = 0.2949s, grad.norm=15.06170082
 13153: 9 [ 1210/ 1327], train_loss/perplexity = 3.83821988/46.4427261 secs/batch = 0.2942s, grad.norm=15.38239002
 13158: 9 [ 1215/ 1327], train_loss/perplexity = 4.02551460/56.0091248 secs/batch = 0.2928s, grad.norm=14.12496853
 13163: 9 [ 1220/ 1327], train_loss/perplexity = 4.21113968/67.4333496 secs/batch = 0.2968s, grad.norm=15.36913395
 13168: 9 [ 1225/ 1327], train_loss/perplexity = 3.88406563/48.6214905 secs/batch = 0.3009s, grad.norm=15.67865086
 13173: 9 [ 1230/ 1327], train_loss/perplexity = 4.19418001/66.2993469 secs/batch = 0.2948s, grad.norm=14.57259083
 13178: 9 [ 1235/ 1327], train_loss/perplexity = 4.13336182/62.3873062 secs/batch = 0.2987s, grad.norm=14.92820549
 13183: 9 [ 1240/ 1327], train_loss/perplexity = 4.38261509/80.0470886 secs/batch = 0.2944s, grad.norm=15.69570637
 13188: 9 [ 1245/ 1327], train_loss/perplexity = 4.21569061/67.7409363 secs/batch = 0.2947s, grad.norm=14.33033657
 13193: 9 [ 1250/ 1327], train_loss/perplexity = 4.38473272/80.2167816 secs/batch = 0.3009s, grad.norm=14.15361691
 13198: 9 [ 1255/ 1327], train_loss/perplexity = 4.42060184/83.1463089 secs/batch = 0.2953s, grad.norm=14.79378891
 13203: 9 [ 1260/ 1327], train_loss/perplexity = 4.19996405/66.6839371 secs/batch = 0.2940s, grad.norm=15.35873508
 13208: 9 [ 1265/ 1327], train_loss/perplexity = 4.42794704/83.7592850 secs/batch = 0.2991s, grad.norm=14.81018353
 13213: 9 [ 1270/ 1327], train_loss/perplexity = 4.17309809/64.9162598 secs/batch = 0.2963s, grad.norm=15.48157501
 13218: 9 [ 1275/ 1327], train_loss/perplexity = 4.41816187/82.9436874 secs/batch = 0.2940s, grad.norm=15.76064396
 13223: 9 [ 1280/ 1327], train_loss/perplexity = 4.18564320/65.7357712 secs/batch = 0.3011s, grad.norm=14.48314095
 13228: 9 [ 1285/ 1327], train_loss/perplexity = 4.14089203/62.8588676 secs/batch = 0.2952s, grad.norm=15.73901367
 13233: 9 [ 1290/ 1327], train_loss/perplexity = 4.32337284/75.4426575 secs/batch = 0.2936s, grad.norm=13.68237209
 13238: 9 [ 1295/ 1327], train_loss/perplexity = 4.35667372/77.9972610 secs/batch = 0.2927s, grad.norm=14.91011715
 13243: 9 [ 1300/ 1327], train_loss/perplexity = 4.51805115/91.6567993 secs/batch = 0.2947s, grad.norm=14.92597008
 13248: 9 [ 1305/ 1327], train_loss/perplexity = 4.63430548/102.9563904 secs/batch = 0.3016s, grad.norm=16.20420837
 13253: 9 [ 1310/ 1327], train_loss/perplexity = 4.84175014/126.6908875 secs/batch = 0.2954s, grad.norm=15.22319317
 13258: 9 [ 1315/ 1327], train_loss/perplexity = 4.62511635/102.0146408 secs/batch = 0.2945s, grad.norm=15.89612865
 13263: 9 [ 1320/ 1327], train_loss/perplexity = 4.66251850/105.9024658 secs/batch = 0.2975s, grad.norm=17.26773643
 13268: 9 [ 1325/ 1327], train_loss/perplexity = 4.42703152/83.6826401 secs/batch = 0.3003s, grad.norm=14.37274170
Epoch training time: 393.13007259368896
	> validation loss = 4.76733589, perplexity = 117.60550690
	> validation loss = 4.64877558, perplexity = 104.45700836
	> validation loss = 4.65737104, perplexity = 105.35873413
	> validation loss = 4.63682365, perplexity = 103.21597290
	> validation loss = 4.82608032, perplexity = 124.72113800
	> validation loss = 4.70566177, perplexity = 110.57143402
	> validation loss = 4.70476055, perplexity = 110.47183228
	> validation loss = 4.52976131, perplexity = 92.73642731
	> validation loss = 4.33350468, perplexity = 76.21091461
	> validation loss = 4.43632317, perplexity = 84.46381378
	> validation loss = 4.57277203, perplexity = 96.81210327
	> validation loss = 4.63413286, perplexity = 102.93861389
	> validation loss = 4.54138851, perplexity = 93.82098389
	> validation loss = 4.35459280, perplexity = 77.83512115
	> validation loss = 4.27663612, perplexity = 71.99784088
	> validation loss = 4.27408028, perplexity = 71.81405640
	> validation loss = 4.71728849, perplexity = 111.86451721
	> validation loss = 4.29361153, perplexity = 73.23046875
	> validation loss = 4.67450380, perplexity = 107.17937469
	> validation loss = 4.61310577, perplexity = 100.79671478
	> validation loss = 4.34517860, perplexity = 77.10580444
at the end of epoch: 9
train loss = 4.48071296, perplexity = 88.29760321
validation loss = 4.55565100, perplexity = 95.16868962
Saved model cv/epoch009_4.5557.model
 13275: 10 [    5/ 1327], train_loss/perplexity = 4.56344318/95.9131622 secs/batch = 0.2984s, grad.norm=14.31416130
 13280: 10 [   10/ 1327], train_loss/perplexity = 4.15912962/64.0157776 secs/batch = 0.2938s, grad.norm=14.08923340
 13285: 10 [   15/ 1327], train_loss/perplexity = 4.52459002/92.2580948 secs/batch = 0.2956s, grad.norm=14.33190155
 13290: 10 [   20/ 1327], train_loss/perplexity = 4.61262417/100.7481842 secs/batch = 0.2957s, grad.norm=15.35150242
 13295: 10 [   25/ 1327], train_loss/perplexity = 4.43074083/83.9936218 secs/batch = 0.2979s, grad.norm=14.96721172
 13300: 10 [   30/ 1327], train_loss/perplexity = 4.54121351/93.8045654 secs/batch = 0.2935s, grad.norm=15.05887890
 13305: 10 [   35/ 1327], train_loss/perplexity = 4.29553223/73.3712540 secs/batch = 0.2993s, grad.norm=14.42310238
 13310: 10 [   40/ 1327], train_loss/perplexity = 4.30593109/74.1382141 secs/batch = 0.2990s, grad.norm=14.99342060
 13315: 10 [   45/ 1327], train_loss/perplexity = 4.07348871/58.7616081 secs/batch = 0.2910s, grad.norm=14.42889690
 13320: 10 [   50/ 1327], train_loss/perplexity = 4.31537342/74.8415680 secs/batch = 0.2962s, grad.norm=14.58016491
 13325: 10 [   55/ 1327], train_loss/perplexity = 4.28803730/72.8233948 secs/batch = 0.3016s, grad.norm=14.68988132
 13330: 10 [   60/ 1327], train_loss/perplexity = 4.56050014/95.6312943 secs/batch = 0.2941s, grad.norm=15.08489037
 13335: 10 [   65/ 1327], train_loss/perplexity = 4.14625788/63.1970673 secs/batch = 0.2933s, grad.norm=13.76768017
 13340: 10 [   70/ 1327], train_loss/perplexity = 3.93246078/51.0324020 secs/batch = 0.2951s, grad.norm=14.11101246
 13345: 10 [   75/ 1327], train_loss/perplexity = 3.78401423/43.9922829 secs/batch = 0.2941s, grad.norm=15.01365376
 13350: 10 [   80/ 1327], train_loss/perplexity = 4.23604584/69.1339417 secs/batch = 0.2992s, grad.norm=15.11419106
 13355: 10 [   85/ 1327], train_loss/perplexity = 4.27762508/72.0690765 secs/batch = 0.2972s, grad.norm=15.83040237
 13360: 10 [   90/ 1327], train_loss/perplexity = 4.32328176/75.4357834 secs/batch = 0.2941s, grad.norm=14.96842957
 13365: 10 [   95/ 1327], train_loss/perplexity = 4.24688768/69.8875580 secs/batch = 0.2951s, grad.norm=14.92713737
 13370: 10 [  100/ 1327], train_loss/perplexity = 4.47298336/87.6177292 secs/batch = 0.2942s, grad.norm=15.03007507
 13375: 10 [  105/ 1327], train_loss/perplexity = 4.31359673/74.7087097 secs/batch = 0.2997s, grad.norm=15.58793449
 13380: 10 [  110/ 1327], train_loss/perplexity = 4.19217253/66.1663818 secs/batch = 0.2958s, grad.norm=14.94378090
 13385: 10 [  115/ 1327], train_loss/perplexity = 4.15088844/63.4903831 secs/batch = 0.2943s, grad.norm=15.69518471
 13390: 10 [  120/ 1327], train_loss/perplexity = 4.29832029/73.5761032 secs/batch = 0.2938s, grad.norm=15.47988415
 13395: 10 [  125/ 1327], train_loss/perplexity = 4.37942886/79.7924500 secs/batch = 0.2923s, grad.norm=15.41353130
 13400: 10 [  130/ 1327], train_loss/perplexity = 4.23035765/68.7418137 secs/batch = 0.2940s, grad.norm=15.30832291
 13405: 10 [  135/ 1327], train_loss/perplexity = 4.16448593/64.3595886 secs/batch = 0.3000s, grad.norm=14.27493572
 13410: 10 [  140/ 1327], train_loss/perplexity = 4.57948971/97.4646454 secs/batch = 0.2950s, grad.norm=15.17168999
 13415: 10 [  145/ 1327], train_loss/perplexity = 4.38693905/80.3939590 secs/batch = 0.2986s, grad.norm=16.19078255
 13420: 10 [  150/ 1327], train_loss/perplexity = 4.37190104/79.1940384 secs/batch = 0.2956s, grad.norm=16.05264282
 13425: 10 [  155/ 1327], train_loss/perplexity = 4.69778347/109.7037430 secs/batch = 0.2953s, grad.norm=14.57868576
 13430: 10 [  160/ 1327], train_loss/perplexity = 4.40214396/81.6256866 secs/batch = 0.2938s, grad.norm=14.09280491
 13435: 10 [  165/ 1327], train_loss/perplexity = 4.45215321/85.8115158 secs/batch = 0.2997s, grad.norm=14.33953094
 13440: 10 [  170/ 1327], train_loss/perplexity = 4.28741598/72.7781677 secs/batch = 0.2969s, grad.norm=14.54593468
 13445: 10 [  175/ 1327], train_loss/perplexity = 4.64404631/103.9641724 secs/batch = 0.2984s, grad.norm=14.91504955
 13450: 10 [  180/ 1327], train_loss/perplexity = 4.41838264/82.9619980 secs/batch = 0.2966s, grad.norm=15.27807999
 13455: 10 [  185/ 1327], train_loss/perplexity = 4.78179169/119.3179398 secs/batch = 0.2965s, grad.norm=15.30807209
 13460: 10 [  190/ 1327], train_loss/perplexity = 4.20139980/66.7797470 secs/batch = 0.2949s, grad.norm=13.63082409
 13465: 10 [  195/ 1327], train_loss/perplexity = 4.55070686/94.6993256 secs/batch = 0.2958s, grad.norm=13.99856853
 13470: 10 [  200/ 1327], train_loss/perplexity = 4.43286324/84.1720734 secs/batch = 0.2968s, grad.norm=14.90592098
 13475: 10 [  205/ 1327], train_loss/perplexity = 4.64061975/103.6085358 secs/batch = 0.2941s, grad.norm=14.58680820
 13480: 10 [  210/ 1327], train_loss/perplexity = 4.50510168/90.4775467 secs/batch = 0.2999s, grad.norm=13.74006844
 13485: 10 [  215/ 1327], train_loss/perplexity = 4.57238865/96.7749939 secs/batch = 0.2942s, grad.norm=14.61001968
 13490: 10 [  220/ 1327], train_loss/perplexity = 4.50619698/90.5766983 secs/batch = 0.2999s, grad.norm=14.26409340
 13495: 10 [  225/ 1327], train_loss/perplexity = 4.72723150/112.9823380 secs/batch = 0.2969s, grad.norm=15.20115948
 13500: 10 [  230/ 1327], train_loss/perplexity = 4.58153963/97.6646423 secs/batch = 0.2951s, grad.norm=15.29281998
 13505: 10 [  235/ 1327], train_loss/perplexity = 4.40842915/82.1403351 secs/batch = 0.2953s, grad.norm=14.74576187
 13510: 10 [  240/ 1327], train_loss/perplexity = 4.16980839/64.7030563 secs/batch = 0.2965s, grad.norm=15.63288021
 13515: 10 [  245/ 1327], train_loss/perplexity = 4.49072456/89.1860428 secs/batch = 0.2984s, grad.norm=15.22352695
 13520: 10 [  250/ 1327], train_loss/perplexity = 4.29773521/73.5330658 secs/batch = 0.2995s, grad.norm=14.25711727
 13525: 10 [  255/ 1327], train_loss/perplexity = 4.31278658/74.6482162 secs/batch = 0.2952s, grad.norm=15.46395016
 13530: 10 [  260/ 1327], train_loss/perplexity = 4.48888206/89.0218658 secs/batch = 0.2959s, grad.norm=15.13137054
 13535: 10 [  265/ 1327], train_loss/perplexity = 4.67980576/107.7491379 secs/batch = 0.2939s, grad.norm=14.04071808
 13540: 10 [  270/ 1327], train_loss/perplexity = 4.79429340/120.8189850 secs/batch = 0.2956s, grad.norm=14.62259483
 13545: 10 [  275/ 1327], train_loss/perplexity = 4.71186924/111.2599335 secs/batch = 0.2955s, grad.norm=14.66453171
 13550: 10 [  280/ 1327], train_loss/perplexity = 4.46010780/86.4968338 secs/batch = 0.2947s, grad.norm=14.70696831
 13555: 10 [  285/ 1327], train_loss/perplexity = 4.74950361/115.5269241 secs/batch = 0.3013s, grad.norm=15.62869644
 13560: 10 [  290/ 1327], train_loss/perplexity = 4.50439548/90.4136734 secs/batch = 0.2940s, grad.norm=15.57441711
 13565: 10 [  295/ 1327], train_loss/perplexity = 4.37088156/79.1133423 secs/batch = 0.2988s, grad.norm=14.96406269
 13570: 10 [  300/ 1327], train_loss/perplexity = 3.89159441/48.9889336 secs/batch = 0.3023s, grad.norm=13.91338921
 13575: 10 [  305/ 1327], train_loss/perplexity = 4.32275391/75.3959732 secs/batch = 0.2951s, grad.norm=15.13500404
 13580: 10 [  310/ 1327], train_loss/perplexity = 4.26966715/71.4978333 secs/batch = 0.2975s, grad.norm=14.73652458
 13585: 10 [  315/ 1327], train_loss/perplexity = 3.87460947/48.1638870 secs/batch = 0.3003s, grad.norm=14.51792622
 13590: 10 [  320/ 1327], train_loss/perplexity = 3.87182069/48.0297546 secs/batch = 0.2969s, grad.norm=15.54808044
 13595: 10 [  325/ 1327], train_loss/perplexity = 3.88920760/48.8721466 secs/batch = 0.2954s, grad.norm=14.49863052
 13600: 10 [  330/ 1327], train_loss/perplexity = 4.37228155/79.2241821 secs/batch = 0.2946s, grad.norm=15.17969036
 13605: 10 [  335/ 1327], train_loss/perplexity = 3.87377214/48.1235733 secs/batch = 0.2953s, grad.norm=14.16040611
 13610: 10 [  340/ 1327], train_loss/perplexity = 4.57090521/96.6315384 secs/batch = 0.3005s, grad.norm=14.69201374
 13615: 10 [  345/ 1327], train_loss/perplexity = 4.35827208/78.1220322 secs/batch = 0.2944s, grad.norm=13.70383167
 13620: 10 [  350/ 1327], train_loss/perplexity = 4.40570354/81.9167557 secs/batch = 0.2962s, grad.norm=14.96340752
 13625: 10 [  355/ 1327], train_loss/perplexity = 4.44152355/84.9041977 secs/batch = 0.2961s, grad.norm=14.71914291
 13630: 10 [  360/ 1327], train_loss/perplexity = 4.49045181/89.1617203 secs/batch = 0.3018s, grad.norm=15.25744629
 13635: 10 [  365/ 1327], train_loss/perplexity = 4.55321836/94.9374619 secs/batch = 0.3010s, grad.norm=14.37829685
 13640: 10 [  370/ 1327], train_loss/perplexity = 4.59516621/99.0045929 secs/batch = 0.2942s, grad.norm=14.58315945
 13645: 10 [  375/ 1327], train_loss/perplexity = 3.97242188/53.1130104 secs/batch = 0.2989s, grad.norm=14.17520428
 13650: 10 [  380/ 1327], train_loss/perplexity = 4.09480238/60.0274734 secs/batch = 0.3015s, grad.norm=15.41427803
 13655: 10 [  385/ 1327], train_loss/perplexity = 4.27298689/71.7355804 secs/batch = 0.2950s, grad.norm=15.43421745
 13660: 10 [  390/ 1327], train_loss/perplexity = 4.44095707/84.8561172 secs/batch = 0.2999s, grad.norm=14.50342369
 13665: 10 [  395/ 1327], train_loss/perplexity = 4.38885689/80.5482941 secs/batch = 0.2963s, grad.norm=15.23291683
 13670: 10 [  400/ 1327], train_loss/perplexity = 4.34854794/77.3660431 secs/batch = 0.2984s, grad.norm=14.14478302
 13675: 10 [  405/ 1327], train_loss/perplexity = 4.70588207/110.5957947 secs/batch = 0.2960s, grad.norm=15.01106834
 13680: 10 [  410/ 1327], train_loss/perplexity = 4.36916733/78.9778442 secs/batch = 0.3006s, grad.norm=14.71193981
 13685: 10 [  415/ 1327], train_loss/perplexity = 4.28285456/72.4469528 secs/batch = 0.2948s, grad.norm=14.81715393
 13690: 10 [  420/ 1327], train_loss/perplexity = 3.86485291/47.6962547 secs/batch = 0.2926s, grad.norm=15.51269913
 13695: 10 [  425/ 1327], train_loss/perplexity = 4.23522806/69.0774307 secs/batch = 0.2932s, grad.norm=15.64162540
 13700: 10 [  430/ 1327], train_loss/perplexity = 4.50958204/90.8838272 secs/batch = 0.2951s, grad.norm=16.25582695
 13705: 10 [  435/ 1327], train_loss/perplexity = 4.54886627/94.5251846 secs/batch = 0.2949s, grad.norm=15.63078403
 13710: 10 [  440/ 1327], train_loss/perplexity = 4.10221720/60.4742241 secs/batch = 0.3016s, grad.norm=16.22286034
 13715: 10 [  445/ 1327], train_loss/perplexity = 4.42059803/83.1459961 secs/batch = 0.2946s, grad.norm=15.44972706
 13720: 10 [  450/ 1327], train_loss/perplexity = 4.33481121/76.3105545 secs/batch = 0.2939s, grad.norm=14.95386410
 13725: 10 [  455/ 1327], train_loss/perplexity = 4.24785042/69.9548798 secs/batch = 0.2960s, grad.norm=14.64079380
 13730: 10 [  460/ 1327], train_loss/perplexity = 4.29571342/73.3845520 secs/batch = 0.2961s, grad.norm=15.80486393
 13735: 10 [  465/ 1327], train_loss/perplexity = 4.02450800/55.9527740 secs/batch = 0.2969s, grad.norm=16.47036362
 13740: 10 [  470/ 1327], train_loss/perplexity = 4.69577456/109.4835739 secs/batch = 0.2945s, grad.norm=13.90580845
 13745: 10 [  475/ 1327], train_loss/perplexity = 4.13928413/62.7578773 secs/batch = 0.2962s, grad.norm=15.16071224
 13750: 10 [  480/ 1327], train_loss/perplexity = 4.29387808/73.2499847 secs/batch = 0.2961s, grad.norm=14.92558479
 13755: 10 [  485/ 1327], train_loss/perplexity = 4.30374527/73.9763336 secs/batch = 0.3023s, grad.norm=14.46759319
 13760: 10 [  490/ 1327], train_loss/perplexity = 4.21500254/67.6943359 secs/batch = 0.2958s, grad.norm=16.10578728
 13765: 10 [  495/ 1327], train_loss/perplexity = 4.20668697/67.1337585 secs/batch = 0.2948s, grad.norm=14.62180996
 13770: 10 [  500/ 1327], train_loss/perplexity = 4.44022131/84.7937088 secs/batch = 0.2966s, grad.norm=15.69245911
 13775: 10 [  505/ 1327], train_loss/perplexity = 4.54849291/94.4898987 secs/batch = 0.3014s, grad.norm=14.02947426
 13780: 10 [  510/ 1327], train_loss/perplexity = 4.89403152/133.4906616 secs/batch = 0.2961s, grad.norm=13.99016190
 13785: 10 [  515/ 1327], train_loss/perplexity = 4.50930786/90.8589096 secs/batch = 0.3002s, grad.norm=14.56210613
 13790: 10 [  520/ 1327], train_loss/perplexity = 4.66444588/106.1067734 secs/batch = 0.3013s, grad.norm=14.34010983
 13795: 10 [  525/ 1327], train_loss/perplexity = 4.25261688/70.2891083 secs/batch = 0.2944s, grad.norm=14.51128387
 13800: 10 [  530/ 1327], train_loss/perplexity = 4.29917908/73.6393204 secs/batch = 0.2932s, grad.norm=15.17534733
 13805: 10 [  535/ 1327], train_loss/perplexity = 4.35361385/77.7589645 secs/batch = 0.2961s, grad.norm=14.80603218
 13810: 10 [  540/ 1327], train_loss/perplexity = 4.49659491/89.7111359 secs/batch = 0.2968s, grad.norm=14.92067146
 13815: 10 [  545/ 1327], train_loss/perplexity = 4.39959335/81.4177551 secs/batch = 0.3017s, grad.norm=15.45555687
 13820: 10 [  550/ 1327], train_loss/perplexity = 4.43488216/84.3421860 secs/batch = 0.2943s, grad.norm=14.87717628
 13825: 10 [  555/ 1327], train_loss/perplexity = 4.22648573/68.4761658 secs/batch = 0.2969s, grad.norm=15.07212925
 13830: 10 [  560/ 1327], train_loss/perplexity = 4.45682478/86.2133255 secs/batch = 0.2953s, grad.norm=16.92654037
 13835: 10 [  565/ 1327], train_loss/perplexity = 4.30036783/73.7269058 secs/batch = 0.3021s, grad.norm=15.57669258
 13840: 10 [  570/ 1327], train_loss/perplexity = 4.28036976/72.2671585 secs/batch = 0.2955s, grad.norm=15.73139000
 13845: 10 [  575/ 1327], train_loss/perplexity = 4.09266376/59.8992386 secs/batch = 0.2955s, grad.norm=15.34053612
 13850: 10 [  580/ 1327], train_loss/perplexity = 4.48194504/88.4064560 secs/batch = 0.2957s, grad.norm=15.25924301
 13855: 10 [  585/ 1327], train_loss/perplexity = 4.03061914/56.2957573 secs/batch = 0.2954s, grad.norm=15.03831673
 13860: 10 [  590/ 1327], train_loss/perplexity = 4.33514357/76.3359146 secs/batch = 0.3037s, grad.norm=14.42914486
 13865: 10 [  595/ 1327], train_loss/perplexity = 4.34578323/77.1524429 secs/batch = 0.3002s, grad.norm=14.76479149
 13870: 10 [  600/ 1327], train_loss/perplexity = 4.56099606/95.6787338 secs/batch = 0.3002s, grad.norm=14.43074608
 13875: 10 [  605/ 1327], train_loss/perplexity = 4.48077726/88.3032837 secs/batch = 0.2935s, grad.norm=14.50123978
 13880: 10 [  610/ 1327], train_loss/perplexity = 4.60003757/99.4880524 secs/batch = 0.2981s, grad.norm=14.94018269
 13885: 10 [  615/ 1327], train_loss/perplexity = 4.24241734/69.5758362 secs/batch = 0.2961s, grad.norm=14.30573845
 13890: 10 [  620/ 1327], train_loss/perplexity = 4.61554050/101.0424271 secs/batch = 0.2957s, grad.norm=14.71480560
 13895: 10 [  625/ 1327], train_loss/perplexity = 4.54657650/94.3089905 secs/batch = 0.3026s, grad.norm=15.49031544
 13900: 10 [  630/ 1327], train_loss/perplexity = 4.71967840/112.1321869 secs/batch = 0.3018s, grad.norm=15.54308987
 13905: 10 [  635/ 1327], train_loss/perplexity = 4.46174860/86.6388702 secs/batch = 0.3003s, grad.norm=15.02957535
 13910: 10 [  640/ 1327], train_loss/perplexity = 4.37433052/79.3866730 secs/batch = 0.2942s, grad.norm=15.74947834
 13915: 10 [  645/ 1327], train_loss/perplexity = 4.64831495/104.4089050 secs/batch = 0.2948s, grad.norm=15.35162830
 13920: 10 [  650/ 1327], train_loss/perplexity = 4.15621710/63.8296051 secs/batch = 0.2998s, grad.norm=14.30762291
 13925: 10 [  655/ 1327], train_loss/perplexity = 4.35567570/77.9194565 secs/batch = 0.2997s, grad.norm=15.44834232
 13930: 10 [  660/ 1327], train_loss/perplexity = 4.21823931/67.9138031 secs/batch = 0.2979s, grad.norm=14.41855907
 13935: 10 [  665/ 1327], train_loss/perplexity = 4.39206219/80.8068848 secs/batch = 0.2942s, grad.norm=14.75449753
 13940: 10 [  670/ 1327], train_loss/perplexity = 4.27611208/71.9601212 secs/batch = 0.2947s, grad.norm=15.23816013
 13945: 10 [  675/ 1327], train_loss/perplexity = 4.16796446/64.5838547 secs/batch = 0.2990s, grad.norm=15.05627441
 13950: 10 [  680/ 1327], train_loss/perplexity = 4.32848549/75.8293533 secs/batch = 0.2972s, grad.norm=15.50061989
 13955: 10 [  685/ 1327], train_loss/perplexity = 4.10466003/60.6221313 secs/batch = 0.2959s, grad.norm=14.93965149
 13960: 10 [  690/ 1327], train_loss/perplexity = 4.60437870/99.9208832 secs/batch = 0.3010s, grad.norm=15.20496559
 13965: 10 [  695/ 1327], train_loss/perplexity = 4.51322222/91.2152634 secs/batch = 0.2958s, grad.norm=15.16600323
 13970: 10 [  700/ 1327], train_loss/perplexity = 4.62307787/101.8069000 secs/batch = 0.2987s, grad.norm=15.18003559
 13975: 10 [  705/ 1327], train_loss/perplexity = 4.38266277/80.0509109 secs/batch = 0.2957s, grad.norm=14.29755211
 13980: 10 [  710/ 1327], train_loss/perplexity = 4.21130943/67.4447937 secs/batch = 0.2963s, grad.norm=15.15417099
 13985: 10 [  715/ 1327], train_loss/perplexity = 4.17274284/64.8932037 secs/batch = 0.3004s, grad.norm=14.97354984
 13990: 10 [  720/ 1327], train_loss/perplexity = 4.26957226/71.4910507 secs/batch = 0.3034s, grad.norm=15.26533413
 13995: 10 [  725/ 1327], train_loss/perplexity = 4.21411848/67.6345215 secs/batch = 0.2914s, grad.norm=14.82109165
 14000: 10 [  730/ 1327], train_loss/perplexity = 4.36259127/78.4601822 secs/batch = 0.2941s, grad.norm=14.70027256
 14005: 10 [  735/ 1327], train_loss/perplexity = 4.41337109/82.5472717 secs/batch = 0.2970s, grad.norm=14.94103336
 14010: 10 [  740/ 1327], train_loss/perplexity = 3.91761684/50.2804756 secs/batch = 0.2931s, grad.norm=14.82972908
 14015: 10 [  745/ 1327], train_loss/perplexity = 4.44364977/85.0849152 secs/batch = 0.2952s, grad.norm=14.66362095
 14020: 10 [  750/ 1327], train_loss/perplexity = 4.27074242/71.5747528 secs/batch = 0.2939s, grad.norm=15.11243916
 14025: 10 [  755/ 1327], train_loss/perplexity = 4.09646988/60.1276550 secs/batch = 0.2950s, grad.norm=15.03035259
 14030: 10 [  760/ 1327], train_loss/perplexity = 4.01002359/55.1481705 secs/batch = 0.2988s, grad.norm=14.34096050
 14035: 10 [  765/ 1327], train_loss/perplexity = 4.02558899/56.0132904 secs/batch = 0.3017s, grad.norm=15.09434223
 14040: 10 [  770/ 1327], train_loss/perplexity = 4.06149292/58.0609283 secs/batch = 0.2958s, grad.norm=14.30247688
 14045: 10 [  775/ 1327], train_loss/perplexity = 4.21418333/67.6389084 secs/batch = 0.2961s, grad.norm=15.67535782
 14050: 10 [  780/ 1327], train_loss/perplexity = 4.57794189/97.3139038 secs/batch = 0.3006s, grad.norm=14.53163815
 14055: 10 [  785/ 1327], train_loss/perplexity = 4.36194992/78.4098816 secs/batch = 0.2946s, grad.norm=15.44999027
 14060: 10 [  790/ 1327], train_loss/perplexity = 4.16768312/64.5656891 secs/batch = 0.2939s, grad.norm=14.87967873
 14065: 10 [  795/ 1327], train_loss/perplexity = 4.53357172/93.0904617 secs/batch = 0.2993s, grad.norm=15.51379585
 14070: 10 [  800/ 1327], train_loss/perplexity = 4.34428835/77.0371933 secs/batch = 0.2941s, grad.norm=14.89567852
 14075: 10 [  805/ 1327], train_loss/perplexity = 4.72334194/112.5437393 secs/batch = 0.2958s, grad.norm=14.20908546
 14080: 10 [  810/ 1327], train_loss/perplexity = 4.36361504/78.5405502 secs/batch = 0.2956s, grad.norm=14.14801502
 14085: 10 [  815/ 1327], train_loss/perplexity = 4.22306252/68.2421570 secs/batch = 0.2989s, grad.norm=13.93729877
 14090: 10 [  820/ 1327], train_loss/perplexity = 4.03223038/56.3865356 secs/batch = 0.2953s, grad.norm=14.13836575
 14095: 10 [  825/ 1327], train_loss/perplexity = 4.31423950/74.7567520 secs/batch = 0.2993s, grad.norm=15.11381435
 14100: 10 [  830/ 1327], train_loss/perplexity = 3.99486208/54.3183479 secs/batch = 0.2949s, grad.norm=14.60203552
 14105: 10 [  835/ 1327], train_loss/perplexity = 4.33392477/76.2429352 secs/batch = 0.2947s, grad.norm=15.53605556
 14110: 10 [  840/ 1327], train_loss/perplexity = 4.32200575/75.3395920 secs/batch = 0.2935s, grad.norm=14.92251778
 14115: 10 [  845/ 1327], train_loss/perplexity = 4.20100975/66.7537003 secs/batch = 0.2972s, grad.norm=15.06123447
 14120: 10 [  850/ 1327], train_loss/perplexity = 4.29437065/73.2860794 secs/batch = 0.2973s, grad.norm=14.06155205
 14125: 10 [  855/ 1327], train_loss/perplexity = 4.38860226/80.5277863 secs/batch = 0.2958s, grad.norm=15.49904537
 14130: 10 [  860/ 1327], train_loss/perplexity = 3.99576116/54.3672066 secs/batch = 0.2948s, grad.norm=14.51999760
 14135: 10 [  865/ 1327], train_loss/perplexity = 4.46429491/86.8597641 secs/batch = 0.2997s, grad.norm=14.93115425
 14140: 10 [  870/ 1327], train_loss/perplexity = 4.38855362/80.5238647 secs/batch = 0.2955s, grad.norm=14.69156837
 14145: 10 [  875/ 1327], train_loss/perplexity = 3.88693023/48.7609711 secs/batch = 0.2947s, grad.norm=14.44938374
 14150: 10 [  880/ 1327], train_loss/perplexity = 4.16825819/64.6028290 secs/batch = 0.2948s, grad.norm=14.70009613
 14155: 10 [  885/ 1327], train_loss/perplexity = 4.29421854/73.2749329 secs/batch = 0.2967s, grad.norm=14.21623135
 14160: 10 [  890/ 1327], train_loss/perplexity = 4.51390696/91.2777405 secs/batch = 0.3005s, grad.norm=14.82437897
 14165: 10 [  895/ 1327], train_loss/perplexity = 4.45487118/86.0450668 secs/batch = 0.3005s, grad.norm=14.19849205
 14170: 10 [  900/ 1327], train_loss/perplexity = 4.36341858/78.5251236 secs/batch = 0.2955s, grad.norm=15.03932476
 14175: 10 [  905/ 1327], train_loss/perplexity = 4.10599041/60.7028351 secs/batch = 0.3000s, grad.norm=14.15438080
 14180: 10 [  910/ 1327], train_loss/perplexity = 4.21490860/67.6879807 secs/batch = 0.2958s, grad.norm=14.13839245
 14185: 10 [  915/ 1327], train_loss/perplexity = 4.41826534/82.9522705 secs/batch = 0.2939s, grad.norm=14.64434624
 14190: 10 [  920/ 1327], train_loss/perplexity = 4.56073332/95.6535950 secs/batch = 0.3008s, grad.norm=14.61673737
 14195: 10 [  925/ 1327], train_loss/perplexity = 4.48478699/88.6580658 secs/batch = 0.3009s, grad.norm=14.14438248
 14200: 10 [  930/ 1327], train_loss/perplexity = 4.40875578/82.1671677 secs/batch = 0.2950s, grad.norm=14.24386787
 14205: 10 [  935/ 1327], train_loss/perplexity = 4.50496435/90.4651184 secs/batch = 0.2948s, grad.norm=13.63390732
 14210: 10 [  940/ 1327], train_loss/perplexity = 4.47372198/87.6824722 secs/batch = 0.2989s, grad.norm=13.77398682
 14215: 10 [  945/ 1327], train_loss/perplexity = 4.63647556/103.1800537 secs/batch = 0.2944s, grad.norm=14.65406704
 14220: 10 [  950/ 1327], train_loss/perplexity = 4.37739658/79.6304550 secs/batch = 0.2989s, grad.norm=15.31690788
 14225: 10 [  955/ 1327], train_loss/perplexity = 4.44883871/85.5275650 secs/batch = 0.2955s, grad.norm=14.87943840
 14230: 10 [  960/ 1327], train_loss/perplexity = 4.67610741/107.3513794 secs/batch = 0.2952s, grad.norm=15.03459549
 14235: 10 [  965/ 1327], train_loss/perplexity = 4.42671013/83.6557465 secs/batch = 0.2986s, grad.norm=15.02075958
 14240: 10 [  970/ 1327], train_loss/perplexity = 4.64259624/103.8135223 secs/batch = 0.2959s, grad.norm=14.36147499
 14245: 10 [  975/ 1327], train_loss/perplexity = 4.33028316/75.9657974 secs/batch = 0.2930s, grad.norm=15.12476063
 14250: 10 [  980/ 1327], train_loss/perplexity = 4.17086983/64.7717667 secs/batch = 0.2943s, grad.norm=14.13818264
 14255: 10 [  985/ 1327], train_loss/perplexity = 4.35774708/78.0810242 secs/batch = 0.2993s, grad.norm=15.88313770
 14260: 10 [  990/ 1327], train_loss/perplexity = 4.59145117/98.6374664 secs/batch = 0.2946s, grad.norm=14.87568665
 14265: 10 [  995/ 1327], train_loss/perplexity = 4.54259491/93.9342346 secs/batch = 0.2945s, grad.norm=14.33951378
 14270: 10 [ 1000/ 1327], train_loss/perplexity = 4.04073334/56.8680305 secs/batch = 0.2986s, grad.norm=13.81406403
 14275: 10 [ 1005/ 1327], train_loss/perplexity = 4.53907776/93.6044312 secs/batch = 0.2992s, grad.norm=14.64208412
 14280: 10 [ 1010/ 1327], train_loss/perplexity = 4.06305599/58.1517525 secs/batch = 0.2987s, grad.norm=13.61514473
 14285: 10 [ 1015/ 1327], train_loss/perplexity = 4.61189747/100.6749954 secs/batch = 0.2929s, grad.norm=14.84951019
 14290: 10 [ 1020/ 1327], train_loss/perplexity = 4.71668100/111.7965851 secs/batch = 0.2954s, grad.norm=14.01774120
 14295: 10 [ 1025/ 1327], train_loss/perplexity = 4.57614183/97.1388931 secs/batch = 0.2942s, grad.norm=14.28902531
 14300: 10 [ 1030/ 1327], train_loss/perplexity = 4.31455564/74.7803879 secs/batch = 0.2979s, grad.norm=14.57821369
 14305: 10 [ 1035/ 1327], train_loss/perplexity = 4.31074667/74.4960938 secs/batch = 0.2928s, grad.norm=14.06505775
 14310: 10 [ 1040/ 1327], train_loss/perplexity = 4.53433847/93.1618652 secs/batch = 0.3000s, grad.norm=15.06535244
 14315: 10 [ 1045/ 1327], train_loss/perplexity = 4.05111122/57.4612732 secs/batch = 0.2993s, grad.norm=13.63889503
 14320: 10 [ 1050/ 1327], train_loss/perplexity = 4.22322989/68.2535782 secs/batch = 0.2951s, grad.norm=15.39605331
 14325: 10 [ 1055/ 1327], train_loss/perplexity = 4.27195501/71.6615982 secs/batch = 0.3011s, grad.norm=16.73305130
 14330: 10 [ 1060/ 1327], train_loss/perplexity = 3.91737652/50.2683945 secs/batch = 0.2928s, grad.norm=15.77261734
 14335: 10 [ 1065/ 1327], train_loss/perplexity = 4.03814173/56.7208405 secs/batch = 0.2966s, grad.norm=15.21891785
 14340: 10 [ 1070/ 1327], train_loss/perplexity = 4.40498447/81.8578720 secs/batch = 0.2955s, grad.norm=15.56927204
 14345: 10 [ 1075/ 1327], train_loss/perplexity = 4.12276411/61.7296333 secs/batch = 0.2932s, grad.norm=14.67771530
 14350: 10 [ 1080/ 1327], train_loss/perplexity = 4.06007910/57.9788971 secs/batch = 0.2974s, grad.norm=14.39175415
 14355: 10 [ 1085/ 1327], train_loss/perplexity = 3.91362953/50.0803909 secs/batch = 0.2984s, grad.norm=14.92754364
 14360: 10 [ 1090/ 1327], train_loss/perplexity = 4.20399094/66.9530029 secs/batch = 0.3004s, grad.norm=16.28882217
 14365: 10 [ 1095/ 1327], train_loss/perplexity = 4.33410740/76.2568588 secs/batch = 0.3010s, grad.norm=14.72681332
 14370: 10 [ 1100/ 1327], train_loss/perplexity = 4.01510668/55.4292068 secs/batch = 0.2989s, grad.norm=16.83579636
 14375: 10 [ 1105/ 1327], train_loss/perplexity = 3.99736977/54.4547310 secs/batch = 0.2926s, grad.norm=14.94987392
 14380: 10 [ 1110/ 1327], train_loss/perplexity = 4.37421799/79.3777390 secs/batch = 0.2960s, grad.norm=16.29069328
 14385: 10 [ 1115/ 1327], train_loss/perplexity = 4.21793318/67.8930130 secs/batch = 0.2998s, grad.norm=14.93354797
 14390: 10 [ 1120/ 1327], train_loss/perplexity = 4.36522007/78.6667099 secs/batch = 0.2995s, grad.norm=14.42240906
 14395: 10 [ 1125/ 1327], train_loss/perplexity = 4.62211370/101.7087860 secs/batch = 0.2983s, grad.norm=15.59724426
 14400: 10 [ 1130/ 1327], train_loss/perplexity = 4.29514122/73.3425674 secs/batch = 0.2918s, grad.norm=15.21688366
 14405: 10 [ 1135/ 1327], train_loss/perplexity = 4.29266882/73.1614609 secs/batch = 0.3000s, grad.norm=15.02216244
 14410: 10 [ 1140/ 1327], train_loss/perplexity = 4.52801895/92.5749817 secs/batch = 0.2938s, grad.norm=16.69698524
 14415: 10 [ 1145/ 1327], train_loss/perplexity = 4.32185936/75.3285599 secs/batch = 0.2933s, grad.norm=15.23587513
 14420: 10 [ 1150/ 1327], train_loss/perplexity = 4.25566244/70.5035095 secs/batch = 0.2953s, grad.norm=14.14265156
 14425: 10 [ 1155/ 1327], train_loss/perplexity = 4.43588305/84.4266434 secs/batch = 0.2948s, grad.norm=15.65517902
 14430: 10 [ 1160/ 1327], train_loss/perplexity = 4.30983543/74.4282379 secs/batch = 0.2945s, grad.norm=14.81043243
 14435: 10 [ 1165/ 1327], train_loss/perplexity = 4.37070084/79.0990448 secs/batch = 0.2989s, grad.norm=15.36350250
 14440: 10 [ 1170/ 1327], train_loss/perplexity = 4.26192379/70.9463348 secs/batch = 0.2960s, grad.norm=14.43477154
 14445: 10 [ 1175/ 1327], train_loss/perplexity = 3.99663949/54.4149818 secs/batch = 0.2945s, grad.norm=14.59792614
 14450: 10 [ 1180/ 1327], train_loss/perplexity = 3.99512601/54.3326874 secs/batch = 0.2983s, grad.norm=14.44631672
 14455: 10 [ 1185/ 1327], train_loss/perplexity = 4.20919323/67.3022232 secs/batch = 0.2985s, grad.norm=14.78743267
 14460: 10 [ 1190/ 1327], train_loss/perplexity = 4.33766890/76.5289307 secs/batch = 0.2996s, grad.norm=15.59602547
 14465: 10 [ 1195/ 1327], train_loss/perplexity = 3.99778771/54.4774971 secs/batch = 0.3010s, grad.norm=14.49923611
 14470: 10 [ 1200/ 1327], train_loss/perplexity = 4.06526661/58.2804451 secs/batch = 0.2957s, grad.norm=14.72131729
 14475: 10 [ 1205/ 1327], train_loss/perplexity = 4.11337614/61.1528282 secs/batch = 0.2931s, grad.norm=14.72800446
 14480: 10 [ 1210/ 1327], train_loss/perplexity = 3.72522235/41.4804535 secs/batch = 0.2946s, grad.norm=15.30015850
 14485: 10 [ 1215/ 1327], train_loss/perplexity = 3.97865939/53.4453354 secs/batch = 0.3006s, grad.norm=14.55593777
 14490: 10 [ 1220/ 1327], train_loss/perplexity = 4.10353994/60.5542679 secs/batch = 0.2942s, grad.norm=14.81771755
 14495: 10 [ 1225/ 1327], train_loss/perplexity = 3.86905789/47.8972397 secs/batch = 0.2986s, grad.norm=15.86156559
 14500: 10 [ 1230/ 1327], train_loss/perplexity = 4.17423344/64.9899979 secs/batch = 0.2987s, grad.norm=14.75521469
 14505: 10 [ 1235/ 1327], train_loss/perplexity = 4.11618567/61.3248825 secs/batch = 0.2996s, grad.norm=14.91371727
 14510: 10 [ 1240/ 1327], train_loss/perplexity = 4.30753183/74.2569809 secs/batch = 0.2952s, grad.norm=15.80619812
 14515: 10 [ 1245/ 1327], train_loss/perplexity = 4.22388887/68.2985764 secs/batch = 0.2948s, grad.norm=14.94046211
 14520: 10 [ 1250/ 1327], train_loss/perplexity = 4.32054329/75.2294922 secs/batch = 0.2987s, grad.norm=14.09284496
 14525: 10 [ 1255/ 1327], train_loss/perplexity = 4.39252138/80.8440018 secs/batch = 0.2988s, grad.norm=14.22007751
 14530: 10 [ 1260/ 1327], train_loss/perplexity = 4.14887333/63.3625717 secs/batch = 0.2985s, grad.norm=14.97540188
 14535: 10 [ 1265/ 1327], train_loss/perplexity = 4.43185186/84.0869904 secs/batch = 0.3013s, grad.norm=14.61667824
 14540: 10 [ 1270/ 1327], train_loss/perplexity = 4.12006140/61.5630226 secs/batch = 0.2938s, grad.norm=15.00695515
 14545: 10 [ 1275/ 1327], train_loss/perplexity = 4.33012676/75.9539108 secs/batch = 0.2996s, grad.norm=15.93283558
 14550: 10 [ 1280/ 1327], train_loss/perplexity = 4.15844774/63.9721451 secs/batch = 0.2942s, grad.norm=14.80295944
 14555: 10 [ 1285/ 1327], train_loss/perplexity = 4.06129789/58.0496025 secs/batch = 0.2981s, grad.norm=15.35973740
 14560: 10 [ 1290/ 1327], train_loss/perplexity = 4.30138922/73.8022461 secs/batch = 0.2990s, grad.norm=14.56885815
 14565: 10 [ 1295/ 1327], train_loss/perplexity = 4.29522228/73.3485184 secs/batch = 0.2923s, grad.norm=16.01654434
 14570: 10 [ 1300/ 1327], train_loss/perplexity = 4.50959444/90.8849487 secs/batch = 0.2941s, grad.norm=14.76865292
 14575: 10 [ 1305/ 1327], train_loss/perplexity = 4.54032850/93.7215805 secs/batch = 0.2937s, grad.norm=15.40086079
 14580: 10 [ 1310/ 1327], train_loss/perplexity = 4.77279377/118.2491455 secs/batch = 0.2947s, grad.norm=15.66294193
 14585: 10 [ 1315/ 1327], train_loss/perplexity = 4.62987804/102.5015640 secs/batch = 0.2941s, grad.norm=15.29421329
 14590: 10 [ 1320/ 1327], train_loss/perplexity = 4.62201500/101.6987457 secs/batch = 0.2955s, grad.norm=15.29091454
 14595: 10 [ 1325/ 1327], train_loss/perplexity = 4.52336645/92.1452789 secs/batch = 0.3010s, grad.norm=14.82916355
Epoch training time: 393.5853707790375
	> validation loss = 4.71220779, perplexity = 111.29760742
	> validation loss = 4.61989117, perplexity = 101.48298645
	> validation loss = 4.63623047, perplexity = 103.15476990
	> validation loss = 4.59642601, perplexity = 99.12939453
	> validation loss = 4.80356026, perplexity = 121.94379425
	> validation loss = 4.69327402, perplexity = 109.21015167
	> validation loss = 4.67848969, perplexity = 107.60742950
	> validation loss = 4.52076340, perplexity = 91.90573120
	> validation loss = 4.31262350, perplexity = 74.63603973
	> validation loss = 4.41837358, perplexity = 82.96124268
	> validation loss = 4.55862331, perplexity = 95.45198059
	> validation loss = 4.59423494, perplexity = 98.91242981
	> validation loss = 4.51817179, perplexity = 91.66785431
	> validation loss = 4.33511925, perplexity = 76.33406067
	> validation loss = 4.29424047, perplexity = 73.27653503
	> validation loss = 4.28717136, perplexity = 72.76036835
	> validation loss = 4.70464706, perplexity = 110.45928955
	> validation loss = 4.24500704, perplexity = 69.75624847
	> validation loss = 4.66762638, perplexity = 106.44478607
	> validation loss = 4.57003117, perplexity = 96.54711914
	> validation loss = 4.34690809, perplexity = 77.23927307
at the end of epoch: 10
train loss = 4.43476435, perplexity = 84.33224974
validation loss = 4.53759065, perplexity = 93.46533824
Saved model cv/epoch010_4.5376.model
 14602: 11 [    5/ 1327], train_loss/perplexity = 4.54760647/94.4061737 secs/batch = 0.2954s, grad.norm=15.65966606
 14607: 11 [   10/ 1327], train_loss/perplexity = 4.14994144/63.4302864 secs/batch = 0.2949s, grad.norm=14.56194687
 14612: 11 [   15/ 1327], train_loss/perplexity = 4.40581417/81.9258194 secs/batch = 0.2950s, grad.norm=14.14245415
 14617: 11 [   20/ 1327], train_loss/perplexity = 4.61462116/100.9495773 secs/batch = 0.2926s, grad.norm=15.62504482
 14622: 11 [   25/ 1327], train_loss/perplexity = 4.48942852/89.0705261 secs/batch = 0.2988s, grad.norm=15.19471073
 14627: 11 [   30/ 1327], train_loss/perplexity = 4.48653698/88.8133469 secs/batch = 0.2951s, grad.norm=15.21080208
 14632: 11 [   35/ 1327], train_loss/perplexity = 4.30047846/73.7350616 secs/batch = 0.2952s, grad.norm=14.29998207
 14637: 11 [   40/ 1327], train_loss/perplexity = 4.29885244/73.6152649 secs/batch = 0.2978s, grad.norm=14.77872467
 14642: 11 [   45/ 1327], train_loss/perplexity = 4.02569962/56.0194855 secs/batch = 0.2936s, grad.norm=14.58838844
 14647: 11 [   50/ 1327], train_loss/perplexity = 4.32976913/75.9267578 secs/batch = 0.2960s, grad.norm=15.31511402
 14652: 11 [   55/ 1327], train_loss/perplexity = 4.29185009/73.1015854 secs/batch = 0.2948s, grad.norm=15.16505337
 14657: 11 [   60/ 1327], train_loss/perplexity = 4.56755829/96.3086624 secs/batch = 0.2928s, grad.norm=15.57394409
 14662: 11 [   65/ 1327], train_loss/perplexity = 4.08593750/59.4976921 secs/batch = 0.2953s, grad.norm=13.79952049
 14667: 11 [   70/ 1327], train_loss/perplexity = 3.95761204/52.3322105 secs/batch = 0.2956s, grad.norm=14.96377468
 14672: 11 [   75/ 1327], train_loss/perplexity = 3.79119635/44.3093796 secs/batch = 0.2968s, grad.norm=14.43402290
 14677: 11 [   80/ 1327], train_loss/perplexity = 4.21881199/67.9527054 secs/batch = 0.2933s, grad.norm=15.38307667
 14682: 11 [   85/ 1327], train_loss/perplexity = 4.26964760/71.4964371 secs/batch = 0.2964s, grad.norm=14.96882915
 14687: 11 [   90/ 1327], train_loss/perplexity = 4.39121914/80.7387924 secs/batch = 0.3011s, grad.norm=15.32523060
 14692: 11 [   95/ 1327], train_loss/perplexity = 4.17331123/64.9300919 secs/batch = 0.2992s, grad.norm=15.25850296
 14697: 11 [  100/ 1327], train_loss/perplexity = 4.40177393/81.5954819 secs/batch = 0.2985s, grad.norm=14.79378891
 14702: 11 [  105/ 1327], train_loss/perplexity = 4.22191620/68.1639786 secs/batch = 0.2983s, grad.norm=15.40023708
 14707: 11 [  110/ 1327], train_loss/perplexity = 4.13570690/62.5337791 secs/batch = 0.2953s, grad.norm=14.39120960
 14712: 11 [  115/ 1327], train_loss/perplexity = 4.08106470/59.2084770 secs/batch = 0.3011s, grad.norm=15.59518051
 14717: 11 [  120/ 1327], train_loss/perplexity = 4.22432899/68.3286362 secs/batch = 0.2926s, grad.norm=15.37576866
 14722: 11 [  125/ 1327], train_loss/perplexity = 4.23119545/68.7994308 secs/batch = 0.2950s, grad.norm=15.29658890
 14727: 11 [  130/ 1327], train_loss/perplexity = 4.21468782/67.6730347 secs/batch = 0.2953s, grad.norm=15.89519978
 14732: 11 [  135/ 1327], train_loss/perplexity = 4.43416691/84.2818832 secs/batch = 0.2966s, grad.norm=20.11676407
 14737: 11 [  140/ 1327], train_loss/perplexity = 4.55491400/95.0985794 secs/batch = 0.2997s, grad.norm=16.00915909
 14742: 11 [  145/ 1327], train_loss/perplexity = 4.40981007/82.2538376 secs/batch = 0.2969s, grad.norm=16.53481483
 14747: 11 [  150/ 1327], train_loss/perplexity = 4.46487522/86.9101868 secs/batch = 0.2952s, grad.norm=16.28696060
 14752: 11 [  155/ 1327], train_loss/perplexity = 4.70000648/109.9478836 secs/batch = 0.2961s, grad.norm=15.10125542
 14757: 11 [  160/ 1327], train_loss/perplexity = 4.28607082/72.6803360 secs/batch = 0.2955s, grad.norm=14.07769108
 14762: 11 [  165/ 1327], train_loss/perplexity = 4.45075369/85.6915054 secs/batch = 0.2967s, grad.norm=14.48002815
 14767: 11 [  170/ 1327], train_loss/perplexity = 4.25690174/70.5909348 secs/batch = 0.3003s, grad.norm=14.41763878
 14772: 11 [  175/ 1327], train_loss/perplexity = 4.59008074/98.5023804 secs/batch = 0.3003s, grad.norm=14.60218334
 14777: 11 [  180/ 1327], train_loss/perplexity = 4.44265366/85.0002060 secs/batch = 0.2950s, grad.norm=15.40532112
 14782: 11 [  185/ 1327], train_loss/perplexity = 4.78902292/120.1838837 secs/batch = 0.2948s, grad.norm=15.12236786
 14787: 11 [  190/ 1327], train_loss/perplexity = 4.18658781/65.7978897 secs/batch = 0.2943s, grad.norm=14.55373478
 14792: 11 [  195/ 1327], train_loss/perplexity = 4.49930334/89.9544449 secs/batch = 0.2957s, grad.norm=13.90015888
 14797: 11 [  200/ 1327], train_loss/perplexity = 4.45686245/86.2165756 secs/batch = 0.2949s, grad.norm=14.93879128
 14802: 11 [  205/ 1327], train_loss/perplexity = 4.56330109/95.8995285 secs/batch = 0.2961s, grad.norm=14.39430046
 14807: 11 [  210/ 1327], train_loss/perplexity = 4.43853569/84.6508942 secs/batch = 0.2959s, grad.norm=14.21289539
 14812: 11 [  215/ 1327], train_loss/perplexity = 4.60100698/99.5845490 secs/batch = 0.2957s, grad.norm=14.00196075
 14817: 11 [  220/ 1327], train_loss/perplexity = 4.50370455/90.3512192 secs/batch = 0.3021s, grad.norm=14.41464901
 14822: 11 [  225/ 1327], train_loss/perplexity = 4.66092920/105.7342834 secs/batch = 0.2949s, grad.norm=14.86411858
 14827: 11 [  230/ 1327], train_loss/perplexity = 4.56551266/96.1118546 secs/batch = 0.2950s, grad.norm=15.07417870
 14832: 11 [  235/ 1327], train_loss/perplexity = 4.35377312/77.7713470 secs/batch = 0.2951s, grad.norm=14.63574696
 14837: 11 [  240/ 1327], train_loss/perplexity = 4.11088514/61.0006866 secs/batch = 0.2939s, grad.norm=15.62137794
 14842: 11 [  245/ 1327], train_loss/perplexity = 4.44678402/85.3520126 secs/batch = 0.2972s, grad.norm=15.38062382
 14847: 11 [  250/ 1327], train_loss/perplexity = 4.27064800/71.5679932 secs/batch = 0.3020s, grad.norm=14.27254772
 14852: 11 [  255/ 1327], train_loss/perplexity = 4.27856970/72.1371918 secs/batch = 0.2942s, grad.norm=14.94218254
 14857: 11 [  260/ 1327], train_loss/perplexity = 4.54854298/94.4946289 secs/batch = 0.2942s, grad.norm=15.25380611
 14862: 11 [  265/ 1327], train_loss/perplexity = 4.96964455/143.9757080 secs/batch = 0.2940s, grad.norm=32.47232437
 14867: 11 [  270/ 1327], train_loss/perplexity = 4.77743435/118.7991638 secs/batch = 0.2946s, grad.norm=15.28162003
 14872: 11 [  275/ 1327], train_loss/perplexity = 4.72267723/112.4689560 secs/batch = 0.2954s, grad.norm=14.59688282
 14877: 11 [  280/ 1327], train_loss/perplexity = 4.47245073/87.5710754 secs/batch = 0.2993s, grad.norm=14.59134483
 14882: 11 [  285/ 1327], train_loss/perplexity = 4.72627115/112.8738861 secs/batch = 0.3000s, grad.norm=15.13265228
 14887: 11 [  290/ 1327], train_loss/perplexity = 4.51473236/91.3531113 secs/batch = 0.2949s, grad.norm=15.56501484
 14892: 11 [  295/ 1327], train_loss/perplexity = 4.24377537/69.6703873 secs/batch = 0.2953s, grad.norm=14.67206764
 14897: 11 [  300/ 1327], train_loss/perplexity = 3.75695777/42.8179665 secs/batch = 0.2976s, grad.norm=14.16088581
 14902: 11 [  305/ 1327], train_loss/perplexity = 4.30189133/73.8393173 secs/batch = 0.2945s, grad.norm=14.32110023
 14907: 11 [  310/ 1327], train_loss/perplexity = 4.22929192/68.6685944 secs/batch = 0.2955s, grad.norm=15.43608379
 14912: 11 [  315/ 1327], train_loss/perplexity = 3.88593435/48.7124367 secs/batch = 0.2960s, grad.norm=15.10741043
 14917: 11 [  320/ 1327], train_loss/perplexity = 3.73442101/41.8637810 secs/batch = 0.2954s, grad.norm=15.86637020
 14922: 11 [  325/ 1327], train_loss/perplexity = 3.85920095/47.4274406 secs/batch = 0.3006s, grad.norm=14.81226635
 14927: 11 [  330/ 1327], train_loss/perplexity = 4.37604618/79.5229950 secs/batch = 0.2949s, grad.norm=14.99107647
 14932: 11 [  335/ 1327], train_loss/perplexity = 3.83484507/46.2862549 secs/batch = 0.2941s, grad.norm=14.05714607
 14937: 11 [  340/ 1327], train_loss/perplexity = 4.57300997/96.8351440 secs/batch = 0.2940s, grad.norm=14.42302227
 14942: 11 [  345/ 1327], train_loss/perplexity = 4.33930254/76.6540604 secs/batch = 0.2954s, grad.norm=13.86250401
 14947: 11 [  350/ 1327], train_loss/perplexity = 4.42532158/83.5396652 secs/batch = 0.2968s, grad.norm=16.12503815
 14952: 11 [  355/ 1327], train_loss/perplexity = 4.36864519/78.9366150 secs/batch = 0.2961s, grad.norm=15.87312508
 14957: 11 [  360/ 1327], train_loss/perplexity = 4.45699453/86.2279663 secs/batch = 0.3012s, grad.norm=15.66763115
 14962: 11 [  365/ 1327], train_loss/perplexity = 4.49462414/89.5345078 secs/batch = 0.2949s, grad.norm=15.00695801
 14967: 11 [  370/ 1327], train_loss/perplexity = 4.55503941/95.1105042 secs/batch = 0.2953s, grad.norm=15.17921829
 14972: 11 [  375/ 1327], train_loss/perplexity = 3.93999672/51.4184341 secs/batch = 0.2957s, grad.norm=14.93392086
 14977: 11 [  380/ 1327], train_loss/perplexity = 4.05876684/57.9028625 secs/batch = 0.2957s, grad.norm=15.53023338
 14982: 11 [  385/ 1327], train_loss/perplexity = 4.27651072/71.9888153 secs/batch = 0.2952s, grad.norm=15.77331448
 14987: 11 [  390/ 1327], train_loss/perplexity = 4.33563995/76.3738174 secs/batch = 0.2946s, grad.norm=14.47895050
 14992: 11 [  395/ 1327], train_loss/perplexity = 4.44129658/84.8849335 secs/batch = 0.2974s, grad.norm=16.06650543
 14997: 11 [  400/ 1327], train_loss/perplexity = 4.29881048/73.6121750 secs/batch = 0.2941s, grad.norm=14.00747776
 15002: 11 [  405/ 1327], train_loss/perplexity = 4.64693356/104.2647705 secs/batch = 0.2955s, grad.norm=14.83058739
 15007: 11 [  410/ 1327], train_loss/perplexity = 4.24594402/69.8216400 secs/batch = 0.3000s, grad.norm=15.11167049
 15012: 11 [  415/ 1327], train_loss/perplexity = 4.22066498/68.0787430 secs/batch = 0.3002s, grad.norm=14.88534164
 15017: 11 [  420/ 1327], train_loss/perplexity = 3.92346644/50.5754585 secs/batch = 0.2962s, grad.norm=15.23063087
 15022: 11 [  425/ 1327], train_loss/perplexity = 4.19614983/66.4300690 secs/batch = 0.2964s, grad.norm=15.90268517
 15027: 11 [  430/ 1327], train_loss/perplexity = 4.47352457/87.6651611 secs/batch = 0.2975s, grad.norm=16.35088158
 15032: 11 [  435/ 1327], train_loss/perplexity = 4.54352808/94.0219345 secs/batch = 0.2942s, grad.norm=15.63332081
 15037: 11 [  440/ 1327], train_loss/perplexity = 4.01325321/55.3265686 secs/batch = 0.2972s, grad.norm=15.27553082
 15042: 11 [  445/ 1327], train_loss/perplexity = 4.34973288/77.4577713 secs/batch = 0.2939s, grad.norm=15.50616455
 15047: 11 [  450/ 1327], train_loss/perplexity = 4.30659199/74.1872253 secs/batch = 0.2959s, grad.norm=15.24224377
 15052: 11 [  455/ 1327], train_loss/perplexity = 4.24560356/69.7978745 secs/batch = 0.2944s, grad.norm=14.32012463
 15057: 11 [  460/ 1327], train_loss/perplexity = 4.26898861/71.4493332 secs/batch = 0.2922s, grad.norm=15.61791801
 15062: 11 [  465/ 1327], train_loss/perplexity = 3.98220348/53.6350899 secs/batch = 0.2973s, grad.norm=18.12168503
 15067: 11 [  470/ 1327], train_loss/perplexity = 4.73835468/114.2460785 secs/batch = 0.3016s, grad.norm=14.94895077
 15072: 11 [  475/ 1327], train_loss/perplexity = 4.13263464/62.3419571 secs/batch = 0.2945s, grad.norm=15.59632301
 15077: 11 [  480/ 1327], train_loss/perplexity = 4.26720190/71.3217926 secs/batch = 0.2954s, grad.norm=15.11445904
 15082: 11 [  485/ 1327], train_loss/perplexity = 4.29929924/73.6481628 secs/batch = 0.2928s, grad.norm=15.35350037
 15087: 11 [  490/ 1327], train_loss/perplexity = 4.16273642/64.2470856 secs/batch = 0.2992s, grad.norm=19.22756577
 15092: 11 [  495/ 1327], train_loss/perplexity = 4.21378136/67.6117249 secs/batch = 0.3005s, grad.norm=14.85657501
 15097: 11 [  500/ 1327], train_loss/perplexity = 4.42905998/83.8525543 secs/batch = 0.2941s, grad.norm=15.53094673
 15102: 11 [  505/ 1327], train_loss/perplexity = 4.49558496/89.6205750 secs/batch = 0.3001s, grad.norm=13.89791965
 15107: 11 [  510/ 1327], train_loss/perplexity = 4.81703472/123.5980453 secs/batch = 0.3004s, grad.norm=14.63213253
 15112: 11 [  515/ 1327], train_loss/perplexity = 4.42926931/83.8701096 secs/batch = 0.2970s, grad.norm=14.51594639
 15117: 11 [  520/ 1327], train_loss/perplexity = 4.60511017/99.9939957 secs/batch = 0.2961s, grad.norm=14.59367275
 15122: 11 [  525/ 1327], train_loss/perplexity = 4.18748093/65.8566818 secs/batch = 0.3004s, grad.norm=14.91842651
 15127: 11 [  530/ 1327], train_loss/perplexity = 4.20174265/66.8026428 secs/batch = 0.2944s, grad.norm=15.59031773
 15132: 11 [  535/ 1327], train_loss/perplexity = 4.29546118/73.3660431 secs/batch = 0.2944s, grad.norm=14.95131016
 15137: 11 [  540/ 1327], train_loss/perplexity = 4.52077866/91.9071350 secs/batch = 0.3005s, grad.norm=15.49309158
 15142: 11 [  545/ 1327], train_loss/perplexity = 4.42538929/83.5453262 secs/batch = 0.2963s, grad.norm=15.63693047
 15147: 11 [  550/ 1327], train_loss/perplexity = 4.36768246/78.8606567 secs/batch = 0.2945s, grad.norm=14.98833370
 15152: 11 [  555/ 1327], train_loss/perplexity = 4.29309273/73.1924820 secs/batch = 0.2938s, grad.norm=15.10323811
 15157: 11 [  560/ 1327], train_loss/perplexity = 4.34696341/77.2435455 secs/batch = 0.2920s, grad.norm=16.36123657
 15162: 11 [  565/ 1327], train_loss/perplexity = 4.23740482/69.2279587 secs/batch = 0.2953s, grad.norm=16.16812134
 15167: 11 [  570/ 1327], train_loss/perplexity = 4.27133703/71.6173248 secs/batch = 0.3013s, grad.norm=16.03477669
 15172: 11 [  575/ 1327], train_loss/perplexity = 4.08383560/59.3727646 secs/batch = 0.2999s, grad.norm=16.07511520
 15177: 11 [  580/ 1327], train_loss/perplexity = 4.47571182/87.8571167 secs/batch = 0.2956s, grad.norm=14.61422539
 15182: 11 [  585/ 1327], train_loss/perplexity = 4.00377941/54.8048897 secs/batch = 0.2975s, grad.norm=15.61527252
 15187: 11 [  590/ 1327], train_loss/perplexity = 4.34267902/76.9133148 secs/batch = 0.3033s, grad.norm=14.99864674
 15192: 11 [  595/ 1327], train_loss/perplexity = 4.34341002/76.9695587 secs/batch = 0.2955s, grad.norm=15.53260899
 15197: 11 [  600/ 1327], train_loss/perplexity = 4.55354691/94.9686584 secs/batch = 0.2963s, grad.norm=14.04803467
 15202: 11 [  605/ 1327], train_loss/perplexity = 4.46409321/86.8422470 secs/batch = 0.2993s, grad.norm=16.02402115
 15207: 11 [  610/ 1327], train_loss/perplexity = 4.63417339/102.9427872 secs/batch = 0.2952s, grad.norm=15.26380539
 15212: 11 [  615/ 1327], train_loss/perplexity = 4.16031265/64.0915604 secs/batch = 0.2963s, grad.norm=14.84250927
 15217: 11 [  620/ 1327], train_loss/perplexity = 4.59422588/98.9115372 secs/batch = 0.2946s, grad.norm=15.06700230
 15222: 11 [  625/ 1327], train_loss/perplexity = 4.58302307/97.8096313 secs/batch = 0.2987s, grad.norm=14.85802841
 15227: 11 [  630/ 1327], train_loss/perplexity = 4.63444185/102.9704285 secs/batch = 0.2941s, grad.norm=14.63956833
 15232: 11 [  635/ 1327], train_loss/perplexity = 4.38325214/80.0980988 secs/batch = 0.2945s, grad.norm=15.48372650
 15237: 11 [  640/ 1327], train_loss/perplexity = 4.30488634/74.0607986 secs/batch = 0.2954s, grad.norm=15.30760193
 15242: 11 [  645/ 1327], train_loss/perplexity = 4.64804745/104.3809738 secs/batch = 0.2945s, grad.norm=15.99473667
 15247: 11 [  650/ 1327], train_loss/perplexity = 4.12376261/61.7913017 secs/batch = 0.2957s, grad.norm=15.13276768
 15252: 11 [  655/ 1327], train_loss/perplexity = 4.38518763/80.2532806 secs/batch = 0.2960s, grad.norm=15.78970623
 15257: 11 [  660/ 1327], train_loss/perplexity = 4.24553967/69.7934113 secs/batch = 0.2930s, grad.norm=15.97581577
 15262: 11 [  665/ 1327], train_loss/perplexity = 4.36493587/78.6443558 secs/batch = 0.2945s, grad.norm=15.43356705
 15267: 11 [  670/ 1327], train_loss/perplexity = 4.25840187/70.6969070 secs/batch = 0.2948s, grad.norm=15.50059509
 15272: 11 [  675/ 1327], train_loss/perplexity = 4.02880955/56.1939735 secs/batch = 0.2960s, grad.norm=15.49497223
 15277: 11 [  680/ 1327], train_loss/perplexity = 4.26941109/71.4795303 secs/batch = 0.2953s, grad.norm=15.65380192
 15282: 11 [  685/ 1327], train_loss/perplexity = 4.14161062/62.9040527 secs/batch = 0.2959s, grad.norm=15.80460739
 15287: 11 [  690/ 1327], train_loss/perplexity = 4.53957558/93.6510468 secs/batch = 0.3006s, grad.norm=16.14675522
 15292: 11 [  695/ 1327], train_loss/perplexity = 4.31207466/74.5950851 secs/batch = 0.2987s, grad.norm=14.67102909
 15297: 11 [  700/ 1327], train_loss/perplexity = 4.66563845/106.2333908 secs/batch = 0.3000s, grad.norm=16.02027702
 15302: 11 [  705/ 1327], train_loss/perplexity = 4.31429958/74.7612381 secs/batch = 0.2956s, grad.norm=13.94753170
 15307: 11 [  710/ 1327], train_loss/perplexity = 4.22640085/68.4703522 secs/batch = 0.2999s, grad.norm=14.56316662
 15312: 11 [  715/ 1327], train_loss/perplexity = 4.07939291/59.1095734 secs/batch = 0.2944s, grad.norm=14.66078758
 15317: 11 [  720/ 1327], train_loss/perplexity = 4.14003134/62.8047905 secs/batch = 0.2986s, grad.norm=15.15682411
 15322: 11 [  725/ 1327], train_loss/perplexity = 4.15320444/63.6375961 secs/batch = 0.2947s, grad.norm=14.99161625
 15327: 11 [  730/ 1327], train_loss/perplexity = 4.29126263/73.0586548 secs/batch = 0.2967s, grad.norm=15.22219849
 15332: 11 [  735/ 1327], train_loss/perplexity = 4.45780563/86.2979355 secs/batch = 0.2930s, grad.norm=16.96941948
 15337: 11 [  740/ 1327], train_loss/perplexity = 3.83510160/46.2981300 secs/batch = 0.2952s, grad.norm=13.99634457
 15342: 11 [  745/ 1327], train_loss/perplexity = 4.35576773/77.9266281 secs/batch = 0.3007s, grad.norm=15.67606926
 15347: 11 [  750/ 1327], train_loss/perplexity = 4.22831059/68.6012421 secs/batch = 0.2992s, grad.norm=14.95519924
 15352: 11 [  755/ 1327], train_loss/perplexity = 4.06384420/58.1976051 secs/batch = 0.2957s, grad.norm=14.58184433
 15357: 11 [  760/ 1327], train_loss/perplexity = 4.00379515/54.8057518 secs/batch = 0.2948s, grad.norm=14.76703262
 15362: 11 [  765/ 1327], train_loss/perplexity = 4.16897964/64.6494522 secs/batch = 0.2959s, grad.norm=15.63578320
 15367: 11 [  770/ 1327], train_loss/perplexity = 4.02903128/56.2064362 secs/batch = 0.3011s, grad.norm=15.05728054
 15372: 11 [  775/ 1327], train_loss/perplexity = 4.13789749/62.6709175 secs/batch = 0.3007s, grad.norm=15.23629951
 15377: 11 [  780/ 1327], train_loss/perplexity = 4.52955103/92.7169266 secs/batch = 0.2940s, grad.norm=14.74941063
 15382: 11 [  785/ 1327], train_loss/perplexity = 4.33651257/76.4404907 secs/batch = 0.2924s, grad.norm=15.76342678
 15387: 11 [  790/ 1327], train_loss/perplexity = 4.12070847/61.6028709 secs/batch = 0.2999s, grad.norm=15.02489948
 15392: 11 [  795/ 1327], train_loss/perplexity = 4.49118900/89.2274780 secs/batch = 0.2944s, grad.norm=15.56294537
 15397: 11 [  800/ 1327], train_loss/perplexity = 4.36086369/78.3247528 secs/batch = 0.2930s, grad.norm=15.57214546
 15402: 11 [  805/ 1327], train_loss/perplexity = 4.77543020/118.5613098 secs/batch = 0.2994s, grad.norm=14.95681000
 15407: 11 [  810/ 1327], train_loss/perplexity = 4.36388206/78.5615234 secs/batch = 0.3002s, grad.norm=14.87327766
 15412: 11 [  815/ 1327], train_loss/perplexity = 4.14133930/62.8869896 secs/batch = 0.2946s, grad.norm=14.60995579
 15417: 11 [  820/ 1327], train_loss/perplexity = 4.02389383/55.9184189 secs/batch = 0.2975s, grad.norm=14.03515053
 15422: 11 [  825/ 1327], train_loss/perplexity = 5.34574556/209.7141876 secs/batch = 0.2984s, grad.norm=57.08450317
 15427: 11 [  830/ 1327], train_loss/perplexity = 4.00948429/55.1184387 secs/batch = 0.3015s, grad.norm=14.93625546
 15432: 11 [  835/ 1327], train_loss/perplexity = 4.27934885/72.1934128 secs/batch = 0.2946s, grad.norm=15.39913177
 15437: 11 [  840/ 1327], train_loss/perplexity = 4.34503794/77.0949631 secs/batch = 0.2993s, grad.norm=15.45591354
 15442: 11 [  845/ 1327], train_loss/perplexity = 4.13971424/62.7848778 secs/batch = 0.2943s, grad.norm=15.32071972
 15447: 11 [  850/ 1327], train_loss/perplexity = 4.30639887/74.1728973 secs/batch = 0.3001s, grad.norm=14.65407562
 15452: 11 [  855/ 1327], train_loss/perplexity = 4.29100513/73.0398483 secs/batch = 0.2950s, grad.norm=14.92676830
 15457: 11 [  860/ 1327], train_loss/perplexity = 4.01121950/55.2141647 secs/batch = 0.2946s, grad.norm=14.86287308
 15462: 11 [  865/ 1327], train_loss/perplexity = 4.47111607/87.4542770 secs/batch = 0.2938s, grad.norm=15.12304401
 15467: 11 [  870/ 1327], train_loss/perplexity = 4.34761000/77.2935104 secs/batch = 0.2989s, grad.norm=15.50588989
 15472: 11 [  875/ 1327], train_loss/perplexity = 3.92958164/50.8856850 secs/batch = 0.2949s, grad.norm=15.18350410
 15477: 11 [  880/ 1327], train_loss/perplexity = 4.12196541/61.6803513 secs/batch = 0.2984s, grad.norm=14.07435226
 15482: 11 [  885/ 1327], train_loss/perplexity = 4.33265257/76.1460037 secs/batch = 0.3005s, grad.norm=14.71339989
 15487: 11 [  890/ 1327], train_loss/perplexity = 4.41349936/82.5578613 secs/batch = 0.2930s, grad.norm=14.52528191
 15492: 11 [  895/ 1327], train_loss/perplexity = 4.50445652/90.4191895 secs/batch = 0.2944s, grad.norm=14.46118450
 15497: 11 [  900/ 1327], train_loss/perplexity = 4.27918530/72.1816101 secs/batch = 0.3010s, grad.norm=15.06091690
 15502: 11 [  905/ 1327], train_loss/perplexity = 4.10988188/60.9395180 secs/batch = 0.2994s, grad.norm=14.66595268
 15507: 11 [  910/ 1327], train_loss/perplexity = 4.17259979/64.8839188 secs/batch = 0.2986s, grad.norm=13.70338821
 15512: 11 [  915/ 1327], train_loss/perplexity = 4.41438961/82.6313858 secs/batch = 0.2950s, grad.norm=14.44557476
 15517: 11 [  920/ 1327], train_loss/perplexity = 4.52088881/91.9172592 secs/batch = 0.2989s, grad.norm=15.14486122
 15522: 11 [  925/ 1327], train_loss/perplexity = 4.43624067/84.4568405 secs/batch = 0.2951s, grad.norm=14.45419788
 15527: 11 [  930/ 1327], train_loss/perplexity = 4.33860636/76.6007080 secs/batch = 0.2999s, grad.norm=14.15253162
 15532: 11 [  935/ 1327], train_loss/perplexity = 4.41981030/83.0805206 secs/batch = 0.2961s, grad.norm=14.19016361
 15537: 11 [  940/ 1327], train_loss/perplexity = 4.36168623/78.3892059 secs/batch = 0.3002s, grad.norm=14.12149048
 15542: 11 [  945/ 1327], train_loss/perplexity = 4.64326668/103.8831482 secs/batch = 0.2944s, grad.norm=14.43309498
 15547: 11 [  950/ 1327], train_loss/perplexity = 4.37879610/79.7419739 secs/batch = 0.2940s, grad.norm=14.85530853
 15552: 11 [  955/ 1327], train_loss/perplexity = 4.39537334/81.0748978 secs/batch = 0.2933s, grad.norm=15.22315216
 15557: 11 [  960/ 1327], train_loss/perplexity = 4.68674231/108.4991455 secs/batch = 0.2952s, grad.norm=14.86972046
 15562: 11 [  965/ 1327], train_loss/perplexity = 4.38710833/80.4075699 secs/batch = 0.2952s, grad.norm=14.95422077
 15567: 11 [  970/ 1327], train_loss/perplexity = 4.63442659/102.9688568 secs/batch = 0.2945s, grad.norm=14.41223907
 15572: 11 [  975/ 1327], train_loss/perplexity = 4.28837490/72.8479843 secs/batch = 0.2943s, grad.norm=15.48210430
 15577: 11 [  980/ 1327], train_loss/perplexity = 4.20689487/67.1477127 secs/batch = 0.2953s, grad.norm=14.53086758
 15582: 11 [  985/ 1327], train_loss/perplexity = 4.33871603/76.6091156 secs/batch = 0.2934s, grad.norm=15.77848816
 15587: 11 [  990/ 1327], train_loss/perplexity = 4.53169394/92.9158249 secs/batch = 0.2931s, grad.norm=14.57966518
 15592: 11 [  995/ 1327], train_loss/perplexity = 4.51552486/91.4255371 secs/batch = 0.2967s, grad.norm=14.93987274
 15597: 11 [ 1000/ 1327], train_loss/perplexity = 4.01697254/55.5327263 secs/batch = 0.2988s, grad.norm=14.51796913
 15602: 11 [ 1005/ 1327], train_loss/perplexity = 4.47703981/87.9738693 secs/batch = 0.2932s, grad.norm=15.07289600
 15607: 11 [ 1010/ 1327], train_loss/perplexity = 4.05413294/57.6351700 secs/batch = 0.2952s, grad.norm=13.83506584
 15612: 11 [ 1015/ 1327], train_loss/perplexity = 4.59761286/99.2471161 secs/batch = 0.2948s, grad.norm=14.89589310
 15617: 11 [ 1020/ 1327], train_loss/perplexity = 4.59770870/99.2566299 secs/batch = 0.2938s, grad.norm=14.57132053
 15622: 11 [ 1025/ 1327], train_loss/perplexity = 4.58388138/97.8936234 secs/batch = 0.2995s, grad.norm=15.12792873
 15627: 11 [ 1030/ 1327], train_loss/perplexity = 4.25162363/70.2193298 secs/batch = 0.2981s, grad.norm=14.14011860
 15632: 11 [ 1035/ 1327], train_loss/perplexity = 4.28143692/72.3443146 secs/batch = 0.2943s, grad.norm=13.65531540
 15637: 11 [ 1040/ 1327], train_loss/perplexity = 4.48709154/88.8626175 secs/batch = 0.2951s, grad.norm=14.83545113
 15642: 11 [ 1045/ 1327], train_loss/perplexity = 4.03747272/56.6829071 secs/batch = 0.2924s, grad.norm=13.94220066
 15647: 11 [ 1050/ 1327], train_loss/perplexity = 4.15612984/63.8240356 secs/batch = 0.2964s, grad.norm=14.78246212
 15652: 11 [ 1055/ 1327], train_loss/perplexity = 4.29556656/73.3737717 secs/batch = 0.2952s, grad.norm=15.11643028
 15657: 11 [ 1060/ 1327], train_loss/perplexity = 3.87315130/48.0937042 secs/batch = 0.2946s, grad.norm=16.11133194
 15662: 11 [ 1065/ 1327], train_loss/perplexity = 4.05830717/57.8762550 secs/batch = 0.2927s, grad.norm=14.99399662
 15667: 11 [ 1070/ 1327], train_loss/perplexity = 4.32116890/75.2765656 secs/batch = 0.2953s, grad.norm=14.92138481
 15672: 11 [ 1075/ 1327], train_loss/perplexity = 4.06233311/58.1097298 secs/batch = 0.2949s, grad.norm=14.54292011
 15677: 11 [ 1080/ 1327], train_loss/perplexity = 4.01436043/55.3878593 secs/batch = 0.3001s, grad.norm=14.24998569
 15682: 11 [ 1085/ 1327], train_loss/perplexity = 3.91300774/50.0492592 secs/batch = 0.2988s, grad.norm=14.69426250
 15687: 11 [ 1090/ 1327], train_loss/perplexity = 4.17449665/65.0071106 secs/batch = 0.2947s, grad.norm=16.18397522
 15692: 11 [ 1095/ 1327], train_loss/perplexity = 4.26900578/71.4505615 secs/batch = 0.2940s, grad.norm=15.60915375
 15697: 11 [ 1100/ 1327], train_loss/perplexity = 4.09839916/60.2437706 secs/batch = 0.2956s, grad.norm=16.47350121
 15702: 11 [ 1105/ 1327], train_loss/perplexity = 3.97773933/53.3961868 secs/batch = 0.2923s, grad.norm=15.27064323
 15707: 11 [ 1110/ 1327], train_loss/perplexity = 4.38890505/80.5521698 secs/batch = 0.3003s, grad.norm=15.60846615
 15712: 11 [ 1115/ 1327], train_loss/perplexity = 4.08961868/59.7171173 secs/batch = 0.2946s, grad.norm=14.45449448
 15717: 11 [ 1120/ 1327], train_loss/perplexity = 4.35903645/78.1817627 secs/batch = 0.2945s, grad.norm=14.59652138
 15722: 11 [ 1125/ 1327], train_loss/perplexity = 4.49423885/89.5000229 secs/batch = 0.2930s, grad.norm=15.30613995
 15727: 11 [ 1130/ 1327], train_loss/perplexity = 4.22267437/68.2156754 secs/batch = 0.2946s, grad.norm=15.18743134
 15732: 11 [ 1135/ 1327], train_loss/perplexity = 4.16403389/64.3305054 secs/batch = 0.2939s, grad.norm=14.82226181
 15737: 11 [ 1140/ 1327], train_loss/perplexity = 4.46459007/86.8854065 secs/batch = 0.2941s, grad.norm=15.92366409
 15742: 11 [ 1145/ 1327], train_loss/perplexity = 4.26028395/70.8300934 secs/batch = 0.2937s, grad.norm=14.79656029
 15747: 11 [ 1150/ 1327], train_loss/perplexity = 4.26379824/71.0794449 secs/batch = 0.2927s, grad.norm=14.80506229
 15752: 11 [ 1155/ 1327], train_loss/perplexity = 4.27128077/71.6132965 secs/batch = 0.2980s, grad.norm=15.22896099
 15757: 11 [ 1160/ 1327], train_loss/perplexity = 4.23791599/69.2633591 secs/batch = 0.2921s, grad.norm=15.53163147
 15762: 11 [ 1165/ 1327], train_loss/perplexity = 4.29901934/73.6275558 secs/batch = 0.2956s, grad.norm=15.45212460
 15767: 11 [ 1170/ 1327], train_loss/perplexity = 4.19221306/66.1690674 secs/batch = 0.2991s, grad.norm=15.24303150
 15772: 11 [ 1175/ 1327], train_loss/perplexity = 3.92798424/50.8044662 secs/batch = 0.2932s, grad.norm=15.27962494
 15777: 11 [ 1180/ 1327], train_loss/perplexity = 4.03361416/56.4646149 secs/batch = 0.2985s, grad.norm=14.65948868
 15782: 11 [ 1185/ 1327], train_loss/perplexity = 4.14992952/63.4295311 secs/batch = 0.2951s, grad.norm=14.90299606
 15787: 11 [ 1190/ 1327], train_loss/perplexity = 4.37478495/79.4227600 secs/batch = 0.2985s, grad.norm=15.28019810
 15792: 11 [ 1195/ 1327], train_loss/perplexity = 4.08035421/59.1664238 secs/batch = 0.2911s, grad.norm=15.42402649
 15797: 11 [ 1200/ 1327], train_loss/perplexity = 4.05382395/57.6173630 secs/batch = 0.2935s, grad.norm=15.09820366
 15802: 11 [ 1205/ 1327], train_loss/perplexity = 4.07458830/58.8262558 secs/batch = 0.2943s, grad.norm=14.98651981
 15807: 11 [ 1210/ 1327], train_loss/perplexity = 3.69865084/40.3927727 secs/batch = 0.2934s, grad.norm=15.84508419
 15812: 11 [ 1215/ 1327], train_loss/perplexity = 3.94192457/51.5176544 secs/batch = 0.2928s, grad.norm=14.42483425
 15817: 11 [ 1220/ 1327], train_loss/perplexity = 4.13426208/62.4434967 secs/batch = 0.2966s, grad.norm=16.08360291
 15822: 11 [ 1225/ 1327], train_loss/perplexity = 3.81329131/45.2992897 secs/batch = 0.2938s, grad.norm=16.17355537
 15827: 11 [ 1230/ 1327], train_loss/perplexity = 4.10373783/60.5662498 secs/batch = 0.2932s, grad.norm=14.83210087
 15832: 11 [ 1235/ 1327], train_loss/perplexity = 4.01452494/55.3969727 secs/batch = 0.2993s, grad.norm=15.07897949
 15837: 11 [ 1240/ 1327], train_loss/perplexity = 4.28457022/72.5713501 secs/batch = 0.2952s, grad.norm=16.52319717
 15842: 11 [ 1245/ 1327], train_loss/perplexity = 4.17246580/64.8752213 secs/batch = 0.3014s, grad.norm=14.15992260
 15847: 11 [ 1250/ 1327], train_loss/perplexity = 4.31172657/74.5691299 secs/batch = 0.2951s, grad.norm=14.32571125
 15852: 11 [ 1255/ 1327], train_loss/perplexity = 4.35747194/78.0595474 secs/batch = 0.2926s, grad.norm=14.46216679
 15857: 11 [ 1260/ 1327], train_loss/perplexity = 4.13708210/62.6198349 secs/batch = 0.2993s, grad.norm=15.90820599
 15862: 11 [ 1265/ 1327], train_loss/perplexity = 4.42227697/83.2857056 secs/batch = 0.2924s, grad.norm=15.02374744
 15867: 11 [ 1270/ 1327], train_loss/perplexity = 4.03400517/56.4866982 secs/batch = 0.2939s, grad.norm=15.30387783
 15872: 11 [ 1275/ 1327], train_loss/perplexity = 4.27476597/71.8633194 secs/batch = 0.2979s, grad.norm=15.68714333
 15877: 11 [ 1280/ 1327], train_loss/perplexity = 4.12368107/61.7862625 secs/batch = 0.2940s, grad.norm=15.87477112
 15882: 11 [ 1285/ 1327], train_loss/perplexity = 4.05619669/57.7542343 secs/batch = 0.2947s, grad.norm=16.02480698
 15887: 11 [ 1290/ 1327], train_loss/perplexity = 4.24296331/69.6138306 secs/batch = 0.2928s, grad.norm=15.09528065
 15892: 11 [ 1295/ 1327], train_loss/perplexity = 4.30992460/74.4348755 secs/batch = 0.2950s, grad.norm=15.91135788
 15897: 11 [ 1300/ 1327], train_loss/perplexity = 4.48807669/88.9502029 secs/batch = 0.2955s, grad.norm=15.17619705
 15902: 11 [ 1305/ 1327], train_loss/perplexity = 4.52612877/92.4001617 secs/batch = 0.2950s, grad.norm=16.25453949
 15907: 11 [ 1310/ 1327], train_loss/perplexity = 4.74850702/115.4118500 secs/batch = 0.2999s, grad.norm=15.04467106
 15912: 11 [ 1315/ 1327], train_loss/perplexity = 4.58464766/97.9686661 secs/batch = 0.2993s, grad.norm=15.58860874
 15917: 11 [ 1320/ 1327], train_loss/perplexity = 4.53139496/92.8880463 secs/batch = 0.2997s, grad.norm=14.57481670
 15922: 11 [ 1325/ 1327], train_loss/perplexity = 4.50495625/90.4643860 secs/batch = 0.2986s, grad.norm=14.68881512
Epoch training time: 393.1927080154419
	> validation loss = 4.72314358, perplexity = 112.52141571
	> validation loss = 4.64049864, perplexity = 103.59599304
	> validation loss = 4.63224602, perplexity = 102.74456787
	> validation loss = 4.62294722, perplexity = 101.79360199
	> validation loss = 4.79696178, perplexity = 121.14179993
	> validation loss = 4.72955227, perplexity = 113.24485016
	> validation loss = 4.68579960, perplexity = 108.39691162
	> validation loss = 4.50987625, perplexity = 90.91056824
	> validation loss = 4.32337093, perplexity = 75.44251251
	> validation loss = 4.42279720, perplexity = 83.32904816
	> validation loss = 4.56001472, perplexity = 95.58488464
	> validation loss = 4.58170223, perplexity = 97.68052673
	> validation loss = 4.50822830, perplexity = 90.76087189
	> validation loss = 4.31047583, perplexity = 74.47592163
	> validation loss = 4.27319336, perplexity = 71.75039673
	> validation loss = 4.26517200, perplexity = 71.17716217
	> validation loss = 4.66257238, perplexity = 105.90816498
	> validation loss = 4.24791050, perplexity = 69.95908356
	> validation loss = 4.68140173, perplexity = 107.92124176
	> validation loss = 4.59194946, perplexity = 98.68663025
	> validation loss = 4.35379791, perplexity = 77.77327728
at the end of epoch: 11
train loss = 4.40324003, perplexity = 81.71519989
validation loss = 4.52919687, perplexity = 92.68409389
Saved model cv/epoch011_4.5292.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 1.0
new learning rate is: 0.5
 15929: 12 [    5/ 1327], train_loss/perplexity = 4.43265772/84.1547775 secs/batch = 0.2955s, grad.norm=14.62465191
 15934: 12 [   10/ 1327], train_loss/perplexity = 3.99856424/54.5198174 secs/batch = 0.2920s, grad.norm=13.85565662
 15939: 12 [   15/ 1327], train_loss/perplexity = 4.31102705/74.5169830 secs/batch = 0.2916s, grad.norm=13.74422169
 15944: 12 [   20/ 1327], train_loss/perplexity = 4.50645733/90.6002808 secs/batch = 0.2938s, grad.norm=13.98162460
 15949: 12 [   25/ 1327], train_loss/perplexity = 4.39774275/81.2672195 secs/batch = 0.2978s, grad.norm=14.97276688
 15954: 12 [   30/ 1327], train_loss/perplexity = 4.46176100/86.6399460 secs/batch = 0.2919s, grad.norm=14.62275600
 15959: 12 [   35/ 1327], train_loss/perplexity = 4.25076437/70.1590195 secs/batch = 0.2922s, grad.norm=14.31856728
 15964: 12 [   40/ 1327], train_loss/perplexity = 4.24611664/69.8336945 secs/batch = 0.2925s, grad.norm=14.80027771
 15969: 12 [   45/ 1327], train_loss/perplexity = 4.03217793/56.3835754 secs/batch = 0.2944s, grad.norm=13.69069386
 15974: 12 [   50/ 1327], train_loss/perplexity = 4.26212215/70.9604111 secs/batch = 0.2983s, grad.norm=15.07028294
 15979: 12 [   55/ 1327], train_loss/perplexity = 4.18769503/65.8707886 secs/batch = 0.2951s, grad.norm=14.89880371
 15984: 12 [   60/ 1327], train_loss/perplexity = 4.45451117/86.0140915 secs/batch = 0.2991s, grad.norm=15.11328888
 15989: 12 [   65/ 1327], train_loss/perplexity = 4.02683735/56.0832596 secs/batch = 0.2993s, grad.norm=14.01021004
 15994: 12 [   70/ 1327], train_loss/perplexity = 3.94345117/51.5963631 secs/batch = 0.2981s, grad.norm=14.65813446
 15999: 12 [   75/ 1327], train_loss/perplexity = 3.69550014/40.2657051 secs/batch = 0.2925s, grad.norm=13.91211987
 16004: 12 [   80/ 1327], train_loss/perplexity = 4.14596462/63.1785355 secs/batch = 0.3009s, grad.norm=14.35418797
 16009: 12 [   85/ 1327], train_loss/perplexity = 4.18470240/65.6739502 secs/batch = 0.2949s, grad.norm=14.88964272
 16014: 12 [   90/ 1327], train_loss/perplexity = 4.22040749/68.0612106 secs/batch = 0.2994s, grad.norm=14.85490417
 16019: 12 [   95/ 1327], train_loss/perplexity = 4.09083700/59.7899132 secs/batch = 0.2953s, grad.norm=14.49410248
 16024: 12 [  100/ 1327], train_loss/perplexity = 4.32410908/75.4982224 secs/batch = 0.2971s, grad.norm=14.72442341
 16029: 12 [  105/ 1327], train_loss/perplexity = 4.13577604/62.5381050 secs/batch = 0.2965s, grad.norm=15.29824829
 16034: 12 [  110/ 1327], train_loss/perplexity = 4.00514984/54.8800468 secs/batch = 0.2997s, grad.norm=13.95858288
 16039: 12 [  115/ 1327], train_loss/perplexity = 4.02487946/55.9735603 secs/batch = 0.2954s, grad.norm=15.11059093
 16044: 12 [  120/ 1327], train_loss/perplexity = 4.10575056/60.6882782 secs/batch = 0.3007s, grad.norm=14.93795681
 16049: 12 [  125/ 1327], train_loss/perplexity = 4.18740511/65.8516922 secs/batch = 0.2951s, grad.norm=15.03225231
 16054: 12 [  130/ 1327], train_loss/perplexity = 4.13016748/62.1883392 secs/batch = 0.2942s, grad.norm=15.08869457
 16059: 12 [  135/ 1327], train_loss/perplexity = 4.15713501/63.8882217 secs/batch = 0.2949s, grad.norm=14.65059376
 16064: 12 [  140/ 1327], train_loss/perplexity = 4.48675442/88.8326645 secs/batch = 0.3007s, grad.norm=15.26884174
 16069: 12 [  145/ 1327], train_loss/perplexity = 4.32038116/75.2172928 secs/batch = 0.2954s, grad.norm=15.78573322
 16074: 12 [  150/ 1327], train_loss/perplexity = 4.36610222/78.7361374 secs/batch = 0.2949s, grad.norm=14.72916126
 16079: 12 [  155/ 1327], train_loss/perplexity = 4.57426977/96.9572144 secs/batch = 0.2956s, grad.norm=14.65361500
 16084: 12 [  160/ 1327], train_loss/perplexity = 4.25405359/70.3901672 secs/batch = 0.2958s, grad.norm=13.94230461
 16089: 12 [  165/ 1327], train_loss/perplexity = 4.40035295/81.4796219 secs/batch = 0.3007s, grad.norm=14.66166210
 16094: 12 [  170/ 1327], train_loss/perplexity = 4.13557625/62.5256119 secs/batch = 0.2945s, grad.norm=13.79662800
 16099: 12 [  175/ 1327], train_loss/perplexity = 4.50851345/90.7867584 secs/batch = 0.2988s, grad.norm=14.52981853
 16104: 12 [  180/ 1327], train_loss/perplexity = 4.27248764/71.6997757 secs/batch = 0.2956s, grad.norm=14.83549690
 16109: 12 [  185/ 1327], train_loss/perplexity = 4.66982079/106.6786194 secs/batch = 0.2936s, grad.norm=15.05253601
 16114: 12 [  190/ 1327], train_loss/perplexity = 4.12139797/61.6453590 secs/batch = 0.3007s, grad.norm=13.88080502
 16119: 12 [  195/ 1327], train_loss/perplexity = 4.48451042/88.6335449 secs/batch = 0.2985s, grad.norm=13.55105782
 16124: 12 [  200/ 1327], train_loss/perplexity = 4.29467010/73.3080292 secs/batch = 0.2935s, grad.norm=14.80920696
 16129: 12 [  205/ 1327], train_loss/perplexity = 4.47373056/87.6832199 secs/batch = 0.3004s, grad.norm=14.44052315
 16134: 12 [  210/ 1327], train_loss/perplexity = 4.31650686/74.9264450 secs/batch = 0.2958s, grad.norm=13.96403694
 16139: 12 [  215/ 1327], train_loss/perplexity = 4.49980354/89.9994507 secs/batch = 0.2941s, grad.norm=14.24729061
 16144: 12 [  220/ 1327], train_loss/perplexity = 4.43592262/84.4299850 secs/batch = 0.2936s, grad.norm=14.45912552
 16149: 12 [  225/ 1327], train_loss/perplexity = 4.59165955/98.6580200 secs/batch = 0.2957s, grad.norm=14.40491962
 16154: 12 [  230/ 1327], train_loss/perplexity = 4.39804459/81.2917557 secs/batch = 0.2938s, grad.norm=15.01643753
 16159: 12 [  235/ 1327], train_loss/perplexity = 4.20975780/67.3402252 secs/batch = 0.3002s, grad.norm=13.69270992
 16164: 12 [  240/ 1327], train_loss/perplexity = 4.01409388/55.3730965 secs/batch = 0.2923s, grad.norm=14.96758366
 16169: 12 [  245/ 1327], train_loss/perplexity = 4.31641293/74.9194031 secs/batch = 0.2953s, grad.norm=14.34911728
 16174: 12 [  250/ 1327], train_loss/perplexity = 4.15245533/63.5899429 secs/batch = 0.2990s, grad.norm=13.78276920
 16179: 12 [  255/ 1327], train_loss/perplexity = 4.16077280/64.1210556 secs/batch = 0.2937s, grad.norm=14.25156784
 16184: 12 [  260/ 1327], train_loss/perplexity = 4.32202482/75.3410263 secs/batch = 0.2988s, grad.norm=14.90453053
 16189: 12 [  265/ 1327], train_loss/perplexity = 4.59840488/99.3257523 secs/batch = 0.3015s, grad.norm=13.93276119
 16194: 12 [  270/ 1327], train_loss/perplexity = 4.62411165/101.9122009 secs/batch = 0.2930s, grad.norm=14.14719200
 16199: 12 [  275/ 1327], train_loss/perplexity = 4.58671045/98.1709595 secs/batch = 0.2940s, grad.norm=14.31945705
 16204: 12 [  280/ 1327], train_loss/perplexity = 4.34590197/77.1616058 secs/batch = 0.2956s, grad.norm=14.44593811
 16209: 12 [  285/ 1327], train_loss/perplexity = 4.63124609/102.6418839 secs/batch = 0.2990s, grad.norm=14.91756248
 16214: 12 [  290/ 1327], train_loss/perplexity = 4.34451103/77.0543518 secs/batch = 0.2939s, grad.norm=14.37520981
 16219: 12 [  295/ 1327], train_loss/perplexity = 4.13136196/62.2626648 secs/batch = 0.3011s, grad.norm=14.25317764
 16224: 12 [  300/ 1327], train_loss/perplexity = 3.68942332/40.0217590 secs/batch = 0.2947s, grad.norm=13.43452930
 16229: 12 [  305/ 1327], train_loss/perplexity = 4.13168907/62.2830353 secs/batch = 0.2952s, grad.norm=14.28299522
 16234: 12 [  310/ 1327], train_loss/perplexity = 4.18282413/65.5507126 secs/batch = 0.2927s, grad.norm=13.84051228
 16239: 12 [  315/ 1327], train_loss/perplexity = 3.72003651/41.2658997 secs/batch = 0.2975s, grad.norm=14.06513882
 16244: 12 [  320/ 1327], train_loss/perplexity = 3.72008467/41.2678871 secs/batch = 0.2990s, grad.norm=15.78591824
 16249: 12 [  325/ 1327], train_loss/perplexity = 3.72872114/41.6258392 secs/batch = 0.2986s, grad.norm=13.76155090
 16254: 12 [  330/ 1327], train_loss/perplexity = 4.33555603/76.3674088 secs/batch = 0.3004s, grad.norm=14.46982002
 16259: 12 [  335/ 1327], train_loss/perplexity = 3.68003511/39.6477852 secs/batch = 0.2924s, grad.norm=13.18026066
 16264: 12 [  340/ 1327], train_loss/perplexity = 4.45471811/86.0318985 secs/batch = 0.2940s, grad.norm=14.22367477
 16269: 12 [  345/ 1327], train_loss/perplexity = 4.23197699/68.8532181 secs/batch = 0.3008s, grad.norm=13.61187649
 16274: 12 [  350/ 1327], train_loss/perplexity = 4.21126604/67.4418716 secs/batch = 0.2933s, grad.norm=14.90243244
 16279: 12 [  355/ 1327], train_loss/perplexity = 4.29515028/73.3432388 secs/batch = 0.2945s, grad.norm=14.34084606
 16284: 12 [  360/ 1327], train_loss/perplexity = 4.30980062/74.4256516 secs/batch = 0.2947s, grad.norm=16.32551575
 16289: 12 [  365/ 1327], train_loss/perplexity = 4.35812855/78.1108170 secs/batch = 0.2943s, grad.norm=14.61380863
 16294: 12 [  370/ 1327], train_loss/perplexity = 4.42082834/83.1651459 secs/batch = 0.2947s, grad.norm=14.51805401
 16299: 12 [  375/ 1327], train_loss/perplexity = 3.81128383/45.2084427 secs/batch = 0.2945s, grad.norm=15.03933907
 16304: 12 [  380/ 1327], train_loss/perplexity = 3.91987610/50.3941994 secs/batch = 0.2935s, grad.norm=14.67596245
 16309: 12 [  385/ 1327], train_loss/perplexity = 4.16478157/64.3786163 secs/batch = 0.2995s, grad.norm=15.46684837
 16314: 12 [  390/ 1327], train_loss/perplexity = 4.29919338/73.6403732 secs/batch = 0.3002s, grad.norm=14.63103580
 16319: 12 [  395/ 1327], train_loss/perplexity = 4.32035065/75.2149963 secs/batch = 0.2950s, grad.norm=13.97953892
 16324: 12 [  400/ 1327], train_loss/perplexity = 4.21511173/67.7017288 secs/batch = 0.2942s, grad.norm=13.91059208
 16329: 12 [  405/ 1327], train_loss/perplexity = 4.49542046/89.6058350 secs/batch = 0.2942s, grad.norm=14.80222321
 16334: 12 [  410/ 1327], train_loss/perplexity = 4.12745667/62.0199852 secs/batch = 0.2946s, grad.norm=14.61896515
 16339: 12 [  415/ 1327], train_loss/perplexity = 4.10124302/60.4153366 secs/batch = 0.2958s, grad.norm=14.70408535
 16344: 12 [  420/ 1327], train_loss/perplexity = 3.69792414/40.3634300 secs/batch = 0.2956s, grad.norm=14.23602867
 16349: 12 [  425/ 1327], train_loss/perplexity = 4.11427116/61.2075882 secs/batch = 0.2931s, grad.norm=15.37218952
 16354: 12 [  430/ 1327], train_loss/perplexity = 4.26197910/70.9502640 secs/batch = 0.2939s, grad.norm=14.79799271
 16359: 12 [  435/ 1327], train_loss/perplexity = 4.36277437/78.4745483 secs/batch = 0.2993s, grad.norm=14.95698357
 16364: 12 [  440/ 1327], train_loss/perplexity = 3.91299558/50.0486526 secs/batch = 0.2922s, grad.norm=14.87500763
 16369: 12 [  445/ 1327], train_loss/perplexity = 4.22745657/68.5426788 secs/batch = 0.2927s, grad.norm=14.81301403
 16374: 12 [  450/ 1327], train_loss/perplexity = 4.18187523/65.4885406 secs/batch = 0.2947s, grad.norm=14.67218304
 16379: 12 [  455/ 1327], train_loss/perplexity = 4.12872410/62.0986404 secs/batch = 0.2949s, grad.norm=14.43520355
 16384: 12 [  460/ 1327], train_loss/perplexity = 4.10203505/60.4632072 secs/batch = 0.2934s, grad.norm=14.83229160
 16389: 12 [  465/ 1327], train_loss/perplexity = 3.87643600/48.2519379 secs/batch = 0.2935s, grad.norm=15.40172100
 16394: 12 [  470/ 1327], train_loss/perplexity = 4.54138470/93.8206253 secs/batch = 0.2985s, grad.norm=14.12154770
 16399: 12 [  475/ 1327], train_loss/perplexity = 3.97443819/53.2202072 secs/batch = 0.2991s, grad.norm=14.69063854
 16404: 12 [  480/ 1327], train_loss/perplexity = 4.11661911/61.3514671 secs/batch = 0.2941s, grad.norm=14.52446175
 16409: 12 [  485/ 1327], train_loss/perplexity = 4.12591171/61.9242401 secs/batch = 0.2951s, grad.norm=14.63702583
 16414: 12 [  490/ 1327], train_loss/perplexity = 4.05693054/57.7966347 secs/batch = 0.2949s, grad.norm=15.73079681
 16419: 12 [  495/ 1327], train_loss/perplexity = 4.07294273/58.7295341 secs/batch = 0.2941s, grad.norm=13.89020634
 16424: 12 [  500/ 1327], train_loss/perplexity = 4.20862865/67.2642365 secs/batch = 0.2934s, grad.norm=14.96084785
 16429: 12 [  505/ 1327], train_loss/perplexity = 4.32563257/75.6133270 secs/batch = 0.2981s, grad.norm=13.52153015
 16434: 12 [  510/ 1327], train_loss/perplexity = 4.66590357/106.2615585 secs/batch = 0.2947s, grad.norm=13.89311314
 16439: 12 [  515/ 1327], train_loss/perplexity = 4.28978729/72.9509506 secs/batch = 0.2989s, grad.norm=14.02962112
 16444: 12 [  520/ 1327], train_loss/perplexity = 4.38221359/80.0149612 secs/batch = 0.2935s, grad.norm=14.10193348
 16449: 12 [  525/ 1327], train_loss/perplexity = 4.09531975/60.0585403 secs/batch = 0.2947s, grad.norm=14.45531940
 16454: 12 [  530/ 1327], train_loss/perplexity = 4.17192554/64.8401871 secs/batch = 0.2948s, grad.norm=15.15458202
 16459: 12 [  535/ 1327], train_loss/perplexity = 4.22129107/68.1213760 secs/batch = 0.2985s, grad.norm=15.01212311
 16464: 12 [  540/ 1327], train_loss/perplexity = 4.32315683/75.4263611 secs/batch = 0.2948s, grad.norm=14.38210964
 16469: 12 [  545/ 1327], train_loss/perplexity = 4.27132463/71.6164398 secs/batch = 0.2983s, grad.norm=15.00848293
 16474: 12 [  550/ 1327], train_loss/perplexity = 4.17988014/65.3580170 secs/batch = 0.3001s, grad.norm=14.33384418
 16479: 12 [  555/ 1327], train_loss/perplexity = 4.06878901/58.4860916 secs/batch = 0.2954s, grad.norm=13.95631695
 16484: 12 [  560/ 1327], train_loss/perplexity = 4.20559692/67.0606155 secs/batch = 0.2952s, grad.norm=15.67038918
 16489: 12 [  565/ 1327], train_loss/perplexity = 4.11073112/60.9912949 secs/batch = 0.2925s, grad.norm=15.65105629
 16494: 12 [  570/ 1327], train_loss/perplexity = 4.05303955/57.5721855 secs/batch = 0.2987s, grad.norm=15.62840557
 16499: 12 [  575/ 1327], train_loss/perplexity = 3.92295694/50.5496979 secs/batch = 0.2935s, grad.norm=15.46625900
 16504: 12 [  580/ 1327], train_loss/perplexity = 4.27907085/72.1733475 secs/batch = 0.2926s, grad.norm=14.66317558
 16509: 12 [  585/ 1327], train_loss/perplexity = 3.88972497/48.8974380 secs/batch = 0.2924s, grad.norm=14.61149120
 16514: 12 [  590/ 1327], train_loss/perplexity = 4.25697708/70.5962524 secs/batch = 0.2955s, grad.norm=14.87018681
 16519: 12 [  595/ 1327], train_loss/perplexity = 4.18746853/65.8558655 secs/batch = 0.3008s, grad.norm=15.10131073
 16524: 12 [  600/ 1327], train_loss/perplexity = 4.38655519/80.3631058 secs/batch = 0.2947s, grad.norm=13.58477020
 16529: 12 [  605/ 1327], train_loss/perplexity = 4.30085897/73.7631302 secs/batch = 0.2924s, grad.norm=14.04203701
 16534: 12 [  610/ 1327], train_loss/perplexity = 4.52421522/92.2235260 secs/batch = 0.2971s, grad.norm=14.76482487
 16539: 12 [  615/ 1327], train_loss/perplexity = 4.07948399/59.1149597 secs/batch = 0.2951s, grad.norm=13.74634933
 16544: 12 [  620/ 1327], train_loss/perplexity = 4.38003445/79.8407822 secs/batch = 0.2954s, grad.norm=14.02670288
 16549: 12 [  625/ 1327], train_loss/perplexity = 4.41855049/82.9759216 secs/batch = 0.2948s, grad.norm=14.73732758
 16554: 12 [  630/ 1327], train_loss/perplexity = 4.44101238/84.8608093 secs/batch = 0.2949s, grad.norm=14.51261902
 16559: 12 [  635/ 1327], train_loss/perplexity = 4.21802092/67.8989716 secs/batch = 0.2959s, grad.norm=14.92501068
 16564: 12 [  640/ 1327], train_loss/perplexity = 4.19394159/66.2835388 secs/batch = 0.2955s, grad.norm=14.44590759
 16569: 12 [  645/ 1327], train_loss/perplexity = 4.47992229/88.2278137 secs/batch = 0.2945s, grad.norm=14.88753605
 16574: 12 [  650/ 1327], train_loss/perplexity = 3.91071653/49.9347191 secs/batch = 0.2934s, grad.norm=14.06795883
 16579: 12 [  655/ 1327], train_loss/perplexity = 4.12308025/61.7491531 secs/batch = 0.2948s, grad.norm=15.25600433
 16584: 12 [  660/ 1327], train_loss/perplexity = 4.07614803/58.9180832 secs/batch = 0.3002s, grad.norm=14.51588440
 16589: 12 [  665/ 1327], train_loss/perplexity = 4.25754929/70.6366577 secs/batch = 0.2924s, grad.norm=14.77045536
 16594: 12 [  670/ 1327], train_loss/perplexity = 4.16532946/64.4139023 secs/batch = 0.2951s, grad.norm=14.51517200
 16599: 12 [  675/ 1327], train_loss/perplexity = 3.99773240/54.4744835 secs/batch = 0.2987s, grad.norm=14.91429329
 16604: 12 [  680/ 1327], train_loss/perplexity = 4.12623024/61.9439697 secs/batch = 0.3014s, grad.norm=15.17840576
 16609: 12 [  685/ 1327], train_loss/perplexity = 3.92060971/50.4311829 secs/batch = 0.3001s, grad.norm=14.08831024
 16614: 12 [  690/ 1327], train_loss/perplexity = 4.35565376/77.9177475 secs/batch = 0.2971s, grad.norm=14.71164989
 16619: 12 [  695/ 1327], train_loss/perplexity = 4.23778391/69.2542114 secs/batch = 0.2998s, grad.norm=14.17733002
 16624: 12 [  700/ 1327], train_loss/perplexity = 4.44031239/84.8014297 secs/batch = 0.2930s, grad.norm=14.69547176
 16629: 12 [  705/ 1327], train_loss/perplexity = 4.11896276/61.4954224 secs/batch = 0.2962s, grad.norm=13.80312443
 16634: 12 [  710/ 1327], train_loss/perplexity = 4.05025482/57.4120865 secs/batch = 0.2950s, grad.norm=15.07895374
 16639: 12 [  715/ 1327], train_loss/perplexity = 4.00317240/54.7716331 secs/batch = 0.2945s, grad.norm=14.46875763
 16644: 12 [  720/ 1327], train_loss/perplexity = 4.05861950/57.8943329 secs/batch = 0.2927s, grad.norm=14.67614269
 16649: 12 [  725/ 1327], train_loss/perplexity = 4.00793171/55.0329285 secs/batch = 0.2940s, grad.norm=14.29050922
 16654: 12 [  730/ 1327], train_loss/perplexity = 4.12657547/61.9653587 secs/batch = 0.3004s, grad.norm=14.46193600
 16659: 12 [  735/ 1327], train_loss/perplexity = 4.28157377/72.3542175 secs/batch = 0.2936s, grad.norm=15.53642654
 16664: 12 [  740/ 1327], train_loss/perplexity = 3.75997710/42.9474411 secs/batch = 0.2926s, grad.norm=13.68131351
 16669: 12 [  745/ 1327], train_loss/perplexity = 4.18670559/65.8056412 secs/batch = 0.2946s, grad.norm=14.59331322
 16674: 12 [  750/ 1327], train_loss/perplexity = 4.06059122/58.0085983 secs/batch = 0.2969s, grad.norm=14.36944199
 16679: 12 [  755/ 1327], train_loss/perplexity = 3.91201878/49.9997902 secs/batch = 0.2945s, grad.norm=14.00976086
 16684: 12 [  760/ 1327], train_loss/perplexity = 3.84130359/46.5861626 secs/batch = 0.2933s, grad.norm=13.78809547
 16689: 12 [  765/ 1327], train_loss/perplexity = 3.82462406/45.8155746 secs/batch = 0.2943s, grad.norm=13.75784492
 16694: 12 [  770/ 1327], train_loss/perplexity = 3.81464934/45.3608475 secs/batch = 0.3015s, grad.norm=13.89792728
 16699: 12 [  775/ 1327], train_loss/perplexity = 3.97238350/53.1109695 secs/batch = 0.2941s, grad.norm=15.09365463
 16704: 12 [  780/ 1327], train_loss/perplexity = 4.33740997/76.5091248 secs/batch = 0.2939s, grad.norm=15.11821938
 16709: 12 [  785/ 1327], train_loss/perplexity = 4.26005507/70.8138809 secs/batch = 0.3004s, grad.norm=15.31636333
 16714: 12 [  790/ 1327], train_loss/perplexity = 3.93611860/51.2194138 secs/batch = 0.2960s, grad.norm=14.42520046
 16719: 12 [  795/ 1327], train_loss/perplexity = 4.31758356/75.0071564 secs/batch = 0.3004s, grad.norm=14.64333916
 16724: 12 [  800/ 1327], train_loss/perplexity = 4.22134638/68.1251450 secs/batch = 0.2941s, grad.norm=14.53639507
 16729: 12 [  805/ 1327], train_loss/perplexity = 4.45691776/86.2213440 secs/batch = 0.2992s, grad.norm=14.19919395
 16734: 12 [  810/ 1327], train_loss/perplexity = 4.22872305/68.6295395 secs/batch = 0.2946s, grad.norm=14.15419674
 16739: 12 [  815/ 1327], train_loss/perplexity = 4.02749014/56.1198807 secs/batch = 0.2995s, grad.norm=14.25894928
 16744: 12 [  820/ 1327], train_loss/perplexity = 3.82181239/45.6869354 secs/batch = 0.2931s, grad.norm=13.59922981
 16749: 12 [  825/ 1327], train_loss/perplexity = 4.17596674/65.1027451 secs/batch = 0.2954s, grad.norm=13.93522167
 16754: 12 [  830/ 1327], train_loss/perplexity = 3.81295061/45.2838554 secs/batch = 0.2981s, grad.norm=14.92566872
 16759: 12 [  835/ 1327], train_loss/perplexity = 4.08053875/59.1773415 secs/batch = 0.2951s, grad.norm=14.87008286
 16764: 12 [  840/ 1327], train_loss/perplexity = 4.18620157/65.7724838 secs/batch = 0.2932s, grad.norm=14.78617668
 16769: 12 [  845/ 1327], train_loss/perplexity = 3.97284245/53.1353531 secs/batch = 0.2941s, grad.norm=14.80047989
 16774: 12 [  850/ 1327], train_loss/perplexity = 4.11302805/61.1315460 secs/batch = 0.3010s, grad.norm=14.25838375
 16779: 12 [  855/ 1327], train_loss/perplexity = 4.17619991/65.1179276 secs/batch = 0.2942s, grad.norm=15.42825222
 16784: 12 [  860/ 1327], train_loss/perplexity = 3.85985541/47.4584885 secs/batch = 0.2955s, grad.norm=14.27906799
 16789: 12 [  865/ 1327], train_loss/perplexity = 4.31410599/74.7467728 secs/batch = 0.2986s, grad.norm=14.35095882
 16794: 12 [  870/ 1327], train_loss/perplexity = 4.12660360/61.9671021 secs/batch = 0.2988s, grad.norm=15.73939514
 16799: 12 [  875/ 1327], train_loss/perplexity = 3.68477821/39.8362846 secs/batch = 0.2943s, grad.norm=14.10412884
 16804: 12 [  880/ 1327], train_loss/perplexity = 4.00091124/54.6479263 secs/batch = 0.2930s, grad.norm=14.22097206
 16809: 12 [  885/ 1327], train_loss/perplexity = 4.12140703/61.6459198 secs/batch = 0.3001s, grad.norm=14.10497570
 16814: 12 [  890/ 1327], train_loss/perplexity = 4.25430727/70.4080276 secs/batch = 0.2994s, grad.norm=14.34332848
 16819: 12 [  895/ 1327], train_loss/perplexity = 4.28775311/72.8027039 secs/batch = 0.2940s, grad.norm=14.76431942
 16824: 12 [  900/ 1327], train_loss/perplexity = 4.10229826/60.4791260 secs/batch = 0.2922s, grad.norm=15.05483913
 16829: 12 [  905/ 1327], train_loss/perplexity = 3.98967886/54.0375328 secs/batch = 0.2942s, grad.norm=13.54030609
 16834: 12 [  910/ 1327], train_loss/perplexity = 4.01020575/55.1582184 secs/batch = 0.2956s, grad.norm=13.52657223
 16839: 12 [  915/ 1327], train_loss/perplexity = 4.24250746/69.5821075 secs/batch = 0.2939s, grad.norm=14.32201290
 16844: 12 [  920/ 1327], train_loss/perplexity = 4.41723919/82.8671875 secs/batch = 0.2956s, grad.norm=14.54195499
 16849: 12 [  925/ 1327], train_loss/perplexity = 4.25753498/70.6356506 secs/batch = 0.2933s, grad.norm=13.88550472
 16854: 12 [  930/ 1327], train_loss/perplexity = 4.20409250/66.9598007 secs/batch = 0.2940s, grad.norm=14.05375767
 16859: 12 [  935/ 1327], train_loss/perplexity = 4.27089691/71.5858154 secs/batch = 0.2990s, grad.norm=14.01008797
 16864: 12 [  940/ 1327], train_loss/perplexity = 4.19901896/66.6209412 secs/batch = 0.2950s, grad.norm=13.83773232
 16869: 12 [  945/ 1327], train_loss/perplexity = 4.43209791/84.1076813 secs/batch = 0.2951s, grad.norm=14.77577019
 16874: 12 [  950/ 1327], train_loss/perplexity = 4.15453291/63.7221947 secs/batch = 0.2934s, grad.norm=14.16246605
 16879: 12 [  955/ 1327], train_loss/perplexity = 4.16470432/64.3736496 secs/batch = 0.2941s, grad.norm=14.19494820
 16884: 12 [  960/ 1327], train_loss/perplexity = 4.46606302/87.0134811 secs/batch = 0.3011s, grad.norm=14.33848476
 16889: 12 [  965/ 1327], train_loss/perplexity = 4.26539278/71.1928787 secs/batch = 0.3005s, grad.norm=14.55531311
 16894: 12 [  970/ 1327], train_loss/perplexity = 4.52248812/92.0643768 secs/batch = 0.3018s, grad.norm=14.41341114
 16899: 12 [  975/ 1327], train_loss/perplexity = 4.11726475/61.3910904 secs/batch = 0.2932s, grad.norm=15.20254230
 16904: 12 [  980/ 1327], train_loss/perplexity = 3.92807961/50.8093109 secs/batch = 0.2954s, grad.norm=13.65891171
 16909: 12 [  985/ 1327], train_loss/perplexity = 4.04494143/57.1078415 secs/batch = 0.2940s, grad.norm=14.83015156
 16914: 12 [  990/ 1327], train_loss/perplexity = 4.31817913/75.0518417 secs/batch = 0.2913s, grad.norm=14.80986214
 16919: 12 [  995/ 1327], train_loss/perplexity = 4.28258228/72.4272232 secs/batch = 0.2956s, grad.norm=14.48926735
 16924: 12 [ 1000/ 1327], train_loss/perplexity = 3.85796595/47.3689041 secs/batch = 0.2940s, grad.norm=14.04317665
 16929: 12 [ 1005/ 1327], train_loss/perplexity = 4.34295273/76.9343719 secs/batch = 0.2942s, grad.norm=14.69787979
 16934: 12 [ 1010/ 1327], train_loss/perplexity = 3.85136271/47.0571442 secs/batch = 0.2921s, grad.norm=13.28679562
 16939: 12 [ 1015/ 1327], train_loss/perplexity = 4.43138695/84.0479050 secs/batch = 0.2984s, grad.norm=14.38510990
 16944: 12 [ 1020/ 1327], train_loss/perplexity = 4.39468813/81.0193558 secs/batch = 0.2938s, grad.norm=13.99957275
 16949: 12 [ 1025/ 1327], train_loss/perplexity = 4.41787863/82.9201965 secs/batch = 0.2944s, grad.norm=14.17883873
 16954: 12 [ 1030/ 1327], train_loss/perplexity = 4.11935043/61.5192680 secs/batch = 0.2994s, grad.norm=13.74102783
 16959: 12 [ 1035/ 1327], train_loss/perplexity = 4.06542015/58.2893944 secs/batch = 0.2928s, grad.norm=13.77885532
 16964: 12 [ 1040/ 1327], train_loss/perplexity = 4.32203865/75.3420715 secs/batch = 0.2932s, grad.norm=14.71879482
 16969: 12 [ 1045/ 1327], train_loss/perplexity = 3.85276747/47.1232948 secs/batch = 0.2942s, grad.norm=13.56899929
 16974: 12 [ 1050/ 1327], train_loss/perplexity = 3.98923874/54.0137558 secs/batch = 0.2926s, grad.norm=14.75134563
 16979: 12 [ 1055/ 1327], train_loss/perplexity = 4.08849239/59.6498947 secs/batch = 0.2949s, grad.norm=15.06453514
 16984: 12 [ 1060/ 1327], train_loss/perplexity = 3.64141417/38.1457443 secs/batch = 0.2994s, grad.norm=15.13020515
 16989: 12 [ 1065/ 1327], train_loss/perplexity = 3.73698211/41.9711342 secs/batch = 0.2922s, grad.norm=14.27682877
 16994: 12 [ 1070/ 1327], train_loss/perplexity = 4.08420229/59.3945389 secs/batch = 0.2921s, grad.norm=14.85425186
 16999: 12 [ 1075/ 1327], train_loss/perplexity = 3.85811043/47.3757477 secs/batch = 0.2955s, grad.norm=14.65741730
 17004: 12 [ 1080/ 1327], train_loss/perplexity = 3.89271712/49.0439644 secs/batch = 0.2940s, grad.norm=14.73482418
 17009: 12 [ 1085/ 1327], train_loss/perplexity = 3.74185252/42.1760483 secs/batch = 0.2922s, grad.norm=14.30475140
 17014: 12 [ 1090/ 1327], train_loss/perplexity = 4.03683281/56.6466484 secs/batch = 0.2941s, grad.norm=15.00412369
 17019: 12 [ 1095/ 1327], train_loss/perplexity = 4.10177279/60.4473534 secs/batch = 0.2977s, grad.norm=14.84112549
 17024: 12 [ 1100/ 1327], train_loss/perplexity = 3.86384749/47.6483269 secs/batch = 0.3006s, grad.norm=15.78201580
 17029: 12 [ 1105/ 1327], train_loss/perplexity = 3.82173157/45.6832428 secs/batch = 0.2927s, grad.norm=15.02636433
 17034: 12 [ 1110/ 1327], train_loss/perplexity = 4.19652176/66.4547806 secs/batch = 0.2978s, grad.norm=15.36891079
 17039: 12 [ 1115/ 1327], train_loss/perplexity = 3.99171567/54.1477089 secs/batch = 0.2944s, grad.norm=14.05918121
 17044: 12 [ 1120/ 1327], train_loss/perplexity = 4.12672949/61.9749031 secs/batch = 0.2941s, grad.norm=14.34997845
 17049: 12 [ 1125/ 1327], train_loss/perplexity = 4.29576254/73.3881531 secs/batch = 0.2945s, grad.norm=15.03569508
 17054: 12 [ 1130/ 1327], train_loss/perplexity = 4.01770067/55.5731773 secs/batch = 0.2993s, grad.norm=14.63071442
 17059: 12 [ 1135/ 1327], train_loss/perplexity = 3.99331760/54.2345200 secs/batch = 0.2956s, grad.norm=14.79801559
 17064: 12 [ 1140/ 1327], train_loss/perplexity = 4.23598194/69.1295242 secs/batch = 0.3013s, grad.norm=15.17354202
 17069: 12 [ 1145/ 1327], train_loss/perplexity = 4.02998877/56.2602806 secs/batch = 0.2945s, grad.norm=14.77904510
 17074: 12 [ 1150/ 1327], train_loss/perplexity = 4.08048534/59.1741829 secs/batch = 0.2957s, grad.norm=14.79347229
 17079: 12 [ 1155/ 1327], train_loss/perplexity = 4.09975147/60.3252945 secs/batch = 0.2958s, grad.norm=14.82892895
 17084: 12 [ 1160/ 1327], train_loss/perplexity = 4.08223677/59.2779121 secs/batch = 0.2959s, grad.norm=14.99556637
 17089: 12 [ 1165/ 1327], train_loss/perplexity = 4.15798330/63.9424400 secs/batch = 0.2943s, grad.norm=14.93644047
 17094: 12 [ 1170/ 1327], train_loss/perplexity = 3.96397495/52.6662560 secs/batch = 0.2938s, grad.norm=14.48211098
 17099: 12 [ 1175/ 1327], train_loss/perplexity = 3.76224422/43.0449219 secs/batch = 0.2945s, grad.norm=14.61298180
 17104: 12 [ 1180/ 1327], train_loss/perplexity = 3.80032492/44.7157097 secs/batch = 0.2945s, grad.norm=14.59971905
 17109: 12 [ 1185/ 1327], train_loss/perplexity = 3.99354601/54.2469101 secs/batch = 0.2941s, grad.norm=15.03954029
 17114: 12 [ 1190/ 1327], train_loss/perplexity = 4.10460615/60.6188660 secs/batch = 0.2953s, grad.norm=14.90815449
 17119: 12 [ 1195/ 1327], train_loss/perplexity = 3.88530302/48.6816902 secs/batch = 0.3000s, grad.norm=14.65476131
 17124: 12 [ 1200/ 1327], train_loss/perplexity = 3.82567310/45.8636627 secs/batch = 0.2995s, grad.norm=14.03588009
 17129: 12 [ 1205/ 1327], train_loss/perplexity = 3.82860827/45.9984779 secs/batch = 0.2979s, grad.norm=14.65893173
 17134: 12 [ 1210/ 1327], train_loss/perplexity = 3.51495314/33.6143532 secs/batch = 0.2927s, grad.norm=14.74853992
 17139: 12 [ 1215/ 1327], train_loss/perplexity = 3.73442817/41.8640785 secs/batch = 0.2952s, grad.norm=14.01366234
 17144: 12 [ 1220/ 1327], train_loss/perplexity = 3.88800383/48.8133507 secs/batch = 0.2986s, grad.norm=14.93828773
 17149: 12 [ 1225/ 1327], train_loss/perplexity = 3.61783719/37.2569008 secs/batch = 0.2991s, grad.norm=15.44259930
 17154: 12 [ 1230/ 1327], train_loss/perplexity = 4.00916243/55.1007004 secs/batch = 0.3001s, grad.norm=14.47668934
 17159: 12 [ 1235/ 1327], train_loss/perplexity = 3.78889918/44.2077103 secs/batch = 0.2963s, grad.norm=14.42943287
 17164: 12 [ 1240/ 1327], train_loss/perplexity = 4.12597513/61.9281693 secs/batch = 0.2947s, grad.norm=15.33942223
 17169: 12 [ 1245/ 1327], train_loss/perplexity = 4.01973248/55.6862068 secs/batch = 0.2952s, grad.norm=14.61146355
 17174: 12 [ 1250/ 1327], train_loss/perplexity = 4.09337091/59.9416084 secs/batch = 0.2929s, grad.norm=13.85195160
 17179: 12 [ 1255/ 1327], train_loss/perplexity = 4.19089174/66.0816956 secs/batch = 0.2975s, grad.norm=14.52865219
 17184: 12 [ 1260/ 1327], train_loss/perplexity = 3.96935415/52.9503212 secs/batch = 0.2998s, grad.norm=15.48160744
 17189: 12 [ 1265/ 1327], train_loss/perplexity = 4.16435575/64.3512115 secs/batch = 0.2949s, grad.norm=14.85854435
 17194: 12 [ 1270/ 1327], train_loss/perplexity = 3.90189505/49.4961586 secs/batch = 0.2943s, grad.norm=15.31335640
 17199: 12 [ 1275/ 1327], train_loss/perplexity = 4.11332655/61.1497955 secs/batch = 0.2977s, grad.norm=14.71553898
 17204: 12 [ 1280/ 1327], train_loss/perplexity = 3.91250658/50.0241852 secs/batch = 0.2995s, grad.norm=14.29780197
 17209: 12 [ 1285/ 1327], train_loss/perplexity = 3.90094161/49.4489899 secs/batch = 0.2957s, grad.norm=14.62088013
 17214: 12 [ 1290/ 1327], train_loss/perplexity = 4.05099583/57.4546432 secs/batch = 0.2937s, grad.norm=14.57540321
 17219: 12 [ 1295/ 1327], train_loss/perplexity = 4.05295420/57.5672722 secs/batch = 0.2999s, grad.norm=14.41354752
 17224: 12 [ 1300/ 1327], train_loss/perplexity = 4.19211674/66.1626892 secs/batch = 0.2935s, grad.norm=13.60218430
 17229: 12 [ 1305/ 1327], train_loss/perplexity = 4.28143311/72.3440399 secs/batch = 0.3023s, grad.norm=14.95804596
 17234: 12 [ 1310/ 1327], train_loss/perplexity = 4.56622887/96.1807175 secs/batch = 0.2965s, grad.norm=15.59290791
 17239: 12 [ 1315/ 1327], train_loss/perplexity = 4.33527946/76.3462906 secs/batch = 0.2948s, grad.norm=15.06994247
 17244: 12 [ 1320/ 1327], train_loss/perplexity = 4.30655813/74.1847153 secs/batch = 0.2942s, grad.norm=14.64264202
 17249: 12 [ 1325/ 1327], train_loss/perplexity = 4.22602367/68.4445343 secs/batch = 0.2935s, grad.norm=14.62189388
Epoch training time: 392.84120535850525
	> validation loss = 4.62235403, perplexity = 101.73323059
	> validation loss = 4.58128262, perplexity = 97.63954926
	> validation loss = 4.57908440, perplexity = 97.42514801
	> validation loss = 4.54352236, perplexity = 94.02139282
	> validation loss = 4.71383905, perplexity = 111.47931671
	> validation loss = 4.65976191, perplexity = 105.61093140
	> validation loss = 4.59895229, perplexity = 99.38014221
	> validation loss = 4.42674255, perplexity = 83.65846252
	> validation loss = 4.25660992, perplexity = 70.57033539
	> validation loss = 4.35181046, perplexity = 77.61885834
	> validation loss = 4.52684498, perplexity = 92.46636963
	> validation loss = 4.51347542, perplexity = 91.23835754
	> validation loss = 4.43050861, perplexity = 83.97411346
	> validation loss = 4.21022558, perplexity = 67.37173462
	> validation loss = 4.18124294, perplexity = 65.44715118
	> validation loss = 4.18937874, perplexity = 65.98178864
	> validation loss = 4.62769890, perplexity = 102.27844238
	> validation loss = 4.16086626, perplexity = 64.12705231
	> validation loss = 4.62393284, perplexity = 101.89397430
	> validation loss = 4.55472183, perplexity = 95.08030701
	> validation loss = 4.26761723, perplexity = 71.35141754
at the end of epoch: 12
train loss = 4.18085576, perplexity = 65.42181450
validation loss = 4.46446756, perplexity = 86.87476139
Saved model cv/epoch012_4.4645.model
 17256: 13 [    5/ 1327], train_loss/perplexity = 4.35106230/77.5608139 secs/batch = 0.2984s, grad.norm=14.82019520
 17261: 13 [   10/ 1327], train_loss/perplexity = 3.93051219/50.9330597 secs/batch = 0.2931s, grad.norm=13.89490318
 17266: 13 [   15/ 1327], train_loss/perplexity = 4.28033495/72.2646408 secs/batch = 0.2945s, grad.norm=13.64404011
 17271: 13 [   20/ 1327], train_loss/perplexity = 4.46150494/86.6177673 secs/batch = 0.2936s, grad.norm=14.02126503
 17276: 13 [   25/ 1327], train_loss/perplexity = 4.24094057/69.4731674 secs/batch = 0.2952s, grad.norm=14.80144405
 17281: 13 [   30/ 1327], train_loss/perplexity = 4.32849979/75.8304367 secs/batch = 0.2941s, grad.norm=14.45749950
 17286: 13 [   35/ 1327], train_loss/perplexity = 4.12958050/62.1518440 secs/batch = 0.2981s, grad.norm=14.11932564
 17291: 13 [   40/ 1327], train_loss/perplexity = 4.06805897/58.4434128 secs/batch = 0.2985s, grad.norm=14.02573776
 17296: 13 [   45/ 1327], train_loss/perplexity = 3.91310835/50.0542984 secs/batch = 0.2924s, grad.norm=13.52171421
 17301: 13 [   50/ 1327], train_loss/perplexity = 4.14879179/63.3574066 secs/batch = 0.2927s, grad.norm=14.64064789
 17306: 13 [   55/ 1327], train_loss/perplexity = 4.07159233/58.6502800 secs/batch = 0.2945s, grad.norm=14.47706223
 17311: 13 [   60/ 1327], train_loss/perplexity = 4.35964775/78.2295761 secs/batch = 0.2928s, grad.norm=14.36087227
 17316: 13 [   65/ 1327], train_loss/perplexity = 3.99293947/54.2140160 secs/batch = 0.3016s, grad.norm=14.04951096
 17321: 13 [   70/ 1327], train_loss/perplexity = 3.77483988/43.5905266 secs/batch = 0.2945s, grad.norm=14.19713211
 17326: 13 [   75/ 1327], train_loss/perplexity = 3.57021761/35.5243225 secs/batch = 0.2980s, grad.norm=13.55552578
 17331: 13 [   80/ 1327], train_loss/perplexity = 3.98756266/53.9232979 secs/batch = 0.2951s, grad.norm=14.21289921
 17336: 13 [   85/ 1327], train_loss/perplexity = 4.10721540/60.7772408 secs/batch = 0.2988s, grad.norm=15.38234901
 17341: 13 [   90/ 1327], train_loss/perplexity = 4.10413837/60.5905151 secs/batch = 0.2952s, grad.norm=14.53995228
 17346: 13 [   95/ 1327], train_loss/perplexity = 3.95136356/52.0062332 secs/batch = 0.2985s, grad.norm=14.30741692
 17351: 13 [  100/ 1327], train_loss/perplexity = 4.26782751/71.3664246 secs/batch = 0.2941s, grad.norm=14.42343807
 17356: 13 [  105/ 1327], train_loss/perplexity = 3.99261713/54.1965446 secs/batch = 0.2953s, grad.norm=15.18416882
 17361: 13 [  110/ 1327], train_loss/perplexity = 3.97617650/53.3128014 secs/batch = 0.2928s, grad.norm=14.24566460
 17366: 13 [  115/ 1327], train_loss/perplexity = 3.95673037/52.2860909 secs/batch = 0.2955s, grad.norm=14.61863804
 17371: 13 [  120/ 1327], train_loss/perplexity = 4.00323772/54.7752113 secs/batch = 0.2924s, grad.norm=14.82294273
 17376: 13 [  125/ 1327], train_loss/perplexity = 4.10655212/60.7369423 secs/batch = 0.2953s, grad.norm=14.91303349
 17381: 13 [  130/ 1327], train_loss/perplexity = 4.04423666/57.0676079 secs/batch = 0.2991s, grad.norm=15.68007565
 17386: 13 [  135/ 1327], train_loss/perplexity = 4.06708241/58.3863678 secs/batch = 0.2991s, grad.norm=14.10376740
 17391: 13 [  140/ 1327], train_loss/perplexity = 4.33521795/76.3415985 secs/batch = 0.2992s, grad.norm=15.14203835
 17396: 13 [  145/ 1327], train_loss/perplexity = 4.12450981/61.8374901 secs/batch = 0.2956s, grad.norm=15.26541805
 17401: 13 [  150/ 1327], train_loss/perplexity = 4.26772833/71.3593445 secs/batch = 0.2952s, grad.norm=15.55118370
 17406: 13 [  155/ 1327], train_loss/perplexity = 4.50382996/90.3625565 secs/batch = 0.2932s, grad.norm=14.51612091
 17411: 13 [  160/ 1327], train_loss/perplexity = 4.09134674/59.8204002 secs/batch = 0.2935s, grad.norm=14.07821369
 17416: 13 [  165/ 1327], train_loss/perplexity = 4.24658155/69.8661728 secs/batch = 0.2964s, grad.norm=14.44644547
 17421: 13 [  170/ 1327], train_loss/perplexity = 4.04008293/56.8310547 secs/batch = 0.2940s, grad.norm=13.91454220
 17426: 13 [  175/ 1327], train_loss/perplexity = 4.38847017/80.5171509 secs/batch = 0.2941s, grad.norm=14.72419453
 17431: 13 [  180/ 1327], train_loss/perplexity = 4.20550823/67.0546646 secs/batch = 0.2924s, grad.norm=14.58843708
 17436: 13 [  185/ 1327], train_loss/perplexity = 4.54983139/94.6164551 secs/batch = 0.3007s, grad.norm=15.03803825
 17441: 13 [  190/ 1327], train_loss/perplexity = 4.04145813/56.9092636 secs/batch = 0.2946s, grad.norm=14.03346539
 17446: 13 [  195/ 1327], train_loss/perplexity = 4.30738783/74.2462921 secs/batch = 0.3002s, grad.norm=13.77917480
 17451: 13 [  200/ 1327], train_loss/perplexity = 4.17387962/64.9670105 secs/batch = 0.2991s, grad.norm=14.79004288
 17456: 13 [  205/ 1327], train_loss/perplexity = 4.39210033/80.8099670 secs/batch = 0.2962s, grad.norm=14.29492474
 17461: 13 [  210/ 1327], train_loss/perplexity = 4.24450207/69.7210388 secs/batch = 0.2938s, grad.norm=13.79197216
 17466: 13 [  215/ 1327], train_loss/perplexity = 4.39890146/81.3614426 secs/batch = 0.2952s, grad.norm=14.22401619
 17471: 13 [  220/ 1327], train_loss/perplexity = 4.26235199/70.9767227 secs/batch = 0.2921s, grad.norm=14.10739708
 17476: 13 [  225/ 1327], train_loss/perplexity = 4.46339750/86.7818527 secs/batch = 0.2936s, grad.norm=14.10008621
 17481: 13 [  230/ 1327], train_loss/perplexity = 4.29418468/73.2724533 secs/batch = 0.2952s, grad.norm=14.96935654
 17486: 13 [  235/ 1327], train_loss/perplexity = 4.14274406/62.9753914 secs/batch = 0.2970s, grad.norm=14.24666691
 17491: 13 [  240/ 1327], train_loss/perplexity = 3.91688800/50.2438431 secs/batch = 0.2912s, grad.norm=14.14672661
 17496: 13 [  245/ 1327], train_loss/perplexity = 4.23986530/69.3985062 secs/batch = 0.2988s, grad.norm=14.59821033
 17501: 13 [  250/ 1327], train_loss/perplexity = 4.12438059/61.8294983 secs/batch = 0.2947s, grad.norm=14.15420151
 17506: 13 [  255/ 1327], train_loss/perplexity = 4.11430216/61.2094841 secs/batch = 0.2947s, grad.norm=14.40834236
 17511: 13 [  260/ 1327], train_loss/perplexity = 4.28456497/72.5709686 secs/batch = 0.2952s, grad.norm=14.90946579
 17516: 13 [  265/ 1327], train_loss/perplexity = 4.49138212/89.2447052 secs/batch = 0.2938s, grad.norm=14.10220432
 17521: 13 [  270/ 1327], train_loss/perplexity = 4.58342218/97.8486786 secs/batch = 0.2954s, grad.norm=14.62744427
 17526: 13 [  275/ 1327], train_loss/perplexity = 4.46684790/87.0818024 secs/batch = 0.2987s, grad.norm=14.40100574
 17531: 13 [  280/ 1327], train_loss/perplexity = 4.23150396/68.8206558 secs/batch = 0.2934s, grad.norm=14.59296417
 17536: 13 [  285/ 1327], train_loss/perplexity = 4.63137388/102.6550064 secs/batch = 0.3009s, grad.norm=15.04386044
 17541: 13 [  290/ 1327], train_loss/perplexity = 4.23206520/68.8592911 secs/batch = 0.2998s, grad.norm=14.11019897
 17546: 13 [  295/ 1327], train_loss/perplexity = 3.98002577/53.5184135 secs/batch = 0.2944s, grad.norm=14.05500317
 17551: 13 [  300/ 1327], train_loss/perplexity = 3.62658119/37.5841026 secs/batch = 0.2941s, grad.norm=13.80269814
 17556: 13 [  305/ 1327], train_loss/perplexity = 4.09760571/60.1959877 secs/batch = 0.2950s, grad.norm=14.09691334
 17561: 13 [  310/ 1327], train_loss/perplexity = 4.07923031/59.0999641 secs/batch = 0.2948s, grad.norm=14.47403908
 17566: 13 [  315/ 1327], train_loss/perplexity = 3.68403196/39.8065681 secs/batch = 0.2954s, grad.norm=14.33995533
 17571: 13 [  320/ 1327], train_loss/perplexity = 3.61881852/37.2934799 secs/batch = 0.2915s, grad.norm=15.19617081
 17576: 13 [  325/ 1327], train_loss/perplexity = 3.63621283/37.9478493 secs/batch = 0.2948s, grad.norm=13.41546631
 17581: 13 [  330/ 1327], train_loss/perplexity = 4.24345875/69.6483307 secs/batch = 0.2949s, grad.norm=15.21103191
 17586: 13 [  335/ 1327], train_loss/perplexity = 3.60731697/36.8670044 secs/batch = 0.2949s, grad.norm=13.84744072
 17591: 13 [  340/ 1327], train_loss/perplexity = 4.40635061/81.9697800 secs/batch = 0.2940s, grad.norm=14.07544518
 17596: 13 [  345/ 1327], train_loss/perplexity = 4.12539768/61.8924179 secs/batch = 0.2923s, grad.norm=14.05808735
 17601: 13 [  350/ 1327], train_loss/perplexity = 4.22647667/68.4755478 secs/batch = 0.3011s, grad.norm=15.41177654
 17606: 13 [  355/ 1327], train_loss/perplexity = 4.21511173/67.7017288 secs/batch = 0.2956s, grad.norm=14.69570065
 17611: 13 [  360/ 1327], train_loss/perplexity = 4.28130388/72.3346939 secs/batch = 0.2997s, grad.norm=15.93013382
 17616: 13 [  365/ 1327], train_loss/perplexity = 4.34623718/77.1874771 secs/batch = 0.2954s, grad.norm=15.20160961
 17621: 13 [  370/ 1327], train_loss/perplexity = 4.42726088/83.7018356 secs/batch = 0.2953s, grad.norm=14.93587780
 17626: 13 [  375/ 1327], train_loss/perplexity = 3.73552847/41.9101677 secs/batch = 0.2941s, grad.norm=14.08359814
 17631: 13 [  380/ 1327], train_loss/perplexity = 3.84733438/46.8679657 secs/batch = 0.2960s, grad.norm=15.01565170
 17636: 13 [  385/ 1327], train_loss/perplexity = 4.05132103/57.4733315 secs/batch = 0.2947s, grad.norm=15.39223480
 17641: 13 [  390/ 1327], train_loss/perplexity = 4.18107796/65.4363556 secs/batch = 0.2993s, grad.norm=14.49452972
 17646: 13 [  395/ 1327], train_loss/perplexity = 4.17111731/64.7877960 secs/batch = 0.2945s, grad.norm=14.63764381
 17651: 13 [  400/ 1327], train_loss/perplexity = 4.11407137/61.1953583 secs/batch = 0.2927s, grad.norm=14.03908825
 17656: 13 [  405/ 1327], train_loss/perplexity = 4.47312117/87.6298065 secs/batch = 0.2990s, grad.norm=14.94548607
 17661: 13 [  410/ 1327], train_loss/perplexity = 4.03031158/56.2784424 secs/batch = 0.2988s, grad.norm=14.13687611
 17666: 13 [  415/ 1327], train_loss/perplexity = 4.08569431/59.4832230 secs/batch = 0.3017s, grad.norm=14.63461685
 17671: 13 [  420/ 1327], train_loss/perplexity = 3.63934779/38.0670013 secs/batch = 0.2949s, grad.norm=14.74989605
 17676: 13 [  425/ 1327], train_loss/perplexity = 3.97517061/53.2592010 secs/batch = 0.2952s, grad.norm=14.79756069
 17681: 13 [  430/ 1327], train_loss/perplexity = 4.18191576/65.4911957 secs/batch = 0.2953s, grad.norm=15.56171989
 17686: 13 [  435/ 1327], train_loss/perplexity = 4.21963644/68.0087509 secs/batch = 0.3006s, grad.norm=15.19012737
 17691: 13 [  440/ 1327], train_loss/perplexity = 3.76679039/43.2410545 secs/batch = 0.2998s, grad.norm=14.26778126
 17696: 13 [  445/ 1327], train_loss/perplexity = 4.11895943/61.4952202 secs/batch = 0.2934s, grad.norm=14.88581181
 17701: 13 [  450/ 1327], train_loss/perplexity = 4.11092901/61.0033646 secs/batch = 0.2960s, grad.norm=15.22040081
 17706: 13 [  455/ 1327], train_loss/perplexity = 4.00989914/55.1413078 secs/batch = 0.2953s, grad.norm=14.10832596
 17711: 13 [  460/ 1327], train_loss/perplexity = 4.11522913/61.2662506 secs/batch = 0.2946s, grad.norm=15.60883713
 17716: 13 [  465/ 1327], train_loss/perplexity = 3.73907137/42.0589142 secs/batch = 0.2924s, grad.norm=16.05765152
 17721: 13 [  470/ 1327], train_loss/perplexity = 4.46082306/86.5587234 secs/batch = 0.2950s, grad.norm=14.50090313
 17726: 13 [  475/ 1327], train_loss/perplexity = 3.93908072/51.3713531 secs/batch = 0.2955s, grad.norm=14.99655819
 17731: 13 [  480/ 1327], train_loss/perplexity = 4.02817154/56.1581345 secs/batch = 0.2941s, grad.norm=14.97142887
 17736: 13 [  485/ 1327], train_loss/perplexity = 4.15780210/63.9308548 secs/batch = 0.2936s, grad.norm=15.21777248
 17741: 13 [  490/ 1327], train_loss/perplexity = 3.93229342/51.0238609 secs/batch = 0.2939s, grad.norm=15.36896706
 17746: 13 [  495/ 1327], train_loss/perplexity = 3.95316172/52.0998306 secs/batch = 0.2941s, grad.norm=14.29317379
 17751: 13 [  500/ 1327], train_loss/perplexity = 4.15661097/63.8547478 secs/batch = 0.2937s, grad.norm=14.32336903
 17756: 13 [  505/ 1327], train_loss/perplexity = 4.23559237/69.1026001 secs/batch = 0.2920s, grad.norm=13.58878231
 17761: 13 [  510/ 1327], train_loss/perplexity = 4.62129641/101.6256943 secs/batch = 0.2944s, grad.norm=14.30053711
 17766: 13 [  515/ 1327], train_loss/perplexity = 4.28438139/72.5576477 secs/batch = 0.2949s, grad.norm=14.37753963
 17771: 13 [  520/ 1327], train_loss/perplexity = 4.40725899/82.0442734 secs/batch = 0.2943s, grad.norm=14.35817051
 17776: 13 [  525/ 1327], train_loss/perplexity = 4.00607920/54.9310722 secs/batch = 0.2946s, grad.norm=14.42517662
 17781: 13 [  530/ 1327], train_loss/perplexity = 4.01704311/55.5366478 secs/batch = 0.3011s, grad.norm=15.19561386
 17786: 13 [  535/ 1327], train_loss/perplexity = 4.14628935/63.1990547 secs/batch = 0.2945s, grad.norm=14.96701431
 17791: 13 [  540/ 1327], train_loss/perplexity = 4.22399902/68.3060989 secs/batch = 0.2942s, grad.norm=14.70218945
 17796: 13 [  545/ 1327], train_loss/perplexity = 4.14467096/63.0968590 secs/batch = 0.2993s, grad.norm=14.39906788
 17801: 13 [  550/ 1327], train_loss/perplexity = 4.15803432/63.9457016 secs/batch = 0.2987s, grad.norm=14.68990993
 17806: 13 [  555/ 1327], train_loss/perplexity = 4.00086021/54.6451378 secs/batch = 0.2988s, grad.norm=14.22059155
 17811: 13 [  560/ 1327], train_loss/perplexity = 4.21191978/67.4859772 secs/batch = 0.2948s, grad.norm=16.23331451
 17816: 13 [  565/ 1327], train_loss/perplexity = 4.09761715/60.1966782 secs/batch = 0.2951s, grad.norm=16.32835197
 17821: 13 [  570/ 1327], train_loss/perplexity = 4.06521702/58.2775536 secs/batch = 0.2947s, grad.norm=15.79820538
 17826: 13 [  575/ 1327], train_loss/perplexity = 3.87266350/48.0702515 secs/batch = 0.2952s, grad.norm=15.02039814
 17831: 13 [  580/ 1327], train_loss/perplexity = 4.21386385/67.6173019 secs/batch = 0.2930s, grad.norm=15.19558525
 17836: 13 [  585/ 1327], train_loss/perplexity = 3.76092887/42.9883385 secs/batch = 0.2934s, grad.norm=14.61362743
 17841: 13 [  590/ 1327], train_loss/perplexity = 4.16568756/64.4369736 secs/batch = 0.2998s, grad.norm=14.26691628
 17846: 13 [  595/ 1327], train_loss/perplexity = 4.18420029/65.6409836 secs/batch = 0.2942s, grad.norm=15.09040928
 17851: 13 [  600/ 1327], train_loss/perplexity = 4.36971664/79.0212402 secs/batch = 0.2941s, grad.norm=13.98890400
 17856: 13 [  605/ 1327], train_loss/perplexity = 4.24685907/69.8855591 secs/batch = 0.3007s, grad.norm=14.58860207
 17861: 13 [  610/ 1327], train_loss/perplexity = 4.44132280/84.8871536 secs/batch = 0.2995s, grad.norm=15.02110577
 17866: 13 [  615/ 1327], train_loss/perplexity = 4.00946474/55.1173592 secs/batch = 0.2957s, grad.norm=14.27798939
 17871: 13 [  620/ 1327], train_loss/perplexity = 4.33386517/76.2383881 secs/batch = 0.2951s, grad.norm=14.33547401
 17876: 13 [  625/ 1327], train_loss/perplexity = 4.35625410/77.9645386 secs/batch = 0.2948s, grad.norm=14.41842270
 17881: 13 [  630/ 1327], train_loss/perplexity = 4.35877609/78.1614151 secs/batch = 0.2944s, grad.norm=14.49711609
 17886: 13 [  635/ 1327], train_loss/perplexity = 4.14096355/62.8633652 secs/batch = 0.2943s, grad.norm=14.34897614
 17891: 13 [  640/ 1327], train_loss/perplexity = 4.11054468/60.9799232 secs/batch = 0.2951s, grad.norm=14.91567230
 17896: 13 [  645/ 1327], train_loss/perplexity = 4.36888981/78.9559250 secs/batch = 0.3002s, grad.norm=15.47666073
 17901: 13 [  650/ 1327], train_loss/perplexity = 3.91703987/50.2514725 secs/batch = 0.2959s, grad.norm=14.54719067
 17906: 13 [  655/ 1327], train_loss/perplexity = 4.08306551/59.3270607 secs/batch = 0.2953s, grad.norm=15.19804859
 17911: 13 [  660/ 1327], train_loss/perplexity = 3.97264457/53.1248360 secs/batch = 0.2944s, grad.norm=14.62092400
 17916: 13 [  665/ 1327], train_loss/perplexity = 4.16093636/64.1315460 secs/batch = 0.2981s, grad.norm=14.37624168
 17921: 13 [  670/ 1327], train_loss/perplexity = 4.03052282/56.2903328 secs/batch = 0.2993s, grad.norm=14.53546715
 17926: 13 [  675/ 1327], train_loss/perplexity = 3.87361884/48.1161957 secs/batch = 0.2988s, grad.norm=14.82585335
 17931: 13 [  680/ 1327], train_loss/perplexity = 4.11260796/61.1058731 secs/batch = 0.2956s, grad.norm=15.41716290
 17936: 13 [  685/ 1327], train_loss/perplexity = 3.84560156/46.7868195 secs/batch = 0.3013s, grad.norm=14.73088837
 17941: 13 [  690/ 1327], train_loss/perplexity = 4.27600908/71.9527054 secs/batch = 0.2958s, grad.norm=14.35910988
 17946: 13 [  695/ 1327], train_loss/perplexity = 4.14226341/62.9451294 secs/batch = 0.3020s, grad.norm=14.57035446
 17951: 13 [  700/ 1327], train_loss/perplexity = 4.38728189/80.4215240 secs/batch = 0.2998s, grad.norm=15.71015549
 17956: 13 [  705/ 1327], train_loss/perplexity = 4.09996843/60.3383827 secs/batch = 0.2948s, grad.norm=13.82541275
 17961: 13 [  710/ 1327], train_loss/perplexity = 3.98128176/53.5856743 secs/batch = 0.2986s, grad.norm=15.01340771
 17966: 13 [  715/ 1327], train_loss/perplexity = 3.90680790/49.7399216 secs/batch = 0.2948s, grad.norm=14.68779278
 17971: 13 [  720/ 1327], train_loss/perplexity = 3.89207983/49.0127182 secs/batch = 0.2949s, grad.norm=14.53666687
 17976: 13 [  725/ 1327], train_loss/perplexity = 3.98584580/53.8307991 secs/batch = 0.2942s, grad.norm=14.98580551
 17981: 13 [  730/ 1327], train_loss/perplexity = 4.01671076/55.5181923 secs/batch = 0.2941s, grad.norm=14.72590923
 17986: 13 [  735/ 1327], train_loss/perplexity = 4.19848490/66.5853729 secs/batch = 0.2924s, grad.norm=15.51258373
 17991: 13 [  740/ 1327], train_loss/perplexity = 3.65482044/38.6605797 secs/batch = 0.2970s, grad.norm=14.02230740
 17996: 13 [  745/ 1327], train_loss/perplexity = 4.13135147/62.2620125 secs/batch = 0.2959s, grad.norm=14.79558945
 18001: 13 [  750/ 1327], train_loss/perplexity = 4.03897762/56.7682762 secs/batch = 0.3003s, grad.norm=14.96038532
 18006: 13 [  755/ 1327], train_loss/perplexity = 3.87361288/48.1159096 secs/batch = 0.3007s, grad.norm=14.08917618
 18011: 13 [  760/ 1327], train_loss/perplexity = 3.73023224/41.6887894 secs/batch = 0.2956s, grad.norm=13.61691380
 18016: 13 [  765/ 1327], train_loss/perplexity = 3.89265728/49.0410309 secs/batch = 0.2956s, grad.norm=14.09508610
 18021: 13 [  770/ 1327], train_loss/perplexity = 3.79445958/44.4542046 secs/batch = 0.2991s, grad.norm=14.10188484
 18026: 13 [  775/ 1327], train_loss/perplexity = 3.96714926/52.8337021 secs/batch = 0.2996s, grad.norm=15.28604698
 18031: 13 [  780/ 1327], train_loss/perplexity = 4.28071260/72.2919388 secs/batch = 0.2941s, grad.norm=15.32775879
 18036: 13 [  785/ 1327], train_loss/perplexity = 4.14283180/62.9809189 secs/batch = 0.2976s, grad.norm=15.03880215
 18041: 13 [  790/ 1327], train_loss/perplexity = 3.85789537/47.3655586 secs/batch = 0.2936s, grad.norm=14.64578533
 18046: 13 [  795/ 1327], train_loss/perplexity = 4.28409100/72.5365829 secs/batch = 0.2949s, grad.norm=15.32384777
 18051: 13 [  800/ 1327], train_loss/perplexity = 4.13724899/62.6302872 secs/batch = 0.2921s, grad.norm=15.57199764
 18056: 13 [  805/ 1327], train_loss/perplexity = 4.42842770/83.7995529 secs/batch = 0.2943s, grad.norm=14.52254009
 18061: 13 [  810/ 1327], train_loss/perplexity = 4.11336660/61.1522484 secs/batch = 0.3007s, grad.norm=14.04474163
 18066: 13 [  815/ 1327], train_loss/perplexity = 3.92390966/50.5978775 secs/batch = 0.2998s, grad.norm=14.11148453
 18071: 13 [  820/ 1327], train_loss/perplexity = 3.88677692/48.7534981 secs/batch = 0.3002s, grad.norm=13.82993698
 18076: 13 [  825/ 1327], train_loss/perplexity = 4.14517641/63.1287575 secs/batch = 0.2950s, grad.norm=14.34096336
 18081: 13 [  830/ 1327], train_loss/perplexity = 3.79232740/44.3595238 secs/batch = 0.2968s, grad.norm=14.65859032
 18086: 13 [  835/ 1327], train_loss/perplexity = 4.07352972/58.7640190 secs/batch = 0.2958s, grad.norm=14.61042213
 18091: 13 [  840/ 1327], train_loss/perplexity = 4.16172981/64.1824493 secs/batch = 0.2949s, grad.norm=14.82099819
 18096: 13 [  845/ 1327], train_loss/perplexity = 3.93884850/51.3594284 secs/batch = 0.2944s, grad.norm=15.06720924
 18101: 13 [  850/ 1327], train_loss/perplexity = 4.09784079/60.2101402 secs/batch = 0.2938s, grad.norm=14.41188908
 18106: 13 [  855/ 1327], train_loss/perplexity = 4.13182735/62.2916489 secs/batch = 0.3009s, grad.norm=14.68456745
 18111: 13 [  860/ 1327], train_loss/perplexity = 3.81908011/45.5622787 secs/batch = 0.3007s, grad.norm=14.32186222
 18116: 13 [  865/ 1327], train_loss/perplexity = 4.29207230/73.1178360 secs/batch = 0.2948s, grad.norm=14.87949085
 18121: 13 [  870/ 1327], train_loss/perplexity = 4.07740641/58.9922676 secs/batch = 0.2930s, grad.norm=15.06538200
 18126: 13 [  875/ 1327], train_loss/perplexity = 3.69519520/40.2534294 secs/batch = 0.2990s, grad.norm=14.21176147
 18131: 13 [  880/ 1327], train_loss/perplexity = 3.90951610/49.8748131 secs/batch = 0.2951s, grad.norm=14.19661617
 18136: 13 [  885/ 1327], train_loss/perplexity = 4.10907459/60.8903427 secs/batch = 0.3001s, grad.norm=14.09347248
 18141: 13 [  890/ 1327], train_loss/perplexity = 4.20751619/67.1894455 secs/batch = 0.2943s, grad.norm=14.68730831
 18146: 13 [  895/ 1327], train_loss/perplexity = 4.20190907/66.8137589 secs/batch = 0.2943s, grad.norm=14.43540764
 18151: 13 [  900/ 1327], train_loss/perplexity = 4.07779264/59.0150604 secs/batch = 0.2943s, grad.norm=14.18429852
 18156: 13 [  905/ 1327], train_loss/perplexity = 3.92991686/50.9027443 secs/batch = 0.2940s, grad.norm=13.76819229
 18161: 13 [  910/ 1327], train_loss/perplexity = 3.95222163/52.0508766 secs/batch = 0.2994s, grad.norm=13.54619598
 18166: 13 [  915/ 1327], train_loss/perplexity = 4.20213795/66.8290558 secs/batch = 0.2951s, grad.norm=14.20427322
 18171: 13 [  920/ 1327], train_loss/perplexity = 4.35693312/78.0174942 secs/batch = 0.2930s, grad.norm=14.65369701
 18176: 13 [  925/ 1327], train_loss/perplexity = 4.19111824/66.0966644 secs/batch = 0.2939s, grad.norm=14.15651226
 18181: 13 [  930/ 1327], train_loss/perplexity = 4.23818922/69.2822800 secs/batch = 0.2931s, grad.norm=14.44893360
 18186: 13 [  935/ 1327], train_loss/perplexity = 4.30573463/74.1236496 secs/batch = 0.3007s, grad.norm=14.38876820
 18191: 13 [  940/ 1327], train_loss/perplexity = 4.20733500/67.1772766 secs/batch = 0.3012s, grad.norm=14.27330494
 18196: 13 [  945/ 1327], train_loss/perplexity = 4.42940903/83.8818283 secs/batch = 0.2938s, grad.norm=14.37178135
 18201: 13 [  950/ 1327], train_loss/perplexity = 4.08918858/59.6914368 secs/batch = 0.2951s, grad.norm=14.69319820
 18206: 13 [  955/ 1327], train_loss/perplexity = 4.12695980/61.9891777 secs/batch = 0.2958s, grad.norm=14.46848488
 18211: 13 [  960/ 1327], train_loss/perplexity = 4.48190069/88.4025421 secs/batch = 0.2956s, grad.norm=14.65819836
 18216: 13 [  965/ 1327], train_loss/perplexity = 4.16058445/64.1089783 secs/batch = 0.2951s, grad.norm=14.51726818
 18221: 13 [  970/ 1327], train_loss/perplexity = 4.39344168/80.9184341 secs/batch = 0.2945s, grad.norm=14.46512604
 18226: 13 [  975/ 1327], train_loss/perplexity = 4.07965851/59.1252747 secs/batch = 0.2984s, grad.norm=15.15176296
 18231: 13 [  980/ 1327], train_loss/perplexity = 3.91218138/50.0079193 secs/batch = 0.2997s, grad.norm=13.96698093
 18236: 13 [  985/ 1327], train_loss/perplexity = 4.06716394/58.3911285 secs/batch = 0.2935s, grad.norm=15.38172340
 18241: 13 [  990/ 1327], train_loss/perplexity = 4.35411978/77.7983170 secs/batch = 0.2952s, grad.norm=14.99940300
 18246: 13 [  995/ 1327], train_loss/perplexity = 4.22564220/68.4184265 secs/batch = 0.2935s, grad.norm=14.88786221
 18251: 13 [ 1000/ 1327], train_loss/perplexity = 3.88679695/48.7544746 secs/batch = 0.2990s, grad.norm=14.58574963
 18256: 13 [ 1005/ 1327], train_loss/perplexity = 4.21999216/68.0329514 secs/batch = 0.2999s, grad.norm=14.49776649
 18261: 13 [ 1010/ 1327], train_loss/perplexity = 3.78255653/43.9282036 secs/batch = 0.2964s, grad.norm=13.33673096
 18266: 13 [ 1015/ 1327], train_loss/perplexity = 4.34170198/76.8382034 secs/batch = 0.3008s, grad.norm=14.21177101
 18271: 13 [ 1020/ 1327], train_loss/perplexity = 4.40769863/82.0803452 secs/batch = 0.2918s, grad.norm=14.31530571
 18276: 13 [ 1025/ 1327], train_loss/perplexity = 4.35658216/77.9901199 secs/batch = 0.2992s, grad.norm=14.60622883
 18281: 13 [ 1030/ 1327], train_loss/perplexity = 4.04305744/57.0003510 secs/batch = 0.2956s, grad.norm=14.03702259
 18286: 13 [ 1035/ 1327], train_loss/perplexity = 3.97660685/53.3357506 secs/batch = 0.2990s, grad.norm=14.12139702
 18291: 13 [ 1040/ 1327], train_loss/perplexity = 4.25255585/70.2848206 secs/batch = 0.2938s, grad.norm=14.88229561
 18296: 13 [ 1045/ 1327], train_loss/perplexity = 3.78314900/43.9542351 secs/batch = 0.2947s, grad.norm=13.78729057
 18301: 13 [ 1050/ 1327], train_loss/perplexity = 3.95129776/52.0028114 secs/batch = 0.2972s, grad.norm=14.77167797
 18306: 13 [ 1055/ 1327], train_loss/perplexity = 3.99280572/54.2067642 secs/batch = 0.2951s, grad.norm=15.24565125
 18311: 13 [ 1060/ 1327], train_loss/perplexity = 3.60790443/36.8886681 secs/batch = 0.2946s, grad.norm=15.62863827
 18316: 13 [ 1065/ 1327], train_loss/perplexity = 3.71518183/41.0660515 secs/batch = 0.2929s, grad.norm=14.71666145
 18321: 13 [ 1070/ 1327], train_loss/perplexity = 4.04034519/56.8459625 secs/batch = 0.2944s, grad.norm=15.50840378
 18326: 13 [ 1075/ 1327], train_loss/perplexity = 3.87229991/48.0527763 secs/batch = 0.2951s, grad.norm=14.88235188
 18331: 13 [ 1080/ 1327], train_loss/perplexity = 3.81618857/45.4307213 secs/batch = 0.2934s, grad.norm=14.71633339
 18336: 13 [ 1085/ 1327], train_loss/perplexity = 3.72881269/41.6296501 secs/batch = 0.2954s, grad.norm=15.13399792
 18341: 13 [ 1090/ 1327], train_loss/perplexity = 3.92056227/50.4287910 secs/batch = 0.2956s, grad.norm=15.20755196
 18346: 13 [ 1095/ 1327], train_loss/perplexity = 4.00742579/55.0050926 secs/batch = 0.3008s, grad.norm=15.11353016
 18351: 13 [ 1100/ 1327], train_loss/perplexity = 3.76833773/43.3080139 secs/batch = 0.3007s, grad.norm=15.88878155
 18356: 13 [ 1105/ 1327], train_loss/perplexity = 3.74658751/42.3762283 secs/batch = 0.2990s, grad.norm=15.66901302
 18361: 13 [ 1110/ 1327], train_loss/perplexity = 4.07853746/59.0590324 secs/batch = 0.2950s, grad.norm=15.73682594
 18366: 13 [ 1115/ 1327], train_loss/perplexity = 3.87945318/48.3977432 secs/batch = 0.2982s, grad.norm=14.32341862
 18371: 13 [ 1120/ 1327], train_loss/perplexity = 4.08642578/59.5267487 secs/batch = 0.2954s, grad.norm=14.92794991
 18376: 13 [ 1125/ 1327], train_loss/perplexity = 4.29806137/73.5570526 secs/batch = 0.2944s, grad.norm=15.42823982
 18381: 13 [ 1130/ 1327], train_loss/perplexity = 3.99801540/54.4899025 secs/batch = 0.2996s, grad.norm=14.81479645
 18386: 13 [ 1135/ 1327], train_loss/perplexity = 3.94851589/51.8583450 secs/batch = 0.2970s, grad.norm=14.56679058
 18391: 13 [ 1140/ 1327], train_loss/perplexity = 4.15851545/63.9764748 secs/batch = 0.3009s, grad.norm=15.56104660
 18396: 13 [ 1145/ 1327], train_loss/perplexity = 3.99329042/54.2330475 secs/batch = 0.2948s, grad.norm=14.80759239
 18401: 13 [ 1150/ 1327], train_loss/perplexity = 4.01371765/55.3522682 secs/batch = 0.2950s, grad.norm=14.51183891
 18406: 13 [ 1155/ 1327], train_loss/perplexity = 4.11360931/61.1670914 secs/batch = 0.2975s, grad.norm=14.96406937
 18411: 13 [ 1160/ 1327], train_loss/perplexity = 3.99465275/54.3069801 secs/batch = 0.2961s, grad.norm=16.10142517
 18416: 13 [ 1165/ 1327], train_loss/perplexity = 4.13045025/62.2059250 secs/batch = 0.3008s, grad.norm=15.53318501
 18421: 13 [ 1170/ 1327], train_loss/perplexity = 3.94161463/51.5016899 secs/batch = 0.2957s, grad.norm=14.71081734
 18426: 13 [ 1175/ 1327], train_loss/perplexity = 3.67917752/39.6138000 secs/batch = 0.2964s, grad.norm=14.63367844
 18431: 13 [ 1180/ 1327], train_loss/perplexity = 3.84378934/46.7021103 secs/batch = 0.2954s, grad.norm=15.13415813
 18436: 13 [ 1185/ 1327], train_loss/perplexity = 3.97046065/53.0089455 secs/batch = 0.2970s, grad.norm=14.70379066
 18441: 13 [ 1190/ 1327], train_loss/perplexity = 4.06609201/58.3285675 secs/batch = 0.3007s, grad.norm=15.72513008
 18446: 13 [ 1195/ 1327], train_loss/perplexity = 3.77688646/43.6798325 secs/batch = 0.2939s, grad.norm=14.16085243
 18451: 13 [ 1200/ 1327], train_loss/perplexity = 3.83735609/46.4026260 secs/batch = 0.2949s, grad.norm=15.21349907
 18456: 13 [ 1205/ 1327], train_loss/perplexity = 3.80878210/45.0954819 secs/batch = 0.2978s, grad.norm=14.81791306
 18461: 13 [ 1210/ 1327], train_loss/perplexity = 3.50852156/33.3988533 secs/batch = 0.2998s, grad.norm=14.93745613
 18466: 13 [ 1215/ 1327], train_loss/perplexity = 3.77740979/43.7026978 secs/batch = 0.2993s, grad.norm=14.74435520
 18471: 13 [ 1220/ 1327], train_loss/perplexity = 3.83058405/46.0894470 secs/batch = 0.2977s, grad.norm=15.01219273
 18476: 13 [ 1225/ 1327], train_loss/perplexity = 3.59620333/36.4595451 secs/batch = 0.2939s, grad.norm=16.32549477
 18481: 13 [ 1230/ 1327], train_loss/perplexity = 3.87331486/48.1015701 secs/batch = 0.2967s, grad.norm=14.35772800
 18486: 13 [ 1235/ 1327], train_loss/perplexity = 3.79077649/44.2907791 secs/batch = 0.2958s, grad.norm=14.79435635
 18491: 13 [ 1240/ 1327], train_loss/perplexity = 4.03683138/56.6465683 secs/batch = 0.2991s, grad.norm=15.45384789
 18496: 13 [ 1245/ 1327], train_loss/perplexity = 3.93298817/51.0593224 secs/batch = 0.2956s, grad.norm=14.33509064
 18501: 13 [ 1250/ 1327], train_loss/perplexity = 4.05664921/57.7803764 secs/batch = 0.2955s, grad.norm=13.82398319
 18506: 13 [ 1255/ 1327], train_loss/perplexity = 4.09935284/60.3012505 secs/batch = 0.2949s, grad.norm=13.91820812
 18511: 13 [ 1260/ 1327], train_loss/perplexity = 3.85950422/47.4418259 secs/batch = 0.3017s, grad.norm=15.05412674
 18516: 13 [ 1265/ 1327], train_loss/perplexity = 4.09865522/60.2591972 secs/batch = 0.3000s, grad.norm=14.78438568
 18521: 13 [ 1270/ 1327], train_loss/perplexity = 3.81352568/45.3099060 secs/batch = 0.2947s, grad.norm=15.11621761
 18526: 13 [ 1275/ 1327], train_loss/perplexity = 4.04792738/57.2786179 secs/batch = 0.2960s, grad.norm=15.38627434
 18531: 13 [ 1280/ 1327], train_loss/perplexity = 3.84970307/46.9791107 secs/batch = 0.2956s, grad.norm=15.06982803
 18536: 13 [ 1285/ 1327], train_loss/perplexity = 3.85628486/47.2893372 secs/batch = 0.3017s, grad.norm=14.85424805
 18541: 13 [ 1290/ 1327], train_loss/perplexity = 4.03954029/56.8002243 secs/batch = 0.2930s, grad.norm=14.48383045
 18546: 13 [ 1295/ 1327], train_loss/perplexity = 4.00555086/54.9020576 secs/batch = 0.2996s, grad.norm=14.25743103
 18551: 13 [ 1300/ 1327], train_loss/perplexity = 4.17909765/65.3069000 secs/batch = 0.2950s, grad.norm=14.54571915
 18556: 13 [ 1305/ 1327], train_loss/perplexity = 4.24068689/69.4555435 secs/batch = 0.2955s, grad.norm=15.22146320
 18561: 13 [ 1310/ 1327], train_loss/perplexity = 4.51271629/91.1691208 secs/batch = 0.2953s, grad.norm=15.21769905
 18566: 13 [ 1315/ 1327], train_loss/perplexity = 4.28634882/72.7005386 secs/batch = 0.2961s, grad.norm=15.03809452
 18571: 13 [ 1320/ 1327], train_loss/perplexity = 4.31723356/74.9809113 secs/batch = 0.3005s, grad.norm=14.95374775
 18576: 13 [ 1325/ 1327], train_loss/perplexity = 4.21168470/67.4701080 secs/batch = 0.2947s, grad.norm=15.40443897
Epoch training time: 393.15151023864746
	> validation loss = 4.63499117, perplexity = 103.02700806
	> validation loss = 4.56768608, perplexity = 96.32097626
	> validation loss = 4.57668352, perplexity = 97.19152832
	> validation loss = 4.54046440, perplexity = 93.73432159
	> validation loss = 4.69467258, perplexity = 109.36299896
	> validation loss = 4.65550327, perplexity = 105.16213226
	> validation loss = 4.59494114, perplexity = 98.98230743
	> validation loss = 4.42956638, perplexity = 83.89502716
	> validation loss = 4.22476101, perplexity = 68.35816193
	> validation loss = 4.34911823, perplexity = 77.41017151
	> validation loss = 4.52208042, perplexity = 92.02685547
	> validation loss = 4.47772789, perplexity = 88.03442383
	> validation loss = 4.46144390, perplexity = 86.61248016
	> validation loss = 4.22298384, perplexity = 68.23678589
	> validation loss = 4.18622255, perplexity = 65.77386475
	> validation loss = 4.17582369, perplexity = 65.09343719
	> validation loss = 4.60074043, perplexity = 99.55800629
	> validation loss = 4.11298180, perplexity = 61.12871933
	> validation loss = 4.59784412, perplexity = 99.27007294
	> validation loss = 4.53955173, perplexity = 93.64881134
	> validation loss = 4.26945734, perplexity = 71.48283386
at the end of epoch: 13
train loss = 4.14882080, perplexity = 63.35924324
validation loss = 4.45690698, perplexity = 86.22041453
Saved model cv/epoch013_4.4569.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.5
new learning rate is: 0.25
 18583: 14 [    5/ 1327], train_loss/perplexity = 4.26674175/71.2889786 secs/batch = 0.2945s, grad.norm=14.93902493
 18588: 14 [   10/ 1327], train_loss/perplexity = 3.88015890/48.4319115 secs/batch = 0.2934s, grad.norm=14.18791008
 18593: 14 [   15/ 1327], train_loss/perplexity = 4.18938684/65.9823227 secs/batch = 0.2963s, grad.norm=13.66911507
 18598: 14 [   20/ 1327], train_loss/perplexity = 4.33572865/76.3805923 secs/batch = 0.2972s, grad.norm=14.24490356
 18603: 14 [   25/ 1327], train_loss/perplexity = 4.23197889/68.8533478 secs/batch = 0.2964s, grad.norm=15.08966160
 18608: 14 [   30/ 1327], train_loss/perplexity = 4.31606197/74.8931122 secs/batch = 0.2981s, grad.norm=15.26788616
 18613: 14 [   35/ 1327], train_loss/perplexity = 4.05189991/57.5066109 secs/batch = 0.2955s, grad.norm=14.58646393
 18618: 14 [   40/ 1327], train_loss/perplexity = 4.09951687/60.3111420 secs/batch = 0.2997s, grad.norm=14.84918785
 18623: 14 [   45/ 1327], train_loss/perplexity = 3.86327195/47.6209106 secs/batch = 0.2939s, grad.norm=13.59989071
 18628: 14 [   50/ 1327], train_loss/perplexity = 4.03696489/56.6541290 secs/batch = 0.2945s, grad.norm=14.59500217
 18633: 14 [   55/ 1327], train_loss/perplexity = 3.95538092/52.2155800 secs/batch = 0.2954s, grad.norm=14.92760658
 18638: 14 [   60/ 1327], train_loss/perplexity = 4.27989149/72.2326050 secs/batch = 0.2995s, grad.norm=14.66822720
 18643: 14 [   65/ 1327], train_loss/perplexity = 3.86860466/47.8755379 secs/batch = 0.3000s, grad.norm=14.04017639
 18648: 14 [   70/ 1327], train_loss/perplexity = 3.69715548/40.3324127 secs/batch = 0.3005s, grad.norm=14.62226868
 18653: 14 [   75/ 1327], train_loss/perplexity = 3.54538417/34.6529961 secs/batch = 0.2994s, grad.norm=14.37307549
 18658: 14 [   80/ 1327], train_loss/perplexity = 3.97760916/53.3892365 secs/batch = 0.2944s, grad.norm=14.63034725
 18663: 14 [   85/ 1327], train_loss/perplexity = 3.97342539/53.1663361 secs/batch = 0.2937s, grad.norm=14.68394661
 18668: 14 [   90/ 1327], train_loss/perplexity = 4.07961988/59.1229935 secs/batch = 0.2953s, grad.norm=14.88540840
 18673: 14 [   95/ 1327], train_loss/perplexity = 3.90385866/49.5934448 secs/batch = 0.2949s, grad.norm=14.48386288
 18678: 14 [  100/ 1327], train_loss/perplexity = 4.22708130/68.5169601 secs/batch = 0.3003s, grad.norm=15.22151184
 18683: 14 [  105/ 1327], train_loss/perplexity = 3.90812349/49.8054047 secs/batch = 0.3014s, grad.norm=15.28258610
 18688: 14 [  110/ 1327], train_loss/perplexity = 3.86850262/47.8706512 secs/batch = 0.2977s, grad.norm=14.49127865
 18693: 14 [  115/ 1327], train_loss/perplexity = 3.92997050/50.9054756 secs/batch = 0.2952s, grad.norm=15.32623386
 18698: 14 [  120/ 1327], train_loss/perplexity = 4.01195240/55.2546463 secs/batch = 0.2935s, grad.norm=15.12737465
 18703: 14 [  125/ 1327], train_loss/perplexity = 4.02490902/55.9752159 secs/batch = 0.2937s, grad.norm=15.12278652
 18708: 14 [  130/ 1327], train_loss/perplexity = 3.95439005/52.1638680 secs/batch = 0.2994s, grad.norm=15.50925922
 18713: 14 [  135/ 1327], train_loss/perplexity = 4.07068014/58.5968018 secs/batch = 0.2970s, grad.norm=14.30167770
 18718: 14 [  140/ 1327], train_loss/perplexity = 4.33152199/76.0599594 secs/batch = 0.2970s, grad.norm=15.26100349
 18723: 14 [  145/ 1327], train_loss/perplexity = 4.09348011/59.9481544 secs/batch = 0.3009s, grad.norm=15.48750496
 18728: 14 [  150/ 1327], train_loss/perplexity = 4.17414379/64.9841766 secs/batch = 0.2954s, grad.norm=14.80974102
 18733: 14 [  155/ 1327], train_loss/perplexity = 4.43495560/84.3483810 secs/batch = 0.2992s, grad.norm=14.79497242
 18738: 14 [  160/ 1327], train_loss/perplexity = 4.10154581/60.4336357 secs/batch = 0.2961s, grad.norm=14.02254486
 18743: 14 [  165/ 1327], train_loss/perplexity = 4.29640579/73.4353790 secs/batch = 0.3012s, grad.norm=14.74940109
 18748: 14 [  170/ 1327], train_loss/perplexity = 4.04313755/57.0049171 secs/batch = 0.2948s, grad.norm=13.81157780
 18753: 14 [  175/ 1327], train_loss/perplexity = 4.32471657/75.5440979 secs/batch = 0.3004s, grad.norm=14.49389553
 18758: 14 [  180/ 1327], train_loss/perplexity = 4.10098505/60.3997536 secs/batch = 0.2955s, grad.norm=14.71030045
 18763: 14 [  185/ 1327], train_loss/perplexity = 4.43047857/83.9715958 secs/batch = 0.2950s, grad.norm=14.62812328
 18768: 14 [  190/ 1327], train_loss/perplexity = 3.97951627/53.4911537 secs/batch = 0.2959s, grad.norm=13.73816109
 18773: 14 [  195/ 1327], train_loss/perplexity = 4.29162741/73.0853119 secs/batch = 0.3003s, grad.norm=14.11300278
 18778: 14 [  200/ 1327], train_loss/perplexity = 4.10015011/60.3493462 secs/batch = 0.2946s, grad.norm=14.98736763
 18783: 14 [  205/ 1327], train_loss/perplexity = 4.34878492/77.3843765 secs/batch = 0.2984s, grad.norm=14.63629627
 18788: 14 [  210/ 1327], train_loss/perplexity = 4.22249222/68.2032471 secs/batch = 0.2960s, grad.norm=13.92698956
 18793: 14 [  215/ 1327], train_loss/perplexity = 4.36107063/78.3409653 secs/batch = 0.2954s, grad.norm=14.00954247
 18798: 14 [  220/ 1327], train_loss/perplexity = 4.23104763/68.7892609 secs/batch = 0.2963s, grad.norm=13.99573231
 18803: 14 [  225/ 1327], train_loss/perplexity = 4.37651539/79.5603104 secs/batch = 0.3003s, grad.norm=14.87435246
 18808: 14 [  230/ 1327], train_loss/perplexity = 4.26174402/70.9335861 secs/batch = 0.3016s, grad.norm=15.07870960
 18813: 14 [  235/ 1327], train_loss/perplexity = 4.13281775/62.3533707 secs/batch = 0.2994s, grad.norm=14.49805260
 18818: 14 [  240/ 1327], train_loss/perplexity = 3.83450651/46.2705879 secs/batch = 0.2958s, grad.norm=14.61352634
 18823: 14 [  245/ 1327], train_loss/perplexity = 4.15101242/63.4982529 secs/batch = 0.2959s, grad.norm=14.72148514
 18828: 14 [  250/ 1327], train_loss/perplexity = 4.06874371/58.4834442 secs/batch = 0.2951s, grad.norm=13.93155098
 18833: 14 [  255/ 1327], train_loss/perplexity = 3.92180276/50.4913864 secs/batch = 0.3009s, grad.norm=14.04229641
 18838: 14 [  260/ 1327], train_loss/perplexity = 4.19622517/66.4350739 secs/batch = 0.2991s, grad.norm=14.92764759
 18843: 14 [  265/ 1327], train_loss/perplexity = 4.44092369/84.8532867 secs/batch = 0.2961s, grad.norm=13.77966499
 18848: 14 [  270/ 1327], train_loss/perplexity = 4.45444679/86.0085602 secs/batch = 0.2954s, grad.norm=13.88637924
 18853: 14 [  275/ 1327], train_loss/perplexity = 4.37307644/79.2871780 secs/batch = 0.3009s, grad.norm=14.94127274
 18858: 14 [  280/ 1327], train_loss/perplexity = 4.13534975/62.5114517 secs/batch = 0.3015s, grad.norm=14.56607246
 18863: 14 [  285/ 1327], train_loss/perplexity = 4.51013231/90.9338531 secs/batch = 0.3000s, grad.norm=14.54280758
 18868: 14 [  290/ 1327], train_loss/perplexity = 4.10940361/60.9103813 secs/batch = 0.2939s, grad.norm=14.69203186
 18873: 14 [  295/ 1327], train_loss/perplexity = 3.97646141/53.3279953 secs/batch = 0.2950s, grad.norm=14.56274891
 18878: 14 [  300/ 1327], train_loss/perplexity = 3.53920269/34.4394493 secs/batch = 0.2939s, grad.norm=13.79714966
 18883: 14 [  305/ 1327], train_loss/perplexity = 4.05953264/57.9472237 secs/batch = 0.2959s, grad.norm=14.63165855
 18888: 14 [  310/ 1327], train_loss/perplexity = 4.01740789/55.5569115 secs/batch = 0.3011s, grad.norm=14.50793552
 18893: 14 [  315/ 1327], train_loss/perplexity = 3.56063414/35.1855011 secs/batch = 0.2947s, grad.norm=13.73710155
 18898: 14 [  320/ 1327], train_loss/perplexity = 3.52141428/33.8322411 secs/batch = 0.2931s, grad.norm=15.05724525
 18903: 14 [  325/ 1327], train_loss/perplexity = 3.45456076/31.6443863 secs/batch = 0.2965s, grad.norm=13.38658619
 18908: 14 [  330/ 1327], train_loss/perplexity = 4.12270832/61.7261887 secs/batch = 0.2945s, grad.norm=14.76821518
 18913: 14 [  335/ 1327], train_loss/perplexity = 3.56056547/35.1830864 secs/batch = 0.3024s, grad.norm=13.62750435
 18918: 14 [  340/ 1327], train_loss/perplexity = 4.26011467/70.8181000 secs/batch = 0.2936s, grad.norm=14.23239613
 18923: 14 [  345/ 1327], train_loss/perplexity = 4.10207510/60.4656296 secs/batch = 0.2962s, grad.norm=13.92753124
 18928: 14 [  350/ 1327], train_loss/perplexity = 4.06939077/58.5212975 secs/batch = 0.2999s, grad.norm=15.21082687
 18933: 14 [  355/ 1327], train_loss/perplexity = 4.08959913/59.7159500 secs/batch = 0.2961s, grad.norm=14.54732895
 18938: 14 [  360/ 1327], train_loss/perplexity = 4.14956760/63.4065781 secs/batch = 0.3000s, grad.norm=16.45101547
 18943: 14 [  365/ 1327], train_loss/perplexity = 4.24875069/70.0178833 secs/batch = 0.2941s, grad.norm=14.79041100
 18948: 14 [  370/ 1327], train_loss/perplexity = 4.23546267/69.0936356 secs/batch = 0.2939s, grad.norm=14.60201740
 18953: 14 [  375/ 1327], train_loss/perplexity = 3.62500668/37.5249748 secs/batch = 0.3010s, grad.norm=14.71104050
 18958: 14 [  380/ 1327], train_loss/perplexity = 3.70704103/40.7331009 secs/batch = 0.2957s, grad.norm=15.07740593
 18963: 14 [  385/ 1327], train_loss/perplexity = 3.98284173/53.6693306 secs/batch = 0.3016s, grad.norm=15.11022472
 18968: 14 [  390/ 1327], train_loss/perplexity = 4.12928867/62.1337090 secs/batch = 0.2947s, grad.norm=14.76219940
 18973: 14 [  395/ 1327], train_loss/perplexity = 4.08076954/59.1910019 secs/batch = 0.2934s, grad.norm=14.96588039
 18978: 14 [  400/ 1327], train_loss/perplexity = 4.02140903/55.7796478 secs/batch = 0.2959s, grad.norm=15.02462578
 18983: 14 [  405/ 1327], train_loss/perplexity = 4.40897560/82.1852264 secs/batch = 0.2940s, grad.norm=14.78462791
 18988: 14 [  410/ 1327], train_loss/perplexity = 3.93114543/50.9653206 secs/batch = 0.3012s, grad.norm=14.35570812
 18993: 14 [  415/ 1327], train_loss/perplexity = 3.94613385/51.7349663 secs/batch = 0.2935s, grad.norm=14.55980206
 18998: 14 [  420/ 1327], train_loss/perplexity = 3.50420976/33.2551537 secs/batch = 0.2959s, grad.norm=14.37430573
 19003: 14 [  425/ 1327], train_loss/perplexity = 3.93212843/51.0154457 secs/batch = 0.2960s, grad.norm=15.27563477
 19008: 14 [  430/ 1327], train_loss/perplexity = 4.11430740/61.2098045 secs/batch = 0.2956s, grad.norm=15.51255417
 19013: 14 [  435/ 1327], train_loss/perplexity = 4.16960287/64.6897583 secs/batch = 0.3005s, grad.norm=15.24486446
 19018: 14 [  440/ 1327], train_loss/perplexity = 3.68512154/39.8499641 secs/batch = 0.2938s, grad.norm=14.56425476
 19023: 14 [  445/ 1327], train_loss/perplexity = 4.03091145/56.3122139 secs/batch = 0.2956s, grad.norm=15.24462891
 19028: 14 [  450/ 1327], train_loss/perplexity = 4.02711773/56.0989838 secs/batch = 0.3028s, grad.norm=14.85656166
 19033: 14 [  455/ 1327], train_loss/perplexity = 3.98186970/53.6171875 secs/batch = 0.3014s, grad.norm=14.42291641
 19038: 14 [  460/ 1327], train_loss/perplexity = 4.02055359/55.7319489 secs/batch = 0.2970s, grad.norm=15.32060242
 19043: 14 [  465/ 1327], train_loss/perplexity = 3.65355492/38.6116829 secs/batch = 0.2993s, grad.norm=15.66689205
 19048: 14 [  470/ 1327], train_loss/perplexity = 4.46731567/87.1225433 secs/batch = 0.2952s, grad.norm=15.08387089
 19053: 14 [  475/ 1327], train_loss/perplexity = 3.84848976/46.9221458 secs/batch = 0.3009s, grad.norm=14.96686840
 19058: 14 [  480/ 1327], train_loss/perplexity = 3.91766167/50.2827301 secs/batch = 0.3005s, grad.norm=14.85469151
 19063: 14 [  485/ 1327], train_loss/perplexity = 3.89400887/49.1073570 secs/batch = 0.2943s, grad.norm=14.52833366
 19068: 14 [  490/ 1327], train_loss/perplexity = 3.86171675/47.5469055 secs/batch = 0.2992s, grad.norm=15.57932281
 19073: 14 [  495/ 1327], train_loss/perplexity = 3.96253538/52.5904922 secs/batch = 0.2932s, grad.norm=14.59579086
 19078: 14 [  500/ 1327], train_loss/perplexity = 4.09691238/60.1542664 secs/batch = 0.2952s, grad.norm=14.90820503
 19083: 14 [  505/ 1327], train_loss/perplexity = 4.20905972/67.2932358 secs/batch = 0.2958s, grad.norm=13.73414135
 19088: 14 [  510/ 1327], train_loss/perplexity = 4.53976822/93.6690903 secs/batch = 0.2964s, grad.norm=14.42784500
 19093: 14 [  515/ 1327], train_loss/perplexity = 4.16365051/64.3058472 secs/batch = 0.2961s, grad.norm=14.32496643
 19098: 14 [  520/ 1327], train_loss/perplexity = 4.26383972/71.0823975 secs/batch = 0.2951s, grad.norm=14.61027908
 19103: 14 [  525/ 1327], train_loss/perplexity = 3.94900990/51.8839722 secs/batch = 0.2968s, grad.norm=14.32959175
 19108: 14 [  530/ 1327], train_loss/perplexity = 3.95208549/52.0437889 secs/batch = 0.2997s, grad.norm=15.30324936
 19113: 14 [  535/ 1327], train_loss/perplexity = 4.03977251/56.8134155 secs/batch = 0.2958s, grad.norm=14.73952579
 19118: 14 [  540/ 1327], train_loss/perplexity = 4.15635872/63.8386459 secs/batch = 0.2950s, grad.norm=14.45789242
 19123: 14 [  545/ 1327], train_loss/perplexity = 4.07596016/58.9070129 secs/batch = 0.3020s, grad.norm=15.21526909
 19128: 14 [  550/ 1327], train_loss/perplexity = 4.11097670/61.0062714 secs/batch = 0.2942s, grad.norm=14.79569817
 19133: 14 [  555/ 1327], train_loss/perplexity = 3.92516279/50.6613235 secs/batch = 0.2934s, grad.norm=14.01094627
 19138: 14 [  560/ 1327], train_loss/perplexity = 4.07726717/58.9840546 secs/batch = 0.2949s, grad.norm=15.59886169
 19143: 14 [  565/ 1327], train_loss/perplexity = 3.92936826/50.8748283 secs/batch = 0.2997s, grad.norm=15.36232185
 19148: 14 [  570/ 1327], train_loss/perplexity = 3.92795634/50.8030472 secs/batch = 0.2961s, grad.norm=15.26946163
 19153: 14 [  575/ 1327], train_loss/perplexity = 3.69718790/40.3337212 secs/batch = 0.2943s, grad.norm=15.07456112
 19158: 14 [  580/ 1327], train_loss/perplexity = 4.05654335/57.7742615 secs/batch = 0.2956s, grad.norm=14.79522228
 19163: 14 [  585/ 1327], train_loss/perplexity = 3.73391342/41.8425369 secs/batch = 0.2956s, grad.norm=14.61248302
 19168: 14 [  590/ 1327], train_loss/perplexity = 4.08233976/59.2840195 secs/batch = 0.2946s, grad.norm=14.76770115
 19173: 14 [  595/ 1327], train_loss/perplexity = 4.01006031/55.1501961 secs/batch = 0.2954s, grad.norm=14.94396210
 19178: 14 [  600/ 1327], train_loss/perplexity = 4.22006416/68.0378494 secs/batch = 0.2946s, grad.norm=14.06693459
 19183: 14 [  605/ 1327], train_loss/perplexity = 4.12548685/61.8979378 secs/batch = 0.2949s, grad.norm=14.56682587
 19188: 14 [  610/ 1327], train_loss/perplexity = 4.33903694/76.6336975 secs/batch = 0.2953s, grad.norm=14.84753513
 19193: 14 [  615/ 1327], train_loss/perplexity = 3.89951515/49.3785019 secs/batch = 0.3004s, grad.norm=14.05822659
 19198: 14 [  620/ 1327], train_loss/perplexity = 4.32061529/75.2349091 secs/batch = 0.2954s, grad.norm=14.89390469
 19203: 14 [  625/ 1327], train_loss/perplexity = 4.25276566/70.2995682 secs/batch = 0.2945s, grad.norm=14.18155766
 19208: 14 [  630/ 1327], train_loss/perplexity = 4.29718447/73.4925842 secs/batch = 0.3008s, grad.norm=14.66148567
 19213: 14 [  635/ 1327], train_loss/perplexity = 4.06267548/58.1296272 secs/batch = 0.2934s, grad.norm=14.93474102
 19218: 14 [  640/ 1327], train_loss/perplexity = 4.02596283/56.0342331 secs/batch = 0.2992s, grad.norm=14.84160805
 19223: 14 [  645/ 1327], train_loss/perplexity = 4.30553722/74.1090164 secs/batch = 0.2954s, grad.norm=15.25621414
 19228: 14 [  650/ 1327], train_loss/perplexity = 3.80829144/45.0733604 secs/batch = 0.2949s, grad.norm=15.13661671
 19233: 14 [  655/ 1327], train_loss/perplexity = 3.93043590/50.9291725 secs/batch = 0.2960s, grad.norm=14.49474430
 19238: 14 [  660/ 1327], train_loss/perplexity = 3.88709164/48.7688408 secs/batch = 0.3002s, grad.norm=14.54837227
 19243: 14 [  665/ 1327], train_loss/perplexity = 4.02426910/55.9394073 secs/batch = 0.2927s, grad.norm=14.52353001
 19248: 14 [  670/ 1327], train_loss/perplexity = 3.97096157/53.0355034 secs/batch = 0.2943s, grad.norm=14.90326977
 19253: 14 [  675/ 1327], train_loss/perplexity = 3.76334810/43.0924644 secs/batch = 0.2951s, grad.norm=14.93917084
 19258: 14 [  680/ 1327], train_loss/perplexity = 3.94098282/51.4691620 secs/batch = 0.3002s, grad.norm=15.30007362
 19263: 14 [  685/ 1327], train_loss/perplexity = 3.78998470/44.2557220 secs/batch = 0.2946s, grad.norm=14.65322208
 19268: 14 [  690/ 1327], train_loss/perplexity = 4.22192955/68.1648865 secs/batch = 0.2960s, grad.norm=14.60015297
 19273: 14 [  695/ 1327], train_loss/perplexity = 4.08856297/59.6541061 secs/batch = 0.3010s, grad.norm=14.60696220
 19278: 14 [  700/ 1327], train_loss/perplexity = 4.23151970/68.8217392 secs/batch = 0.3002s, grad.norm=15.21220112
 19283: 14 [  705/ 1327], train_loss/perplexity = 4.03519726/56.5540733 secs/batch = 0.3019s, grad.norm=14.38128567
 19288: 14 [  710/ 1327], train_loss/perplexity = 3.89881349/49.3438683 secs/batch = 0.2956s, grad.norm=15.55708790
 19293: 14 [  715/ 1327], train_loss/perplexity = 3.78344345/43.9671783 secs/batch = 0.2925s, grad.norm=14.84152985
 19298: 14 [  720/ 1327], train_loss/perplexity = 3.75568986/42.7637100 secs/batch = 0.3024s, grad.norm=14.99129963
 19303: 14 [  725/ 1327], train_loss/perplexity = 3.84032345/46.5405273 secs/batch = 0.3007s, grad.norm=14.93194389
 19308: 14 [  730/ 1327], train_loss/perplexity = 3.98982358/54.0453529 secs/batch = 0.2960s, grad.norm=15.06414032
 19313: 14 [  735/ 1327], train_loss/perplexity = 4.04043484/56.8510590 secs/batch = 0.2938s, grad.norm=15.20071793
 19318: 14 [  740/ 1327], train_loss/perplexity = 3.56130433/35.2090912 secs/batch = 0.2986s, grad.norm=14.23455524
 19323: 14 [  745/ 1327], train_loss/perplexity = 4.03858519/56.7460022 secs/batch = 0.2924s, grad.norm=14.99072742
 19328: 14 [  750/ 1327], train_loss/perplexity = 3.94803524/51.8334274 secs/batch = 0.2956s, grad.norm=15.07200527
 19333: 14 [  755/ 1327], train_loss/perplexity = 3.76145959/43.0111580 secs/batch = 0.2940s, grad.norm=13.88644981
 19338: 14 [  760/ 1327], train_loss/perplexity = 3.63409638/37.8676186 secs/batch = 0.2986s, grad.norm=13.77997875
 19343: 14 [  765/ 1327], train_loss/perplexity = 3.80240488/44.8088150 secs/batch = 0.2941s, grad.norm=14.19203568
 19348: 14 [  770/ 1327], train_loss/perplexity = 3.67428660/39.4205246 secs/batch = 0.2934s, grad.norm=14.31106091
 19353: 14 [  775/ 1327], train_loss/perplexity = 3.80708337/45.0189438 secs/batch = 0.2946s, grad.norm=14.56119156
 19358: 14 [  780/ 1327], train_loss/perplexity = 4.18665886/65.8025665 secs/batch = 0.2946s, grad.norm=15.31216908
 19363: 14 [  785/ 1327], train_loss/perplexity = 4.08391762/59.3776321 secs/batch = 0.2931s, grad.norm=15.18793869
 19368: 14 [  790/ 1327], train_loss/perplexity = 3.76980114/43.3714409 secs/batch = 0.2953s, grad.norm=14.66620255
 19373: 14 [  795/ 1327], train_loss/perplexity = 4.12463808/61.8454208 secs/batch = 0.2988s, grad.norm=14.86301231
 19378: 14 [  800/ 1327], train_loss/perplexity = 4.03605556/56.6026382 secs/batch = 0.3002s, grad.norm=15.30986977
 19383: 14 [  805/ 1327], train_loss/perplexity = 4.35425901/77.8091507 secs/batch = 0.2956s, grad.norm=14.81428909
 19388: 14 [  810/ 1327], train_loss/perplexity = 3.97383857/53.1883049 secs/batch = 0.3001s, grad.norm=13.97504044
 19393: 14 [  815/ 1327], train_loss/perplexity = 3.81379414/45.3220711 secs/batch = 0.2950s, grad.norm=14.41582203
 19398: 14 [  820/ 1327], train_loss/perplexity = 3.77694559/43.6824150 secs/batch = 0.2950s, grad.norm=13.89895916
 19403: 14 [  825/ 1327], train_loss/perplexity = 4.01075506/55.1885262 secs/batch = 0.2953s, grad.norm=14.67312622
 19408: 14 [  830/ 1327], train_loss/perplexity = 3.71250629/40.9563255 secs/batch = 0.2949s, grad.norm=14.81627464
 19413: 14 [  835/ 1327], train_loss/perplexity = 3.91015863/49.9068680 secs/batch = 0.3018s, grad.norm=14.97222614
 19418: 14 [  840/ 1327], train_loss/perplexity = 3.99478889/54.3143730 secs/batch = 0.2920s, grad.norm=14.56376648
 19423: 14 [  845/ 1327], train_loss/perplexity = 3.89006233/48.9139366 secs/batch = 0.2939s, grad.norm=15.24545288
 19428: 14 [  850/ 1327], train_loss/perplexity = 3.99868584/54.5264473 secs/batch = 0.2922s, grad.norm=14.52738953
 19433: 14 [  855/ 1327], train_loss/perplexity = 4.01981258/55.6906662 secs/batch = 0.2962s, grad.norm=15.46993256
 19438: 14 [  860/ 1327], train_loss/perplexity = 3.75493622/42.7314949 secs/batch = 0.2987s, grad.norm=14.37377453
 19443: 14 [  865/ 1327], train_loss/perplexity = 4.08291435/59.3180923 secs/batch = 0.2988s, grad.norm=14.91806507
 19448: 14 [  870/ 1327], train_loss/perplexity = 4.02592754/56.0322571 secs/batch = 0.2941s, grad.norm=15.15716743
 19453: 14 [  875/ 1327], train_loss/perplexity = 3.56170678/35.2232628 secs/batch = 0.2923s, grad.norm=13.86689472
 19458: 14 [  880/ 1327], train_loss/perplexity = 3.80516434/44.9326324 secs/batch = 0.2972s, grad.norm=14.18028069
 19463: 14 [  885/ 1327], train_loss/perplexity = 3.99254990/54.1928978 secs/batch = 0.2955s, grad.norm=14.41893387
 19468: 14 [  890/ 1327], train_loss/perplexity = 4.15103817/63.4998894 secs/batch = 0.2999s, grad.norm=14.30605125
 19473: 14 [  895/ 1327], train_loss/perplexity = 4.05710506/57.8067207 secs/batch = 0.2950s, grad.norm=14.35336494
 19478: 14 [  900/ 1327], train_loss/perplexity = 3.92455959/50.6307755 secs/batch = 0.2938s, grad.norm=14.16926098
 19483: 14 [  905/ 1327], train_loss/perplexity = 3.82795525/45.9684486 secs/batch = 0.2944s, grad.norm=14.20925903
 19488: 14 [  910/ 1327], train_loss/perplexity = 3.85344720/47.1553383 secs/batch = 0.2955s, grad.norm=13.84663868
 19493: 14 [  915/ 1327], train_loss/perplexity = 4.07961512/59.1227112 secs/batch = 0.3011s, grad.norm=14.00850582
 19498: 14 [  920/ 1327], train_loss/perplexity = 4.24188757/69.5389862 secs/batch = 0.2956s, grad.norm=15.18605423
 19503: 14 [  925/ 1327], train_loss/perplexity = 4.03846836/56.7393723 secs/batch = 0.2993s, grad.norm=14.06088924
 19508: 14 [  930/ 1327], train_loss/perplexity = 4.08618164/59.5122185 secs/batch = 0.2984s, grad.norm=14.62360191
 19513: 14 [  935/ 1327], train_loss/perplexity = 4.16525173/64.4088974 secs/batch = 0.2999s, grad.norm=14.35752201
 19518: 14 [  940/ 1327], train_loss/perplexity = 4.07029247/58.5740929 secs/batch = 0.2948s, grad.norm=14.05373192
 19523: 14 [  945/ 1327], train_loss/perplexity = 4.28663635/72.7214508 secs/batch = 0.2949s, grad.norm=14.18307590
 19528: 14 [  950/ 1327], train_loss/perplexity = 4.01918936/55.6559715 secs/batch = 0.2951s, grad.norm=14.59718037
 19533: 14 [  955/ 1327], train_loss/perplexity = 4.01798344/55.5888939 secs/batch = 0.2940s, grad.norm=14.15907383
 19538: 14 [  960/ 1327], train_loss/perplexity = 4.26952744/71.4878464 secs/batch = 0.3003s, grad.norm=14.78137112
 19543: 14 [  965/ 1327], train_loss/perplexity = 4.04830790/57.3004150 secs/batch = 0.2938s, grad.norm=15.06731129
 19548: 14 [  970/ 1327], train_loss/perplexity = 4.36607027/78.7336197 secs/batch = 0.2982s, grad.norm=14.98424053
 19553: 14 [  975/ 1327], train_loss/perplexity = 3.94852972/51.8590622 secs/batch = 0.3016s, grad.norm=15.51173401
 19558: 14 [  980/ 1327], train_loss/perplexity = 3.81569910/45.4084892 secs/batch = 0.2976s, grad.norm=14.02391434
 19563: 14 [  985/ 1327], train_loss/perplexity = 3.97868943/53.4469414 secs/batch = 0.2941s, grad.norm=15.25522232
 19568: 14 [  990/ 1327], train_loss/perplexity = 4.13321066/62.3778763 secs/batch = 0.2950s, grad.norm=14.86724567
 19573: 14 [  995/ 1327], train_loss/perplexity = 4.21190071/67.4846878 secs/batch = 0.2948s, grad.norm=14.54397297
 19578: 14 [ 1000/ 1327], train_loss/perplexity = 3.76772738/43.2815895 secs/batch = 0.2989s, grad.norm=14.19018173
 19583: 14 [ 1005/ 1327], train_loss/perplexity = 4.15558004/63.7889557 secs/batch = 0.2961s, grad.norm=14.98550129
 19588: 14 [ 1010/ 1327], train_loss/perplexity = 3.75904083/42.9072495 secs/batch = 0.2965s, grad.norm=13.87622929
 19593: 14 [ 1015/ 1327], train_loss/perplexity = 4.22133255/68.1242065 secs/batch = 0.3004s, grad.norm=14.49164009
 19598: 14 [ 1020/ 1327], train_loss/perplexity = 4.34910536/77.4091797 secs/batch = 0.2974s, grad.norm=14.82367706
 19603: 14 [ 1025/ 1327], train_loss/perplexity = 4.27139854/71.6217346 secs/batch = 0.3020s, grad.norm=14.16323662
 19608: 14 [ 1030/ 1327], train_loss/perplexity = 4.02561378/56.0146790 secs/batch = 0.2933s, grad.norm=14.32833767
 19613: 14 [ 1035/ 1327], train_loss/perplexity = 3.95623732/52.2603149 secs/batch = 0.2945s, grad.norm=14.28011608
 19618: 14 [ 1040/ 1327], train_loss/perplexity = 4.13786936/62.6691551 secs/batch = 0.2992s, grad.norm=15.74541759
 19623: 14 [ 1045/ 1327], train_loss/perplexity = 3.65946436/38.8405342 secs/batch = 0.2947s, grad.norm=13.77067375
 19628: 14 [ 1050/ 1327], train_loss/perplexity = 3.82315779/45.7484436 secs/batch = 0.2961s, grad.norm=14.53561306
 19633: 14 [ 1055/ 1327], train_loss/perplexity = 3.86544847/47.7246704 secs/batch = 0.3008s, grad.norm=15.02829456
 19638: 14 [ 1060/ 1327], train_loss/perplexity = 3.49620366/32.9899712 secs/batch = 0.2927s, grad.norm=15.44495678
 19643: 14 [ 1065/ 1327], train_loss/perplexity = 3.65067911/38.5008049 secs/batch = 0.2932s, grad.norm=14.69412804
 19648: 14 [ 1070/ 1327], train_loss/perplexity = 3.94242120/51.5432472 secs/batch = 0.2946s, grad.norm=15.09259987
 19653: 14 [ 1075/ 1327], train_loss/perplexity = 3.78011894/43.8212547 secs/batch = 0.2929s, grad.norm=15.31353760
 19658: 14 [ 1080/ 1327], train_loss/perplexity = 3.77402496/43.5550194 secs/batch = 0.3002s, grad.norm=14.83472633
 19663: 14 [ 1085/ 1327], train_loss/perplexity = 3.60916877/36.9353371 secs/batch = 0.2953s, grad.norm=14.89263439
 19668: 14 [ 1090/ 1327], train_loss/perplexity = 3.78327608/43.9598236 secs/batch = 0.2942s, grad.norm=15.15266323
 19673: 14 [ 1095/ 1327], train_loss/perplexity = 3.94442511/51.6466370 secs/batch = 0.2994s, grad.norm=15.17406940
 19678: 14 [ 1100/ 1327], train_loss/perplexity = 3.63668346/37.9657135 secs/batch = 0.2934s, grad.norm=15.85058594
 19683: 14 [ 1105/ 1327], train_loss/perplexity = 3.62383819/37.4811516 secs/batch = 0.2947s, grad.norm=15.22229385
 19688: 14 [ 1110/ 1327], train_loss/perplexity = 4.02452850/55.9539223 secs/batch = 0.2964s, grad.norm=15.70088863
 19693: 14 [ 1115/ 1327], train_loss/perplexity = 3.71636844/41.1148109 secs/batch = 0.3020s, grad.norm=14.41586208
 19698: 14 [ 1120/ 1327], train_loss/perplexity = 4.02014160/55.7089920 secs/batch = 0.2937s, grad.norm=14.85593319
 19703: 14 [ 1125/ 1327], train_loss/perplexity = 4.23139286/68.8130112 secs/batch = 0.2954s, grad.norm=15.79737186
 19708: 14 [ 1130/ 1327], train_loss/perplexity = 3.80108500/44.7497101 secs/batch = 0.2946s, grad.norm=14.89826393
 19713: 14 [ 1135/ 1327], train_loss/perplexity = 3.74685049/42.3873711 secs/batch = 0.2960s, grad.norm=14.66480827
 19718: 14 [ 1140/ 1327], train_loss/perplexity = 4.09441948/60.0044937 secs/batch = 0.2951s, grad.norm=15.35625839
 19723: 14 [ 1145/ 1327], train_loss/perplexity = 3.93833804/51.3332176 secs/batch = 0.3011s, grad.norm=14.51807213
 19728: 14 [ 1150/ 1327], train_loss/perplexity = 3.88418603/48.6273460 secs/batch = 0.2957s, grad.norm=14.32481670
 19733: 14 [ 1155/ 1327], train_loss/perplexity = 3.96462512/52.7005081 secs/batch = 0.2954s, grad.norm=15.13626862
 19738: 14 [ 1160/ 1327], train_loss/perplexity = 3.91947937/50.3742104 secs/batch = 0.2942s, grad.norm=15.23767853
 19743: 14 [ 1165/ 1327], train_loss/perplexity = 4.00043583/54.6219521 secs/batch = 0.2963s, grad.norm=15.31869793
 19748: 14 [ 1170/ 1327], train_loss/perplexity = 3.79142523/44.3195190 secs/batch = 0.2944s, grad.norm=14.95268917
 19753: 14 [ 1175/ 1327], train_loss/perplexity = 3.58590555/36.0860214 secs/batch = 0.2938s, grad.norm=14.77770138
 19758: 14 [ 1180/ 1327], train_loss/perplexity = 3.64458537/38.2669029 secs/batch = 0.3022s, grad.norm=15.16367340
 19763: 14 [ 1185/ 1327], train_loss/perplexity = 3.84842968/46.9193268 secs/batch = 0.2967s, grad.norm=14.95511913
 19768: 14 [ 1190/ 1327], train_loss/perplexity = 3.90754700/49.7766991 secs/batch = 0.2959s, grad.norm=15.43375206
 19773: 14 [ 1195/ 1327], train_loss/perplexity = 3.67556238/39.4708481 secs/batch = 0.2979s, grad.norm=14.83484554
 19778: 14 [ 1200/ 1327], train_loss/perplexity = 3.68374109/39.7949944 secs/batch = 0.2947s, grad.norm=14.76324081
 19783: 14 [ 1205/ 1327], train_loss/perplexity = 3.73320246/41.8127975 secs/batch = 0.3011s, grad.norm=15.20866394
 19788: 14 [ 1210/ 1327], train_loss/perplexity = 3.31233573/27.4491653 secs/batch = 0.2974s, grad.norm=14.89716148
 19793: 14 [ 1215/ 1327], train_loss/perplexity = 3.58986449/36.2291679 secs/batch = 0.2993s, grad.norm=14.41259384
 19798: 14 [ 1220/ 1327], train_loss/perplexity = 3.68657517/39.9079361 secs/batch = 0.2949s, grad.norm=15.23901272
 19803: 14 [ 1225/ 1327], train_loss/perplexity = 3.42794251/30.8131790 secs/batch = 0.2969s, grad.norm=16.11304665
 19808: 14 [ 1230/ 1327], train_loss/perplexity = 3.76402926/43.1218262 secs/batch = 0.2942s, grad.norm=14.44884396
 19813: 14 [ 1235/ 1327], train_loss/perplexity = 3.61234403/37.0528030 secs/batch = 0.3017s, grad.norm=14.88188744
 19818: 14 [ 1240/ 1327], train_loss/perplexity = 3.91698241/50.2485847 secs/batch = 0.2971s, grad.norm=15.43850136
 19823: 14 [ 1245/ 1327], train_loss/perplexity = 3.80514050/44.9315605 secs/batch = 0.2942s, grad.norm=14.46471214
 19828: 14 [ 1250/ 1327], train_loss/perplexity = 3.99372602/54.2566757 secs/batch = 0.2957s, grad.norm=14.47769260
 19833: 14 [ 1255/ 1327], train_loss/perplexity = 3.97262716/53.1239128 secs/batch = 0.2957s, grad.norm=14.44477272
 19838: 14 [ 1260/ 1327], train_loss/perplexity = 3.78028250/43.8284225 secs/batch = 0.2954s, grad.norm=15.63796425
 19843: 14 [ 1265/ 1327], train_loss/perplexity = 3.93698740/51.2639313 secs/batch = 0.2950s, grad.norm=15.44910717
 19848: 14 [ 1270/ 1327], train_loss/perplexity = 3.66132498/38.9128685 secs/batch = 0.2913s, grad.norm=15.16220951
 19853: 14 [ 1275/ 1327], train_loss/perplexity = 3.86881113/47.8854218 secs/batch = 0.2967s, grad.norm=14.99256134
 19858: 14 [ 1280/ 1327], train_loss/perplexity = 3.78746319/44.1442719 secs/batch = 0.2952s, grad.norm=17.52905655
 19863: 14 [ 1285/ 1327], train_loss/perplexity = 3.68169355/39.7135925 secs/batch = 0.3017s, grad.norm=15.30817223
 19868: 14 [ 1290/ 1327], train_loss/perplexity = 3.85825396/47.3825455 secs/batch = 0.3012s, grad.norm=14.51730156
 19873: 14 [ 1295/ 1327], train_loss/perplexity = 3.90471363/49.6358643 secs/batch = 0.3007s, grad.norm=15.19280529
 19878: 14 [ 1300/ 1327], train_loss/perplexity = 4.06414700/58.2152290 secs/batch = 0.2948s, grad.norm=14.54665565
 19883: 14 [ 1305/ 1327], train_loss/perplexity = 4.12195158/61.6794968 secs/batch = 0.2940s, grad.norm=15.21181870
 19888: 14 [ 1310/ 1327], train_loss/perplexity = 4.39140987/80.7541885 secs/batch = 0.3000s, grad.norm=15.33519173
 19893: 14 [ 1315/ 1327], train_loss/perplexity = 4.19572735/66.4020081 secs/batch = 0.2999s, grad.norm=15.33505154
 19898: 14 [ 1320/ 1327], train_loss/perplexity = 4.12767076/62.0332642 secs/batch = 0.2990s, grad.norm=14.84809399
 19903: 14 [ 1325/ 1327], train_loss/perplexity = 4.02927732/56.2202682 secs/batch = 0.3018s, grad.norm=15.16289711
Epoch training time: 393.8227322101593
	> validation loss = 4.58410645, perplexity = 97.91565704
	> validation loss = 4.55691862, perplexity = 95.28940582
	> validation loss = 4.53921413, perplexity = 93.61720276
	> validation loss = 4.50825930, perplexity = 90.76368713
	> validation loss = 4.67967176, perplexity = 107.73470306
	> validation loss = 4.62289286, perplexity = 101.78806305
	> validation loss = 4.56123447, perplexity = 95.70154572
	> validation loss = 4.40176582, perplexity = 81.59482574
	> validation loss = 4.19435024, perplexity = 66.31063080
	> validation loss = 4.31708193, perplexity = 74.96954346
	> validation loss = 4.53073978, perplexity = 92.82720947
	> validation loss = 4.48070908, perplexity = 88.29725647
	> validation loss = 4.44480848, perplexity = 85.18356323
	> validation loss = 4.18990850, perplexity = 66.01675415
	> validation loss = 4.15007544, perplexity = 63.43878555
	> validation loss = 4.16234875, perplexity = 64.22219086
	> validation loss = 4.58821630, perplexity = 98.31890106
	> validation loss = 4.07592058, perplexity = 58.90468216
	> validation loss = 4.57104778, perplexity = 96.64531708
	> validation loss = 4.52652979, perplexity = 92.43722534
	> validation loss = 4.24907541, perplexity = 70.04062653
at the end of epoch: 14
train loss = 4.00856093, perplexity = 55.06756748
validation loss = 4.43017104, perplexity = 83.94577416
Saved model cv/epoch014_4.4302.model
 19910: 15 [    5/ 1327], train_loss/perplexity = 4.22751522/68.5466995 secs/batch = 0.2983s, grad.norm=15.16207314
 19915: 15 [   10/ 1327], train_loss/perplexity = 3.79566813/44.5079651 secs/batch = 0.2944s, grad.norm=14.52386761
 19920: 15 [   15/ 1327], train_loss/perplexity = 4.15455008/63.7232857 secs/batch = 0.2949s, grad.norm=14.17274857
 19925: 15 [   20/ 1327], train_loss/perplexity = 4.28719902/72.7623749 secs/batch = 0.2941s, grad.norm=14.14788628
 19930: 15 [   25/ 1327], train_loss/perplexity = 4.11526251/61.2682953 secs/batch = 0.2949s, grad.norm=15.13414764
 19935: 15 [   30/ 1327], train_loss/perplexity = 4.22569275/68.4218826 secs/batch = 0.2993s, grad.norm=14.98726559
 19940: 15 [   35/ 1327], train_loss/perplexity = 3.94139338/51.4902954 secs/batch = 0.2986s, grad.norm=14.31471825
 19945: 15 [   40/ 1327], train_loss/perplexity = 3.94254971/51.5498695 secs/batch = 0.2950s, grad.norm=14.77390194
 19950: 15 [   45/ 1327], train_loss/perplexity = 3.74768376/42.4227066 secs/batch = 0.2930s, grad.norm=13.60702038
 19955: 15 [   50/ 1327], train_loss/perplexity = 3.97512627/53.2568398 secs/batch = 0.2951s, grad.norm=14.72570133
 19960: 15 [   55/ 1327], train_loss/perplexity = 3.97791290/53.4054565 secs/batch = 0.2993s, grad.norm=15.20074177
 19965: 15 [   60/ 1327], train_loss/perplexity = 4.16356134/64.3001099 secs/batch = 0.2990s, grad.norm=14.88414097
 19970: 15 [   65/ 1327], train_loss/perplexity = 3.79989672/44.6965675 secs/batch = 0.2947s, grad.norm=14.36951733
 19975: 15 [   70/ 1327], train_loss/perplexity = 3.64945102/38.4535484 secs/batch = 0.2930s, grad.norm=14.59889889
 19980: 15 [   75/ 1327], train_loss/perplexity = 3.50286961/33.2106171 secs/batch = 0.2992s, grad.norm=14.07945251
 19985: 15 [   80/ 1327], train_loss/perplexity = 3.91426730/50.1123390 secs/batch = 0.2959s, grad.norm=14.63786030
 19990: 15 [   85/ 1327], train_loss/perplexity = 3.91324329/50.0610504 secs/batch = 0.2938s, grad.norm=15.11483002
 19995: 15 [   90/ 1327], train_loss/perplexity = 4.03764820/56.6928558 secs/batch = 0.2942s, grad.norm=15.16200542
 20000: 15 [   95/ 1327], train_loss/perplexity = 3.85375023/47.1696281 secs/batch = 0.2962s, grad.norm=14.77492142
 20005: 15 [  100/ 1327], train_loss/perplexity = 4.13580656/62.5400124 secs/batch = 0.2937s, grad.norm=14.83642197
 20010: 15 [  105/ 1327], train_loss/perplexity = 3.88585711/48.7086716 secs/batch = 0.2961s, grad.norm=15.60852528
 20015: 15 [  110/ 1327], train_loss/perplexity = 3.78383970/43.9846039 secs/batch = 0.2989s, grad.norm=14.73110104
 20020: 15 [  115/ 1327], train_loss/perplexity = 3.86122584/47.5235710 secs/batch = 0.2953s, grad.norm=15.10580254
 20025: 15 [  120/ 1327], train_loss/perplexity = 3.85999966/47.4653358 secs/batch = 0.2949s, grad.norm=15.04695034
 20030: 15 [  125/ 1327], train_loss/perplexity = 3.92139673/50.4708900 secs/batch = 0.2935s, grad.norm=15.17100048
 20035: 15 [  130/ 1327], train_loss/perplexity = 3.92241764/50.5224419 secs/batch = 0.3001s, grad.norm=15.66570091
 20040: 15 [  135/ 1327], train_loss/perplexity = 3.93136644/50.9765854 secs/batch = 0.2932s, grad.norm=14.95097828
 20045: 15 [  140/ 1327], train_loss/perplexity = 4.26040888/70.8389435 secs/batch = 0.2989s, grad.norm=15.37831593
 20050: 15 [  145/ 1327], train_loss/perplexity = 4.02044678/55.7259979 secs/batch = 0.2960s, grad.norm=15.79451752
 20055: 15 [  150/ 1327], train_loss/perplexity = 4.15328264/63.6425743 secs/batch = 0.3011s, grad.norm=15.47043705
 20060: 15 [  155/ 1327], train_loss/perplexity = 4.34072018/76.7628021 secs/batch = 0.2943s, grad.norm=14.82815933
 20065: 15 [  160/ 1327], train_loss/perplexity = 4.03188705/56.3671799 secs/batch = 0.2973s, grad.norm=14.02254105
 20070: 15 [  165/ 1327], train_loss/perplexity = 4.24529266/69.7761765 secs/batch = 0.2952s, grad.norm=14.85539150
 20075: 15 [  170/ 1327], train_loss/perplexity = 3.93320251/51.0702705 secs/batch = 0.3014s, grad.norm=14.24307823
 20080: 15 [  175/ 1327], train_loss/perplexity = 4.19687223/66.4780807 secs/batch = 0.3002s, grad.norm=14.88032150
 20085: 15 [  180/ 1327], train_loss/perplexity = 4.10368347/60.5629578 secs/batch = 0.3015s, grad.norm=14.88954449
 20090: 15 [  185/ 1327], train_loss/perplexity = 4.36209393/78.4211731 secs/batch = 0.2951s, grad.norm=15.10274410
 20095: 15 [  190/ 1327], train_loss/perplexity = 3.95527029/52.2098045 secs/batch = 0.2947s, grad.norm=14.08849335
 20100: 15 [  195/ 1327], train_loss/perplexity = 4.21105099/67.4273682 secs/batch = 0.2945s, grad.norm=14.02197647
 20105: 15 [  200/ 1327], train_loss/perplexity = 4.07816648/59.0371246 secs/batch = 0.2948s, grad.norm=15.13890457
 20110: 15 [  205/ 1327], train_loss/perplexity = 4.26265001/70.9978790 secs/batch = 0.2944s, grad.norm=15.04650879
 20115: 15 [  210/ 1327], train_loss/perplexity = 4.06323099/58.1619301 secs/batch = 0.2951s, grad.norm=14.29201126
 20120: 15 [  215/ 1327], train_loss/perplexity = 4.28303719/72.4601822 secs/batch = 0.2954s, grad.norm=14.74812222
 20125: 15 [  220/ 1327], train_loss/perplexity = 4.16588688/64.4498138 secs/batch = 0.2962s, grad.norm=14.32607841
 20130: 15 [  225/ 1327], train_loss/perplexity = 4.36896229/78.9616470 secs/batch = 0.2955s, grad.norm=14.93100643
 20135: 15 [  230/ 1327], train_loss/perplexity = 4.17069721/64.7605896 secs/batch = 0.2954s, grad.norm=15.46132183
 20140: 15 [  235/ 1327], train_loss/perplexity = 4.09974098/60.3246613 secs/batch = 0.2947s, grad.norm=14.80819130
 20145: 15 [  240/ 1327], train_loss/perplexity = 3.79787445/44.6062698 secs/batch = 0.2959s, grad.norm=14.83458614
 20150: 15 [  245/ 1327], train_loss/perplexity = 4.09702778/60.1612091 secs/batch = 0.2994s, grad.norm=15.02411175
 20155: 15 [  250/ 1327], train_loss/perplexity = 3.98221493/53.6357002 secs/batch = 0.2931s, grad.norm=14.14957714
 20160: 15 [  255/ 1327], train_loss/perplexity = 3.91014862/49.9063683 secs/batch = 0.2969s, grad.norm=14.44263458
 20165: 15 [  260/ 1327], train_loss/perplexity = 4.19225359/66.1717453 secs/batch = 0.2951s, grad.norm=15.60798836
 20170: 15 [  265/ 1327], train_loss/perplexity = 4.31567955/74.8644791 secs/batch = 0.2954s, grad.norm=14.10021782
 20175: 15 [  270/ 1327], train_loss/perplexity = 4.39283371/80.8692551 secs/batch = 0.2938s, grad.norm=14.58943462
 20180: 15 [  275/ 1327], train_loss/perplexity = 4.31700420/74.9637146 secs/batch = 0.2943s, grad.norm=14.75628662
 20185: 15 [  280/ 1327], train_loss/perplexity = 4.14631224/63.2005005 secs/batch = 0.2961s, grad.norm=14.51130581
 20190: 15 [  285/ 1327], train_loss/perplexity = 4.44992590/85.6205978 secs/batch = 0.2952s, grad.norm=14.75134563
 20195: 15 [  290/ 1327], train_loss/perplexity = 4.11446285/61.2193222 secs/batch = 0.2953s, grad.norm=14.94551086
 20200: 15 [  295/ 1327], train_loss/perplexity = 3.91733718/50.2664146 secs/batch = 0.3004s, grad.norm=15.17674541
 20205: 15 [  300/ 1327], train_loss/perplexity = 3.44273996/31.2725258 secs/batch = 0.2988s, grad.norm=13.90328312
 20210: 15 [  305/ 1327], train_loss/perplexity = 3.96476555/52.7079086 secs/batch = 0.3012s, grad.norm=14.79192734
 20215: 15 [  310/ 1327], train_loss/perplexity = 3.95502162/52.1968231 secs/batch = 0.2958s, grad.norm=14.77193356
 20220: 15 [  315/ 1327], train_loss/perplexity = 3.51966405/33.7730789 secs/batch = 0.3000s, grad.norm=13.94274998
 20225: 15 [  320/ 1327], train_loss/perplexity = 3.48084044/32.4870148 secs/batch = 0.3007s, grad.norm=15.22189617
 20230: 15 [  325/ 1327], train_loss/perplexity = 3.49344277/32.8990173 secs/batch = 0.2935s, grad.norm=13.86651230
 20235: 15 [  330/ 1327], train_loss/perplexity = 4.06673002/58.3657951 secs/batch = 0.2951s, grad.norm=14.93249607
 20240: 15 [  335/ 1327], train_loss/perplexity = 3.52853680/34.0740738 secs/batch = 0.3010s, grad.norm=13.84686375
 20245: 15 [  340/ 1327], train_loss/perplexity = 4.17748547/65.2016983 secs/batch = 0.2949s, grad.norm=14.90231323
 20250: 15 [  345/ 1327], train_loss/perplexity = 4.04809189/57.2880402 secs/batch = 0.2949s, grad.norm=14.46428204
 20255: 15 [  350/ 1327], train_loss/perplexity = 4.00794268/55.0335312 secs/batch = 0.2924s, grad.norm=15.11646652
 20260: 15 [  355/ 1327], train_loss/perplexity = 4.05397511/57.6260719 secs/batch = 0.3007s, grad.norm=15.08016872
 20265: 15 [  360/ 1327], train_loss/perplexity = 4.07833099/59.0468369 secs/batch = 0.2948s, grad.norm=15.99681664
 20270: 15 [  365/ 1327], train_loss/perplexity = 4.16503572/64.3949814 secs/batch = 0.2959s, grad.norm=14.77967739
 20275: 15 [  370/ 1327], train_loss/perplexity = 4.28712988/72.7573471 secs/batch = 0.2950s, grad.norm=16.24871826
 20280: 15 [  375/ 1327], train_loss/perplexity = 3.67334723/39.3835106 secs/batch = 0.2956s, grad.norm=14.79510498
 20285: 15 [  380/ 1327], train_loss/perplexity = 3.60746741/36.8725510 secs/batch = 0.2975s, grad.norm=15.03595924
 20290: 15 [  385/ 1327], train_loss/perplexity = 3.94841409/51.8530655 secs/batch = 0.3001s, grad.norm=15.34174061
 20295: 15 [  390/ 1327], train_loss/perplexity = 3.97417569/53.2062416 secs/batch = 0.2962s, grad.norm=14.70097828
 20300: 15 [  395/ 1327], train_loss/perplexity = 4.07028675/58.5737572 secs/batch = 0.2947s, grad.norm=14.94997787
 20305: 15 [  400/ 1327], train_loss/perplexity = 3.96650028/52.7994232 secs/batch = 0.3012s, grad.norm=14.94808674
 20310: 15 [  405/ 1327], train_loss/perplexity = 4.26001167/70.8108063 secs/batch = 0.2973s, grad.norm=14.97297287
 20315: 15 [  410/ 1327], train_loss/perplexity = 3.89970446/49.3878517 secs/batch = 0.3010s, grad.norm=14.87043095
 20320: 15 [  415/ 1327], train_loss/perplexity = 3.92674398/50.7414932 secs/batch = 0.2947s, grad.norm=14.69842434
 20325: 15 [  420/ 1327], train_loss/perplexity = 3.49859095/33.0688248 secs/batch = 0.2983s, grad.norm=14.74893856
 20330: 15 [  425/ 1327], train_loss/perplexity = 3.88690829/48.7599030 secs/batch = 0.2949s, grad.norm=15.45409584
 20335: 15 [  430/ 1327], train_loss/perplexity = 4.04878998/57.3280487 secs/batch = 0.3002s, grad.norm=15.19473553
 20340: 15 [  435/ 1327], train_loss/perplexity = 4.09326839/59.9354630 secs/batch = 0.2944s, grad.norm=15.61900330
 20345: 15 [  440/ 1327], train_loss/perplexity = 3.63017297/37.7193413 secs/batch = 0.3005s, grad.norm=14.62823582
 20350: 15 [  445/ 1327], train_loss/perplexity = 4.03217125/56.3832016 secs/batch = 0.2987s, grad.norm=15.95818996
 20355: 15 [  450/ 1327], train_loss/perplexity = 4.02873993/56.1900635 secs/batch = 0.2958s, grad.norm=14.99825954
 20360: 15 [  455/ 1327], train_loss/perplexity = 3.91531515/50.1648788 secs/batch = 0.2955s, grad.norm=14.82958984
 20365: 15 [  460/ 1327], train_loss/perplexity = 3.92561197/50.6840858 secs/batch = 0.2967s, grad.norm=15.31644249
 20370: 15 [  465/ 1327], train_loss/perplexity = 3.61169815/37.0288811 secs/batch = 0.2957s, grad.norm=15.79779148
 20375: 15 [  470/ 1327], train_loss/perplexity = 4.35993862/78.2523346 secs/batch = 0.2993s, grad.norm=14.87876797
 20380: 15 [  475/ 1327], train_loss/perplexity = 3.82007861/45.6077919 secs/batch = 0.2957s, grad.norm=15.08885956
 20385: 15 [  480/ 1327], train_loss/perplexity = 3.89321804/49.0685387 secs/batch = 0.2945s, grad.norm=14.73157215
 20390: 15 [  485/ 1327], train_loss/perplexity = 3.95136309/52.0062065 secs/batch = 0.2948s, grad.norm=15.03693199
 20395: 15 [  490/ 1327], train_loss/perplexity = 3.81773925/45.5012245 secs/batch = 0.2944s, grad.norm=16.12060356
 20400: 15 [  495/ 1327], train_loss/perplexity = 3.86619329/47.7602310 secs/batch = 0.2989s, grad.norm=15.23707771
 20405: 15 [  500/ 1327], train_loss/perplexity = 4.07627010/58.9252739 secs/batch = 0.2943s, grad.norm=15.03751755
 20410: 15 [  505/ 1327], train_loss/perplexity = 4.08023357/59.1592865 secs/batch = 0.2940s, grad.norm=14.07630539
 20415: 15 [  510/ 1327], train_loss/perplexity = 4.46933413/87.2985764 secs/batch = 0.2947s, grad.norm=14.60008717
 20420: 15 [  515/ 1327], train_loss/perplexity = 4.08877230/59.6665955 secs/batch = 0.3007s, grad.norm=14.49280548
 20425: 15 [  520/ 1327], train_loss/perplexity = 4.22380447/68.2928085 secs/batch = 0.2934s, grad.norm=14.82662773
 20430: 15 [  525/ 1327], train_loss/perplexity = 3.91165233/49.9814682 secs/batch = 0.2932s, grad.norm=14.69008541
 20435: 15 [  530/ 1327], train_loss/perplexity = 3.88235998/48.5386314 secs/batch = 0.2957s, grad.norm=15.34948158
 20440: 15 [  535/ 1327], train_loss/perplexity = 4.01773071/55.5748482 secs/batch = 0.2949s, grad.norm=15.23716640
 20445: 15 [  540/ 1327], train_loss/perplexity = 4.10263157/60.4992867 secs/batch = 0.2956s, grad.norm=15.19813442
 20450: 15 [  545/ 1327], train_loss/perplexity = 4.08191204/59.2586670 secs/batch = 0.2938s, grad.norm=15.10243797
 20455: 15 [  550/ 1327], train_loss/perplexity = 4.02157068/55.7886620 secs/batch = 0.2956s, grad.norm=14.99998379
 20460: 15 [  555/ 1327], train_loss/perplexity = 3.88347721/48.5928879 secs/batch = 0.2945s, grad.norm=14.73765755
 20465: 15 [  560/ 1327], train_loss/perplexity = 4.05982780/57.9643288 secs/batch = 0.2937s, grad.norm=16.26609612
 20470: 15 [  565/ 1327], train_loss/perplexity = 3.86102891/47.5142136 secs/batch = 0.2930s, grad.norm=15.74899864
 20475: 15 [  570/ 1327], train_loss/perplexity = 3.92105603/50.4536972 secs/batch = 0.2934s, grad.norm=15.99304867
 20480: 15 [  575/ 1327], train_loss/perplexity = 3.67654347/39.5095901 secs/batch = 0.2968s, grad.norm=15.64920330
 20485: 15 [  580/ 1327], train_loss/perplexity = 4.07143354/58.6409683 secs/batch = 0.2953s, grad.norm=15.81412697
 20490: 15 [  585/ 1327], train_loss/perplexity = 3.71652579/41.1212807 secs/batch = 0.2933s, grad.norm=14.69196892
 20495: 15 [  590/ 1327], train_loss/perplexity = 4.06958246/58.5325165 secs/batch = 0.2984s, grad.norm=15.75933075
 20500: 15 [  595/ 1327], train_loss/perplexity = 4.04356098/57.0290604 secs/batch = 0.3001s, grad.norm=15.44743347
 20505: 15 [  600/ 1327], train_loss/perplexity = 4.14451599/63.0870781 secs/batch = 0.2955s, grad.norm=14.39164639
 20510: 15 [  605/ 1327], train_loss/perplexity = 4.07009840/58.5627251 secs/batch = 0.2955s, grad.norm=14.48273945
 20515: 15 [  610/ 1327], train_loss/perplexity = 4.30116653/73.7858200 secs/batch = 0.2954s, grad.norm=14.73086834
 20520: 15 [  615/ 1327], train_loss/perplexity = 3.87340188/48.1057587 secs/batch = 0.2932s, grad.norm=14.34615707
 20525: 15 [  620/ 1327], train_loss/perplexity = 4.23101759/68.7871933 secs/batch = 0.3000s, grad.norm=14.87164402
 20530: 15 [  625/ 1327], train_loss/perplexity = 4.21529436/67.7140961 secs/batch = 0.2951s, grad.norm=15.06207466
 20535: 15 [  630/ 1327], train_loss/perplexity = 4.32160378/75.3093109 secs/batch = 0.3011s, grad.norm=15.04104996
 20540: 15 [  635/ 1327], train_loss/perplexity = 3.97572613/53.2887955 secs/batch = 0.2941s, grad.norm=14.79782772
 20545: 15 [  640/ 1327], train_loss/perplexity = 4.00906372/55.0952606 secs/batch = 0.2951s, grad.norm=14.83843327
 20550: 15 [  645/ 1327], train_loss/perplexity = 4.24523687/69.7722855 secs/batch = 0.2949s, grad.norm=15.88584709
 20555: 15 [  650/ 1327], train_loss/perplexity = 3.70548010/40.6695671 secs/batch = 0.3006s, grad.norm=14.98514462
 20560: 15 [  655/ 1327], train_loss/perplexity = 3.94101262/51.4706955 secs/batch = 0.2950s, grad.norm=15.23032570
 20565: 15 [  660/ 1327], train_loss/perplexity = 3.85445952/47.2030983 secs/batch = 0.2940s, grad.norm=14.83554459
 20570: 15 [  665/ 1327], train_loss/perplexity = 3.99473286/54.3113289 secs/batch = 0.2953s, grad.norm=15.53242683
 20575: 15 [  670/ 1327], train_loss/perplexity = 3.88738441/48.7831230 secs/batch = 0.2949s, grad.norm=15.17291451
 20580: 15 [  675/ 1327], train_loss/perplexity = 3.72218943/41.3548393 secs/batch = 0.2932s, grad.norm=15.24932098
 20585: 15 [  680/ 1327], train_loss/perplexity = 3.92353487/50.5789185 secs/batch = 0.3007s, grad.norm=15.58183575
 20590: 15 [  685/ 1327], train_loss/perplexity = 3.69054174/40.0665474 secs/batch = 0.2979s, grad.norm=14.48694229
 20595: 15 [  690/ 1327], train_loss/perplexity = 4.07279778/58.7210197 secs/batch = 0.2962s, grad.norm=14.55529690
 20600: 15 [  695/ 1327], train_loss/perplexity = 4.02687693/56.0854797 secs/batch = 0.2942s, grad.norm=14.68908882
 20605: 15 [  700/ 1327], train_loss/perplexity = 4.20636272/67.1119919 secs/batch = 0.2939s, grad.norm=15.64962292
 20610: 15 [  705/ 1327], train_loss/perplexity = 3.93583298/51.2047844 secs/batch = 0.2963s, grad.norm=14.17606735
 20615: 15 [  710/ 1327], train_loss/perplexity = 3.85004401/46.9951324 secs/batch = 0.2959s, grad.norm=15.11311436
 20620: 15 [  715/ 1327], train_loss/perplexity = 3.73681092/41.9639511 secs/batch = 0.3015s, grad.norm=15.24485111
 20625: 15 [  720/ 1327], train_loss/perplexity = 3.74963093/42.5053902 secs/batch = 0.2933s, grad.norm=15.23455906
 20630: 15 [  725/ 1327], train_loss/perplexity = 3.81883001/45.5508842 secs/batch = 0.2949s, grad.norm=15.41071129
 20635: 15 [  730/ 1327], train_loss/perplexity = 3.96341085/52.6365547 secs/batch = 0.2994s, grad.norm=15.87837219
 20640: 15 [  735/ 1327], train_loss/perplexity = 3.99341130/54.2396011 secs/batch = 0.2933s, grad.norm=15.73401165
 20645: 15 [  740/ 1327], train_loss/perplexity = 3.50994229/33.4463387 secs/batch = 0.2959s, grad.norm=14.32678032
 20650: 15 [  745/ 1327], train_loss/perplexity = 4.00703859/54.9837990 secs/batch = 0.2937s, grad.norm=15.22834396
 20655: 15 [  750/ 1327], train_loss/perplexity = 3.85156727/47.0667725 secs/batch = 0.2921s, grad.norm=14.83481884
 20660: 15 [  755/ 1327], train_loss/perplexity = 3.72550774/41.4922943 secs/batch = 0.3037s, grad.norm=14.68007374
 20665: 15 [  760/ 1327], train_loss/perplexity = 3.61951327/37.3194008 secs/batch = 0.3014s, grad.norm=14.45547867
 20670: 15 [  765/ 1327], train_loss/perplexity = 3.71796703/41.1805916 secs/batch = 0.2954s, grad.norm=14.43331432
 20675: 15 [  770/ 1327], train_loss/perplexity = 3.68380690/39.7976112 secs/batch = 0.2953s, grad.norm=14.72704983
 20680: 15 [  775/ 1327], train_loss/perplexity = 3.74368238/42.2532959 secs/batch = 0.2962s, grad.norm=15.43797302
 20685: 15 [  780/ 1327], train_loss/perplexity = 4.11920643/61.5104103 secs/batch = 0.2954s, grad.norm=15.42484474
 20690: 15 [  785/ 1327], train_loss/perplexity = 4.01213074/55.2644997 secs/batch = 0.3006s, grad.norm=15.23055077
 20695: 15 [  790/ 1327], train_loss/perplexity = 3.76310515/43.0819931 secs/batch = 0.3009s, grad.norm=15.20276451
 20700: 15 [  795/ 1327], train_loss/perplexity = 4.08391571/59.3775215 secs/batch = 0.2949s, grad.norm=15.33646011
 20705: 15 [  800/ 1327], train_loss/perplexity = 4.00522947/54.8844185 secs/batch = 0.2954s, grad.norm=15.54073906
 20710: 15 [  805/ 1327], train_loss/perplexity = 4.25017881/70.1179504 secs/batch = 0.2954s, grad.norm=14.96114826
 20715: 15 [  810/ 1327], train_loss/perplexity = 3.92451525/50.6285286 secs/batch = 0.2979s, grad.norm=14.38794899
 20720: 15 [  815/ 1327], train_loss/perplexity = 3.74393153/42.2638245 secs/batch = 0.2935s, grad.norm=14.68330097
 20725: 15 [  820/ 1327], train_loss/perplexity = 3.72450352/41.4506493 secs/batch = 0.2922s, grad.norm=14.31166267
 20730: 15 [  825/ 1327], train_loss/perplexity = 3.89102840/48.9612122 secs/batch = 0.2959s, grad.norm=14.66607475
 20735: 15 [  830/ 1327], train_loss/perplexity = 3.62334299/37.4625969 secs/batch = 0.2950s, grad.norm=14.99090385
 20740: 15 [  835/ 1327], train_loss/perplexity = 3.88759661/48.7934761 secs/batch = 0.3018s, grad.norm=15.10600471
 20745: 15 [  840/ 1327], train_loss/perplexity = 3.95592308/52.2438965 secs/batch = 0.3001s, grad.norm=15.40692616
 20750: 15 [  845/ 1327], train_loss/perplexity = 3.80172253/44.7782516 secs/batch = 0.2997s, grad.norm=15.20705986
 20755: 15 [  850/ 1327], train_loss/perplexity = 3.96597695/52.7718010 secs/batch = 0.2991s, grad.norm=15.15850925
 20760: 15 [  855/ 1327], train_loss/perplexity = 3.96975565/52.9715843 secs/batch = 0.2940s, grad.norm=15.14399529
 20765: 15 [  860/ 1327], train_loss/perplexity = 3.63008809/37.7161407 secs/batch = 0.3004s, grad.norm=14.46870995
 20770: 15 [  865/ 1327], train_loss/perplexity = 4.09080839/59.7882042 secs/batch = 0.2943s, grad.norm=15.00488663
 20775: 15 [  870/ 1327], train_loss/perplexity = 3.96008492/52.4617805 secs/batch = 0.2930s, grad.norm=16.64325714
 20780: 15 [  875/ 1327], train_loss/perplexity = 3.56345105/35.2847557 secs/batch = 0.2960s, grad.norm=14.30330276
 20785: 15 [  880/ 1327], train_loss/perplexity = 3.77450633/43.5759926 secs/batch = 0.2974s, grad.norm=14.96333694
 20790: 15 [  885/ 1327], train_loss/perplexity = 3.95601559/52.2487297 secs/batch = 0.2944s, grad.norm=14.61453819
 20795: 15 [  890/ 1327], train_loss/perplexity = 4.09902287/60.2813568 secs/batch = 0.3003s, grad.norm=15.01691246
 20800: 15 [  895/ 1327], train_loss/perplexity = 4.01345158/55.3375435 secs/batch = 0.2963s, grad.norm=14.26915932
 20805: 15 [  900/ 1327], train_loss/perplexity = 3.93988991/51.4129410 secs/batch = 0.3009s, grad.norm=14.73714352
 20810: 15 [  905/ 1327], train_loss/perplexity = 3.73263979/41.7892761 secs/batch = 0.2955s, grad.norm=14.43133259
 20815: 15 [  910/ 1327], train_loss/perplexity = 3.78150988/43.8822479 secs/batch = 0.3008s, grad.norm=13.62683010
 20820: 15 [  915/ 1327], train_loss/perplexity = 4.06228924/58.1071815 secs/batch = 0.2996s, grad.norm=14.55416107
 20825: 15 [  920/ 1327], train_loss/perplexity = 4.18567944/65.7381516 secs/batch = 0.2951s, grad.norm=15.30539799
 20830: 15 [  925/ 1327], train_loss/perplexity = 4.03224516/56.3873672 secs/batch = 0.2933s, grad.norm=14.21166134
 20835: 15 [  930/ 1327], train_loss/perplexity = 4.06602335/58.3245659 secs/batch = 0.2949s, grad.norm=14.88918018
 20840: 15 [  935/ 1327], train_loss/perplexity = 4.07002783/58.5585938 secs/batch = 0.2990s, grad.norm=14.59047985
 20845: 15 [  940/ 1327], train_loss/perplexity = 4.04322529/57.0099182 secs/batch = 0.2943s, grad.norm=14.38662052
 20850: 15 [  945/ 1327], train_loss/perplexity = 4.27781105/72.0824814 secs/batch = 0.2981s, grad.norm=14.71948051
 20855: 15 [  950/ 1327], train_loss/perplexity = 4.02486086/55.9725189 secs/batch = 0.2963s, grad.norm=14.69436455
 20860: 15 [  955/ 1327], train_loss/perplexity = 3.94161892/51.5019112 secs/batch = 0.2952s, grad.norm=14.21576595
 20865: 15 [  960/ 1327], train_loss/perplexity = 4.31748247/74.9995804 secs/batch = 0.2931s, grad.norm=15.39653683
 20870: 15 [  965/ 1327], train_loss/perplexity = 4.01569366/55.4617538 secs/batch = 0.2950s, grad.norm=15.12712193
 20875: 15 [  970/ 1327], train_loss/perplexity = 4.19801998/66.5544205 secs/batch = 0.2961s, grad.norm=14.59055614
 20880: 15 [  975/ 1327], train_loss/perplexity = 3.87569284/48.2160912 secs/batch = 0.3015s, grad.norm=15.64369297
 20885: 15 [  980/ 1327], train_loss/perplexity = 3.78331804/43.9616661 secs/batch = 0.2947s, grad.norm=14.53325653
 20890: 15 [  985/ 1327], train_loss/perplexity = 3.95212936/52.0460739 secs/batch = 0.2982s, grad.norm=15.56690502
 20895: 15 [  990/ 1327], train_loss/perplexity = 4.11526966/61.2687340 secs/batch = 0.2960s, grad.norm=15.28124332
 20900: 15 [  995/ 1327], train_loss/perplexity = 4.10185957/60.4525986 secs/batch = 0.2976s, grad.norm=15.18939877
 20905: 15 [ 1000/ 1327], train_loss/perplexity = 3.70631695/40.7036171 secs/batch = 0.3004s, grad.norm=14.68659019
 20910: 15 [ 1005/ 1327], train_loss/perplexity = 4.12628508/61.9473648 secs/batch = 0.2969s, grad.norm=15.09909725
 20915: 15 [ 1010/ 1327], train_loss/perplexity = 3.68469834/39.8331032 secs/batch = 0.2944s, grad.norm=14.01763535
 20920: 15 [ 1015/ 1327], train_loss/perplexity = 4.23268223/68.9017944 secs/batch = 0.2963s, grad.norm=14.69885921
 20925: 15 [ 1020/ 1327], train_loss/perplexity = 4.28740168/72.7771225 secs/batch = 0.2942s, grad.norm=14.92832279
 20930: 15 [ 1025/ 1327], train_loss/perplexity = 4.11764431/61.4143982 secs/batch = 0.2940s, grad.norm=14.51102066
 20935: 15 [ 1030/ 1327], train_loss/perplexity = 3.99603415/54.3820496 secs/batch = 0.2959s, grad.norm=14.61451054
 20940: 15 [ 1035/ 1327], train_loss/perplexity = 3.90346289/49.5738220 secs/batch = 0.2956s, grad.norm=14.40295601
 20945: 15 [ 1040/ 1327], train_loss/perplexity = 4.11904001/61.5001755 secs/batch = 0.2937s, grad.norm=15.53190327
 20950: 15 [ 1045/ 1327], train_loss/perplexity = 3.66159821/38.9235001 secs/batch = 0.2956s, grad.norm=14.19962788
 20955: 15 [ 1050/ 1327], train_loss/perplexity = 3.77697468/43.6836853 secs/batch = 0.3012s, grad.norm=14.69964600
 20960: 15 [ 1055/ 1327], train_loss/perplexity = 3.81204176/45.2427177 secs/batch = 0.2950s, grad.norm=15.50283337
 20965: 15 [ 1060/ 1327], train_loss/perplexity = 3.43593383/31.0604038 secs/batch = 0.2986s, grad.norm=15.60094166
 20970: 15 [ 1065/ 1327], train_loss/perplexity = 3.59992552/36.5955086 secs/batch = 0.2944s, grad.norm=15.68179417
 20975: 15 [ 1070/ 1327], train_loss/perplexity = 3.88164163/48.5037766 secs/batch = 0.2962s, grad.norm=15.55197811
 20980: 15 [ 1075/ 1327], train_loss/perplexity = 3.67526698/39.4591904 secs/batch = 0.2949s, grad.norm=14.96509743
 20985: 15 [ 1080/ 1327], train_loss/perplexity = 3.68301988/39.7663040 secs/batch = 0.3003s, grad.norm=15.27288723
 20990: 15 [ 1085/ 1327], train_loss/perplexity = 3.52238154/33.8649826 secs/batch = 0.2960s, grad.norm=14.92233944
 20995: 15 [ 1090/ 1327], train_loss/perplexity = 3.76741028/43.2678680 secs/batch = 0.2948s, grad.norm=16.10638809
 21000: 15 [ 1095/ 1327], train_loss/perplexity = 3.88261366/48.5509453 secs/batch = 0.3000s, grad.norm=15.51616764
 21005: 15 [ 1100/ 1327], train_loss/perplexity = 3.62190032/37.4085884 secs/batch = 0.2946s, grad.norm=16.50002861
 21010: 15 [ 1105/ 1327], train_loss/perplexity = 3.60559511/36.8035812 secs/batch = 0.2950s, grad.norm=15.06940365
 21015: 15 [ 1110/ 1327], train_loss/perplexity = 3.89312148/49.0638008 secs/batch = 0.2957s, grad.norm=15.79451275
 21020: 15 [ 1115/ 1327], train_loss/perplexity = 3.75995350/42.9464302 secs/batch = 0.3009s, grad.norm=14.97746658
 21025: 15 [ 1120/ 1327], train_loss/perplexity = 3.96834040/52.8966713 secs/batch = 0.2947s, grad.norm=15.16450882
 21030: 15 [ 1125/ 1327], train_loss/perplexity = 4.14560461/63.1557961 secs/batch = 0.2958s, grad.norm=15.57974243
 21035: 15 [ 1130/ 1327], train_loss/perplexity = 3.80062413/44.7290916 secs/batch = 0.2943s, grad.norm=15.06616688
 21040: 15 [ 1135/ 1327], train_loss/perplexity = 3.78250861/43.9260979 secs/batch = 0.2953s, grad.norm=15.29538727
 21045: 15 [ 1140/ 1327], train_loss/perplexity = 4.11010933/60.9533806 secs/batch = 0.2948s, grad.norm=16.14474297
 21050: 15 [ 1145/ 1327], train_loss/perplexity = 3.84833741/46.9149971 secs/batch = 0.2952s, grad.norm=14.86975574
 21055: 15 [ 1150/ 1327], train_loss/perplexity = 3.86634874/47.7676544 secs/batch = 0.2945s, grad.norm=15.01791096
 21060: 15 [ 1155/ 1327], train_loss/perplexity = 3.95555973/52.2249184 secs/batch = 0.2961s, grad.norm=15.71933556
 21065: 15 [ 1160/ 1327], train_loss/perplexity = 3.84491062/46.7545052 secs/batch = 0.3006s, grad.norm=15.43037987
 21070: 15 [ 1165/ 1327], train_loss/perplexity = 3.94066191/51.4526482 secs/batch = 0.2945s, grad.norm=15.18555546
 21075: 15 [ 1170/ 1327], train_loss/perplexity = 3.79859757/44.6385384 secs/batch = 0.2963s, grad.norm=15.24311543
 21080: 15 [ 1175/ 1327], train_loss/perplexity = 3.63535857/37.9154472 secs/batch = 0.2996s, grad.norm=15.61225891
 21085: 15 [ 1180/ 1327], train_loss/perplexity = 3.62987638/37.7081528 secs/batch = 0.2947s, grad.norm=15.24469376
 21090: 15 [ 1185/ 1327], train_loss/perplexity = 3.80018139/44.7092934 secs/batch = 0.2957s, grad.norm=15.22678089
 21095: 15 [ 1190/ 1327], train_loss/perplexity = 3.86244702/47.5816422 secs/batch = 0.2946s, grad.norm=15.61432457
 21100: 15 [ 1195/ 1327], train_loss/perplexity = 3.66542768/39.0728416 secs/batch = 0.3003s, grad.norm=14.80695248
 21105: 15 [ 1200/ 1327], train_loss/perplexity = 3.68832946/39.9780045 secs/batch = 0.2939s, grad.norm=15.48297310
 21110: 15 [ 1205/ 1327], train_loss/perplexity = 3.71389890/41.0134010 secs/batch = 0.2979s, grad.norm=15.14552116
 21115: 15 [ 1210/ 1327], train_loss/perplexity = 3.28352165/26.6695290 secs/batch = 0.3002s, grad.norm=15.25700092
 21120: 15 [ 1215/ 1327], train_loss/perplexity = 3.57260275/35.6091537 secs/batch = 0.3007s, grad.norm=15.03748608
 21125: 15 [ 1220/ 1327], train_loss/perplexity = 3.71337366/40.9918671 secs/batch = 0.2937s, grad.norm=15.23100758
 21130: 15 [ 1225/ 1327], train_loss/perplexity = 3.34935617/28.4843884 secs/batch = 0.2961s, grad.norm=15.74267387
 21135: 15 [ 1230/ 1327], train_loss/perplexity = 3.74542999/42.3272018 secs/batch = 0.2953s, grad.norm=14.71880531
 21140: 15 [ 1235/ 1327], train_loss/perplexity = 3.64476871/38.2739182 secs/batch = 0.2944s, grad.norm=15.01069355
 21145: 15 [ 1240/ 1327], train_loss/perplexity = 3.87974691/48.4119606 secs/batch = 0.2992s, grad.norm=15.80997276
 21150: 15 [ 1245/ 1327], train_loss/perplexity = 3.83133268/46.1239662 secs/batch = 0.2990s, grad.norm=15.11268234
 21155: 15 [ 1250/ 1327], train_loss/perplexity = 3.88859797/48.8423615 secs/batch = 0.3000s, grad.norm=14.46278095
 21160: 15 [ 1255/ 1327], train_loss/perplexity = 3.93052268/50.9335938 secs/batch = 0.3000s, grad.norm=14.88700104
 21165: 15 [ 1260/ 1327], train_loss/perplexity = 3.76010752/42.9530449 secs/batch = 0.2937s, grad.norm=15.70040226
 21170: 15 [ 1265/ 1327], train_loss/perplexity = 3.89898014/49.3520927 secs/batch = 0.2932s, grad.norm=15.37508392
 21175: 15 [ 1270/ 1327], train_loss/perplexity = 3.72770810/41.5836945 secs/batch = 0.2979s, grad.norm=15.83294392
 21180: 15 [ 1275/ 1327], train_loss/perplexity = 3.90247774/49.5250092 secs/batch = 0.2968s, grad.norm=15.31171513
 21185: 15 [ 1280/ 1327], train_loss/perplexity = 3.70878720/40.8042908 secs/batch = 0.2986s, grad.norm=15.27369404
 21190: 15 [ 1285/ 1327], train_loss/perplexity = 3.69495201/40.2436409 secs/batch = 0.3008s, grad.norm=15.55859661
 21195: 15 [ 1290/ 1327], train_loss/perplexity = 3.87465787/48.1662178 secs/batch = 0.2991s, grad.norm=14.83484459
 21200: 15 [ 1295/ 1327], train_loss/perplexity = 3.87701797/48.2800293 secs/batch = 0.2997s, grad.norm=14.95184898
 21205: 15 [ 1300/ 1327], train_loss/perplexity = 4.06427431/58.2226410 secs/batch = 0.2952s, grad.norm=14.95821095
 21210: 15 [ 1305/ 1327], train_loss/perplexity = 4.02733994/56.1114540 secs/batch = 0.2949s, grad.norm=14.86364079
 21215: 15 [ 1310/ 1327], train_loss/perplexity = 4.36080694/78.3203125 secs/batch = 0.2991s, grad.norm=15.88008595
 21220: 15 [ 1315/ 1327], train_loss/perplexity = 4.13837385/62.7007790 secs/batch = 0.2990s, grad.norm=15.53270149
 21225: 15 [ 1320/ 1327], train_loss/perplexity = 4.17156458/64.8167801 secs/batch = 0.2951s, grad.norm=15.28782845
 21230: 15 [ 1325/ 1327], train_loss/perplexity = 4.06208706/58.0954323 secs/batch = 0.3003s, grad.norm=15.65506458
Epoch training time: 393.7115213871002
	> validation loss = 4.59125662, perplexity = 98.61827850
	> validation loss = 4.53075552, perplexity = 92.82866669
	> validation loss = 4.53489590, perplexity = 93.21381378
	> validation loss = 4.49871683, perplexity = 89.90169525
	> validation loss = 4.66145849, perplexity = 105.79026031
	> validation loss = 4.63041162, perplexity = 102.55626678
	> validation loss = 4.55382347, perplexity = 94.99492645
	> validation loss = 4.40575695, perplexity = 81.92112732
	> validation loss = 4.19060946, perplexity = 66.06304169
	> validation loss = 4.32451582, perplexity = 75.52893066
	> validation loss = 4.52067995, perplexity = 91.89806366
	> validation loss = 4.47882700, perplexity = 88.13123322
	> validation loss = 4.42354488, perplexity = 83.39137268
	> validation loss = 4.17324162, perplexity = 64.92557526
	> validation loss = 4.16324329, perplexity = 64.27966309
	> validation loss = 4.16182518, perplexity = 64.18856812
	> validation loss = 4.59859133, perplexity = 99.34427643
	> validation loss = 4.04992437, perplexity = 57.39311600
	> validation loss = 4.57081747, perplexity = 96.62306213
	> validation loss = 4.52067518, perplexity = 91.89762115
	> validation loss = 4.24529076, perplexity = 69.77604675
at the end of epoch: 15
train loss = 3.98845689, perplexity = 53.97154121
validation loss = 4.42583282, perplexity = 83.58238742
Saved model cv/epoch015_4.4258.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.25
new learning rate is: 0.125
 21237: 16 [    5/ 1327], train_loss/perplexity = 4.19963980/66.6623154 secs/batch = 0.2951s, grad.norm=15.34269619
 21242: 16 [   10/ 1327], train_loss/perplexity = 3.76366091/43.1059456 secs/batch = 0.2951s, grad.norm=14.74796677
 21247: 16 [   15/ 1327], train_loss/perplexity = 4.09774303/60.2042542 secs/batch = 0.2938s, grad.norm=14.60877609
 21252: 16 [   20/ 1327], train_loss/perplexity = 4.26183319/70.9399109 secs/batch = 0.2931s, grad.norm=14.37326241
 21257: 16 [   25/ 1327], train_loss/perplexity = 4.07461071/58.8275757 secs/batch = 0.2941s, grad.norm=15.26057434
 21262: 16 [   30/ 1327], train_loss/perplexity = 4.15648127/63.8464699 secs/batch = 0.2951s, grad.norm=15.47571564
 21267: 16 [   35/ 1327], train_loss/perplexity = 3.85881567/47.4091682 secs/batch = 0.2985s, grad.norm=14.52752972
 21272: 16 [   40/ 1327], train_loss/perplexity = 3.94698238/51.7788811 secs/batch = 0.2950s, grad.norm=14.90464973
 21277: 16 [   45/ 1327], train_loss/perplexity = 3.78637457/44.0962410 secs/batch = 0.2984s, grad.norm=14.57383347
 21282: 16 [   50/ 1327], train_loss/perplexity = 3.96638274/52.7932167 secs/batch = 0.2993s, grad.norm=15.30621815
 21287: 16 [   55/ 1327], train_loss/perplexity = 3.94875574/51.8707848 secs/batch = 0.3008s, grad.norm=15.35675335
 21292: 16 [   60/ 1327], train_loss/perplexity = 4.16714573/64.5309982 secs/batch = 0.2950s, grad.norm=15.38508511
 21297: 16 [   65/ 1327], train_loss/perplexity = 3.75806761/42.8655128 secs/batch = 0.2953s, grad.norm=14.46635628
 21302: 16 [   70/ 1327], train_loss/perplexity = 3.58211875/35.9496269 secs/batch = 0.2948s, grad.norm=14.94064140
 21307: 16 [   75/ 1327], train_loss/perplexity = 3.37834787/29.3222866 secs/batch = 0.2927s, grad.norm=14.02829075
 21312: 16 [   80/ 1327], train_loss/perplexity = 3.85756779/47.3500443 secs/batch = 0.2997s, grad.norm=15.36156368
 21317: 16 [   85/ 1327], train_loss/perplexity = 3.91757083/50.2781639 secs/batch = 0.2960s, grad.norm=15.19472218
 21322: 16 [   90/ 1327], train_loss/perplexity = 3.93385005/51.1033516 secs/batch = 0.3012s, grad.norm=15.08797932
 21327: 16 [   95/ 1327], train_loss/perplexity = 3.82354450/45.7661400 secs/batch = 0.2954s, grad.norm=15.08691502
 21332: 16 [  100/ 1327], train_loss/perplexity = 4.06108189/58.0370674 secs/batch = 0.2941s, grad.norm=15.69391632
 21337: 16 [  105/ 1327], train_loss/perplexity = 3.89121628/48.9704132 secs/batch = 0.2959s, grad.norm=16.01841164
 21342: 16 [  110/ 1327], train_loss/perplexity = 3.78817272/44.1756058 secs/batch = 0.2995s, grad.norm=15.08888054
 21347: 16 [  115/ 1327], train_loss/perplexity = 3.81343412/45.3057594 secs/batch = 0.2931s, grad.norm=15.25792503
 21352: 16 [  120/ 1327], train_loss/perplexity = 3.86560535/47.7321587 secs/batch = 0.2992s, grad.norm=15.50443554
 21357: 16 [  125/ 1327], train_loss/perplexity = 3.85911632/47.4234238 secs/batch = 0.2983s, grad.norm=15.28547382
 21362: 16 [  130/ 1327], train_loss/perplexity = 3.91519117/50.1586609 secs/batch = 0.2988s, grad.norm=16.22117424
 21367: 16 [  135/ 1327], train_loss/perplexity = 3.83390760/46.2428856 secs/batch = 0.2955s, grad.norm=15.11721516
 21372: 16 [  140/ 1327], train_loss/perplexity = 4.13662434/62.5911789 secs/batch = 0.2991s, grad.norm=15.49194336
 21377: 16 [  145/ 1327], train_loss/perplexity = 3.97736859/53.3763924 secs/batch = 0.2979s, grad.norm=15.79384995
 21382: 16 [  150/ 1327], train_loss/perplexity = 4.03407049/56.4903870 secs/batch = 0.2948s, grad.norm=15.69818306
 21387: 16 [  155/ 1327], train_loss/perplexity = 4.24030495/69.4290237 secs/batch = 0.3002s, grad.norm=15.24559784
 21392: 16 [  160/ 1327], train_loss/perplexity = 4.00265741/54.7434311 secs/batch = 0.2955s, grad.norm=14.87680435
 21397: 16 [  165/ 1327], train_loss/perplexity = 4.14916086/63.3807945 secs/batch = 0.2949s, grad.norm=15.28379917
 21402: 16 [  170/ 1327], train_loss/perplexity = 3.87261128/48.0677414 secs/batch = 0.2944s, grad.norm=14.66678143
 21407: 16 [  175/ 1327], train_loss/perplexity = 4.19638014/66.4453735 secs/batch = 0.3003s, grad.norm=14.96238327
 21412: 16 [  180/ 1327], train_loss/perplexity = 4.01010847/55.1528511 secs/batch = 0.2943s, grad.norm=15.37296200
 21417: 16 [  185/ 1327], train_loss/perplexity = 4.34793568/77.3186874 secs/batch = 0.2942s, grad.norm=15.16264629
 21422: 16 [  190/ 1327], train_loss/perplexity = 3.82956433/46.0424728 secs/batch = 0.2920s, grad.norm=14.24802017
 21427: 16 [  195/ 1327], train_loss/perplexity = 4.22846985/68.6121674 secs/batch = 0.2956s, grad.norm=14.88218498
 21432: 16 [  200/ 1327], train_loss/perplexity = 4.07193470/58.6703606 secs/batch = 0.2998s, grad.norm=15.49872971
 21437: 16 [  205/ 1327], train_loss/perplexity = 4.25034904/70.1298828 secs/batch = 0.2940s, grad.norm=15.01886654
 21442: 16 [  210/ 1327], train_loss/perplexity = 4.05630207/57.7603226 secs/batch = 0.2994s, grad.norm=14.75114727
 21447: 16 [  215/ 1327], train_loss/perplexity = 4.19511843/66.3615875 secs/batch = 0.2988s, grad.norm=15.08755493
 21452: 16 [  220/ 1327], train_loss/perplexity = 4.12731886/62.0114403 secs/batch = 0.2962s, grad.norm=14.91661835
 21457: 16 [  225/ 1327], train_loss/perplexity = 4.27468443/71.8574600 secs/batch = 0.3000s, grad.norm=14.98165894
 21462: 16 [  230/ 1327], train_loss/perplexity = 4.13890743/62.7342415 secs/batch = 0.2949s, grad.norm=15.83536625
 21467: 16 [  235/ 1327], train_loss/perplexity = 4.08987808/59.7326088 secs/batch = 0.3009s, grad.norm=14.77723694
 21472: 16 [  240/ 1327], train_loss/perplexity = 3.74101591/42.1407814 secs/batch = 0.2932s, grad.norm=15.15538979
 21477: 16 [  245/ 1327], train_loss/perplexity = 4.00445700/54.8420372 secs/batch = 0.2991s, grad.norm=15.02046013
 21482: 16 [  250/ 1327], train_loss/perplexity = 3.96990061/52.9792633 secs/batch = 0.2993s, grad.norm=14.34479141
 21487: 16 [  255/ 1327], train_loss/perplexity = 3.89692974/49.2510033 secs/batch = 0.2943s, grad.norm=15.20731449
 21492: 16 [  260/ 1327], train_loss/perplexity = 4.15957451/64.0442657 secs/batch = 0.2926s, grad.norm=15.68245888
 21497: 16 [  265/ 1327], train_loss/perplexity = 4.30865288/74.3402786 secs/batch = 0.2981s, grad.norm=14.35909081
 21502: 16 [  270/ 1327], train_loss/perplexity = 4.35616875/77.9578857 secs/batch = 0.2999s, grad.norm=14.92261696
 21507: 16 [  275/ 1327], train_loss/perplexity = 4.26932621/71.4734650 secs/batch = 0.2942s, grad.norm=15.16564751
 21512: 16 [  280/ 1327], train_loss/perplexity = 4.11427689/61.2079391 secs/batch = 0.2945s, grad.norm=15.06747723
 21517: 16 [  285/ 1327], train_loss/perplexity = 4.40809155/82.1126022 secs/batch = 0.2953s, grad.norm=14.97621536
 21522: 16 [  290/ 1327], train_loss/perplexity = 4.09808969/60.2251282 secs/batch = 0.2934s, grad.norm=15.48832417
 21527: 16 [  295/ 1327], train_loss/perplexity = 3.85094976/47.0377159 secs/batch = 0.2947s, grad.norm=15.08077908
 21532: 16 [  300/ 1327], train_loss/perplexity = 3.38718867/29.5826683 secs/batch = 0.2953s, grad.norm=14.26999855
 21537: 16 [  305/ 1327], train_loss/perplexity = 3.94141698/51.4915123 secs/batch = 0.2946s, grad.norm=14.83438683
 21542: 16 [  310/ 1327], train_loss/perplexity = 3.90059471/49.4318390 secs/batch = 0.2991s, grad.norm=14.73834324
 21547: 16 [  315/ 1327], train_loss/perplexity = 3.45679617/31.7152042 secs/batch = 0.2936s, grad.norm=14.42065811
 21552: 16 [  320/ 1327], train_loss/perplexity = 3.39567304/29.8347263 secs/batch = 0.2949s, grad.norm=15.22966099
 21557: 16 [  325/ 1327], train_loss/perplexity = 3.42452955/30.7081947 secs/batch = 0.2924s, grad.norm=13.80812359
 21562: 16 [  330/ 1327], train_loss/perplexity = 3.94017744/51.4277267 secs/batch = 0.2946s, grad.norm=14.74040127
 21567: 16 [  335/ 1327], train_loss/perplexity = 3.39852715/29.9200001 secs/batch = 0.2935s, grad.norm=14.13290024
 21572: 16 [  340/ 1327], train_loss/perplexity = 4.20717907/67.1668015 secs/batch = 0.2951s, grad.norm=14.76430416
 21577: 16 [  345/ 1327], train_loss/perplexity = 3.97004485/52.9869080 secs/batch = 0.2935s, grad.norm=14.52930450
 21582: 16 [  350/ 1327], train_loss/perplexity = 3.93898654/51.3665161 secs/batch = 0.2984s, grad.norm=15.13563156
 21587: 16 [  355/ 1327], train_loss/perplexity = 3.99989367/54.5923462 secs/batch = 0.2990s, grad.norm=15.38152599
 21592: 16 [  360/ 1327], train_loss/perplexity = 4.05075645/57.4408913 secs/batch = 0.3003s, grad.norm=16.48531532
 21597: 16 [  365/ 1327], train_loss/perplexity = 4.12175369/61.6672935 secs/batch = 0.2941s, grad.norm=15.07639122
 21602: 16 [  370/ 1327], train_loss/perplexity = 4.23933887/69.3619766 secs/batch = 0.2950s, grad.norm=15.45334148
 21607: 16 [  375/ 1327], train_loss/perplexity = 3.56917787/35.4874039 secs/batch = 0.2954s, grad.norm=14.87469196
 21612: 16 [  380/ 1327], train_loss/perplexity = 3.59238863/36.3207283 secs/batch = 0.2934s, grad.norm=15.04147816
 21617: 16 [  385/ 1327], train_loss/perplexity = 3.83894920/46.4766121 secs/batch = 0.2940s, grad.norm=15.53386879
 21622: 16 [  390/ 1327], train_loss/perplexity = 3.95335078/52.1096840 secs/batch = 0.3000s, grad.norm=15.16148663
 21627: 16 [  395/ 1327], train_loss/perplexity = 4.03781414/56.7022629 secs/batch = 0.2950s, grad.norm=15.64461994
 21632: 16 [  400/ 1327], train_loss/perplexity = 3.94641733/51.7496338 secs/batch = 0.2953s, grad.norm=14.74765778
 21637: 16 [  405/ 1327], train_loss/perplexity = 4.25637341/70.5536499 secs/batch = 0.2934s, grad.norm=15.38170433
 21642: 16 [  410/ 1327], train_loss/perplexity = 3.83582211/46.3315010 secs/batch = 0.2926s, grad.norm=15.00409794
 21647: 16 [  415/ 1327], train_loss/perplexity = 3.88310814/48.5749588 secs/batch = 0.2946s, grad.norm=14.79947090
 21652: 16 [  420/ 1327], train_loss/perplexity = 3.46859074/32.0914841 secs/batch = 0.3003s, grad.norm=14.96476364
 21657: 16 [  425/ 1327], train_loss/perplexity = 3.81064939/45.1797676 secs/batch = 0.2981s, grad.norm=15.70703793
 21662: 16 [  430/ 1327], train_loss/perplexity = 4.03643560/56.6241531 secs/batch = 0.2985s, grad.norm=15.58481216
 21667: 16 [  435/ 1327], train_loss/perplexity = 4.07959652/59.1216087 secs/batch = 0.2948s, grad.norm=15.74752617
 21672: 16 [  440/ 1327], train_loss/perplexity = 3.56076503/35.1901093 secs/batch = 0.3005s, grad.norm=14.42266083
 21677: 16 [  445/ 1327], train_loss/perplexity = 3.92656422/50.7323723 secs/batch = 0.2932s, grad.norm=15.75712395
 21682: 16 [  450/ 1327], train_loss/perplexity = 3.91822863/50.3112450 secs/batch = 0.2928s, grad.norm=15.59389973
 21687: 16 [  455/ 1327], train_loss/perplexity = 3.88332653/48.5855675 secs/batch = 0.2950s, grad.norm=14.70367718
 21692: 16 [  460/ 1327], train_loss/perplexity = 3.83030438/46.0765610 secs/batch = 0.2993s, grad.norm=15.56343269
 21697: 16 [  465/ 1327], train_loss/perplexity = 3.59844303/36.5412979 secs/batch = 0.2950s, grad.norm=16.00364113
 21702: 16 [  470/ 1327], train_loss/perplexity = 4.30429077/74.0167007 secs/batch = 0.2929s, grad.norm=15.08340645
 21707: 16 [  475/ 1327], train_loss/perplexity = 3.80892754/45.1020432 secs/batch = 0.2988s, grad.norm=15.69866943
 21712: 16 [  480/ 1327], train_loss/perplexity = 3.87882948/48.3675652 secs/batch = 0.2984s, grad.norm=15.28680801
 21717: 16 [  485/ 1327], train_loss/perplexity = 3.83825016/46.4441338 secs/batch = 0.2958s, grad.norm=15.18103886
 21722: 16 [  490/ 1327], train_loss/perplexity = 3.79096508/44.2991333 secs/batch = 0.2993s, grad.norm=15.98801231
 21727: 16 [  495/ 1327], train_loss/perplexity = 3.86313343/47.6143150 secs/batch = 0.2942s, grad.norm=15.54222775
 21732: 16 [  500/ 1327], train_loss/perplexity = 3.97891116/53.4587936 secs/batch = 0.3010s, grad.norm=15.53629303
 21737: 16 [  505/ 1327], train_loss/perplexity = 4.06472111/58.2486610 secs/batch = 0.2946s, grad.norm=14.44249153
 21742: 16 [  510/ 1327], train_loss/perplexity = 4.43145084/84.0532761 secs/batch = 0.2980s, grad.norm=14.69678879
 21747: 16 [  515/ 1327], train_loss/perplexity = 4.07489491/58.8442955 secs/batch = 0.2956s, grad.norm=14.58930492
 21752: 16 [  520/ 1327], train_loss/perplexity = 4.23355007/68.9616165 secs/batch = 0.2956s, grad.norm=15.22487640
 21757: 16 [  525/ 1327], train_loss/perplexity = 3.84852481/46.9237900 secs/batch = 0.2986s, grad.norm=15.32543182
 21762: 16 [  530/ 1327], train_loss/perplexity = 3.80142641/44.7649918 secs/batch = 0.2930s, grad.norm=15.49355412
 21767: 16 [  535/ 1327], train_loss/perplexity = 3.97138309/53.0578651 secs/batch = 0.2963s, grad.norm=15.03201866
 21772: 16 [  540/ 1327], train_loss/perplexity = 4.05040646/57.4207916 secs/batch = 0.2952s, grad.norm=15.04570866
 21777: 16 [  545/ 1327], train_loss/perplexity = 3.99198771/54.1624413 secs/batch = 0.2953s, grad.norm=15.01855755
 21782: 16 [  550/ 1327], train_loss/perplexity = 3.94815779/51.8397789 secs/batch = 0.2935s, grad.norm=14.92726231
 21787: 16 [  555/ 1327], train_loss/perplexity = 3.85058355/47.0204926 secs/batch = 0.2921s, grad.norm=15.23442936
 21792: 16 [  560/ 1327], train_loss/perplexity = 3.96642208/52.7952957 secs/batch = 0.2934s, grad.norm=15.77254295
 21797: 16 [  565/ 1327], train_loss/perplexity = 3.82653618/45.9032631 secs/batch = 0.2946s, grad.norm=16.04670143
 21802: 16 [  570/ 1327], train_loss/perplexity = 3.80860186/45.0873566 secs/batch = 0.2988s, grad.norm=15.35764599
 21807: 16 [  575/ 1327], train_loss/perplexity = 3.62320685/37.4574966 secs/batch = 0.2982s, grad.norm=15.40727615
 21812: 16 [  580/ 1327], train_loss/perplexity = 4.04660845/57.2031212 secs/batch = 0.2958s, grad.norm=15.58538151
 21817: 16 [  585/ 1327], train_loss/perplexity = 3.62212396/37.4169540 secs/batch = 0.2960s, grad.norm=14.92938709
 21822: 16 [  590/ 1327], train_loss/perplexity = 4.01243973/55.2815781 secs/batch = 0.2985s, grad.norm=15.49914074
 21827: 16 [  595/ 1327], train_loss/perplexity = 3.91234970/50.0163383 secs/batch = 0.2927s, grad.norm=15.36333179
 21832: 16 [  600/ 1327], train_loss/perplexity = 4.15725803/63.8960800 secs/batch = 0.2996s, grad.norm=14.76156712
 21837: 16 [  605/ 1327], train_loss/perplexity = 4.05077887/57.4421806 secs/batch = 0.2956s, grad.norm=14.77454853
 21842: 16 [  610/ 1327], train_loss/perplexity = 4.15250587/63.5931587 secs/batch = 0.2939s, grad.norm=15.24932575
 21847: 16 [  615/ 1327], train_loss/perplexity = 3.82414317/45.7935448 secs/batch = 0.2979s, grad.norm=14.56252003
 21852: 16 [  620/ 1327], train_loss/perplexity = 4.19568300/66.3990631 secs/batch = 0.2941s, grad.norm=15.20973682
 21857: 16 [  625/ 1327], train_loss/perplexity = 4.17085266/64.7706528 secs/batch = 0.2945s, grad.norm=15.09175396
 21862: 16 [  630/ 1327], train_loss/perplexity = 4.21901703/67.9666443 secs/batch = 0.2941s, grad.norm=14.87380409
 21867: 16 [  635/ 1327], train_loss/perplexity = 3.94128203/51.4845657 secs/batch = 0.2929s, grad.norm=15.09478951
 21872: 16 [  640/ 1327], train_loss/perplexity = 3.89597702/49.2041016 secs/batch = 0.2981s, grad.norm=15.31301880
 21877: 16 [  645/ 1327], train_loss/perplexity = 4.18822575/65.9057541 secs/batch = 0.2952s, grad.norm=16.31259346
 21882: 16 [  650/ 1327], train_loss/perplexity = 3.68274426/39.7553444 secs/batch = 0.2924s, grad.norm=14.74683952
 21887: 16 [  655/ 1327], train_loss/perplexity = 3.85664558/47.3064003 secs/batch = 0.2958s, grad.norm=15.67464066
 21892: 16 [  660/ 1327], train_loss/perplexity = 3.78824663/44.1788712 secs/batch = 0.2989s, grad.norm=15.43892670
 21897: 16 [  665/ 1327], train_loss/perplexity = 3.96666622/52.8081856 secs/batch = 0.2978s, grad.norm=15.23304462
 21902: 16 [  670/ 1327], train_loss/perplexity = 3.82725811/45.9364128 secs/batch = 0.2995s, grad.norm=14.87905312
 21907: 16 [  675/ 1327], train_loss/perplexity = 3.70305181/40.5709305 secs/batch = 0.2963s, grad.norm=15.15423965
 21912: 16 [  680/ 1327], train_loss/perplexity = 3.88466740/48.6507568 secs/batch = 0.2976s, grad.norm=15.96113396
 21917: 16 [  685/ 1327], train_loss/perplexity = 3.65667963/38.7325211 secs/batch = 0.2945s, grad.norm=14.79149055
 21922: 16 [  690/ 1327], train_loss/perplexity = 4.07005978/58.5604630 secs/batch = 0.2983s, grad.norm=14.96473408
 21927: 16 [  695/ 1327], train_loss/perplexity = 3.90812755/49.8056068 secs/batch = 0.2916s, grad.norm=14.80293846
 21932: 16 [  700/ 1327], train_loss/perplexity = 4.14538145/63.1417046 secs/batch = 0.2960s, grad.norm=15.63214397
 21937: 16 [  705/ 1327], train_loss/perplexity = 3.93513584/51.1691017 secs/batch = 0.2994s, grad.norm=14.87107086
 21942: 16 [  710/ 1327], train_loss/perplexity = 3.83671880/46.3730659 secs/batch = 0.2948s, grad.norm=15.14625835
 21947: 16 [  715/ 1327], train_loss/perplexity = 3.60992956/36.9634476 secs/batch = 0.2930s, grad.norm=14.96214485
 21952: 16 [  720/ 1327], train_loss/perplexity = 3.65361762/38.6141052 secs/batch = 0.2941s, grad.norm=15.53771973
 21957: 16 [  725/ 1327], train_loss/perplexity = 3.87156153/48.0173073 secs/batch = 0.2957s, grad.norm=15.44278240
 21962: 16 [  730/ 1327], train_loss/perplexity = 3.87864733/48.3587570 secs/batch = 0.2999s, grad.norm=15.70483780
 21967: 16 [  735/ 1327], train_loss/perplexity = 3.91723490/50.2612762 secs/batch = 0.2986s, grad.norm=16.09587860
 21972: 16 [  740/ 1327], train_loss/perplexity = 3.45210505/31.5667725 secs/batch = 0.2949s, grad.norm=14.67056847
 21977: 16 [  745/ 1327], train_loss/perplexity = 3.99837351/54.5094185 secs/batch = 0.2975s, grad.norm=15.70555115
 21982: 16 [  750/ 1327], train_loss/perplexity = 3.82750225/45.9476280 secs/batch = 0.2986s, grad.norm=15.50364208
 21987: 16 [  755/ 1327], train_loss/perplexity = 3.67050266/39.2716408 secs/batch = 0.2981s, grad.norm=14.61856556
 21992: 16 [  760/ 1327], train_loss/perplexity = 3.53038239/34.1370201 secs/batch = 0.2953s, grad.norm=14.15787697
 21997: 16 [  765/ 1327], train_loss/perplexity = 3.64057589/38.1137810 secs/batch = 0.2957s, grad.norm=14.26597977
 22002: 16 [  770/ 1327], train_loss/perplexity = 3.60948920/36.9471741 secs/batch = 0.2952s, grad.norm=14.73856831
 22007: 16 [  775/ 1327], train_loss/perplexity = 3.71344304/40.9947090 secs/batch = 0.2922s, grad.norm=15.47850418
 22012: 16 [  780/ 1327], train_loss/perplexity = 4.02134132/55.7758675 secs/batch = 0.2957s, grad.norm=15.41803837
 22017: 16 [  785/ 1327], train_loss/perplexity = 3.91322732/50.0602531 secs/batch = 0.2925s, grad.norm=15.54751205
 22022: 16 [  790/ 1327], train_loss/perplexity = 3.66653490/39.1161308 secs/batch = 0.2930s, grad.norm=15.34909916
 22027: 16 [  795/ 1327], train_loss/perplexity = 4.02572489/56.0209045 secs/batch = 0.2946s, grad.norm=15.58101940
 22032: 16 [  800/ 1327], train_loss/perplexity = 3.96088505/52.5037727 secs/batch = 0.2949s, grad.norm=16.07526970
 22037: 16 [  805/ 1327], train_loss/perplexity = 4.22084284/68.0908508 secs/batch = 0.2983s, grad.norm=15.37130165
 22042: 16 [  810/ 1327], train_loss/perplexity = 3.91119337/49.9585342 secs/batch = 0.2984s, grad.norm=14.57110977
 22047: 16 [  815/ 1327], train_loss/perplexity = 3.69628954/40.2975044 secs/batch = 0.2981s, grad.norm=14.77646637
 22052: 16 [  820/ 1327], train_loss/perplexity = 3.66664672/39.1205025 secs/batch = 0.2937s, grad.norm=14.31417656
 22057: 16 [  825/ 1327], train_loss/perplexity = 3.84628296/46.8187141 secs/batch = 0.2935s, grad.norm=14.89544487
 22062: 16 [  830/ 1327], train_loss/perplexity = 3.48491979/32.6198120 secs/batch = 0.2997s, grad.norm=15.28602314
 22067: 16 [  835/ 1327], train_loss/perplexity = 3.86587000/47.7447929 secs/batch = 0.2952s, grad.norm=15.36826324
 22072: 16 [  840/ 1327], train_loss/perplexity = 3.94132376/51.4867134 secs/batch = 0.2964s, grad.norm=15.32111073
 22077: 16 [  845/ 1327], train_loss/perplexity = 3.76549959/43.1852760 secs/batch = 0.2928s, grad.norm=15.48534584
 22082: 16 [  850/ 1327], train_loss/perplexity = 3.89042616/48.9317360 secs/batch = 0.2987s, grad.norm=15.30885315
 22087: 16 [  855/ 1327], train_loss/perplexity = 3.91062808/49.9303017 secs/batch = 0.2975s, grad.norm=15.26399040
 22092: 16 [  860/ 1327], train_loss/perplexity = 3.60444641/36.7613258 secs/batch = 0.2989s, grad.norm=14.39451694
 22097: 16 [  865/ 1327], train_loss/perplexity = 4.03113842/56.3249969 secs/batch = 0.2908s, grad.norm=15.54896927
 22102: 16 [  870/ 1327], train_loss/perplexity = 3.89120007/48.9696198 secs/batch = 0.2960s, grad.norm=15.84823418
 22107: 16 [  875/ 1327], train_loss/perplexity = 3.49534655/32.9617081 secs/batch = 0.2954s, grad.norm=14.45127964
 22112: 16 [  880/ 1327], train_loss/perplexity = 3.72906733/41.6402550 secs/batch = 0.2945s, grad.norm=14.47762012
 22117: 16 [  885/ 1327], train_loss/perplexity = 3.89310813/49.0631447 secs/batch = 0.2976s, grad.norm=14.67807388
 22122: 16 [  890/ 1327], train_loss/perplexity = 4.11726809/61.3912964 secs/batch = 0.2941s, grad.norm=15.11332130
 22127: 16 [  895/ 1327], train_loss/perplexity = 3.93923736/51.3794022 secs/batch = 0.2936s, grad.norm=14.72371197
 22132: 16 [  900/ 1327], train_loss/perplexity = 3.86511517/47.7087669 secs/batch = 0.2923s, grad.norm=14.48025513
 22137: 16 [  905/ 1327], train_loss/perplexity = 3.72619557/41.5208435 secs/batch = 0.2951s, grad.norm=14.27290440
 22142: 16 [  910/ 1327], train_loss/perplexity = 3.77489662/43.5930023 secs/batch = 0.2931s, grad.norm=13.67399406
 22147: 16 [  915/ 1327], train_loss/perplexity = 4.01410866/55.3739166 secs/batch = 0.2946s, grad.norm=14.58685875
 22152: 16 [  920/ 1327], train_loss/perplexity = 4.08853531/59.6524544 secs/batch = 0.2936s, grad.norm=15.58897495
 22157: 16 [  925/ 1327], train_loss/perplexity = 3.93039417/50.9270477 secs/batch = 0.2910s, grad.norm=14.50389767
 22162: 16 [  930/ 1327], train_loss/perplexity = 4.04857397/57.3156662 secs/batch = 0.2950s, grad.norm=15.32995415
 22167: 16 [  935/ 1327], train_loss/perplexity = 4.05005217/57.4004517 secs/batch = 0.2939s, grad.norm=14.68230534
 22172: 16 [  940/ 1327], train_loss/perplexity = 3.91454220/50.1261177 secs/batch = 0.2934s, grad.norm=14.70401764
 22177: 16 [  945/ 1327], train_loss/perplexity = 4.13705635/62.6182251 secs/batch = 0.2942s, grad.norm=14.72105312
 22182: 16 [  950/ 1327], train_loss/perplexity = 3.94113636/51.4770660 secs/batch = 0.2998s, grad.norm=15.30903912
 22187: 16 [  955/ 1327], train_loss/perplexity = 3.87886310/48.3691940 secs/batch = 0.2982s, grad.norm=14.62491322
 22192: 16 [  960/ 1327], train_loss/perplexity = 4.21162939/67.4663773 secs/batch = 0.2925s, grad.norm=15.51045990
 22197: 16 [  965/ 1327], train_loss/perplexity = 4.00395393/54.8144531 secs/batch = 0.2994s, grad.norm=15.79077244
 22202: 16 [  970/ 1327], train_loss/perplexity = 4.18903351/65.9590149 secs/batch = 0.3004s, grad.norm=15.16989708
 22207: 16 [  975/ 1327], train_loss/perplexity = 3.83348227/46.2232208 secs/batch = 0.2987s, grad.norm=15.96825790
 22212: 16 [  980/ 1327], train_loss/perplexity = 3.70895362/40.8110809 secs/batch = 0.2957s, grad.norm=14.47282982
 22217: 16 [  985/ 1327], train_loss/perplexity = 3.86696815/47.7972527 secs/batch = 0.2952s, grad.norm=15.76698208
 22222: 16 [  990/ 1327], train_loss/perplexity = 4.09769201/60.2011833 secs/batch = 0.2994s, grad.norm=15.36829853
 22227: 16 [  995/ 1327], train_loss/perplexity = 4.07643700/58.9351082 secs/batch = 0.2995s, grad.norm=15.33136749
 22232: 16 [ 1000/ 1327], train_loss/perplexity = 3.59905624/36.5637093 secs/batch = 0.2922s, grad.norm=14.59811592
 22237: 16 [ 1005/ 1327], train_loss/perplexity = 4.07083607/58.6059418 secs/batch = 0.2999s, grad.norm=15.04725361
 22242: 16 [ 1010/ 1327], train_loss/perplexity = 3.57919097/35.8445282 secs/batch = 0.2986s, grad.norm=14.11421680
 22247: 16 [ 1015/ 1327], train_loss/perplexity = 4.15500450/63.7522507 secs/batch = 0.2937s, grad.norm=15.02819347
 22252: 16 [ 1020/ 1327], train_loss/perplexity = 4.18480110/65.6804352 secs/batch = 0.2945s, grad.norm=14.84520149
 22257: 16 [ 1025/ 1327], train_loss/perplexity = 4.12166500/61.6618233 secs/batch = 0.2978s, grad.norm=15.04188061
 22262: 16 [ 1030/ 1327], train_loss/perplexity = 3.86745691/47.8206177 secs/batch = 0.2943s, grad.norm=14.82941246
 22267: 16 [ 1035/ 1327], train_loss/perplexity = 3.85456347/47.2080040 secs/batch = 0.2930s, grad.norm=14.59122562
 22272: 16 [ 1040/ 1327], train_loss/perplexity = 4.05285931/57.5618095 secs/batch = 0.2989s, grad.norm=15.37942886
 22277: 16 [ 1045/ 1327], train_loss/perplexity = 3.57799149/35.8015594 secs/batch = 0.2943s, grad.norm=14.56356621
 22282: 16 [ 1050/ 1327], train_loss/perplexity = 3.73687601/41.9666824 secs/batch = 0.2937s, grad.norm=15.06120300
 22287: 16 [ 1055/ 1327], train_loss/perplexity = 3.77521896/43.6070557 secs/batch = 0.2945s, grad.norm=15.50445175
 22292: 16 [ 1060/ 1327], train_loss/perplexity = 3.39132977/29.7054272 secs/batch = 0.2956s, grad.norm=15.65608501
 22297: 16 [ 1065/ 1327], train_loss/perplexity = 3.48872137/32.7440529 secs/batch = 0.2949s, grad.norm=14.97982788
 22302: 16 [ 1070/ 1327], train_loss/perplexity = 3.79355073/44.4138222 secs/batch = 0.2990s, grad.norm=15.34041309
 22307: 16 [ 1075/ 1327], train_loss/perplexity = 3.64706063/38.3617401 secs/batch = 0.2921s, grad.norm=15.05797672
 22312: 16 [ 1080/ 1327], train_loss/perplexity = 3.59216261/36.3125191 secs/batch = 0.2941s, grad.norm=15.11757851
 22317: 16 [ 1085/ 1327], train_loss/perplexity = 3.55751610/35.0759621 secs/batch = 0.2946s, grad.norm=15.48905563
 22322: 16 [ 1090/ 1327], train_loss/perplexity = 3.70807338/40.7751732 secs/batch = 0.2927s, grad.norm=15.32939053
 22327: 16 [ 1095/ 1327], train_loss/perplexity = 3.78016353/43.8232079 secs/batch = 0.2934s, grad.norm=16.13092804
 22332: 16 [ 1100/ 1327], train_loss/perplexity = 3.50935483/33.4266968 secs/batch = 0.2934s, grad.norm=16.13173676
 22337: 16 [ 1105/ 1327], train_loss/perplexity = 3.56153226/35.2171173 secs/batch = 0.2928s, grad.norm=15.35140038
 22342: 16 [ 1110/ 1327], train_loss/perplexity = 3.76956892/43.3613701 secs/batch = 0.2990s, grad.norm=15.67072105
 22347: 16 [ 1115/ 1327], train_loss/perplexity = 3.65039039/38.4896889 secs/batch = 0.2979s, grad.norm=14.86446571
 22352: 16 [ 1120/ 1327], train_loss/perplexity = 3.86862707/47.8766098 secs/batch = 0.2977s, grad.norm=14.98741150
 22357: 16 [ 1125/ 1327], train_loss/perplexity = 4.07050991/58.5868301 secs/batch = 0.2958s, grad.norm=15.66930103
 22362: 16 [ 1130/ 1327], train_loss/perplexity = 3.73182487/41.7552376 secs/batch = 0.2925s, grad.norm=15.48389912
 22367: 16 [ 1135/ 1327], train_loss/perplexity = 3.69880581/40.3990326 secs/batch = 0.2944s, grad.norm=15.40233898
 22372: 16 [ 1140/ 1327], train_loss/perplexity = 3.99472260/54.3107719 secs/batch = 0.2945s, grad.norm=15.70950317
 22377: 16 [ 1145/ 1327], train_loss/perplexity = 3.74885511/42.4724274 secs/batch = 0.2918s, grad.norm=15.12520599
 22382: 16 [ 1150/ 1327], train_loss/perplexity = 3.75558543/42.7592468 secs/batch = 0.2960s, grad.norm=15.08067894
 22387: 16 [ 1155/ 1327], train_loss/perplexity = 3.86323738/47.6192627 secs/batch = 0.2949s, grad.norm=15.60770416
 22392: 16 [ 1160/ 1327], train_loss/perplexity = 3.82933378/46.0318604 secs/batch = 0.2939s, grad.norm=15.57069778
 22397: 16 [ 1165/ 1327], train_loss/perplexity = 3.89670730/49.2400475 secs/batch = 0.2983s, grad.norm=15.17641926
 22402: 16 [ 1170/ 1327], train_loss/perplexity = 3.74435806/42.2818565 secs/batch = 0.2947s, grad.norm=15.27999401
 22407: 16 [ 1175/ 1327], train_loss/perplexity = 3.44292378/31.2782764 secs/batch = 0.2932s, grad.norm=15.01357079
 22412: 16 [ 1180/ 1327], train_loss/perplexity = 3.59227347/36.3165474 secs/batch = 0.2936s, grad.norm=15.68372059
 22417: 16 [ 1185/ 1327], train_loss/perplexity = 3.61648583/37.2065887 secs/batch = 0.2999s, grad.norm=15.26577663
 22422: 16 [ 1190/ 1327], train_loss/perplexity = 3.81286263/45.2798729 secs/batch = 0.2947s, grad.norm=16.16131401
 22427: 16 [ 1195/ 1327], train_loss/perplexity = 3.59654379/36.4719620 secs/batch = 0.2935s, grad.norm=15.59704971
 22432: 16 [ 1200/ 1327], train_loss/perplexity = 3.59656906/36.4728851 secs/batch = 0.2931s, grad.norm=15.23513031
 22437: 16 [ 1205/ 1327], train_loss/perplexity = 3.64636302/38.3349876 secs/batch = 0.2995s, grad.norm=15.23271179
 22442: 16 [ 1210/ 1327], train_loss/perplexity = 3.25414658/25.8975029 secs/batch = 0.2944s, grad.norm=15.64169502
 22447: 16 [ 1215/ 1327], train_loss/perplexity = 3.48559856/32.6419601 secs/batch = 0.2996s, grad.norm=14.93458748
 22452: 16 [ 1220/ 1327], train_loss/perplexity = 3.64848351/38.4163628 secs/batch = 0.2948s, grad.norm=15.68807697
 22457: 16 [ 1225/ 1327], train_loss/perplexity = 3.35659552/28.6913452 secs/batch = 0.2987s, grad.norm=16.05253983
 22462: 16 [ 1230/ 1327], train_loss/perplexity = 3.63875484/38.0444374 secs/batch = 0.2974s, grad.norm=14.78944111
 22467: 16 [ 1235/ 1327], train_loss/perplexity = 3.52377367/33.9121590 secs/batch = 0.2944s, grad.norm=14.99302006
 22472: 16 [ 1240/ 1327], train_loss/perplexity = 3.79221892/44.3547096 secs/batch = 0.2925s, grad.norm=15.53160858
 22477: 16 [ 1245/ 1327], train_loss/perplexity = 3.71248531/40.9554672 secs/batch = 0.2925s, grad.norm=14.79641533
 22482: 16 [ 1250/ 1327], train_loss/perplexity = 3.85780239/47.3611565 secs/batch = 0.2933s, grad.norm=14.90409470
 22487: 16 [ 1255/ 1327], train_loss/perplexity = 3.91766787/50.2830429 secs/batch = 0.2942s, grad.norm=15.04795742
 22492: 16 [ 1260/ 1327], train_loss/perplexity = 3.62200046/37.4123344 secs/batch = 0.2954s, grad.norm=15.78972721
 22497: 16 [ 1265/ 1327], train_loss/perplexity = 3.86595559/47.7488785 secs/batch = 0.2958s, grad.norm=15.64764404
 22502: 16 [ 1270/ 1327], train_loss/perplexity = 3.66318393/38.9852715 secs/batch = 0.2952s, grad.norm=15.97397995
 22507: 16 [ 1275/ 1327], train_loss/perplexity = 3.79643750/44.5422211 secs/batch = 0.2936s, grad.norm=15.91715717
 22512: 16 [ 1280/ 1327], train_loss/perplexity = 3.64458275/38.2668037 secs/batch = 0.2929s, grad.norm=15.55680656
 22517: 16 [ 1285/ 1327], train_loss/perplexity = 3.55103350/34.8493156 secs/batch = 0.2937s, grad.norm=15.76382065
 22522: 16 [ 1290/ 1327], train_loss/perplexity = 3.78552055/44.0585976 secs/batch = 0.2920s, grad.norm=14.96825886
 22527: 16 [ 1295/ 1327], train_loss/perplexity = 3.80176592/44.7801933 secs/batch = 0.2955s, grad.norm=15.41206932
 22532: 16 [ 1300/ 1327], train_loss/perplexity = 3.92347598/50.5759392 secs/batch = 0.2940s, grad.norm=14.63487530
 22537: 16 [ 1305/ 1327], train_loss/perplexity = 3.96979737/52.9737968 secs/batch = 0.2996s, grad.norm=15.45358562
 22542: 16 [ 1310/ 1327], train_loss/perplexity = 4.28590775/72.6684799 secs/batch = 0.2990s, grad.norm=16.21209526
 22547: 16 [ 1315/ 1327], train_loss/perplexity = 4.09648371/60.1284866 secs/batch = 0.2950s, grad.norm=15.69821644
 22552: 16 [ 1320/ 1327], train_loss/perplexity = 4.05581760/57.7323456 secs/batch = 0.2985s, grad.norm=15.57108879
 22557: 16 [ 1325/ 1327], train_loss/perplexity = 3.96010113/52.4626312 secs/batch = 0.2953s, grad.norm=15.74046707
Epoch training time: 392.75431871414185
	> validation loss = 4.58340025, perplexity = 97.84653473
	> validation loss = 4.51968765, perplexity = 91.80691528
	> validation loss = 4.51788521, perplexity = 91.64158630
	> validation loss = 4.50184631, perplexity = 90.18348694
	> validation loss = 4.65547371, perplexity = 105.15901947
	> validation loss = 4.61265373, perplexity = 100.75115967
	> validation loss = 4.55569649, perplexity = 95.17301941
	> validation loss = 4.39223242, perplexity = 80.82064056
	> validation loss = 4.18580151, perplexity = 65.74617767
	> validation loss = 4.30946636, perplexity = 74.40077209
	> validation loss = 4.51686621, perplexity = 91.54825592
	> validation loss = 4.46994877, perplexity = 87.35224915
	> validation loss = 4.41435528, perplexity = 82.62854767
	> validation loss = 4.15437889, perplexity = 63.71237946
	> validation loss = 4.14183187, perplexity = 62.91797256
	> validation loss = 4.14896727, perplexity = 63.36852264
	> validation loss = 4.59049082, perplexity = 98.54278564
	> validation loss = 4.05019379, perplexity = 57.40858078
	> validation loss = 4.56851482, perplexity = 96.40083313
	> validation loss = 4.51538420, perplexity = 91.41268158
	> validation loss = 4.23519993, perplexity = 69.07548523
at the end of epoch: 16
train loss = 3.91761270, perplexity = 50.28026706
validation loss = 4.41771470, perplexity = 82.90660264
Saved model cv/epoch016_4.4177.model
 22564: 17 [    5/ 1327], train_loss/perplexity = 4.14334106/63.0130005 secs/batch = 0.2978s, grad.norm=15.50805378
 22569: 17 [   10/ 1327], train_loss/perplexity = 3.70165706/40.5143852 secs/batch = 0.2924s, grad.norm=15.09669399
 22574: 17 [   15/ 1327], train_loss/perplexity = 4.05946112/57.9430771 secs/batch = 0.2927s, grad.norm=14.88802528
 22579: 17 [   20/ 1327], train_loss/perplexity = 4.21599483/67.7615433 secs/batch = 0.2989s, grad.norm=14.85766220
 22584: 17 [   25/ 1327], train_loss/perplexity = 4.04374075/57.0393143 secs/batch = 0.2929s, grad.norm=15.80485249
 22589: 17 [   30/ 1327], train_loss/perplexity = 4.10778141/60.8116531 secs/batch = 0.2920s, grad.norm=15.85638142
 22594: 17 [   35/ 1327], train_loss/perplexity = 3.92360735/50.5825844 secs/batch = 0.2929s, grad.norm=14.82592297
 22599: 17 [   40/ 1327], train_loss/perplexity = 3.90638876/49.7190781 secs/batch = 0.2992s, grad.norm=15.24236107
 22604: 17 [   45/ 1327], train_loss/perplexity = 3.69581723/40.2784767 secs/batch = 0.2919s, grad.norm=14.64163113
 22609: 17 [   50/ 1327], train_loss/perplexity = 3.90272665/49.5373344 secs/batch = 0.2923s, grad.norm=15.50835323
 22614: 17 [   55/ 1327], train_loss/perplexity = 3.85540295/47.2476501 secs/batch = 0.2939s, grad.norm=15.72435665
 22619: 17 [   60/ 1327], train_loss/perplexity = 4.09146976/59.8277588 secs/batch = 0.2988s, grad.norm=15.77982712
 22624: 17 [   65/ 1327], train_loss/perplexity = 3.68569636/39.8728790 secs/batch = 0.2998s, grad.norm=14.69428921
 22629: 17 [   70/ 1327], train_loss/perplexity = 3.53267860/34.2154961 secs/batch = 0.2929s, grad.norm=15.09158707
 22634: 17 [   75/ 1327], train_loss/perplexity = 3.40446568/30.0982094 secs/batch = 0.2938s, grad.norm=14.11798859
 22639: 17 [   80/ 1327], train_loss/perplexity = 3.80444264/44.9002190 secs/batch = 0.2996s, grad.norm=15.55480862
 22644: 17 [   85/ 1327], train_loss/perplexity = 3.86919475/47.9037971 secs/batch = 0.2954s, grad.norm=15.65667343
 22649: 17 [   90/ 1327], train_loss/perplexity = 3.94932723/51.9004364 secs/batch = 0.2987s, grad.norm=15.52752590
 22654: 17 [   95/ 1327], train_loss/perplexity = 3.78994823/44.2541084 secs/batch = 0.2969s, grad.norm=15.27880764
 22659: 17 [  100/ 1327], train_loss/perplexity = 4.05551720/57.7150040 secs/batch = 0.2923s, grad.norm=15.68671131
 22664: 17 [  105/ 1327], train_loss/perplexity = 3.78625154/44.0908165 secs/batch = 0.2965s, grad.norm=15.69068241
 22669: 17 [  110/ 1327], train_loss/perplexity = 3.76260018/43.0602455 secs/batch = 0.2935s, grad.norm=15.43186474
 22674: 17 [  115/ 1327], train_loss/perplexity = 3.73326468/41.8153992 secs/batch = 0.2958s, grad.norm=15.59394169
 22679: 17 [  120/ 1327], train_loss/perplexity = 3.81006646/45.1534386 secs/batch = 0.2926s, grad.norm=15.76949120
 22684: 17 [  125/ 1327], train_loss/perplexity = 3.92330861/50.5674782 secs/batch = 0.2938s, grad.norm=15.44973660
 22689: 17 [  130/ 1327], train_loss/perplexity = 3.76748323/43.2710228 secs/batch = 0.2958s, grad.norm=15.96008587
 22694: 17 [  135/ 1327], train_loss/perplexity = 3.87211800/48.0440369 secs/batch = 0.2973s, grad.norm=15.48786831
 22699: 17 [  140/ 1327], train_loss/perplexity = 4.13396740/62.4250984 secs/batch = 0.2921s, grad.norm=16.00901031
 22704: 17 [  145/ 1327], train_loss/perplexity = 3.90604305/49.7018929 secs/batch = 0.2986s, grad.norm=16.28564644
 22709: 17 [  150/ 1327], train_loss/perplexity = 4.00232840/54.7254257 secs/batch = 0.2993s, grad.norm=15.54135799
 22714: 17 [  155/ 1327], train_loss/perplexity = 4.22412205/68.3144989 secs/batch = 0.2949s, grad.norm=15.43947411
 22719: 17 [  160/ 1327], train_loss/perplexity = 3.95831299/52.3689041 secs/batch = 0.2951s, grad.norm=14.42160988
 22724: 17 [  165/ 1327], train_loss/perplexity = 4.07052851/58.5879173 secs/batch = 0.2947s, grad.norm=15.22141552
 22729: 17 [  170/ 1327], train_loss/perplexity = 3.88740778/48.7842636 secs/batch = 0.2938s, grad.norm=14.70642376
 22734: 17 [  175/ 1327], train_loss/perplexity = 4.14303255/62.9935646 secs/batch = 0.2984s, grad.norm=15.23412800
 22739: 17 [  180/ 1327], train_loss/perplexity = 4.01872492/55.6301270 secs/batch = 0.2939s, grad.norm=15.47991180
 22744: 17 [  185/ 1327], train_loss/perplexity = 4.34383965/77.0026321 secs/batch = 0.2949s, grad.norm=15.31409264
 22749: 17 [  190/ 1327], train_loss/perplexity = 3.84106970/46.5752678 secs/batch = 0.2943s, grad.norm=14.62852192
 22754: 17 [  195/ 1327], train_loss/perplexity = 4.11110926/61.0143623 secs/batch = 0.2939s, grad.norm=14.69805336
 22759: 17 [  200/ 1327], train_loss/perplexity = 3.92211032/50.5069199 secs/batch = 0.2952s, grad.norm=15.64990425
 22764: 17 [  205/ 1327], train_loss/perplexity = 4.23014498/68.7271957 secs/batch = 0.2966s, grad.norm=15.65782833
 22769: 17 [  210/ 1327], train_loss/perplexity = 3.95549774/52.2216797 secs/batch = 0.2956s, grad.norm=14.46586132
 22774: 17 [  215/ 1327], train_loss/perplexity = 4.19219875/66.1681213 secs/batch = 0.2941s, grad.norm=14.68487072
 22779: 17 [  220/ 1327], train_loss/perplexity = 4.07640982/58.9335060 secs/batch = 0.2998s, grad.norm=14.87959194
 22784: 17 [  225/ 1327], train_loss/perplexity = 4.26695776/71.3043823 secs/batch = 0.2952s, grad.norm=15.71222591
 22789: 17 [  230/ 1327], train_loss/perplexity = 4.11432362/61.2107964 secs/batch = 0.2937s, grad.norm=16.36309242
 22794: 17 [  235/ 1327], train_loss/perplexity = 4.00975847/55.1335526 secs/batch = 0.2985s, grad.norm=15.08607006
 22799: 17 [  240/ 1327], train_loss/perplexity = 3.74194980/42.1801529 secs/batch = 0.2916s, grad.norm=15.17820644
 22804: 17 [  245/ 1327], train_loss/perplexity = 4.01650620/55.5068359 secs/batch = 0.2946s, grad.norm=15.27870750
 22809: 17 [  250/ 1327], train_loss/perplexity = 3.91086888/49.9423256 secs/batch = 0.2970s, grad.norm=14.48924255
 22814: 17 [  255/ 1327], train_loss/perplexity = 3.90540028/49.6699562 secs/batch = 0.2939s, grad.norm=15.43286228
 22819: 17 [  260/ 1327], train_loss/perplexity = 4.11141825/61.0332146 secs/batch = 0.2949s, grad.norm=15.57065964
 22824: 17 [  265/ 1327], train_loss/perplexity = 4.24470615/69.7352676 secs/batch = 0.2947s, grad.norm=14.97621059
 22829: 17 [  270/ 1327], train_loss/perplexity = 4.28970432/72.9449005 secs/batch = 0.3006s, grad.norm=15.06011772
 22834: 17 [  275/ 1327], train_loss/perplexity = 4.22204018/68.1724243 secs/batch = 0.2924s, grad.norm=15.06622505
 22839: 17 [  280/ 1327], train_loss/perplexity = 4.04054070/56.8570786 secs/batch = 0.2990s, grad.norm=15.02892685
 22844: 17 [  285/ 1327], train_loss/perplexity = 4.38719225/80.4143219 secs/batch = 0.2944s, grad.norm=15.28416538
 22849: 17 [  290/ 1327], train_loss/perplexity = 4.08112001/59.2117500 secs/batch = 0.2952s, grad.norm=15.61790371
 22854: 17 [  295/ 1327], train_loss/perplexity = 3.79315042/44.3960457 secs/batch = 0.2954s, grad.norm=14.99866486
 22859: 17 [  300/ 1327], train_loss/perplexity = 3.39800024/29.9042397 secs/batch = 0.2984s, grad.norm=14.52189827
 22864: 17 [  305/ 1327], train_loss/perplexity = 3.86557508/47.7307129 secs/batch = 0.2942s, grad.norm=14.86115456
 22869: 17 [  310/ 1327], train_loss/perplexity = 3.87171173/48.0245209 secs/batch = 0.2937s, grad.norm=15.31922150
 22874: 17 [  315/ 1327], train_loss/perplexity = 3.40128422/30.0026054 secs/batch = 0.2939s, grad.norm=13.76478672
 22879: 17 [  320/ 1327], train_loss/perplexity = 3.43637896/31.0742321 secs/batch = 0.2960s, grad.norm=15.26666260
 22884: 17 [  325/ 1327], train_loss/perplexity = 3.37890792/29.3387127 secs/batch = 0.2955s, grad.norm=14.41923428
 22889: 17 [  330/ 1327], train_loss/perplexity = 4.01385641/55.3599510 secs/batch = 0.2989s, grad.norm=15.15244865
 22894: 17 [  335/ 1327], train_loss/perplexity = 3.48015261/32.4646759 secs/batch = 0.2947s, grad.norm=14.29377079
 22899: 17 [  340/ 1327], train_loss/perplexity = 4.14397478/63.0529442 secs/batch = 0.2936s, grad.norm=15.19072437
 22904: 17 [  345/ 1327], train_loss/perplexity = 3.95115018/51.9951363 secs/batch = 0.2951s, grad.norm=14.72220039
 22909: 17 [  350/ 1327], train_loss/perplexity = 3.91540194/50.1692314 secs/batch = 0.2951s, grad.norm=15.42369652
 22914: 17 [  355/ 1327], train_loss/perplexity = 3.92955804/50.8844833 secs/batch = 0.2945s, grad.norm=15.45179749
 22919: 17 [  360/ 1327], train_loss/perplexity = 3.97835302/53.4289665 secs/batch = 0.2927s, grad.norm=16.52042580
 22924: 17 [  365/ 1327], train_loss/perplexity = 4.08959389/59.7156372 secs/batch = 0.2959s, grad.norm=15.51078033
 22929: 17 [  370/ 1327], train_loss/perplexity = 4.16915607/64.6608582 secs/batch = 0.2972s, grad.norm=15.68026829
 22934: 17 [  375/ 1327], train_loss/perplexity = 3.51550245/33.6328239 secs/batch = 0.2937s, grad.norm=14.94120121
 22939: 17 [  380/ 1327], train_loss/perplexity = 3.53705335/34.3655052 secs/batch = 0.2929s, grad.norm=15.31068420
 22944: 17 [  385/ 1327], train_loss/perplexity = 3.82080936/45.6411324 secs/batch = 0.2951s, grad.norm=15.84627247
 22949: 17 [  390/ 1327], train_loss/perplexity = 3.95883155/52.3960686 secs/batch = 0.2955s, grad.norm=14.98332691
 22954: 17 [  395/ 1327], train_loss/perplexity = 4.02522945/55.9931564 secs/batch = 0.2926s, grad.norm=15.44380379
 22959: 17 [  400/ 1327], train_loss/perplexity = 3.86307859/47.6117020 secs/batch = 0.2988s, grad.norm=15.30190372
 22964: 17 [  405/ 1327], train_loss/perplexity = 4.23022032/68.7323761 secs/batch = 0.2977s, grad.norm=15.83615112
 22969: 17 [  410/ 1327], train_loss/perplexity = 3.84067297/46.5567970 secs/batch = 0.2998s, grad.norm=15.05074501
 22974: 17 [  415/ 1327], train_loss/perplexity = 3.81514120/45.3831635 secs/batch = 0.2946s, grad.norm=15.26014328
 22979: 17 [  420/ 1327], train_loss/perplexity = 3.46356988/31.9307613 secs/batch = 0.2988s, grad.norm=14.89003754
 22984: 17 [  425/ 1327], train_loss/perplexity = 3.77286625/43.5045815 secs/batch = 0.2941s, grad.norm=15.69460392
 22989: 17 [  430/ 1327], train_loss/perplexity = 4.01396275/55.3658371 secs/batch = 0.2998s, grad.norm=15.67260838
 22994: 17 [  435/ 1327], train_loss/perplexity = 4.05928755/57.9330215 secs/batch = 0.3008s, grad.norm=15.56352329
 22999: 17 [  440/ 1327], train_loss/perplexity = 3.55884719/35.1226845 secs/batch = 0.2971s, grad.norm=14.95192242
 23004: 17 [  445/ 1327], train_loss/perplexity = 3.94863796/51.8646774 secs/batch = 0.2958s, grad.norm=15.99768448
 23009: 17 [  450/ 1327], train_loss/perplexity = 3.88579941/48.7058640 secs/batch = 0.2956s, grad.norm=15.74605942
 23014: 17 [  455/ 1327], train_loss/perplexity = 3.87079954/47.9807320 secs/batch = 0.2951s, grad.norm=15.20634270
 23019: 17 [  460/ 1327], train_loss/perplexity = 3.86861205/47.8758888 secs/batch = 0.2956s, grad.norm=16.36750793
 23024: 17 [  465/ 1327], train_loss/perplexity = 3.51563621/33.6373215 secs/batch = 0.2980s, grad.norm=15.35578537
 23029: 17 [  470/ 1327], train_loss/perplexity = 4.29791451/73.5462570 secs/batch = 0.2940s, grad.norm=15.23869801
 23034: 17 [  475/ 1327], train_loss/perplexity = 3.75181389/42.5982819 secs/batch = 0.2995s, grad.norm=15.27850246
 23039: 17 [  480/ 1327], train_loss/perplexity = 3.74157071/42.1641655 secs/batch = 0.2982s, grad.norm=15.00001812
 23044: 17 [  485/ 1327], train_loss/perplexity = 3.81345248/45.3065910 secs/batch = 0.2988s, grad.norm=15.13401604
 23049: 17 [  490/ 1327], train_loss/perplexity = 3.69685459/40.3202820 secs/batch = 0.2940s, grad.norm=15.96119499
 23054: 17 [  495/ 1327], train_loss/perplexity = 3.78137279/43.8762321 secs/batch = 0.2939s, grad.norm=15.61952114
 23059: 17 [  500/ 1327], train_loss/perplexity = 3.88250494/48.5456657 secs/batch = 0.2969s, grad.norm=15.23055649
 23064: 17 [  505/ 1327], train_loss/perplexity = 4.00984240/55.1381798 secs/batch = 0.2943s, grad.norm=14.61030960
 23069: 17 [  510/ 1327], train_loss/perplexity = 4.35756588/78.0668793 secs/batch = 0.2995s, grad.norm=14.85119629
 23074: 17 [  515/ 1327], train_loss/perplexity = 4.03331184/56.4475479 secs/batch = 0.2941s, grad.norm=14.90781212
 23079: 17 [  520/ 1327], train_loss/perplexity = 4.12039042/61.5832825 secs/batch = 0.2987s, grad.norm=15.11651611
 23084: 17 [  525/ 1327], train_loss/perplexity = 3.81199741/45.2407112 secs/batch = 0.2929s, grad.norm=14.90819836
 23089: 17 [  530/ 1327], train_loss/perplexity = 3.77665472/43.6697083 secs/batch = 0.2981s, grad.norm=15.90332985
 23094: 17 [  535/ 1327], train_loss/perplexity = 3.95906019/52.4080505 secs/batch = 0.2951s, grad.norm=15.61224079
 23099: 17 [  540/ 1327], train_loss/perplexity = 4.02451801/55.9533348 secs/batch = 0.2934s, grad.norm=15.59643078
 23104: 17 [  545/ 1327], train_loss/perplexity = 3.93305826/51.0629044 secs/batch = 0.2947s, grad.norm=15.39801407
 23109: 17 [  550/ 1327], train_loss/perplexity = 3.90925121/49.8616028 secs/batch = 0.2947s, grad.norm=15.14021873
 23114: 17 [  555/ 1327], train_loss/perplexity = 3.79506922/44.4813156 secs/batch = 0.2928s, grad.norm=15.10669994
 23119: 17 [  560/ 1327], train_loss/perplexity = 3.96212387/52.5688553 secs/batch = 0.2922s, grad.norm=16.13075638
 23124: 17 [  565/ 1327], train_loss/perplexity = 3.76583862/43.1999207 secs/batch = 0.2954s, grad.norm=16.29614639
 23129: 17 [  570/ 1327], train_loss/perplexity = 3.78012657/43.8215866 secs/batch = 0.2943s, grad.norm=16.18699074
 23134: 17 [  575/ 1327], train_loss/perplexity = 3.61564350/37.1752625 secs/batch = 0.2949s, grad.norm=15.68507004
 23139: 17 [  580/ 1327], train_loss/perplexity = 4.00922394/55.1040916 secs/batch = 0.2951s, grad.norm=16.02361107
 23144: 17 [  585/ 1327], train_loss/perplexity = 3.62658024/37.5840683 secs/batch = 0.2962s, grad.norm=15.41020298
 23149: 17 [  590/ 1327], train_loss/perplexity = 3.98331904/53.6949539 secs/batch = 0.2969s, grad.norm=15.53910542
 23154: 17 [  595/ 1327], train_loss/perplexity = 3.97449446/53.2232018 secs/batch = 0.2932s, grad.norm=16.05560112
 23159: 17 [  600/ 1327], train_loss/perplexity = 4.14426279/63.0711098 secs/batch = 0.2995s, grad.norm=14.80012131
 23164: 17 [  605/ 1327], train_loss/perplexity = 4.01694345/55.5311127 secs/batch = 0.2953s, grad.norm=15.06037140
 23169: 17 [  610/ 1327], train_loss/perplexity = 4.22090626/68.0951691 secs/batch = 0.2939s, grad.norm=15.51392460
 23174: 17 [  615/ 1327], train_loss/perplexity = 3.74601197/42.3518448 secs/batch = 0.2951s, grad.norm=14.60877800
 23179: 17 [  620/ 1327], train_loss/perplexity = 4.17804289/65.2380524 secs/batch = 0.2944s, grad.norm=15.32932663
 23184: 17 [  625/ 1327], train_loss/perplexity = 4.03688383/56.6495361 secs/batch = 0.2969s, grad.norm=15.52394581
 23189: 17 [  630/ 1327], train_loss/perplexity = 4.22332048/68.2597656 secs/batch = 0.2930s, grad.norm=15.37831020
 23194: 17 [  635/ 1327], train_loss/perplexity = 3.88650417/48.7402000 secs/batch = 0.2952s, grad.norm=15.33257389
 23199: 17 [  640/ 1327], train_loss/perplexity = 3.92982578/50.8981094 secs/batch = 0.2950s, grad.norm=15.39067173
 23204: 17 [  645/ 1327], train_loss/perplexity = 4.11773968/61.4202538 secs/batch = 0.2982s, grad.norm=16.33046913
 23209: 17 [  650/ 1327], train_loss/perplexity = 3.64916515/38.4425583 secs/batch = 0.2938s, grad.norm=15.35822296
 23214: 17 [  655/ 1327], train_loss/perplexity = 3.87589216/48.2257042 secs/batch = 0.2990s, grad.norm=15.83653641
 23219: 17 [  660/ 1327], train_loss/perplexity = 3.84325314/46.6770744 secs/batch = 0.2938s, grad.norm=15.63090324
 23224: 17 [  665/ 1327], train_loss/perplexity = 3.93443775/51.1333923 secs/batch = 0.2944s, grad.norm=15.85137844
 23229: 17 [  670/ 1327], train_loss/perplexity = 3.81596875/45.4207382 secs/batch = 0.2982s, grad.norm=15.38035011
 23234: 17 [  675/ 1327], train_loss/perplexity = 3.63156724/37.7719688 secs/batch = 0.2918s, grad.norm=15.94497013
 23239: 17 [  680/ 1327], train_loss/perplexity = 3.88230467/48.5359459 secs/batch = 0.3008s, grad.norm=15.95876980
 23244: 17 [  685/ 1327], train_loss/perplexity = 3.63059878/37.7354050 secs/batch = 0.2949s, grad.norm=15.22450066
 23249: 17 [  690/ 1327], train_loss/perplexity = 4.06453323/58.2377205 secs/batch = 0.2925s, grad.norm=14.98951435
 23254: 17 [  695/ 1327], train_loss/perplexity = 3.95094800/51.9846268 secs/batch = 0.2944s, grad.norm=15.15211487
 23259: 17 [  700/ 1327], train_loss/perplexity = 4.12165785/61.6613846 secs/batch = 0.2989s, grad.norm=16.07465935
 23264: 17 [  705/ 1327], train_loss/perplexity = 3.87290001/48.0816231 secs/batch = 0.2943s, grad.norm=15.13005257
 23269: 17 [  710/ 1327], train_loss/perplexity = 3.72991514/41.6755714 secs/batch = 0.2939s, grad.norm=15.74101162
 23274: 17 [  715/ 1327], train_loss/perplexity = 3.67536163/39.4629250 secs/batch = 0.2945s, grad.norm=15.78300858
 23279: 17 [  720/ 1327], train_loss/perplexity = 3.66207814/38.9421844 secs/batch = 0.2929s, grad.norm=15.73049545
 23284: 17 [  725/ 1327], train_loss/perplexity = 3.76901817/43.3374939 secs/batch = 0.2942s, grad.norm=15.90834808
 23289: 17 [  730/ 1327], train_loss/perplexity = 3.87078500/47.9800339 secs/batch = 0.2995s, grad.norm=15.86728001
 23294: 17 [  735/ 1327], train_loss/perplexity = 3.90553093/49.6764488 secs/batch = 0.2921s, grad.norm=16.04039764
 23299: 17 [  740/ 1327], train_loss/perplexity = 3.46142435/31.8623276 secs/batch = 0.2959s, grad.norm=14.91842461
 23304: 17 [  745/ 1327], train_loss/perplexity = 3.90127277/49.4653664 secs/batch = 0.2944s, grad.norm=15.59349632
 23309: 17 [  750/ 1327], train_loss/perplexity = 3.84077048/46.5613365 secs/batch = 0.2938s, grad.norm=15.88371181
 23314: 17 [  755/ 1327], train_loss/perplexity = 3.73124266/41.7309341 secs/batch = 0.2989s, grad.norm=14.76442146
 23319: 17 [  760/ 1327], train_loss/perplexity = 3.46683645/32.0352364 secs/batch = 0.2937s, grad.norm=14.27924728
 23324: 17 [  765/ 1327], train_loss/perplexity = 3.61449003/37.1324043 secs/batch = 0.2972s, grad.norm=14.79878044
 23329: 17 [  770/ 1327], train_loss/perplexity = 3.63023043/37.7215080 secs/batch = 0.2926s, grad.norm=15.14922905
 23334: 17 [  775/ 1327], train_loss/perplexity = 3.62407732/37.4901161 secs/batch = 0.2932s, grad.norm=15.69490814
 23339: 17 [  780/ 1327], train_loss/perplexity = 3.97852254/53.4380226 secs/batch = 0.2928s, grad.norm=15.72271633
 23344: 17 [  785/ 1327], train_loss/perplexity = 3.93406630/51.1144028 secs/batch = 0.2974s, grad.norm=16.29540253
 23349: 17 [  790/ 1327], train_loss/perplexity = 3.67720604/39.5357780 secs/batch = 0.2940s, grad.norm=15.45597744
 23354: 17 [  795/ 1327], train_loss/perplexity = 4.02034998/55.7206039 secs/batch = 0.2934s, grad.norm=15.47665119
 23359: 17 [  800/ 1327], train_loss/perplexity = 3.87116480/47.9982605 secs/batch = 0.2991s, grad.norm=15.67270756
 23364: 17 [  805/ 1327], train_loss/perplexity = 4.21316862/67.5703049 secs/batch = 0.2982s, grad.norm=15.20027542
 23369: 17 [  810/ 1327], train_loss/perplexity = 3.85278344/47.1240463 secs/batch = 0.2936s, grad.norm=14.68216991
 23374: 17 [  815/ 1327], train_loss/perplexity = 3.72930169/41.6500130 secs/batch = 0.2946s, grad.norm=15.19297600
 23379: 17 [  820/ 1327], train_loss/perplexity = 3.64420152/38.2522163 secs/batch = 0.2927s, grad.norm=14.60978222
 23384: 17 [  825/ 1327], train_loss/perplexity = 3.87050724/47.9667091 secs/batch = 0.2922s, grad.norm=15.21353054
 23389: 17 [  830/ 1327], train_loss/perplexity = 3.55242538/34.8978539 secs/batch = 0.2925s, grad.norm=15.54113865
 23394: 17 [  835/ 1327], train_loss/perplexity = 3.81133389/45.2107048 secs/batch = 0.2937s, grad.norm=15.59393120
 23399: 17 [  840/ 1327], train_loss/perplexity = 3.87918687/48.3848572 secs/batch = 0.2938s, grad.norm=15.60359287
 23404: 17 [  845/ 1327], train_loss/perplexity = 3.72292304/41.3851891 secs/batch = 0.2990s, grad.norm=15.75286961
 23409: 17 [  850/ 1327], train_loss/perplexity = 3.85716200/47.3308372 secs/batch = 0.2939s, grad.norm=15.44881630
 23414: 17 [  855/ 1327], train_loss/perplexity = 3.84484124/46.7512627 secs/batch = 0.2936s, grad.norm=15.44875050
 23419: 17 [  860/ 1327], train_loss/perplexity = 3.59337735/36.3566589 secs/batch = 0.2917s, grad.norm=15.15071678
 23424: 17 [  865/ 1327], train_loss/perplexity = 4.08967304/59.7203636 secs/batch = 0.2946s, grad.norm=15.98089409
 23429: 17 [  870/ 1327], train_loss/perplexity = 3.87220478/48.0482063 secs/batch = 0.2948s, grad.norm=15.63702297
 23434: 17 [  875/ 1327], train_loss/perplexity = 3.42580271/30.7473164 secs/batch = 0.2986s, grad.norm=14.46690464
 23439: 17 [  880/ 1327], train_loss/perplexity = 3.75161314/42.5897293 secs/batch = 0.2982s, grad.norm=14.84534359
 23444: 17 [  885/ 1327], train_loss/perplexity = 3.90391755/49.5963669 secs/batch = 0.2971s, grad.norm=15.04428101
 23449: 17 [  890/ 1327], train_loss/perplexity = 3.99029446/54.0708084 secs/batch = 0.2973s, grad.norm=15.39689255
 23454: 17 [  895/ 1327], train_loss/perplexity = 3.93194103/51.0058861 secs/batch = 0.2987s, grad.norm=14.71232033
 23459: 17 [  900/ 1327], train_loss/perplexity = 3.85140824/47.0592880 secs/batch = 0.2940s, grad.norm=15.08329773
 23464: 17 [  905/ 1327], train_loss/perplexity = 3.66515565/39.0622139 secs/batch = 0.2935s, grad.norm=14.45887184
 23469: 17 [  910/ 1327], train_loss/perplexity = 3.74981976/42.5134201 secs/batch = 0.2915s, grad.norm=13.79178047
 23474: 17 [  915/ 1327], train_loss/perplexity = 3.94309545/51.5780106 secs/batch = 0.2959s, grad.norm=14.80569839
 23479: 17 [  920/ 1327], train_loss/perplexity = 4.06577349/58.3099937 secs/batch = 0.2948s, grad.norm=15.52726078
 23484: 17 [  925/ 1327], train_loss/perplexity = 3.94459152/51.6552353 secs/batch = 0.2918s, grad.norm=14.89469528
 23489: 17 [  930/ 1327], train_loss/perplexity = 4.00324583/54.7756538 secs/batch = 0.2982s, grad.norm=15.26668262
 23494: 17 [  935/ 1327], train_loss/perplexity = 4.02418423/55.9346619 secs/batch = 0.2934s, grad.norm=15.06464386
 23499: 17 [  940/ 1327], train_loss/perplexity = 3.96865797/52.9134712 secs/batch = 0.2930s, grad.norm=14.94877529
 23504: 17 [  945/ 1327], train_loss/perplexity = 4.12877226/62.1016312 secs/batch = 0.2974s, grad.norm=14.90440559
 23509: 17 [  950/ 1327], train_loss/perplexity = 3.91125679/49.9617043 secs/batch = 0.2915s, grad.norm=15.33373833
 23514: 17 [  955/ 1327], train_loss/perplexity = 3.80836058/45.0764771 secs/batch = 0.2934s, grad.norm=15.00978470
 23519: 17 [  960/ 1327], train_loss/perplexity = 4.17073154/64.7628098 secs/batch = 0.2975s, grad.norm=15.57542419
 23524: 17 [  965/ 1327], train_loss/perplexity = 3.95656443/52.2774124 secs/batch = 0.2939s, grad.norm=15.59560871
 23529: 17 [  970/ 1327], train_loss/perplexity = 4.13418388/62.4386139 secs/batch = 0.2994s, grad.norm=15.26467037
 23534: 17 [  975/ 1327], train_loss/perplexity = 3.79577708/44.5128136 secs/batch = 0.2933s, grad.norm=16.38106346
 23539: 17 [  980/ 1327], train_loss/perplexity = 3.71000409/40.8539734 secs/batch = 0.2950s, grad.norm=14.59415340
 23544: 17 [  985/ 1327], train_loss/perplexity = 3.89229417/49.0232239 secs/batch = 0.2945s, grad.norm=15.66215038
 23549: 17 [  990/ 1327], train_loss/perplexity = 4.09158993/59.8349495 secs/batch = 0.2985s, grad.norm=16.06179047
 23554: 17 [  995/ 1327], train_loss/perplexity = 4.00358486/54.7942276 secs/batch = 0.2926s, grad.norm=15.00449562
 23559: 17 [ 1000/ 1327], train_loss/perplexity = 3.58878231/36.1899796 secs/batch = 0.2912s, grad.norm=15.56393719
 23564: 17 [ 1005/ 1327], train_loss/perplexity = 4.02168703/55.7951546 secs/batch = 0.2996s, grad.norm=15.17353821
 23569: 17 [ 1010/ 1327], train_loss/perplexity = 3.61926699/37.3102074 secs/batch = 0.2926s, grad.norm=14.73991680
 23574: 17 [ 1015/ 1327], train_loss/perplexity = 4.11132288/61.0273972 secs/batch = 0.2941s, grad.norm=15.10770798
 23579: 17 [ 1020/ 1327], train_loss/perplexity = 4.10373116/60.5658455 secs/batch = 0.2938s, grad.norm=14.97494125
 23584: 17 [ 1025/ 1327], train_loss/perplexity = 4.10506916/60.6469383 secs/batch = 0.2935s, grad.norm=15.24103069
 23589: 17 [ 1030/ 1327], train_loss/perplexity = 3.82284904/45.7343216 secs/batch = 0.2952s, grad.norm=14.79807758
 23594: 17 [ 1035/ 1327], train_loss/perplexity = 3.85542107/47.2485085 secs/batch = 0.2982s, grad.norm=14.61230564
 23599: 17 [ 1040/ 1327], train_loss/perplexity = 4.00610113/54.9322777 secs/batch = 0.2934s, grad.norm=15.67095852
 23604: 17 [ 1045/ 1327], train_loss/perplexity = 3.54502821/34.6406631 secs/batch = 0.2942s, grad.norm=14.30521965
 23609: 17 [ 1050/ 1327], train_loss/perplexity = 3.71703267/41.1421318 secs/batch = 0.2933s, grad.norm=15.42897320
 23614: 17 [ 1055/ 1327], train_loss/perplexity = 3.77618098/43.6490250 secs/batch = 0.2938s, grad.norm=15.66927910
 23619: 17 [ 1060/ 1327], train_loss/perplexity = 3.39691615/29.8718376 secs/batch = 0.2975s, grad.norm=15.99760914
 23624: 17 [ 1065/ 1327], train_loss/perplexity = 3.51966500/33.7731133 secs/batch = 0.2942s, grad.norm=15.80342484
 23629: 17 [ 1070/ 1327], train_loss/perplexity = 3.76636839/43.2228127 secs/batch = 0.2924s, grad.norm=15.82194519
 23634: 17 [ 1075/ 1327], train_loss/perplexity = 3.53336668/34.2390442 secs/batch = 0.2976s, grad.norm=15.16696548
 23639: 17 [ 1080/ 1327], train_loss/perplexity = 3.58749104/36.1432800 secs/batch = 0.2953s, grad.norm=14.92290115
 23644: 17 [ 1085/ 1327], train_loss/perplexity = 3.47176456/32.1935005 secs/batch = 0.2943s, grad.norm=14.97242355
 23649: 17 [ 1090/ 1327], train_loss/perplexity = 3.68437648/39.8202858 secs/batch = 0.2921s, grad.norm=15.82525444
 23654: 17 [ 1095/ 1327], train_loss/perplexity = 3.81294417/45.2835655 secs/batch = 0.2993s, grad.norm=15.87794495
 23659: 17 [ 1100/ 1327], train_loss/perplexity = 3.44773388/31.4290886 secs/batch = 0.2945s, grad.norm=16.60280800
 23664: 17 [ 1105/ 1327], train_loss/perplexity = 3.57936764/35.8508644 secs/batch = 0.2949s, grad.norm=15.80576801
 23669: 17 [ 1110/ 1327], train_loss/perplexity = 3.89149141/48.9838867 secs/batch = 0.2950s, grad.norm=16.29448700
 23674: 17 [ 1115/ 1327], train_loss/perplexity = 3.66338801/38.9932289 secs/batch = 0.2948s, grad.norm=15.18023872
 23679: 17 [ 1120/ 1327], train_loss/perplexity = 3.85632205/47.2910957 secs/batch = 0.2946s, grad.norm=15.30986977
 23684: 17 [ 1125/ 1327], train_loss/perplexity = 4.09067774/59.7803917 secs/batch = 0.2999s, grad.norm=16.51795387
 23689: 17 [ 1130/ 1327], train_loss/perplexity = 3.74034739/42.1126175 secs/batch = 0.2955s, grad.norm=15.67603207
 23694: 17 [ 1135/ 1327], train_loss/perplexity = 3.78260851/43.9304848 secs/batch = 0.2948s, grad.norm=15.06843567
 23699: 17 [ 1140/ 1327], train_loss/perplexity = 3.97870803/53.4479370 secs/batch = 0.2940s, grad.norm=16.04954529
 23704: 17 [ 1145/ 1327], train_loss/perplexity = 3.79643917/44.5422935 secs/batch = 0.2987s, grad.norm=15.22332573
 23709: 17 [ 1150/ 1327], train_loss/perplexity = 3.78537774/44.0523071 secs/batch = 0.2938s, grad.norm=15.04697037
 23714: 17 [ 1155/ 1327], train_loss/perplexity = 3.85864806/47.4012260 secs/batch = 0.2992s, grad.norm=15.92576027
 23719: 17 [ 1160/ 1327], train_loss/perplexity = 3.79966784/44.6863403 secs/batch = 0.2928s, grad.norm=15.73652554
 23724: 17 [ 1165/ 1327], train_loss/perplexity = 3.81028748/45.1634216 secs/batch = 0.2946s, grad.norm=15.59376335
 23729: 17 [ 1170/ 1327], train_loss/perplexity = 3.73257065/41.7863884 secs/batch = 0.2925s, grad.norm=15.35912895
 23734: 17 [ 1175/ 1327], train_loss/perplexity = 3.48187113/32.5205154 secs/batch = 0.2992s, grad.norm=15.60643959
 23739: 17 [ 1180/ 1327], train_loss/perplexity = 3.50138593/33.1613808 secs/batch = 0.2945s, grad.norm=15.77215195
 23744: 17 [ 1185/ 1327], train_loss/perplexity = 3.67357969/39.3926659 secs/batch = 0.2944s, grad.norm=15.72677803
 23749: 17 [ 1190/ 1327], train_loss/perplexity = 3.85065174/47.0237007 secs/batch = 0.2937s, grad.norm=16.31362534
 23754: 17 [ 1195/ 1327], train_loss/perplexity = 3.61537790/37.1653900 secs/batch = 0.2945s, grad.norm=15.51905346
 23759: 17 [ 1200/ 1327], train_loss/perplexity = 3.58672118/36.1154671 secs/batch = 0.3006s, grad.norm=15.44606209
 23764: 17 [ 1205/ 1327], train_loss/perplexity = 3.58628607/36.0997543 secs/batch = 0.2923s, grad.norm=15.82504272
 23769: 17 [ 1210/ 1327], train_loss/perplexity = 3.17869234/24.0153294 secs/batch = 0.2931s, grad.norm=15.42665195
 23774: 17 [ 1215/ 1327], train_loss/perplexity = 3.43901539/31.1562672 secs/batch = 0.2939s, grad.norm=15.34861660
 23779: 17 [ 1220/ 1327], train_loss/perplexity = 3.65898991/38.8221092 secs/batch = 0.2931s, grad.norm=15.65056419
 23784: 17 [ 1225/ 1327], train_loss/perplexity = 3.29927039/27.0928650 secs/batch = 0.2956s, grad.norm=16.33931923
 23789: 17 [ 1230/ 1327], train_loss/perplexity = 3.60064912/36.6219978 secs/batch = 0.2983s, grad.norm=14.96863079
 23794: 17 [ 1235/ 1327], train_loss/perplexity = 3.55339503/34.9317093 secs/batch = 0.2923s, grad.norm=15.16322136
 23799: 17 [ 1240/ 1327], train_loss/perplexity = 3.82406282/45.7898674 secs/batch = 0.2922s, grad.norm=16.50984955
 23804: 17 [ 1245/ 1327], train_loss/perplexity = 3.72034311/41.2785530 secs/batch = 0.2998s, grad.norm=14.95582294
 23809: 17 [ 1250/ 1327], train_loss/perplexity = 3.81349230/45.3083916 secs/batch = 0.2939s, grad.norm=15.00491810
 23814: 17 [ 1255/ 1327], train_loss/perplexity = 3.91673660/50.2362366 secs/batch = 0.2923s, grad.norm=15.29515839
 23819: 17 [ 1260/ 1327], train_loss/perplexity = 3.62060881/37.3603058 secs/batch = 0.2938s, grad.norm=16.40814209
 23824: 17 [ 1265/ 1327], train_loss/perplexity = 3.89191031/49.0044098 secs/batch = 0.2996s, grad.norm=16.00273323
 23829: 17 [ 1270/ 1327], train_loss/perplexity = 3.58680820/36.1186104 secs/batch = 0.2998s, grad.norm=15.79089069
 23834: 17 [ 1275/ 1327], train_loss/perplexity = 3.82038522/45.6217804 secs/batch = 0.2939s, grad.norm=16.13501930
 23839: 17 [ 1280/ 1327], train_loss/perplexity = 3.64910483/38.4402390 secs/batch = 0.2941s, grad.norm=15.95922661
 23844: 17 [ 1285/ 1327], train_loss/perplexity = 3.56486893/35.3348198 secs/batch = 0.2927s, grad.norm=16.02134705
 23849: 17 [ 1290/ 1327], train_loss/perplexity = 3.76636219/43.2225418 secs/batch = 0.2944s, grad.norm=15.33400536
 23854: 17 [ 1295/ 1327], train_loss/perplexity = 3.73833036/42.0277596 secs/batch = 0.2939s, grad.norm=15.21256256
 23859: 17 [ 1300/ 1327], train_loss/perplexity = 4.00831032/55.0537682 secs/batch = 0.2927s, grad.norm=15.26077080
 23864: 17 [ 1305/ 1327], train_loss/perplexity = 4.02256155/55.8439713 secs/batch = 0.2952s, grad.norm=15.64624310
 23869: 17 [ 1310/ 1327], train_loss/perplexity = 4.27756500/72.0647507 secs/batch = 0.2973s, grad.norm=16.31280327
 23874: 17 [ 1315/ 1327], train_loss/perplexity = 4.05942869/57.9412003 secs/batch = 0.2992s, grad.norm=16.08680725
 23879: 17 [ 1320/ 1327], train_loss/perplexity = 4.06497717/58.2635765 secs/batch = 0.2952s, grad.norm=15.80945206
 23884: 17 [ 1325/ 1327], train_loss/perplexity = 3.90640211/49.7197418 secs/batch = 0.2975s, grad.norm=15.74774265
Epoch training time: 392.0117335319519
	> validation loss = 4.57489729, perplexity = 97.01807404
	> validation loss = 4.52677250, perplexity = 92.45966339
	> validation loss = 4.50845194, perplexity = 90.78117371
	> validation loss = 4.49906635, perplexity = 89.93312836
	> validation loss = 4.64991665, perplexity = 104.57627106
	> validation loss = 4.60825491, perplexity = 100.30895233
	> validation loss = 4.55351925, perplexity = 94.96602631
	> validation loss = 4.37772846, perplexity = 79.65688324
	> validation loss = 4.16700506, perplexity = 64.52192688
	> validation loss = 4.31072998, perplexity = 74.49485016
	> validation loss = 4.51159286, perplexity = 91.06675720
	> validation loss = 4.47132540, perplexity = 87.47257996
	> validation loss = 4.41983604, perplexity = 83.08266449
	> validation loss = 4.14505911, perplexity = 63.12135315
	> validation loss = 4.15237045, perplexity = 63.58454514
	> validation loss = 4.14656639, perplexity = 63.21656799
	> validation loss = 4.58743811, perplexity = 98.24242401
	> validation loss = 4.05560732, perplexity = 57.72020721
	> validation loss = 4.57149744, perplexity = 96.68878937
	> validation loss = 4.52730560, perplexity = 92.50897217
	> validation loss = 4.23979807, perplexity = 69.39383698
at the end of epoch: 17
train loss = 3.89897168, perplexity = 49.35167371
validation loss = 4.41543445, perplexity = 82.71776957
Saved model cv/epoch017_4.4154.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.125
new learning rate is: 0.0625
 23891: 18 [    5/ 1327], train_loss/perplexity = 4.17434931/64.9975357 secs/batch = 0.2915s, grad.norm=16.37018776
 23896: 18 [   10/ 1327], train_loss/perplexity = 3.73832369/42.0274811 secs/batch = 0.2988s, grad.norm=14.95553207
 23901: 18 [   15/ 1327], train_loss/perplexity = 4.04271746/56.9809761 secs/batch = 0.2936s, grad.norm=14.50912476
 23906: 18 [   20/ 1327], train_loss/perplexity = 4.23701334/69.2008667 secs/batch = 0.2943s, grad.norm=14.78112888
 23911: 18 [   25/ 1327], train_loss/perplexity = 3.96048117/52.4825745 secs/batch = 0.2944s, grad.norm=15.58012772
 23916: 18 [   30/ 1327], train_loss/perplexity = 4.09950399/60.3103676 secs/batch = 0.2934s, grad.norm=15.91331673
 23921: 18 [   35/ 1327], train_loss/perplexity = 3.83016658/46.0702133 secs/batch = 0.2947s, grad.norm=15.47760773
 23926: 18 [   40/ 1327], train_loss/perplexity = 3.88367176/48.6023445 secs/batch = 0.2968s, grad.norm=15.73344898
 23931: 18 [   45/ 1327], train_loss/perplexity = 3.71233845/40.9494514 secs/batch = 0.2930s, grad.norm=14.99723244
 23936: 18 [   50/ 1327], train_loss/perplexity = 3.92301321/50.5525398 secs/batch = 0.2933s, grad.norm=15.64664268
 23941: 18 [   55/ 1327], train_loss/perplexity = 3.76154399/43.0147896 secs/batch = 0.2927s, grad.norm=15.59251976
 23946: 18 [   60/ 1327], train_loss/perplexity = 4.07163334/58.6526833 secs/batch = 0.2978s, grad.norm=15.73030186
 23951: 18 [   65/ 1327], train_loss/perplexity = 3.69695354/40.3242722 secs/batch = 0.2976s, grad.norm=15.51557064
 23956: 18 [   70/ 1327], train_loss/perplexity = 3.53178334/34.1848755 secs/batch = 0.2946s, grad.norm=15.55494785
 23961: 18 [   75/ 1327], train_loss/perplexity = 3.37722659/29.2894268 secs/batch = 0.2948s, grad.norm=14.71939659
 23966: 18 [   80/ 1327], train_loss/perplexity = 3.81144071/45.2155342 secs/batch = 0.2983s, grad.norm=15.56631374
 23971: 18 [   85/ 1327], train_loss/perplexity = 3.82329297/45.7546310 secs/batch = 0.3001s, grad.norm=15.78388596
 23976: 18 [   90/ 1327], train_loss/perplexity = 3.87588716/48.2254639 secs/batch = 0.2944s, grad.norm=15.98222065
 23981: 18 [   95/ 1327], train_loss/perplexity = 3.81618547/45.4305801 secs/batch = 0.2944s, grad.norm=16.23417664
 23986: 18 [  100/ 1327], train_loss/perplexity = 4.10953283/60.9182510 secs/batch = 0.2977s, grad.norm=16.03210831
 23991: 18 [  105/ 1327], train_loss/perplexity = 3.71824431/41.1920090 secs/batch = 0.2949s, grad.norm=16.26665688
 23996: 18 [  110/ 1327], train_loss/perplexity = 3.76670909/43.2375412 secs/batch = 0.2946s, grad.norm=15.73828220
 24001: 18 [  115/ 1327], train_loss/perplexity = 3.74181604/42.1745110 secs/batch = 0.2994s, grad.norm=16.09212494
 24006: 18 [  120/ 1327], train_loss/perplexity = 3.79597712/44.5217171 secs/batch = 0.2934s, grad.norm=15.89530945
 24011: 18 [  125/ 1327], train_loss/perplexity = 3.81833243/45.5282249 secs/batch = 0.2941s, grad.norm=15.96529293
 24016: 18 [  130/ 1327], train_loss/perplexity = 3.78704596/44.1258583 secs/batch = 0.2967s, grad.norm=16.64665604
 24021: 18 [  135/ 1327], train_loss/perplexity = 3.88313580/48.5763016 secs/batch = 0.2925s, grad.norm=15.72706032
 24026: 18 [  140/ 1327], train_loss/perplexity = 4.11799431/61.4358978 secs/batch = 0.2941s, grad.norm=16.58981895
 24031: 18 [  145/ 1327], train_loss/perplexity = 3.89616156/49.2131844 secs/batch = 0.3001s, grad.norm=16.17700386
 24036: 18 [  150/ 1327], train_loss/perplexity = 3.96043396/52.4800949 secs/batch = 0.2937s, grad.norm=16.00493431
 24041: 18 [  155/ 1327], train_loss/perplexity = 4.21079779/67.4103012 secs/batch = 0.2914s, grad.norm=15.67129612
 24046: 18 [  160/ 1327], train_loss/perplexity = 3.87065220/47.9736633 secs/batch = 0.2961s, grad.norm=14.96815681
 24051: 18 [  165/ 1327], train_loss/perplexity = 4.03813314/56.7203560 secs/batch = 0.2933s, grad.norm=15.74096870
 24056: 18 [  170/ 1327], train_loss/perplexity = 3.82788277/45.9651146 secs/batch = 0.2941s, grad.norm=14.95039177
 24061: 18 [  175/ 1327], train_loss/perplexity = 4.24845886/69.9974518 secs/batch = 0.2951s, grad.norm=15.70093727
 24066: 18 [  180/ 1327], train_loss/perplexity = 4.01655722/55.5096703 secs/batch = 0.2946s, grad.norm=15.59315395
 24071: 18 [  185/ 1327], train_loss/perplexity = 4.25715399/70.6087418 secs/batch = 0.2931s, grad.norm=15.88309288
 24076: 18 [  190/ 1327], train_loss/perplexity = 3.77422476/43.5637245 secs/batch = 0.2928s, grad.norm=14.83625031
 24081: 18 [  195/ 1327], train_loss/perplexity = 4.15498209/63.7508240 secs/batch = 0.2947s, grad.norm=15.10853386
 24086: 18 [  200/ 1327], train_loss/perplexity = 3.95353460/52.1192627 secs/batch = 0.3009s, grad.norm=15.87749481
 24091: 18 [  205/ 1327], train_loss/perplexity = 4.20712328/67.1630554 secs/batch = 0.2955s, grad.norm=15.57722092
 24096: 18 [  210/ 1327], train_loss/perplexity = 4.05274391/57.5551682 secs/batch = 0.2998s, grad.norm=15.22881603
 24101: 18 [  215/ 1327], train_loss/perplexity = 4.14811373/63.3144608 secs/batch = 0.2930s, grad.norm=15.30456924
 24106: 18 [  220/ 1327], train_loss/perplexity = 4.00759459/55.0143776 secs/batch = 0.2992s, grad.norm=15.32872486
 24111: 18 [  225/ 1327], train_loss/perplexity = 4.22879410/68.6344147 secs/batch = 0.2998s, grad.norm=15.39913654
 24116: 18 [  230/ 1327], train_loss/perplexity = 4.01731348/55.5516663 secs/batch = 0.2926s, grad.norm=16.15421486
 24121: 18 [  235/ 1327], train_loss/perplexity = 3.92906189/50.8592453 secs/batch = 0.2953s, grad.norm=15.23332405
 24126: 18 [  240/ 1327], train_loss/perplexity = 3.67888069/39.6020432 secs/batch = 0.2954s, grad.norm=15.64832497
 24131: 18 [  245/ 1327], train_loss/perplexity = 4.06158686/58.0663795 secs/batch = 0.2948s, grad.norm=15.60696220
 24136: 18 [  250/ 1327], train_loss/perplexity = 3.92049503/50.4253998 secs/batch = 0.2920s, grad.norm=15.10237217
 24141: 18 [  255/ 1327], train_loss/perplexity = 3.91411066/50.1044922 secs/batch = 0.2949s, grad.norm=15.64372540
 24146: 18 [  260/ 1327], train_loss/perplexity = 4.07575417/58.8948822 secs/batch = 0.2948s, grad.norm=16.05200386
 24151: 18 [  265/ 1327], train_loss/perplexity = 4.23452520/69.0288925 secs/batch = 0.3006s, grad.norm=15.15969181
 24156: 18 [  270/ 1327], train_loss/perplexity = 4.27811337/72.1042786 secs/batch = 0.2943s, grad.norm=15.54863262
 24161: 18 [  275/ 1327], train_loss/perplexity = 4.19190645/66.1487808 secs/batch = 0.2932s, grad.norm=15.54385948
 24166: 18 [  280/ 1327], train_loss/perplexity = 3.95712209/52.3065758 secs/batch = 0.2989s, grad.norm=15.65677357
 24171: 18 [  285/ 1327], train_loss/perplexity = 4.36476660/78.6310425 secs/batch = 0.2943s, grad.norm=15.49744511
 24176: 18 [  290/ 1327], train_loss/perplexity = 4.00976896/55.1341324 secs/batch = 0.2948s, grad.norm=15.75451565
 24181: 18 [  295/ 1327], train_loss/perplexity = 3.80742669/45.0344009 secs/batch = 0.2924s, grad.norm=15.59401989
 24186: 18 [  300/ 1327], train_loss/perplexity = 3.37153244/29.1231232 secs/batch = 0.3003s, grad.norm=14.81967449
 24191: 18 [  305/ 1327], train_loss/perplexity = 3.86903048/47.8959274 secs/batch = 0.2943s, grad.norm=15.11486816
 24196: 18 [  310/ 1327], train_loss/perplexity = 3.89803600/49.3055191 secs/batch = 0.2944s, grad.norm=15.51843739
 24201: 18 [  315/ 1327], train_loss/perplexity = 3.39002895/29.6668110 secs/batch = 0.2944s, grad.norm=14.37763691
 24206: 18 [  320/ 1327], train_loss/perplexity = 3.32895279/27.9090996 secs/batch = 0.3008s, grad.norm=15.18073559
 24211: 18 [  325/ 1327], train_loss/perplexity = 3.38591766/29.5450935 secs/batch = 0.2944s, grad.norm=14.40725136
 24216: 18 [  330/ 1327], train_loss/perplexity = 3.96577191/52.7609787 secs/batch = 0.2942s, grad.norm=15.39308929
 24221: 18 [  335/ 1327], train_loss/perplexity = 3.42208195/30.6331253 secs/batch = 0.2946s, grad.norm=14.22308159
 24226: 18 [  340/ 1327], train_loss/perplexity = 4.13496923/62.4876671 secs/batch = 0.2989s, grad.norm=15.26349163
 24231: 18 [  345/ 1327], train_loss/perplexity = 3.98264289/53.6586609 secs/batch = 0.2987s, grad.norm=14.99321938
 24236: 18 [  350/ 1327], train_loss/perplexity = 3.88842392/48.8338585 secs/batch = 0.2948s, grad.norm=15.49583912
 24241: 18 [  355/ 1327], train_loss/perplexity = 3.91840839/50.3202896 secs/batch = 0.2960s, grad.norm=15.66784286
 24246: 18 [  360/ 1327], train_loss/perplexity = 3.95072794/51.9731865 secs/batch = 0.2950s, grad.norm=16.56054878
 24251: 18 [  365/ 1327], train_loss/perplexity = 4.03656578/56.6315231 secs/batch = 0.2944s, grad.norm=15.81722164
 24256: 18 [  370/ 1327], train_loss/perplexity = 4.07109404/58.6210594 secs/batch = 0.2916s, grad.norm=15.42053890
 24261: 18 [  375/ 1327], train_loss/perplexity = 3.55217838/34.8892365 secs/batch = 0.3004s, grad.norm=15.53205490
 24266: 18 [  380/ 1327], train_loss/perplexity = 3.55516386/34.9935532 secs/batch = 0.2939s, grad.norm=15.56628036
 24271: 18 [  385/ 1327], train_loss/perplexity = 3.85462189/47.2107620 secs/batch = 0.2925s, grad.norm=16.38996315
 24276: 18 [  390/ 1327], train_loss/perplexity = 3.92690706/50.7497673 secs/batch = 0.2932s, grad.norm=15.25963974
 24281: 18 [  395/ 1327], train_loss/perplexity = 4.02492046/55.9758568 secs/batch = 0.2996s, grad.norm=15.89538288
 24286: 18 [  400/ 1327], train_loss/perplexity = 3.91567707/50.1830368 secs/batch = 0.2991s, grad.norm=15.61182022
 24291: 18 [  405/ 1327], train_loss/perplexity = 4.17027140/64.7330170 secs/batch = 0.2939s, grad.norm=15.47273731
 24296: 18 [  410/ 1327], train_loss/perplexity = 3.80222511/44.8007622 secs/batch = 0.2973s, grad.norm=15.02526760
 24301: 18 [  415/ 1327], train_loss/perplexity = 3.83452296/46.2713509 secs/batch = 0.2944s, grad.norm=15.39463711
 24306: 18 [  420/ 1327], train_loss/perplexity = 3.38750935/29.5921574 secs/batch = 0.2993s, grad.norm=15.50376034
 24311: 18 [  425/ 1327], train_loss/perplexity = 3.76014638/42.9547119 secs/batch = 0.2936s, grad.norm=15.66230965
 24316: 18 [  430/ 1327], train_loss/perplexity = 3.94641948/51.7497444 secs/batch = 0.2952s, grad.norm=15.76694775
 24321: 18 [  435/ 1327], train_loss/perplexity = 4.03181887/56.3633347 secs/batch = 0.2940s, grad.norm=16.32191277
 24326: 18 [  440/ 1327], train_loss/perplexity = 3.49513102/32.9546051 secs/batch = 0.2929s, grad.norm=15.83230972
 24331: 18 [  445/ 1327], train_loss/perplexity = 3.88245583/48.5432816 secs/batch = 0.2942s, grad.norm=16.04313278
 24336: 18 [  450/ 1327], train_loss/perplexity = 3.87505388/48.1852951 secs/batch = 0.2937s, grad.norm=15.66358662
 24341: 18 [  455/ 1327], train_loss/perplexity = 3.82705355/45.9270172 secs/batch = 0.2994s, grad.norm=15.38584995
 24346: 18 [  460/ 1327], train_loss/perplexity = 3.80084705/44.7390633 secs/batch = 0.2936s, grad.norm=16.18542671
 24351: 18 [  465/ 1327], train_loss/perplexity = 3.48612833/32.6592560 secs/batch = 0.2952s, grad.norm=16.23649788
 24356: 18 [  470/ 1327], train_loss/perplexity = 4.30036449/73.7266617 secs/batch = 0.2942s, grad.norm=15.40686131
 24361: 18 [  475/ 1327], train_loss/perplexity = 3.74590540/42.3473320 secs/batch = 0.2990s, grad.norm=15.97349262
 24366: 18 [  480/ 1327], train_loss/perplexity = 3.79317474/44.3971252 secs/batch = 0.2932s, grad.norm=15.55977821
 24371: 18 [  485/ 1327], train_loss/perplexity = 3.81335163/45.3020210 secs/batch = 0.2937s, grad.norm=15.76370811
 24376: 18 [  490/ 1327], train_loss/perplexity = 3.72596598/41.5113106 secs/batch = 0.2967s, grad.norm=16.19779015
 24381: 18 [  495/ 1327], train_loss/perplexity = 3.79969406/44.6875114 secs/batch = 0.2960s, grad.norm=15.46584702
 24386: 18 [  500/ 1327], train_loss/perplexity = 3.85424566/47.1930046 secs/batch = 0.2921s, grad.norm=15.53930092
 24391: 18 [  505/ 1327], train_loss/perplexity = 4.00513935/54.8794708 secs/batch = 0.2946s, grad.norm=14.86032867
 24396: 18 [  510/ 1327], train_loss/perplexity = 4.37256241/79.2464371 secs/batch = 0.3000s, grad.norm=15.04164982
 24401: 18 [  515/ 1327], train_loss/perplexity = 3.95854759/52.3811913 secs/batch = 0.2932s, grad.norm=14.98006439
 24406: 18 [  520/ 1327], train_loss/perplexity = 4.12195921/61.6799698 secs/batch = 0.2946s, grad.norm=15.53194523
 24411: 18 [  525/ 1327], train_loss/perplexity = 3.71859884/41.2066154 secs/batch = 0.2959s, grad.norm=15.00893497
 24416: 18 [  530/ 1327], train_loss/perplexity = 3.80751848/45.0385361 secs/batch = 0.2937s, grad.norm=15.78540134
 24421: 18 [  535/ 1327], train_loss/perplexity = 3.90467858/49.6341248 secs/batch = 0.2929s, grad.norm=15.43115520
 24426: 18 [  540/ 1327], train_loss/perplexity = 4.01527596/55.4385910 secs/batch = 0.2937s, grad.norm=15.39552498
 24431: 18 [  545/ 1327], train_loss/perplexity = 3.89261651/49.0390282 secs/batch = 0.2942s, grad.norm=15.84168434
 24436: 18 [  550/ 1327], train_loss/perplexity = 3.88053894/48.4503212 secs/batch = 0.2939s, grad.norm=15.58597088
 24441: 18 [  555/ 1327], train_loss/perplexity = 3.76608133/43.2104073 secs/batch = 0.2938s, grad.norm=15.20459557
 24446: 18 [  560/ 1327], train_loss/perplexity = 3.92090249/50.4459496 secs/batch = 0.2942s, grad.norm=16.78029442
 24451: 18 [  565/ 1327], train_loss/perplexity = 3.77291012/43.5064888 secs/batch = 0.2950s, grad.norm=16.36093140
 24456: 18 [  570/ 1327], train_loss/perplexity = 3.72637892/41.5284576 secs/batch = 0.2989s, grad.norm=16.42216682
 24461: 18 [  575/ 1327], train_loss/perplexity = 3.64196444/38.1667404 secs/batch = 0.2958s, grad.norm=16.45322418
 24466: 18 [  580/ 1327], train_loss/perplexity = 3.96920586/52.9424706 secs/batch = 0.2979s, grad.norm=16.22646904
 24471: 18 [  585/ 1327], train_loss/perplexity = 3.56895733/35.4795799 secs/batch = 0.2939s, grad.norm=15.23623085
 24476: 18 [  590/ 1327], train_loss/perplexity = 3.94033408/51.4357834 secs/batch = 0.2962s, grad.norm=15.40506363
 24481: 18 [  595/ 1327], train_loss/perplexity = 3.91264486/50.0311012 secs/batch = 0.2943s, grad.norm=15.99590015
 24486: 18 [  600/ 1327], train_loss/perplexity = 4.08326626/59.3389702 secs/batch = 0.2937s, grad.norm=15.02471447
 24491: 18 [  605/ 1327], train_loss/perplexity = 3.97852755/53.4382896 secs/batch = 0.2916s, grad.norm=15.09260273
 24496: 18 [  610/ 1327], train_loss/perplexity = 4.19209194/66.1610489 secs/batch = 0.2947s, grad.norm=15.75681686
 24501: 18 [  615/ 1327], train_loss/perplexity = 3.76439333/43.1375275 secs/batch = 0.3001s, grad.norm=14.55013847
 24506: 18 [  620/ 1327], train_loss/perplexity = 4.12829208/62.0718193 secs/batch = 0.2945s, grad.norm=15.55751896
 24511: 18 [  625/ 1327], train_loss/perplexity = 4.11022568/60.9604721 secs/batch = 0.2936s, grad.norm=15.35725117
 24516: 18 [  630/ 1327], train_loss/perplexity = 4.14595556/63.1779633 secs/batch = 0.2944s, grad.norm=15.27894402
 24521: 18 [  635/ 1327], train_loss/perplexity = 3.85376120/47.1701469 secs/batch = 0.2939s, grad.norm=15.22635269
 24526: 18 [  640/ 1327], train_loss/perplexity = 3.91704512/50.2517357 secs/batch = 0.2983s, grad.norm=15.30218792
 24531: 18 [  645/ 1327], train_loss/perplexity = 4.09637737/60.1220932 secs/batch = 0.2960s, grad.norm=16.19621658
 24536: 18 [  650/ 1327], train_loss/perplexity = 3.64596391/38.3196907 secs/batch = 0.2950s, grad.norm=15.43211555
 24541: 18 [  655/ 1327], train_loss/perplexity = 3.82767224/45.9554405 secs/batch = 0.3005s, grad.norm=16.01181030
 24546: 18 [  660/ 1327], train_loss/perplexity = 3.71970010/41.2520218 secs/batch = 0.2943s, grad.norm=15.33938313
 24551: 18 [  665/ 1327], train_loss/perplexity = 3.89338017/49.0764923 secs/batch = 0.2945s, grad.norm=15.32972813
 24556: 18 [  670/ 1327], train_loss/perplexity = 3.78607869/44.0831985 secs/batch = 0.2947s, grad.norm=15.62256908
 24561: 18 [  675/ 1327], train_loss/perplexity = 3.57383752/35.6531525 secs/batch = 0.2939s, grad.norm=15.72463608
 24566: 18 [  680/ 1327], train_loss/perplexity = 3.81724739/45.4788513 secs/batch = 0.2942s, grad.norm=16.16772079
 24571: 18 [  685/ 1327], train_loss/perplexity = 3.58484721/36.0478477 secs/batch = 0.2944s, grad.norm=15.10812378
 24576: 18 [  690/ 1327], train_loss/perplexity = 4.09258461/59.8944969 secs/batch = 0.2937s, grad.norm=15.64695740
 24581: 18 [  695/ 1327], train_loss/perplexity = 3.94518566/51.6859322 secs/batch = 0.2942s, grad.norm=15.40611839
 24586: 18 [  700/ 1327], train_loss/perplexity = 4.07983685/59.1358223 secs/batch = 0.2950s, grad.norm=16.18445587
 24591: 18 [  705/ 1327], train_loss/perplexity = 3.85951543/47.4423561 secs/batch = 0.2943s, grad.norm=15.14496994
 24596: 18 [  710/ 1327], train_loss/perplexity = 3.74907994/42.4819794 secs/batch = 0.2927s, grad.norm=15.98116016
 24601: 18 [  715/ 1327], train_loss/perplexity = 3.60935259/36.9421272 secs/batch = 0.2960s, grad.norm=15.50617504
 24606: 18 [  720/ 1327], train_loss/perplexity = 3.59820437/36.5325775 secs/batch = 0.2999s, grad.norm=15.90328312
 24611: 18 [  725/ 1327], train_loss/perplexity = 3.75878096/42.8961029 secs/batch = 0.2944s, grad.norm=15.71509171
 24616: 18 [  730/ 1327], train_loss/perplexity = 3.77638030/43.6577263 secs/batch = 0.2932s, grad.norm=15.99307251
 24621: 18 [  735/ 1327], train_loss/perplexity = 3.86521149/47.7133636 secs/batch = 0.2945s, grad.norm=16.17547989
 24626: 18 [  740/ 1327], train_loss/perplexity = 3.36466360/28.9237652 secs/batch = 0.2989s, grad.norm=14.50461388
 24631: 18 [  745/ 1327], train_loss/perplexity = 3.90073204/49.4386253 secs/batch = 0.2953s, grad.norm=16.24551392
 24636: 18 [  750/ 1327], train_loss/perplexity = 3.76038671/42.9650383 secs/batch = 0.2951s, grad.norm=15.89424992
 24641: 18 [  755/ 1327], train_loss/perplexity = 3.62023568/37.3463669 secs/batch = 0.2934s, grad.norm=14.94522095
 24646: 18 [  760/ 1327], train_loss/perplexity = 3.48936009/32.7649727 secs/batch = 0.2986s, grad.norm=14.33208942
 24651: 18 [  765/ 1327], train_loss/perplexity = 3.69950390/40.4272423 secs/batch = 0.2943s, grad.norm=14.67983437
 24656: 18 [  770/ 1327], train_loss/perplexity = 3.55334330/34.9299049 secs/batch = 0.2942s, grad.norm=15.19748020
 24661: 18 [  775/ 1327], train_loss/perplexity = 3.64589095/38.3168945 secs/batch = 0.2938s, grad.norm=15.98343849
 24666: 18 [  780/ 1327], train_loss/perplexity = 3.91013384/49.9056320 secs/batch = 0.2944s, grad.norm=15.65719509
 24671: 18 [  785/ 1327], train_loss/perplexity = 3.84799123/46.8987579 secs/batch = 0.2955s, grad.norm=15.70119095
 24676: 18 [  790/ 1327], train_loss/perplexity = 3.59784555/36.5194702 secs/batch = 0.3008s, grad.norm=15.39944649
 24681: 18 [  795/ 1327], train_loss/perplexity = 4.01829386/55.6061516 secs/batch = 0.2990s, grad.norm=15.70498657
 24686: 18 [  800/ 1327], train_loss/perplexity = 3.88316727/48.5778313 secs/batch = 0.3018s, grad.norm=16.32970238
 24691: 18 [  805/ 1327], train_loss/perplexity = 4.22997761/68.7156906 secs/batch = 0.2956s, grad.norm=16.35634613
 24696: 18 [  810/ 1327], train_loss/perplexity = 3.82171392/45.6824379 secs/batch = 0.2961s, grad.norm=14.81991768
 24701: 18 [  815/ 1327], train_loss/perplexity = 3.69163942/40.1105499 secs/batch = 0.2954s, grad.norm=15.17479420
 24706: 18 [  820/ 1327], train_loss/perplexity = 3.58523703/36.0619049 secs/batch = 0.2950s, grad.norm=14.87807083
 24711: 18 [  825/ 1327], train_loss/perplexity = 3.84315634/46.6725578 secs/batch = 0.2999s, grad.norm=15.04446316
 24716: 18 [  830/ 1327], train_loss/perplexity = 3.51102471/33.4825592 secs/batch = 0.2955s, grad.norm=15.82583046
 24721: 18 [  835/ 1327], train_loss/perplexity = 3.80679202/45.0058289 secs/batch = 0.3005s, grad.norm=16.02919006
 24726: 18 [  840/ 1327], train_loss/perplexity = 3.92668009/50.7382507 secs/batch = 0.2942s, grad.norm=16.36620903
 24731: 18 [  845/ 1327], train_loss/perplexity = 3.73360896/41.8297997 secs/batch = 0.2943s, grad.norm=16.01437759
 24736: 18 [  850/ 1327], train_loss/perplexity = 3.87157035/48.0177307 secs/batch = 0.2942s, grad.norm=15.56604576
 24741: 18 [  855/ 1327], train_loss/perplexity = 3.87204170/48.0403709 secs/batch = 0.2948s, grad.norm=15.83767891
 24746: 18 [  860/ 1327], train_loss/perplexity = 3.57730579/35.7770195 secs/batch = 0.2951s, grad.norm=14.84223747
 24751: 18 [  865/ 1327], train_loss/perplexity = 3.97686863/53.3497162 secs/batch = 0.2938s, grad.norm=15.62086201
 24756: 18 [  870/ 1327], train_loss/perplexity = 3.83815503/46.4397163 secs/batch = 0.2978s, grad.norm=16.08287430
 24761: 18 [  875/ 1327], train_loss/perplexity = 3.43987679/31.1831169 secs/batch = 0.3000s, grad.norm=14.62107086
 24766: 18 [  880/ 1327], train_loss/perplexity = 3.64553690/38.3033333 secs/batch = 0.3014s, grad.norm=15.08630371
 24771: 18 [  885/ 1327], train_loss/perplexity = 3.83112240/46.1142693 secs/batch = 0.2949s, grad.norm=14.91633129
 24776: 18 [  890/ 1327], train_loss/perplexity = 3.94005108/51.4212265 secs/batch = 0.2997s, grad.norm=15.26146412
 24781: 18 [  895/ 1327], train_loss/perplexity = 3.91814661/50.3071213 secs/batch = 0.2940s, grad.norm=15.26561069
 24786: 18 [  900/ 1327], train_loss/perplexity = 3.75323367/42.6588020 secs/batch = 0.2950s, grad.norm=14.69685268
 24791: 18 [  905/ 1327], train_loss/perplexity = 3.64142346/38.1460991 secs/batch = 0.2951s, grad.norm=14.37512112
 24796: 18 [  910/ 1327], train_loss/perplexity = 3.70443082/40.6269150 secs/batch = 0.2962s, grad.norm=13.83673286
 24801: 18 [  915/ 1327], train_loss/perplexity = 3.97406983/53.2006073 secs/batch = 0.2938s, grad.norm=14.82579231
 24806: 18 [  920/ 1327], train_loss/perplexity = 4.07187986/58.6671448 secs/batch = 0.2996s, grad.norm=16.15410995
 24811: 18 [  925/ 1327], train_loss/perplexity = 3.89149737/48.9841805 secs/batch = 0.2961s, grad.norm=15.04032326
 24816: 18 [  930/ 1327], train_loss/perplexity = 3.98883486/53.9919434 secs/batch = 0.2951s, grad.norm=15.29080391
 24821: 18 [  935/ 1327], train_loss/perplexity = 3.97839999/53.4314766 secs/batch = 0.2948s, grad.norm=14.69649315
 24826: 18 [  940/ 1327], train_loss/perplexity = 3.97424126/53.2097282 secs/batch = 0.2974s, grad.norm=15.14518642
 24831: 18 [  945/ 1327], train_loss/perplexity = 4.12978458/62.1645317 secs/batch = 0.2971s, grad.norm=15.05979729
 24836: 18 [  950/ 1327], train_loss/perplexity = 3.87266636/48.0703888 secs/batch = 0.3013s, grad.norm=15.53356552
 24841: 18 [  955/ 1327], train_loss/perplexity = 3.88845897/48.8355713 secs/batch = 0.2989s, grad.norm=15.34651089
 24846: 18 [  960/ 1327], train_loss/perplexity = 4.15715504/63.8894997 secs/batch = 0.2907s, grad.norm=16.05829620
 24851: 18 [  965/ 1327], train_loss/perplexity = 3.94643283/51.7504349 secs/batch = 0.2946s, grad.norm=15.38395596
 24856: 18 [  970/ 1327], train_loss/perplexity = 4.13910770/62.7468071 secs/batch = 0.3001s, grad.norm=15.75604630
 24861: 18 [  975/ 1327], train_loss/perplexity = 3.78271127/43.9350014 secs/batch = 0.2998s, grad.norm=16.13506508
 24866: 18 [  980/ 1327], train_loss/perplexity = 3.73383522/41.8392639 secs/batch = 0.2947s, grad.norm=15.22602081
 24871: 18 [  985/ 1327], train_loss/perplexity = 3.77033448/43.3945770 secs/batch = 0.3002s, grad.norm=16.03118324
 24876: 18 [  990/ 1327], train_loss/perplexity = 4.08432484/59.4018173 secs/batch = 0.2963s, grad.norm=15.74245071
 24881: 18 [  995/ 1327], train_loss/perplexity = 4.07641077/58.9335632 secs/batch = 0.2949s, grad.norm=15.43173790
 24886: 18 [ 1000/ 1327], train_loss/perplexity = 3.61915588/37.3060646 secs/batch = 0.3007s, grad.norm=15.45209217
 24891: 18 [ 1005/ 1327], train_loss/perplexity = 4.04184484/56.9312744 secs/batch = 0.2926s, grad.norm=15.62897205
 24896: 18 [ 1010/ 1327], train_loss/perplexity = 3.54500389/34.6398201 secs/batch = 0.2945s, grad.norm=14.44592094
 24901: 18 [ 1015/ 1327], train_loss/perplexity = 4.13393450/62.4230423 secs/batch = 0.2952s, grad.norm=15.27919006
 24906: 18 [ 1020/ 1327], train_loss/perplexity = 4.11695671/61.3721848 secs/batch = 0.3017s, grad.norm=15.08071232
 24911: 18 [ 1025/ 1327], train_loss/perplexity = 4.11071920/60.9905663 secs/batch = 0.2921s, grad.norm=15.38089657
 24916: 18 [ 1030/ 1327], train_loss/perplexity = 3.77325678/43.5215759 secs/batch = 0.3007s, grad.norm=14.97353649
 24921: 18 [ 1035/ 1327], train_loss/perplexity = 3.79343796/44.4088135 secs/batch = 0.2944s, grad.norm=14.94731140
 24926: 18 [ 1040/ 1327], train_loss/perplexity = 3.93114233/50.9651642 secs/batch = 0.2946s, grad.norm=16.10659790
 24931: 18 [ 1045/ 1327], train_loss/perplexity = 3.51305771/33.5507011 secs/batch = 0.2930s, grad.norm=14.52530670
 24936: 18 [ 1050/ 1327], train_loss/perplexity = 3.59741044/36.5035820 secs/batch = 0.2992s, grad.norm=15.20501614
 24941: 18 [ 1055/ 1327], train_loss/perplexity = 3.67901921/39.6075287 secs/batch = 0.2953s, grad.norm=15.63586235
 24946: 18 [ 1060/ 1327], train_loss/perplexity = 3.29881334/27.0804844 secs/batch = 0.3001s, grad.norm=16.08692169
 24951: 18 [ 1065/ 1327], train_loss/perplexity = 3.53330040/34.2367783 secs/batch = 0.2924s, grad.norm=15.71014214
 24956: 18 [ 1070/ 1327], train_loss/perplexity = 3.75268602/42.6354485 secs/batch = 0.3013s, grad.norm=16.09997749
 24961: 18 [ 1075/ 1327], train_loss/perplexity = 3.53624272/34.3376617 secs/batch = 0.3017s, grad.norm=15.00543308
 24966: 18 [ 1080/ 1327], train_loss/perplexity = 3.57440329/35.6733284 secs/batch = 0.2953s, grad.norm=15.46416569
 24971: 18 [ 1085/ 1327], train_loss/perplexity = 3.43768215/31.1147556 secs/batch = 0.2949s, grad.norm=15.17076683
 24976: 18 [ 1090/ 1327], train_loss/perplexity = 3.67993236/39.6437111 secs/batch = 0.2954s, grad.norm=15.69697189
 24981: 18 [ 1095/ 1327], train_loss/perplexity = 3.77156734/43.4481087 secs/batch = 0.2965s, grad.norm=15.49620438
 24986: 18 [ 1100/ 1327], train_loss/perplexity = 3.49594164/32.9813309 secs/batch = 0.2941s, grad.norm=16.32821655
 24991: 18 [ 1105/ 1327], train_loss/perplexity = 3.44825196/31.4453773 secs/batch = 0.2938s, grad.norm=15.66156864
 24996: 18 [ 1110/ 1327], train_loss/perplexity = 3.76811004/43.2981567 secs/batch = 0.2936s, grad.norm=16.18677139
 25001: 18 [ 1115/ 1327], train_loss/perplexity = 3.62732434/37.6120453 secs/batch = 0.2948s, grad.norm=15.39647198
 25006: 18 [ 1120/ 1327], train_loss/perplexity = 3.83814621/46.4393044 secs/batch = 0.2957s, grad.norm=15.40012932
 25011: 18 [ 1125/ 1327], train_loss/perplexity = 4.02181053/55.8020439 secs/batch = 0.2943s, grad.norm=16.29860878
 25016: 18 [ 1130/ 1327], train_loss/perplexity = 3.60011959/36.6026115 secs/batch = 0.2956s, grad.norm=15.44750500
 25021: 18 [ 1135/ 1327], train_loss/perplexity = 3.60711432/36.8595352 secs/batch = 0.3018s, grad.norm=15.17076778
 25026: 18 [ 1140/ 1327], train_loss/perplexity = 3.96658897/52.8041077 secs/batch = 0.2944s, grad.norm=15.94623947
 25031: 18 [ 1145/ 1327], train_loss/perplexity = 3.77641296/43.6591530 secs/batch = 0.3012s, grad.norm=15.14936829
 25036: 18 [ 1150/ 1327], train_loss/perplexity = 3.75626469/42.7882996 secs/batch = 0.2963s, grad.norm=15.06574917
 25041: 18 [ 1155/ 1327], train_loss/perplexity = 3.78030896/43.8295822 secs/batch = 0.2996s, grad.norm=15.66906548
 25046: 18 [ 1160/ 1327], train_loss/perplexity = 3.78875637/44.2013969 secs/batch = 0.2942s, grad.norm=15.54809761
 25051: 18 [ 1165/ 1327], train_loss/perplexity = 3.84588814/46.8002319 secs/batch = 0.2944s, grad.norm=16.27902412
 25056: 18 [ 1170/ 1327], train_loss/perplexity = 3.70457506/40.6327782 secs/batch = 0.3001s, grad.norm=15.58121395
 25061: 18 [ 1175/ 1327], train_loss/perplexity = 3.46286249/31.9081821 secs/batch = 0.2930s, grad.norm=15.55574703
 25066: 18 [ 1180/ 1327], train_loss/perplexity = 3.52848315/34.0722466 secs/batch = 0.2953s, grad.norm=15.70526791
 25071: 18 [ 1185/ 1327], train_loss/perplexity = 3.64597940/38.3202858 secs/batch = 0.2956s, grad.norm=15.32263660
 25076: 18 [ 1190/ 1327], train_loss/perplexity = 3.73107791/41.7240601 secs/batch = 0.3000s, grad.norm=15.92464447
 25081: 18 [ 1195/ 1327], train_loss/perplexity = 3.52717018/34.0275383 secs/batch = 0.3021s, grad.norm=15.20620155
 25086: 18 [ 1200/ 1327], train_loss/perplexity = 3.54238367/34.5491753 secs/batch = 0.2949s, grad.norm=15.57323360
 25091: 18 [ 1205/ 1327], train_loss/perplexity = 3.55136895/34.8610077 secs/batch = 0.2958s, grad.norm=15.92529011
 25096: 18 [ 1210/ 1327], train_loss/perplexity = 3.11464763/22.5254917 secs/batch = 0.2964s, grad.norm=15.51174927
 25101: 18 [ 1215/ 1327], train_loss/perplexity = 3.43737745/31.1052761 secs/batch = 0.2942s, grad.norm=14.95102215
 25106: 18 [ 1220/ 1327], train_loss/perplexity = 3.53187346/34.1879578 secs/batch = 0.2989s, grad.norm=15.82117462
 25111: 18 [ 1225/ 1327], train_loss/perplexity = 3.28770852/26.7814236 secs/batch = 0.3007s, grad.norm=16.61254120
 25116: 18 [ 1230/ 1327], train_loss/perplexity = 3.62190676/37.4088287 secs/batch = 0.2982s, grad.norm=15.46106434
 25121: 18 [ 1235/ 1327], train_loss/perplexity = 3.49574852/32.9749603 secs/batch = 0.2953s, grad.norm=15.32384777
 25126: 18 [ 1240/ 1327], train_loss/perplexity = 3.74357533/42.2487755 secs/batch = 0.2947s, grad.norm=16.08559036
 25131: 18 [ 1245/ 1327], train_loss/perplexity = 3.65644217/38.7233276 secs/batch = 0.2962s, grad.norm=15.27491760
 25136: 18 [ 1250/ 1327], train_loss/perplexity = 3.82649517/45.9013786 secs/batch = 0.2944s, grad.norm=15.43962574
 25141: 18 [ 1255/ 1327], train_loss/perplexity = 3.84363699/46.6949959 secs/batch = 0.2929s, grad.norm=15.86115837
 25146: 18 [ 1260/ 1327], train_loss/perplexity = 3.61521649/37.1593895 secs/batch = 0.2965s, grad.norm=16.23556328
 25151: 18 [ 1265/ 1327], train_loss/perplexity = 3.81112456/45.2012405 secs/batch = 0.2949s, grad.norm=16.06680489
 25156: 18 [ 1270/ 1327], train_loss/perplexity = 3.53068709/34.1474228 secs/batch = 0.2941s, grad.norm=15.84170341
 25161: 18 [ 1275/ 1327], train_loss/perplexity = 3.72733688/41.5682602 secs/batch = 0.2925s, grad.norm=16.37514687
 25166: 18 [ 1280/ 1327], train_loss/perplexity = 3.65660357/38.7295761 secs/batch = 0.2987s, grad.norm=15.65345860
 25171: 18 [ 1285/ 1327], train_loss/perplexity = 3.53032708/34.1351318 secs/batch = 0.2969s, grad.norm=15.44171333
 25176: 18 [ 1290/ 1327], train_loss/perplexity = 3.71877432/41.2138481 secs/batch = 0.2992s, grad.norm=15.04321098
 25181: 18 [ 1295/ 1327], train_loss/perplexity = 3.81348276/45.3079605 secs/batch = 0.2951s, grad.norm=15.82988167
 25186: 18 [ 1300/ 1327], train_loss/perplexity = 3.95571494/52.2330246 secs/batch = 0.2993s, grad.norm=15.59434414
 25191: 18 [ 1305/ 1327], train_loss/perplexity = 3.95399928/52.1434860 secs/batch = 0.2950s, grad.norm=16.39941216
 25196: 18 [ 1310/ 1327], train_loss/perplexity = 4.20074797/66.7362289 secs/batch = 0.2947s, grad.norm=16.38194656
 25201: 18 [ 1315/ 1327], train_loss/perplexity = 4.00097084/54.6511803 secs/batch = 0.2996s, grad.norm=15.92961884
 25206: 18 [ 1320/ 1327], train_loss/perplexity = 4.01519394/55.4340439 secs/batch = 0.2948s, grad.norm=15.80087280
 25211: 18 [ 1325/ 1327], train_loss/perplexity = 3.86152411/47.5377502 secs/batch = 0.2944s, grad.norm=15.77531052
Epoch training time: 392.7519094944
	> validation loss = 4.57497549, perplexity = 97.02565765
	> validation loss = 4.52613544, perplexity = 92.40077972
	> validation loss = 4.50596809, perplexity = 90.55596924
	> validation loss = 4.49541092, perplexity = 89.60498047
	> validation loss = 4.64182854, perplexity = 103.73385620
	> validation loss = 4.59839249, perplexity = 99.32452393
	> validation loss = 4.54128551, perplexity = 93.81131744
	> validation loss = 4.37573528, perplexity = 79.49826813
	> validation loss = 4.16746235, perplexity = 64.55143738
	> validation loss = 4.30553532, perplexity = 74.10887909
	> validation loss = 4.50211763, perplexity = 90.20795441
	> validation loss = 4.46705818, perplexity = 87.10011292
	> validation loss = 4.40881062, perplexity = 82.17166901
	> validation loss = 4.14244747, perplexity = 62.95671844
	> validation loss = 4.13169527, perplexity = 62.28342056
	> validation loss = 4.14173365, perplexity = 62.91179276
	> validation loss = 4.58904076, perplexity = 98.39999390
	> validation loss = 4.04827642, perplexity = 57.29861450
	> validation loss = 4.55666018, perplexity = 95.26477814
	> validation loss = 4.51446152, perplexity = 91.32837677
	> validation loss = 4.22459888, perplexity = 68.34708405
at the end of epoch: 18
train loss = 3.86515461, perplexity = 47.71064852
validation loss = 4.40958913, perplexity = 82.23566853
Saved model cv/epoch018_4.4096.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.0625
new learning rate is: 0.03125
 25218: 19 [    5/ 1327], train_loss/perplexity = 4.11508608/61.2574883 secs/batch = 0.2984s, grad.norm=16.06979752
 25223: 19 [   10/ 1327], train_loss/perplexity = 3.64419270/38.2518806 secs/batch = 0.2970s, grad.norm=15.07242584
 25228: 19 [   15/ 1327], train_loss/perplexity = 4.07246208/58.7013130 secs/batch = 0.2941s, grad.norm=15.27859211
 25233: 19 [   20/ 1327], train_loss/perplexity = 4.23510027/69.0686035 secs/batch = 0.2935s, grad.norm=15.20032787
 25238: 19 [   25/ 1327], train_loss/perplexity = 4.02393436/55.9206848 secs/batch = 0.2945s, grad.norm=16.42879295
 25243: 19 [   30/ 1327], train_loss/perplexity = 4.13402224/62.4285202 secs/batch = 0.2984s, grad.norm=16.90645409
 25248: 19 [   35/ 1327], train_loss/perplexity = 3.77825022/43.7394409 secs/batch = 0.2952s, grad.norm=15.54097652
 25253: 19 [   40/ 1327], train_loss/perplexity = 3.91729069/50.2640800 secs/batch = 0.2943s, grad.norm=15.96901321
 25258: 19 [   45/ 1327], train_loss/perplexity = 3.67424083/39.4187202 secs/batch = 0.2934s, grad.norm=15.02114105
 25263: 19 [   50/ 1327], train_loss/perplexity = 3.87860274/48.3566017 secs/batch = 0.2992s, grad.norm=15.64925194
 25268: 19 [   55/ 1327], train_loss/perplexity = 3.79831290/44.6258316 secs/batch = 0.2943s, grad.norm=16.22686577
 25273: 19 [   60/ 1327], train_loss/perplexity = 4.08215427/59.2730217 secs/batch = 0.2987s, grad.norm=16.11498833
 25278: 19 [   65/ 1327], train_loss/perplexity = 3.67737031/39.5422745 secs/batch = 0.2980s, grad.norm=15.46028042
 25283: 19 [   70/ 1327], train_loss/perplexity = 3.51533842/33.6273079 secs/batch = 0.2960s, grad.norm=15.56990528
 25288: 19 [   75/ 1327], train_loss/perplexity = 3.36720037/28.9972324 secs/batch = 0.2949s, grad.norm=14.63615990
 25293: 19 [   80/ 1327], train_loss/perplexity = 3.83503914/46.2952385 secs/batch = 0.2932s, grad.norm=15.99873638
 25298: 19 [   85/ 1327], train_loss/perplexity = 3.81858778/45.5398521 secs/batch = 0.3001s, grad.norm=15.98665333
 25303: 19 [   90/ 1327], train_loss/perplexity = 3.93273234/51.0462646 secs/batch = 0.2991s, grad.norm=16.28314018
 25308: 19 [   95/ 1327], train_loss/perplexity = 3.76777840/43.2837982 secs/batch = 0.3017s, grad.norm=15.77265263
 25313: 19 [  100/ 1327], train_loss/perplexity = 4.03368998/56.4688950 secs/batch = 0.2958s, grad.norm=16.38217735
 25318: 19 [  105/ 1327], train_loss/perplexity = 3.77538681/43.6143761 secs/batch = 0.3004s, grad.norm=16.48363495
 25323: 19 [  110/ 1327], train_loss/perplexity = 3.67689157/39.5233459 secs/batch = 0.2954s, grad.norm=15.87747860
 25328: 19 [  115/ 1327], train_loss/perplexity = 3.69125128/40.0949860 secs/batch = 0.2950s, grad.norm=16.20170784
 25333: 19 [  120/ 1327], train_loss/perplexity = 3.79696226/44.5656013 secs/batch = 0.2941s, grad.norm=16.22642708
 25338: 19 [  125/ 1327], train_loss/perplexity = 3.82576108/45.8676949 secs/batch = 0.3007s, grad.norm=16.21103859
 25343: 19 [  130/ 1327], train_loss/perplexity = 3.81910443/45.5633850 secs/batch = 0.2949s, grad.norm=17.21412277
 25348: 19 [  135/ 1327], train_loss/perplexity = 3.80590534/44.9659424 secs/batch = 0.2924s, grad.norm=15.75338364
 25353: 19 [  140/ 1327], train_loss/perplexity = 4.10260582/60.4977303 secs/batch = 0.2955s, grad.norm=16.59101105
 25358: 19 [  145/ 1327], train_loss/perplexity = 3.92679024/50.7438393 secs/batch = 0.2953s, grad.norm=16.83923721
 25363: 19 [  150/ 1327], train_loss/perplexity = 3.98583174/53.8300438 secs/batch = 0.2947s, grad.norm=16.53065491
 25368: 19 [  155/ 1327], train_loss/perplexity = 4.21581268/67.7491989 secs/batch = 0.3006s, grad.norm=16.07841492
 25373: 19 [  160/ 1327], train_loss/perplexity = 3.88533735/48.6833649 secs/batch = 0.2959s, grad.norm=15.03472424
 25378: 19 [  165/ 1327], train_loss/perplexity = 4.04954815/57.3715286 secs/batch = 0.2950s, grad.norm=15.94127560
 25383: 19 [  170/ 1327], train_loss/perplexity = 3.82082176/45.6417007 secs/batch = 0.3019s, grad.norm=15.38515472
 25388: 19 [  175/ 1327], train_loss/perplexity = 4.19334173/66.2437897 secs/batch = 0.2948s, grad.norm=15.77147579
 25393: 19 [  180/ 1327], train_loss/perplexity = 3.88746023/48.7868233 secs/batch = 0.2950s, grad.norm=16.07640076
 25398: 19 [  185/ 1327], train_loss/perplexity = 4.29866886/73.6017532 secs/batch = 0.2985s, grad.norm=16.12498283
 25403: 19 [  190/ 1327], train_loss/perplexity = 3.85199046/47.0866928 secs/batch = 0.2997s, grad.norm=14.97058868
 25408: 19 [  195/ 1327], train_loss/perplexity = 4.10929441/60.9037285 secs/batch = 0.3017s, grad.norm=15.32285690
 25413: 19 [  200/ 1327], train_loss/perplexity = 3.90818501/49.8084679 secs/batch = 0.2939s, grad.norm=15.62419891
 25418: 19 [  205/ 1327], train_loss/perplexity = 4.21767521/67.8755035 secs/batch = 0.2952s, grad.norm=15.84961224
 25423: 19 [  210/ 1327], train_loss/perplexity = 4.04087114/56.8758698 secs/batch = 0.2949s, grad.norm=15.30311298
 25428: 19 [  215/ 1327], train_loss/perplexity = 4.10493469/60.6387825 secs/batch = 0.2992s, grad.norm=14.88821983
 25433: 19 [  220/ 1327], train_loss/perplexity = 4.05600023/57.7428894 secs/batch = 0.3007s, grad.norm=15.60029125
 25438: 19 [  225/ 1327], train_loss/perplexity = 4.28025532/72.2588882 secs/batch = 0.2957s, grad.norm=15.73431110
 25443: 19 [  230/ 1327], train_loss/perplexity = 4.12271643/61.7266922 secs/batch = 0.2992s, grad.norm=16.86956024
 25448: 19 [  235/ 1327], train_loss/perplexity = 3.95438313/52.1635056 secs/batch = 0.2939s, grad.norm=15.67538071
 25453: 19 [  240/ 1327], train_loss/perplexity = 3.64129734/38.1412888 secs/batch = 0.3018s, grad.norm=15.64038563
 25458: 19 [  245/ 1327], train_loss/perplexity = 3.95243859/52.0621719 secs/batch = 0.2993s, grad.norm=15.88000584
 25463: 19 [  250/ 1327], train_loss/perplexity = 3.86667156/47.7830772 secs/batch = 0.2943s, grad.norm=15.09448910
 25468: 19 [  255/ 1327], train_loss/perplexity = 3.82804370/45.9725151 secs/batch = 0.2953s, grad.norm=15.63075638
 25473: 19 [  260/ 1327], train_loss/perplexity = 4.08024216/59.1597939 secs/batch = 0.2945s, grad.norm=16.56041908
 25478: 19 [  265/ 1327], train_loss/perplexity = 4.27497387/71.8782578 secs/batch = 0.2990s, grad.norm=15.53867626
 25483: 19 [  270/ 1327], train_loss/perplexity = 4.23587799/69.1223373 secs/batch = 0.2960s, grad.norm=15.62762165
 25488: 19 [  275/ 1327], train_loss/perplexity = 4.13700581/62.6150589 secs/batch = 0.2998s, grad.norm=15.54562283
 25493: 19 [  280/ 1327], train_loss/perplexity = 3.97292423/53.1396980 secs/batch = 0.2952s, grad.norm=15.80349636
 25498: 19 [  285/ 1327], train_loss/perplexity = 4.36033344/78.2832336 secs/batch = 0.2999s, grad.norm=15.91283417
 25503: 19 [  290/ 1327], train_loss/perplexity = 3.97703123/53.3583908 secs/batch = 0.2988s, grad.norm=15.81788445
 25508: 19 [  295/ 1327], train_loss/perplexity = 3.75356722/42.6730347 secs/batch = 0.3006s, grad.norm=15.45177841
 25513: 19 [  300/ 1327], train_loss/perplexity = 3.35848522/28.7456150 secs/batch = 0.2948s, grad.norm=14.69498348
 25518: 19 [  305/ 1327], train_loss/perplexity = 3.79800439/44.6120682 secs/batch = 0.3009s, grad.norm=14.97057438
 25523: 19 [  310/ 1327], train_loss/perplexity = 3.87238383/48.0568085 secs/batch = 0.3023s, grad.norm=15.57289124
 25528: 19 [  315/ 1327], train_loss/perplexity = 3.41457415/30.4039993 secs/batch = 0.2998s, grad.norm=15.21524143
 25533: 19 [  320/ 1327], train_loss/perplexity = 3.38644433/29.5606575 secs/batch = 0.2956s, grad.norm=15.94970131
 25538: 19 [  325/ 1327], train_loss/perplexity = 3.39346385/29.7688885 secs/batch = 0.2958s, grad.norm=14.79915714
 25543: 19 [  330/ 1327], train_loss/perplexity = 3.94413090/51.6314468 secs/batch = 0.2945s, grad.norm=15.64902973
 25548: 19 [  335/ 1327], train_loss/perplexity = 3.33828139/28.1706715 secs/batch = 0.2945s, grad.norm=14.10875416
 25553: 19 [  340/ 1327], train_loss/perplexity = 4.08656359/59.5349541 secs/batch = 0.2931s, grad.norm=15.33740139
 25558: 19 [  345/ 1327], train_loss/perplexity = 3.95050573/51.9616394 secs/batch = 0.2951s, grad.norm=14.71400928
 25563: 19 [  350/ 1327], train_loss/perplexity = 3.83402109/46.2481346 secs/batch = 0.2948s, grad.norm=15.73149967
 25568: 19 [  355/ 1327], train_loss/perplexity = 3.86380005/47.6460648 secs/batch = 0.2940s, grad.norm=15.27650452
 25573: 19 [  360/ 1327], train_loss/perplexity = 3.94975734/51.9227638 secs/batch = 0.2951s, grad.norm=16.76957130
 25578: 19 [  365/ 1327], train_loss/perplexity = 4.01113653/55.2095833 secs/batch = 0.3003s, grad.norm=16.04764557
 25583: 19 [  370/ 1327], train_loss/perplexity = 4.10713053/60.7720833 secs/batch = 0.2998s, grad.norm=15.72713947
 25588: 19 [  375/ 1327], train_loss/perplexity = 3.50553656/33.2993050 secs/batch = 0.2958s, grad.norm=15.79895115
 25593: 19 [  380/ 1327], train_loss/perplexity = 3.62280822/37.4425659 secs/batch = 0.2955s, grad.norm=15.52751732
 25598: 19 [  385/ 1327], train_loss/perplexity = 3.81832957/45.5280952 secs/batch = 0.2993s, grad.norm=15.74670696
 25603: 19 [  390/ 1327], train_loss/perplexity = 3.90856600/49.8274498 secs/batch = 0.2947s, grad.norm=15.51737118
 25608: 19 [  395/ 1327], train_loss/perplexity = 3.95483899/52.1872902 secs/batch = 0.2958s, grad.norm=15.52260208
 25613: 19 [  400/ 1327], train_loss/perplexity = 3.90289688/49.5457687 secs/batch = 0.2963s, grad.norm=15.70596123
 25618: 19 [  405/ 1327], train_loss/perplexity = 4.20026255/66.7038422 secs/batch = 0.2952s, grad.norm=15.73183250
 25623: 19 [  410/ 1327], train_loss/perplexity = 3.77997112/43.8147774 secs/batch = 0.2949s, grad.norm=15.45232964
 25628: 19 [  415/ 1327], train_loss/perplexity = 3.85974741/47.4533653 secs/batch = 0.3010s, grad.norm=15.48816395
 25633: 19 [  420/ 1327], train_loss/perplexity = 3.38861322/29.6248417 secs/batch = 0.2951s, grad.norm=15.02158833
 25638: 19 [  425/ 1327], train_loss/perplexity = 3.72274637/41.3778763 secs/batch = 0.3013s, grad.norm=15.93742180
 25643: 19 [  430/ 1327], train_loss/perplexity = 3.95389128/52.1378555 secs/batch = 0.2952s, grad.norm=16.53216362
 25648: 19 [  435/ 1327], train_loss/perplexity = 3.98568487/53.8221397 secs/batch = 0.2926s, grad.norm=16.13228798
 25653: 19 [  440/ 1327], train_loss/perplexity = 3.49065042/32.8072777 secs/batch = 0.2990s, grad.norm=15.64146805
 25658: 19 [  445/ 1327], train_loss/perplexity = 3.88818836/48.8223572 secs/batch = 0.2949s, grad.norm=16.57814026
 25663: 19 [  450/ 1327], train_loss/perplexity = 3.97665215/53.3381653 secs/batch = 0.2922s, grad.norm=15.98574543
 25668: 19 [  455/ 1327], train_loss/perplexity = 3.79982042/44.6931572 secs/batch = 0.2943s, grad.norm=15.38842201
 25673: 19 [  460/ 1327], train_loss/perplexity = 3.77930403/43.7855568 secs/batch = 0.2948s, grad.norm=16.18204689
 25678: 19 [  465/ 1327], train_loss/perplexity = 3.47337389/32.2453499 secs/batch = 0.3002s, grad.norm=16.10003853
 25683: 19 [  470/ 1327], train_loss/perplexity = 4.24791718/69.9595490 secs/batch = 0.2959s, grad.norm=15.66299725
 25688: 19 [  475/ 1327], train_loss/perplexity = 3.70228076/40.5396614 secs/batch = 0.2958s, grad.norm=16.13048172
 25693: 19 [  480/ 1327], train_loss/perplexity = 3.74469137/42.2959518 secs/batch = 0.2943s, grad.norm=15.67807579
 25698: 19 [  485/ 1327], train_loss/perplexity = 3.82794070/45.9677811 secs/batch = 0.2963s, grad.norm=15.67881489
 25703: 19 [  490/ 1327], train_loss/perplexity = 3.71485353/41.0525742 secs/batch = 0.2954s, grad.norm=16.95197487
 25708: 19 [  495/ 1327], train_loss/perplexity = 3.76589298/43.2022667 secs/batch = 0.3015s, grad.norm=15.62903881
 25713: 19 [  500/ 1327], train_loss/perplexity = 3.87094426/47.9876785 secs/batch = 0.2953s, grad.norm=15.43679714
 25718: 19 [  505/ 1327], train_loss/perplexity = 4.05713892/57.8086777 secs/batch = 0.2979s, grad.norm=15.09935284
 25723: 19 [  510/ 1327], train_loss/perplexity = 4.35424423/77.8079987 secs/batch = 0.2940s, grad.norm=15.69083881
 25728: 19 [  515/ 1327], train_loss/perplexity = 4.02918100/56.2148514 secs/batch = 0.2952s, grad.norm=15.50213242
 25733: 19 [  520/ 1327], train_loss/perplexity = 4.14311695/62.9988785 secs/batch = 0.2935s, grad.norm=15.57974052
 25738: 19 [  525/ 1327], train_loss/perplexity = 3.76715708/43.2569122 secs/batch = 0.2982s, grad.norm=15.35803509
 25743: 19 [  530/ 1327], train_loss/perplexity = 3.72986913/41.6736526 secs/batch = 0.2961s, grad.norm=15.53272438
 25748: 19 [  535/ 1327], train_loss/perplexity = 3.92025781/50.4134407 secs/batch = 0.3002s, grad.norm=15.55149841
 25753: 19 [  540/ 1327], train_loss/perplexity = 3.97514248/53.2577057 secs/batch = 0.2962s, grad.norm=15.51927185
 25758: 19 [  545/ 1327], train_loss/perplexity = 3.85999465/47.4650955 secs/batch = 0.2997s, grad.norm=15.96428204
 25763: 19 [  550/ 1327], train_loss/perplexity = 3.88438988/48.6372604 secs/batch = 0.2961s, grad.norm=15.54267025
 25768: 19 [  555/ 1327], train_loss/perplexity = 3.77360010/43.5365181 secs/batch = 0.2949s, grad.norm=15.26453400
 25773: 19 [  560/ 1327], train_loss/perplexity = 4.00535583/54.8913536 secs/batch = 0.2949s, grad.norm=16.93829346
 25778: 19 [  565/ 1327], train_loss/perplexity = 3.74152541/42.1622543 secs/batch = 0.3019s, grad.norm=16.47106743
 25783: 19 [  570/ 1327], train_loss/perplexity = 3.74431944/42.2802238 secs/batch = 0.2976s, grad.norm=16.10484886
 25788: 19 [  575/ 1327], train_loss/perplexity = 3.55451012/34.9706841 secs/batch = 0.2958s, grad.norm=15.73771572
 25793: 19 [  580/ 1327], train_loss/perplexity = 3.94101787/51.4709663 secs/batch = 0.2953s, grad.norm=16.50843048
 25798: 19 [  585/ 1327], train_loss/perplexity = 3.61343765/37.0933495 secs/batch = 0.3003s, grad.norm=15.32897091
 25803: 19 [  590/ 1327], train_loss/perplexity = 3.92207336/50.5050507 secs/batch = 0.2948s, grad.norm=15.93128872
 25808: 19 [  595/ 1327], train_loss/perplexity = 3.86151624/47.5373764 secs/batch = 0.3007s, grad.norm=16.54661942
 25813: 19 [  600/ 1327], train_loss/perplexity = 4.10733509/60.7845154 secs/batch = 0.2934s, grad.norm=15.26346302
 25818: 19 [  605/ 1327], train_loss/perplexity = 3.97714114/53.3642540 secs/batch = 0.2957s, grad.norm=15.15284729
 25823: 19 [  610/ 1327], train_loss/perplexity = 4.18780041/65.8777237 secs/batch = 0.2996s, grad.norm=15.72232819
 25828: 19 [  615/ 1327], train_loss/perplexity = 3.76188016/43.0292511 secs/batch = 0.2994s, grad.norm=14.89797497
 25833: 19 [  620/ 1327], train_loss/perplexity = 4.09088278/59.7926521 secs/batch = 0.2977s, grad.norm=15.49853325
 25838: 19 [  625/ 1327], train_loss/perplexity = 4.07134104/58.6355438 secs/batch = 0.2939s, grad.norm=15.37036610
 25843: 19 [  630/ 1327], train_loss/perplexity = 4.15960217/64.0460358 secs/batch = 0.3002s, grad.norm=15.61035347
 25848: 19 [  635/ 1327], train_loss/perplexity = 3.82279181/45.7317047 secs/batch = 0.2953s, grad.norm=15.15085793
 25853: 19 [  640/ 1327], train_loss/perplexity = 3.85312009/47.1399155 secs/batch = 0.2947s, grad.norm=15.39764500
 25858: 19 [  645/ 1327], train_loss/perplexity = 4.17273855/64.8929214 secs/batch = 0.2939s, grad.norm=16.31992912
 25863: 19 [  650/ 1327], train_loss/perplexity = 3.59789371/36.5212288 secs/batch = 0.2959s, grad.norm=15.78090858
 25868: 19 [  655/ 1327], train_loss/perplexity = 3.79267335/44.3748703 secs/batch = 0.3008s, grad.norm=15.89785576
 25873: 19 [  660/ 1327], train_loss/perplexity = 3.76964808/43.3647995 secs/batch = 0.3021s, grad.norm=15.72533607
 25878: 19 [  665/ 1327], train_loss/perplexity = 3.87039232/47.9611969 secs/batch = 0.2949s, grad.norm=15.53125381
 25883: 19 [  670/ 1327], train_loss/perplexity = 3.73333383/41.8182907 secs/batch = 0.2957s, grad.norm=15.38848209
 25888: 19 [  675/ 1327], train_loss/perplexity = 3.60623026/36.8269615 secs/batch = 0.2960s, grad.norm=15.80326843
 25893: 19 [  680/ 1327], train_loss/perplexity = 3.83425617/46.2590065 secs/batch = 0.2941s, grad.norm=16.36122322
 25898: 19 [  685/ 1327], train_loss/perplexity = 3.60932827/36.9412308 secs/batch = 0.2930s, grad.norm=15.00302124
 25903: 19 [  690/ 1327], train_loss/perplexity = 4.10219288/60.4727516 secs/batch = 0.2948s, grad.norm=15.90651512
 25908: 19 [  695/ 1327], train_loss/perplexity = 3.92407560/50.6062775 secs/batch = 0.2962s, grad.norm=15.34487438
 25913: 19 [  700/ 1327], train_loss/perplexity = 4.03862047/56.7480049 secs/batch = 0.2955s, grad.norm=15.73117638
 25918: 19 [  705/ 1327], train_loss/perplexity = 3.90953708/49.8758583 secs/batch = 0.2945s, grad.norm=15.05251598
 25923: 19 [  710/ 1327], train_loss/perplexity = 3.71471000/41.0466805 secs/batch = 0.2950s, grad.norm=15.90742016
 25928: 19 [  715/ 1327], train_loss/perplexity = 3.57623315/35.7386665 secs/batch = 0.3005s, grad.norm=15.76078796
 25933: 19 [  720/ 1327], train_loss/perplexity = 3.63857341/38.0375328 secs/batch = 0.3023s, grad.norm=16.42548943
 25938: 19 [  725/ 1327], train_loss/perplexity = 3.65786552/38.7784843 secs/batch = 0.2950s, grad.norm=15.62052917
 25943: 19 [  730/ 1327], train_loss/perplexity = 3.79144573/44.3204308 secs/batch = 0.3010s, grad.norm=16.65718460
 25948: 19 [  735/ 1327], train_loss/perplexity = 3.84057856/46.5523987 secs/batch = 0.3012s, grad.norm=16.11229134
 25953: 19 [  740/ 1327], train_loss/perplexity = 3.42566490/30.7430782 secs/batch = 0.2996s, grad.norm=14.99616623
 25958: 19 [  745/ 1327], train_loss/perplexity = 3.92287469/50.5455399 secs/batch = 0.2959s, grad.norm=15.84670162
 25963: 19 [  750/ 1327], train_loss/perplexity = 3.74571872/42.3394279 secs/batch = 0.2960s, grad.norm=15.72059631
 25968: 19 [  755/ 1327], train_loss/perplexity = 3.55284023/34.9123344 secs/batch = 0.3005s, grad.norm=15.40253830
 25973: 19 [  760/ 1327], train_loss/perplexity = 3.48759675/32.7072487 secs/batch = 0.2967s, grad.norm=14.66821194
 25978: 19 [  765/ 1327], train_loss/perplexity = 3.56151962/35.2166710 secs/batch = 0.2958s, grad.norm=15.03106594
 25983: 19 [  770/ 1327], train_loss/perplexity = 3.49761391/33.0365295 secs/batch = 0.2956s, grad.norm=14.91370106
 25988: 19 [  775/ 1327], train_loss/perplexity = 3.61229396/37.0509491 secs/batch = 0.2949s, grad.norm=15.62847424
 25993: 19 [  780/ 1327], train_loss/perplexity = 3.98323369/53.6903725 secs/batch = 0.3021s, grad.norm=15.59863377
 25998: 19 [  785/ 1327], train_loss/perplexity = 3.90326309/49.5639153 secs/batch = 0.2953s, grad.norm=15.74006081
 26003: 19 [  790/ 1327], train_loss/perplexity = 3.60089612/36.6310463 secs/batch = 0.2978s, grad.norm=15.68516731
 26008: 19 [  795/ 1327], train_loss/perplexity = 3.94396758/51.6230125 secs/batch = 0.2957s, grad.norm=15.70335102
 26013: 19 [  800/ 1327], train_loss/perplexity = 3.85959768/47.4462585 secs/batch = 0.3021s, grad.norm=16.54328728
 26018: 19 [  805/ 1327], train_loss/perplexity = 4.13154173/62.2738571 secs/batch = 0.2943s, grad.norm=15.73717022
 26023: 19 [  810/ 1327], train_loss/perplexity = 3.79546547/44.4989433 secs/batch = 0.2947s, grad.norm=15.00516415
 26028: 19 [  815/ 1327], train_loss/perplexity = 3.67548037/39.4676094 secs/batch = 0.2958s, grad.norm=14.94914532
 26033: 19 [  820/ 1327], train_loss/perplexity = 3.55651665/35.0409241 secs/batch = 0.2957s, grad.norm=14.87118053
 26038: 19 [  825/ 1327], train_loss/perplexity = 3.79142547/44.3195305 secs/batch = 0.2953s, grad.norm=15.36191845
 26043: 19 [  830/ 1327], train_loss/perplexity = 3.42617178/30.7586670 secs/batch = 0.3014s, grad.norm=15.73926449
 26048: 19 [  835/ 1327], train_loss/perplexity = 3.74028540/42.1100082 secs/batch = 0.2958s, grad.norm=15.58748055
 26053: 19 [  840/ 1327], train_loss/perplexity = 3.84877682/46.9356155 secs/batch = 0.2976s, grad.norm=15.81760693
 26058: 19 [  845/ 1327], train_loss/perplexity = 3.72390652/41.4259109 secs/batch = 0.3021s, grad.norm=15.84943199
 26063: 19 [  850/ 1327], train_loss/perplexity = 3.78869295/44.1985931 secs/batch = 0.3012s, grad.norm=15.66618729
 26068: 19 [  855/ 1327], train_loss/perplexity = 3.72194314/41.3446541 secs/batch = 0.3013s, grad.norm=15.57512283
 26073: 19 [  860/ 1327], train_loss/perplexity = 3.50055170/33.1337280 secs/batch = 0.2973s, grad.norm=14.94647408
 26078: 19 [  865/ 1327], train_loss/perplexity = 3.98629403/53.8549347 secs/batch = 0.2962s, grad.norm=15.84607506
 26083: 19 [  870/ 1327], train_loss/perplexity = 3.80423069/44.8907013 secs/batch = 0.3012s, grad.norm=15.98886776
 26088: 19 [  875/ 1327], train_loss/perplexity = 3.40927267/30.2432404 secs/batch = 0.2950s, grad.norm=14.51152897
 26093: 19 [  880/ 1327], train_loss/perplexity = 3.60046482/36.6152496 secs/batch = 0.2942s, grad.norm=14.80073166
 26098: 19 [  885/ 1327], train_loss/perplexity = 3.83439159/46.2652702 secs/batch = 0.2995s, grad.norm=15.21776485
 26103: 19 [  890/ 1327], train_loss/perplexity = 3.98151684/53.5982742 secs/batch = 0.2981s, grad.norm=15.33093643
 26108: 19 [  895/ 1327], train_loss/perplexity = 3.93180609/50.9990044 secs/batch = 0.2953s, grad.norm=15.26599026
 26113: 19 [  900/ 1327], train_loss/perplexity = 3.79605627/44.5252419 secs/batch = 0.3020s, grad.norm=14.91298676
 26118: 19 [  905/ 1327], train_loss/perplexity = 3.68946218/40.0233154 secs/batch = 0.2927s, grad.norm=14.73221970
 26123: 19 [  910/ 1327], train_loss/perplexity = 3.68257046/39.7484360 secs/batch = 0.2946s, grad.norm=14.11853790
 26128: 19 [  915/ 1327], train_loss/perplexity = 3.97492313/53.2460251 secs/batch = 0.2947s, grad.norm=14.73035336
 26133: 19 [  920/ 1327], train_loss/perplexity = 4.07233095/58.6936150 secs/batch = 0.2958s, grad.norm=15.83213234
 26138: 19 [  925/ 1327], train_loss/perplexity = 3.91556573/50.1774521 secs/batch = 0.3009s, grad.norm=15.36056709
 26143: 19 [  930/ 1327], train_loss/perplexity = 3.95242143/52.0612755 secs/batch = 0.2973s, grad.norm=15.41883183
 26148: 19 [  935/ 1327], train_loss/perplexity = 3.99304914/54.2199631 secs/batch = 0.2987s, grad.norm=15.20493603
 26153: 19 [  940/ 1327], train_loss/perplexity = 3.85963678/47.4481125 secs/batch = 0.2954s, grad.norm=15.10495853
 26158: 19 [  945/ 1327], train_loss/perplexity = 4.14508915/63.1232491 secs/batch = 0.3006s, grad.norm=15.04242134
 26163: 19 [  950/ 1327], train_loss/perplexity = 3.86550331/47.7272873 secs/batch = 0.2986s, grad.norm=15.64656448
 26168: 19 [  955/ 1327], train_loss/perplexity = 3.80923772/45.1160355 secs/batch = 0.3014s, grad.norm=15.05597878
 26173: 19 [  960/ 1327], train_loss/perplexity = 4.17220783/64.8584900 secs/batch = 0.2949s, grad.norm=15.98907089
 26178: 19 [  965/ 1327], train_loss/perplexity = 3.97406101/53.2001381 secs/batch = 0.2952s, grad.norm=15.71875191
 26183: 19 [  970/ 1327], train_loss/perplexity = 4.10135603/60.4221649 secs/batch = 0.2985s, grad.norm=15.27926064
 26188: 19 [  975/ 1327], train_loss/perplexity = 3.72288084/41.3834419 secs/batch = 0.2919s, grad.norm=15.99036312
 26193: 19 [  980/ 1327], train_loss/perplexity = 3.61642551/37.2043419 secs/batch = 0.2941s, grad.norm=15.20483303
 26198: 19 [  985/ 1327], train_loss/perplexity = 3.76153231/43.0142860 secs/batch = 0.3015s, grad.norm=15.93639183
 26203: 19 [  990/ 1327], train_loss/perplexity = 3.96030855/52.4735146 secs/batch = 0.2940s, grad.norm=15.59287739
 26208: 19 [  995/ 1327], train_loss/perplexity = 4.04404974/57.0569420 secs/batch = 0.2913s, grad.norm=15.73088551
 26213: 19 [ 1000/ 1327], train_loss/perplexity = 3.58852053/36.1805077 secs/batch = 0.2951s, grad.norm=15.33470345
 26218: 19 [ 1005/ 1327], train_loss/perplexity = 3.95557880/52.2259140 secs/batch = 0.2954s, grad.norm=15.24674511
 26223: 19 [ 1010/ 1327], train_loss/perplexity = 3.61857629/37.2844467 secs/batch = 0.2960s, grad.norm=14.56061363
 26228: 19 [ 1015/ 1327], train_loss/perplexity = 4.09698820/60.1588287 secs/batch = 0.2951s, grad.norm=15.33581924
 26233: 19 [ 1020/ 1327], train_loss/perplexity = 4.16978502/64.7015381 secs/batch = 0.3006s, grad.norm=15.10679817
 26238: 19 [ 1025/ 1327], train_loss/perplexity = 4.13717890/62.6258965 secs/batch = 0.3023s, grad.norm=15.24468803
 26243: 19 [ 1030/ 1327], train_loss/perplexity = 3.80355740/44.8604889 secs/batch = 0.2959s, grad.norm=14.89719200
 26248: 19 [ 1035/ 1327], train_loss/perplexity = 3.79616332/44.5300102 secs/batch = 0.2942s, grad.norm=15.08430004
 26253: 19 [ 1040/ 1327], train_loss/perplexity = 3.97032523/53.0017662 secs/batch = 0.2954s, grad.norm=15.49801731
 26258: 19 [ 1045/ 1327], train_loss/perplexity = 3.51494431/33.6140556 secs/batch = 0.2986s, grad.norm=14.64554119
 26263: 19 [ 1050/ 1327], train_loss/perplexity = 3.64362288/38.2300873 secs/batch = 0.3021s, grad.norm=15.30459881
 26268: 19 [ 1055/ 1327], train_loss/perplexity = 3.74234843/42.1969719 secs/batch = 0.3018s, grad.norm=15.68203068
 26273: 19 [ 1060/ 1327], train_loss/perplexity = 3.27527213/26.4504223 secs/batch = 0.2948s, grad.norm=16.24071884
 26278: 19 [ 1065/ 1327], train_loss/perplexity = 3.44006371/31.1889458 secs/batch = 0.3003s, grad.norm=15.31659889
 26283: 19 [ 1070/ 1327], train_loss/perplexity = 3.71162033/40.9200554 secs/batch = 0.2942s, grad.norm=15.87887192
 26288: 19 [ 1075/ 1327], train_loss/perplexity = 3.52315712/33.8912582 secs/batch = 0.2963s, grad.norm=15.27267647
 26293: 19 [ 1080/ 1327], train_loss/perplexity = 3.54547215/34.6560440 secs/batch = 0.2963s, grad.norm=15.14403534
 26298: 19 [ 1085/ 1327], train_loss/perplexity = 3.46827698/32.0814171 secs/batch = 0.2951s, grad.norm=15.46871185
 26303: 19 [ 1090/ 1327], train_loss/perplexity = 3.64852548/38.4179764 secs/batch = 0.3009s, grad.norm=15.48777962
 26308: 19 [ 1095/ 1327], train_loss/perplexity = 3.78880978/44.2037582 secs/batch = 0.2997s, grad.norm=15.89082050
 26313: 19 [ 1100/ 1327], train_loss/perplexity = 3.53806806/34.4003944 secs/batch = 0.2956s, grad.norm=16.61438751
 26318: 19 [ 1105/ 1327], train_loss/perplexity = 3.50545025/33.2964325 secs/batch = 0.2954s, grad.norm=15.67524719
 26323: 19 [ 1110/ 1327], train_loss/perplexity = 3.74912214/42.4837723 secs/batch = 0.2997s, grad.norm=16.31200790
 26328: 19 [ 1115/ 1327], train_loss/perplexity = 3.68509436/39.8488808 secs/batch = 0.2952s, grad.norm=15.12364197
 26333: 19 [ 1120/ 1327], train_loss/perplexity = 3.80590367/44.9658661 secs/batch = 0.2967s, grad.norm=15.44382286
 26338: 19 [ 1125/ 1327], train_loss/perplexity = 4.06271076/58.1316795 secs/batch = 0.2937s, grad.norm=16.51078033
 26343: 19 [ 1130/ 1327], train_loss/perplexity = 3.71658850/41.1238594 secs/batch = 0.2954s, grad.norm=15.91699219
 26348: 19 [ 1135/ 1327], train_loss/perplexity = 3.60860300/36.9144478 secs/batch = 0.2953s, grad.norm=15.28907967
 26353: 19 [ 1140/ 1327], train_loss/perplexity = 3.92932153/50.8724518 secs/batch = 0.2999s, grad.norm=16.39733696
 26358: 19 [ 1145/ 1327], train_loss/perplexity = 3.74038792/42.1143227 secs/batch = 0.2944s, grad.norm=15.01053715
 26363: 19 [ 1150/ 1327], train_loss/perplexity = 3.76144195/43.0103989 secs/batch = 0.2948s, grad.norm=15.23405552
 26368: 19 [ 1155/ 1327], train_loss/perplexity = 3.80803013/45.0615845 secs/batch = 0.2960s, grad.norm=15.61693954
 26373: 19 [ 1160/ 1327], train_loss/perplexity = 3.77476311/43.5871811 secs/batch = 0.2954s, grad.norm=15.54817104
 26378: 19 [ 1165/ 1327], train_loss/perplexity = 3.86230993/47.5751190 secs/batch = 0.2954s, grad.norm=15.80111504
 26383: 19 [ 1170/ 1327], train_loss/perplexity = 3.67821002/39.5754929 secs/batch = 0.3013s, grad.norm=15.35944843
 26388: 19 [ 1175/ 1327], train_loss/perplexity = 3.39242911/29.7381020 secs/batch = 0.2952s, grad.norm=15.10533905
 26393: 19 [ 1180/ 1327], train_loss/perplexity = 3.48544502/32.6369476 secs/batch = 0.2959s, grad.norm=15.65796852
 26398: 19 [ 1185/ 1327], train_loss/perplexity = 3.64440918/38.2601624 secs/batch = 0.2959s, grad.norm=15.56752396
 26403: 19 [ 1190/ 1327], train_loss/perplexity = 3.67223191/39.3396111 secs/batch = 0.2942s, grad.norm=15.69664574
 26408: 19 [ 1195/ 1327], train_loss/perplexity = 3.52634716/33.9995461 secs/batch = 0.3005s, grad.norm=15.16742229
 26413: 19 [ 1200/ 1327], train_loss/perplexity = 3.54258704/34.5562019 secs/batch = 0.2947s, grad.norm=15.20762062
 26418: 19 [ 1205/ 1327], train_loss/perplexity = 3.53456879/34.2802277 secs/batch = 0.2994s, grad.norm=15.62701893
 26423: 19 [ 1210/ 1327], train_loss/perplexity = 3.14523220/23.2250671 secs/batch = 0.2942s, grad.norm=15.35497856
 26428: 19 [ 1215/ 1327], train_loss/perplexity = 3.37994504/29.3691578 secs/batch = 0.2998s, grad.norm=14.88026524
 26433: 19 [ 1220/ 1327], train_loss/perplexity = 3.55816984/35.0989037 secs/batch = 0.2946s, grad.norm=15.51333427
 26438: 19 [ 1225/ 1327], train_loss/perplexity = 3.29418087/26.9553242 secs/batch = 0.2938s, grad.norm=16.14102936
 26443: 19 [ 1230/ 1327], train_loss/perplexity = 3.57939553/35.8518639 secs/batch = 0.2965s, grad.norm=15.28433704
 26448: 19 [ 1235/ 1327], train_loss/perplexity = 3.50688982/33.3443985 secs/batch = 0.2988s, grad.norm=15.31659222
 26453: 19 [ 1240/ 1327], train_loss/perplexity = 3.81379414/45.3220711 secs/batch = 0.2954s, grad.norm=16.14814186
 26458: 19 [ 1245/ 1327], train_loss/perplexity = 3.64170647/38.1568947 secs/batch = 0.2946s, grad.norm=15.07240009
 26463: 19 [ 1250/ 1327], train_loss/perplexity = 3.84958935/46.9737701 secs/batch = 0.2959s, grad.norm=15.17408371
 26468: 19 [ 1255/ 1327], train_loss/perplexity = 3.80150509/44.7685127 secs/batch = 0.2943s, grad.norm=15.11463165
 26473: 19 [ 1260/ 1327], train_loss/perplexity = 3.58031058/35.8846855 secs/batch = 0.2993s, grad.norm=16.11460114
 26478: 19 [ 1265/ 1327], train_loss/perplexity = 3.79430366/44.4472733 secs/batch = 0.2939s, grad.norm=16.05357170
 26483: 19 [ 1270/ 1327], train_loss/perplexity = 3.54422402/34.6128159 secs/batch = 0.2996s, grad.norm=15.76666069
 26488: 19 [ 1275/ 1327], train_loss/perplexity = 3.70185995/40.5226059 secs/batch = 0.2963s, grad.norm=16.32396126
 26493: 19 [ 1280/ 1327], train_loss/perplexity = 3.58350062/35.9993401 secs/batch = 0.2932s, grad.norm=15.93111992
 26498: 19 [ 1285/ 1327], train_loss/perplexity = 3.49096489/32.8175964 secs/batch = 0.2945s, grad.norm=15.57348537
 26503: 19 [ 1290/ 1327], train_loss/perplexity = 3.69608092/40.2890968 secs/batch = 0.2957s, grad.norm=15.53430843
 26508: 19 [ 1295/ 1327], train_loss/perplexity = 3.72874022/41.6266327 secs/batch = 0.2944s, grad.norm=15.66562748
 26513: 19 [ 1300/ 1327], train_loss/perplexity = 3.84203053/46.6200409 secs/batch = 0.2950s, grad.norm=15.33753109
 26518: 19 [ 1305/ 1327], train_loss/perplexity = 3.96599746/52.7728806 secs/batch = 0.2946s, grad.norm=15.95896626
 26523: 19 [ 1310/ 1327], train_loss/perplexity = 4.25042248/70.1350403 secs/batch = 0.3022s, grad.norm=16.26809120
 26528: 19 [ 1315/ 1327], train_loss/perplexity = 3.93546486/51.1859398 secs/batch = 0.2949s, grad.norm=15.91192055
 26533: 19 [ 1320/ 1327], train_loss/perplexity = 3.97201991/53.0916634 secs/batch = 0.2958s, grad.norm=15.65823364
 26538: 19 [ 1325/ 1327], train_loss/perplexity = 3.86772585/47.8334808 secs/batch = 0.2925s, grad.norm=16.06326866
Epoch training time: 393.85798716545105
	> validation loss = 4.56862736, perplexity = 96.41168213
	> validation loss = 4.51544285, perplexity = 91.41804504
	> validation loss = 4.50803804, perplexity = 90.74360657
	> validation loss = 4.48575020, perplexity = 88.74349976
	> validation loss = 4.64062452, perplexity = 103.60903168
	> validation loss = 4.59640503, perplexity = 99.12731171
	> validation loss = 4.53530693, perplexity = 93.25213623
	> validation loss = 4.37169409, perplexity = 79.17765045
	> validation loss = 4.16366673, perplexity = 64.30688477
	> validation loss = 4.30207634, perplexity = 73.85298157
	> validation loss = 4.50512743, perplexity = 90.47987366
	> validation loss = 4.45982742, perplexity = 86.47258759
	> validation loss = 4.40342760, perplexity = 81.73052979
	> validation loss = 4.14179039, perplexity = 62.91536331
	> validation loss = 4.13645029, perplexity = 62.58028412
	> validation loss = 4.13617802, perplexity = 62.56324768
	> validation loss = 4.58354998, perplexity = 97.86118317
	> validation loss = 4.04331684, perplexity = 57.01514053
	> validation loss = 4.55491257, perplexity = 95.09844208
	> validation loss = 4.51495171, perplexity = 91.37315369
	> validation loss = 4.22231483, perplexity = 68.19115448
at the end of epoch: 19
train loss = 3.83304131, perplexity = 46.20284197
validation loss = 4.40568654, perplexity = 81.91536188
Saved model cv/epoch019_4.4057.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.03125
new learning rate is: 0.015625
 26545: 20 [    5/ 1327], train_loss/perplexity = 4.09770679/60.2020721 secs/batch = 0.2949s, grad.norm=16.25286293
 26550: 20 [   10/ 1327], train_loss/perplexity = 3.70859313/40.7963715 secs/batch = 0.2935s, grad.norm=15.41641045
 26555: 20 [   15/ 1327], train_loss/perplexity = 4.01469040/55.4061394 secs/batch = 0.2948s, grad.norm=15.18117428
 26560: 20 [   20/ 1327], train_loss/perplexity = 4.19827175/66.5711823 secs/batch = 0.2950s, grad.norm=14.90591049
 26565: 20 [   25/ 1327], train_loss/perplexity = 3.93301892/51.0608940 secs/batch = 0.2943s, grad.norm=16.23907280
 26570: 20 [   30/ 1327], train_loss/perplexity = 4.12294340/61.7407036 secs/batch = 0.2958s, grad.norm=16.27792168
 26575: 20 [   35/ 1327], train_loss/perplexity = 3.85362911/47.1639175 secs/batch = 0.2989s, grad.norm=15.39470577
 26580: 20 [   40/ 1327], train_loss/perplexity = 3.81881952/45.5504036 secs/batch = 0.2964s, grad.norm=16.16223717
 26585: 20 [   45/ 1327], train_loss/perplexity = 3.64955091/38.4573898 secs/batch = 0.2959s, grad.norm=15.03431606
 26590: 20 [   50/ 1327], train_loss/perplexity = 3.88831854/48.8287125 secs/batch = 0.2957s, grad.norm=15.71101189
 26595: 20 [   55/ 1327], train_loss/perplexity = 3.75902462/42.9065552 secs/batch = 0.2934s, grad.norm=16.20831490
 26600: 20 [   60/ 1327], train_loss/perplexity = 4.09039259/59.7633514 secs/batch = 0.2954s, grad.norm=15.88050842
 26605: 20 [   65/ 1327], train_loss/perplexity = 3.63627362/37.9501572 secs/batch = 0.3012s, grad.norm=15.53932381
 26610: 20 [   70/ 1327], train_loss/perplexity = 3.51068306/33.4711227 secs/batch = 0.2957s, grad.norm=15.86480999
 26615: 20 [   75/ 1327], train_loss/perplexity = 3.37045264/29.0916920 secs/batch = 0.2994s, grad.norm=14.68227386
 26620: 20 [   80/ 1327], train_loss/perplexity = 3.85070229/47.0260773 secs/batch = 0.2989s, grad.norm=16.63595772
 26625: 20 [   85/ 1327], train_loss/perplexity = 3.78800774/44.1683159 secs/batch = 0.2963s, grad.norm=16.09607506
 26630: 20 [   90/ 1327], train_loss/perplexity = 3.88196659/48.5195389 secs/batch = 0.2969s, grad.norm=16.20084190
 26635: 20 [   95/ 1327], train_loss/perplexity = 3.66564608/39.0813789 secs/batch = 0.2934s, grad.norm=15.44960403
 26640: 20 [  100/ 1327], train_loss/perplexity = 4.03720331/56.6676407 secs/batch = 0.2930s, grad.norm=16.44129562
 26645: 20 [  105/ 1327], train_loss/perplexity = 3.73433161/41.8600388 secs/batch = 0.2982s, grad.norm=16.35741997
 26650: 20 [  110/ 1327], train_loss/perplexity = 3.67423964/39.4186745 secs/batch = 0.2940s, grad.norm=16.02524948
 26655: 20 [  115/ 1327], train_loss/perplexity = 3.73696637/41.9704742 secs/batch = 0.2959s, grad.norm=16.18316841
 26660: 20 [  120/ 1327], train_loss/perplexity = 3.83434963/46.2633286 secs/batch = 0.2967s, grad.norm=16.38820076
 26665: 20 [  125/ 1327], train_loss/perplexity = 3.86017966/47.4738808 secs/batch = 0.2957s, grad.norm=16.68445396
 26670: 20 [  130/ 1327], train_loss/perplexity = 3.76665258/43.2350960 secs/batch = 0.3009s, grad.norm=16.74332809
 26675: 20 [  135/ 1327], train_loss/perplexity = 3.81191373/45.2369270 secs/batch = 0.2958s, grad.norm=15.67013741
 26680: 20 [  140/ 1327], train_loss/perplexity = 4.02920580/56.2162476 secs/batch = 0.2958s, grad.norm=16.41170883
 26685: 20 [  145/ 1327], train_loss/perplexity = 3.90175533/49.4892426 secs/batch = 0.2958s, grad.norm=16.69563675
 26690: 20 [  150/ 1327], train_loss/perplexity = 4.01908493/55.6501579 secs/batch = 0.2949s, grad.norm=16.51622200
 26695: 20 [  155/ 1327], train_loss/perplexity = 4.24837208/69.9913788 secs/batch = 0.2960s, grad.norm=16.41240501
 26700: 20 [  160/ 1327], train_loss/perplexity = 3.95721221/52.3112907 secs/batch = 0.3007s, grad.norm=15.31183624
 26705: 20 [  165/ 1327], train_loss/perplexity = 3.94873905/51.8699188 secs/batch = 0.3001s, grad.norm=15.65764141
 26710: 20 [  170/ 1327], train_loss/perplexity = 3.85307264/47.1376801 secs/batch = 0.2958s, grad.norm=15.39963722
 26715: 20 [  175/ 1327], train_loss/perplexity = 4.14721441/63.2575455 secs/batch = 0.2952s, grad.norm=16.00427055
 26720: 20 [  180/ 1327], train_loss/perplexity = 4.01970243/55.6845322 secs/batch = 0.3005s, grad.norm=15.92041111
 26725: 20 [  185/ 1327], train_loss/perplexity = 4.22805882/68.5839691 secs/batch = 0.2945s, grad.norm=16.02695847
 26730: 20 [  190/ 1327], train_loss/perplexity = 3.84550166/46.7821465 secs/batch = 0.2963s, grad.norm=15.56395721
 26735: 20 [  195/ 1327], train_loss/perplexity = 4.10482025/60.6318436 secs/batch = 0.2960s, grad.norm=15.50029659
 26740: 20 [  200/ 1327], train_loss/perplexity = 3.94668889/51.7636871 secs/batch = 0.3028s, grad.norm=16.16927147
 26745: 20 [  205/ 1327], train_loss/perplexity = 4.14558268/63.1544113 secs/batch = 0.2977s, grad.norm=16.29860687
 26750: 20 [  210/ 1327], train_loss/perplexity = 3.95950127/52.4311714 secs/batch = 0.2957s, grad.norm=15.47819424
 26755: 20 [  215/ 1327], train_loss/perplexity = 4.13964367/62.7804451 secs/batch = 0.3011s, grad.norm=15.53298569
 26760: 20 [  220/ 1327], train_loss/perplexity = 4.02471638/55.9644356 secs/batch = 0.2995s, grad.norm=15.67167473
 26765: 20 [  225/ 1327], train_loss/perplexity = 4.18040943/65.3926239 secs/batch = 0.2965s, grad.norm=15.99097061
 26770: 20 [  230/ 1327], train_loss/perplexity = 4.07919073/59.0976257 secs/batch = 0.2962s, grad.norm=16.88086700
 26775: 20 [  235/ 1327], train_loss/perplexity = 3.91377163/50.0875092 secs/batch = 0.2952s, grad.norm=15.85642624
 26780: 20 [  240/ 1327], train_loss/perplexity = 3.61273885/37.0674362 secs/batch = 0.2933s, grad.norm=15.64956474
 26785: 20 [  245/ 1327], train_loss/perplexity = 3.95505881/52.1987648 secs/batch = 0.2939s, grad.norm=16.29515648
 26790: 20 [  250/ 1327], train_loss/perplexity = 3.91863918/50.3319054 secs/batch = 0.2961s, grad.norm=15.25546074
 26795: 20 [  255/ 1327], train_loss/perplexity = 3.77288389/43.5053482 secs/batch = 0.2995s, grad.norm=15.71145439
 26800: 20 [  260/ 1327], train_loss/perplexity = 4.05623531/57.7564659 secs/batch = 0.2996s, grad.norm=16.40818024
 26805: 20 [  265/ 1327], train_loss/perplexity = 4.19297123/66.2192535 secs/batch = 0.2965s, grad.norm=15.29471302
 26810: 20 [  270/ 1327], train_loss/perplexity = 4.23013353/68.7264099 secs/batch = 0.2940s, grad.norm=15.48179150
 26815: 20 [  275/ 1327], train_loss/perplexity = 4.13785982/62.6685562 secs/batch = 0.2990s, grad.norm=15.76559448
 26820: 20 [  280/ 1327], train_loss/perplexity = 4.01657295/55.5105438 secs/batch = 0.3020s, grad.norm=15.47785568
 26825: 20 [  285/ 1327], train_loss/perplexity = 4.35751820/78.0631561 secs/batch = 0.2957s, grad.norm=15.73400497
 26830: 20 [  290/ 1327], train_loss/perplexity = 3.95888162/52.3986931 secs/batch = 0.2958s, grad.norm=16.37231827
 26835: 20 [  295/ 1327], train_loss/perplexity = 3.75385952/42.6855087 secs/batch = 0.3002s, grad.norm=15.53119659
 26840: 20 [  300/ 1327], train_loss/perplexity = 3.37322903/29.1725750 secs/batch = 0.2955s, grad.norm=14.72944450
 26845: 20 [  305/ 1327], train_loss/perplexity = 3.85530925/47.2432251 secs/batch = 0.2956s, grad.norm=15.45463276
 26850: 20 [  310/ 1327], train_loss/perplexity = 3.80784321/45.0531654 secs/batch = 0.2960s, grad.norm=15.80632687
 26855: 20 [  315/ 1327], train_loss/perplexity = 3.43312454/30.9732685 secs/batch = 0.2945s, grad.norm=14.98783970
 26860: 20 [  320/ 1327], train_loss/perplexity = 3.32911873/27.9137306 secs/batch = 0.3014s, grad.norm=16.57950401
 26865: 20 [  325/ 1327], train_loss/perplexity = 3.38120580/29.4062080 secs/batch = 0.3010s, grad.norm=14.49271011
 26870: 20 [  330/ 1327], train_loss/perplexity = 3.90018034/49.4113579 secs/batch = 0.2960s, grad.norm=15.58974934
 26875: 20 [  335/ 1327], train_loss/perplexity = 3.44106078/31.2200584 secs/batch = 0.2960s, grad.norm=15.28922272
 26880: 20 [  340/ 1327], train_loss/perplexity = 4.11573410/61.2971954 secs/batch = 0.2952s, grad.norm=15.62416172
 26885: 20 [  345/ 1327], train_loss/perplexity = 3.85567212/47.2603722 secs/batch = 0.2986s, grad.norm=14.97565746
 26890: 20 [  350/ 1327], train_loss/perplexity = 3.92171836/50.4871254 secs/batch = 0.2945s, grad.norm=16.18642235
 26895: 20 [  355/ 1327], train_loss/perplexity = 3.92380619/50.5926437 secs/batch = 0.3019s, grad.norm=15.51682377
 26900: 20 [  360/ 1327], train_loss/perplexity = 3.94819689/51.8418045 secs/batch = 0.2946s, grad.norm=16.34580040
 26905: 20 [  365/ 1327], train_loss/perplexity = 4.03582144/56.5893860 secs/batch = 0.3004s, grad.norm=15.85930347
 26910: 20 [  370/ 1327], train_loss/perplexity = 4.13098431/62.2391548 secs/batch = 0.2950s, grad.norm=15.98551464
 26915: 20 [  375/ 1327], train_loss/perplexity = 3.48608780/32.6579323 secs/batch = 0.2969s, grad.norm=15.79115295
 26920: 20 [  380/ 1327], train_loss/perplexity = 3.57295632/35.6217461 secs/batch = 0.2956s, grad.norm=16.19006157
 26925: 20 [  385/ 1327], train_loss/perplexity = 3.71399927/41.0175209 secs/batch = 0.2926s, grad.norm=15.89188766
 26930: 20 [  390/ 1327], train_loss/perplexity = 3.92872381/50.8420525 secs/batch = 0.2956s, grad.norm=15.49498940
 26935: 20 [  395/ 1327], train_loss/perplexity = 3.90476274/49.6383018 secs/batch = 0.2944s, grad.norm=15.68738461
 26940: 20 [  400/ 1327], train_loss/perplexity = 3.88735580/48.7817268 secs/batch = 0.2956s, grad.norm=15.05733204
 26945: 20 [  405/ 1327], train_loss/perplexity = 4.15149117/63.5286598 secs/batch = 0.2925s, grad.norm=16.02826118
 26950: 20 [  410/ 1327], train_loss/perplexity = 3.72479296/41.4626465 secs/batch = 0.2954s, grad.norm=15.45264244
 26955: 20 [  415/ 1327], train_loss/perplexity = 3.78562832/44.0633469 secs/batch = 0.3006s, grad.norm=15.18017578
 26960: 20 [  420/ 1327], train_loss/perplexity = 3.43846631/31.1391640 secs/batch = 0.2951s, grad.norm=15.53798294
 26965: 20 [  425/ 1327], train_loss/perplexity = 3.70029569/40.4592667 secs/batch = 0.2940s, grad.norm=16.04532814
 26970: 20 [  430/ 1327], train_loss/perplexity = 3.96490192/52.7150993 secs/batch = 0.2964s, grad.norm=16.45583725
 26975: 20 [  435/ 1327], train_loss/perplexity = 3.99709797/54.4399338 secs/batch = 0.3028s, grad.norm=16.64169121
 26980: 20 [  440/ 1327], train_loss/perplexity = 3.50405645/33.2500572 secs/batch = 0.2952s, grad.norm=15.03696918
 26985: 20 [  445/ 1327], train_loss/perplexity = 3.89046669/48.9337196 secs/batch = 0.2943s, grad.norm=15.98823643
 26990: 20 [  450/ 1327], train_loss/perplexity = 3.84547067/46.7806969 secs/batch = 0.2960s, grad.norm=16.19962311
 26995: 20 [  455/ 1327], train_loss/perplexity = 3.77346015/43.5304260 secs/batch = 0.2947s, grad.norm=15.59601402
 27000: 20 [  460/ 1327], train_loss/perplexity = 3.78499436/44.0354233 secs/batch = 0.3000s, grad.norm=16.74380302
 27005: 20 [  465/ 1327], train_loss/perplexity = 3.44548988/31.3586426 secs/batch = 0.2996s, grad.norm=16.53070831
 27010: 20 [  470/ 1327], train_loss/perplexity = 4.24261141/69.5893402 secs/batch = 0.2924s, grad.norm=15.66885948
 27015: 20 [  475/ 1327], train_loss/perplexity = 3.66729069/39.1457024 secs/batch = 0.2938s, grad.norm=15.32464886
 27020: 20 [  480/ 1327], train_loss/perplexity = 3.72241616/41.3642159 secs/batch = 0.2947s, grad.norm=15.84102058
 27025: 20 [  485/ 1327], train_loss/perplexity = 3.72667623/41.5408058 secs/batch = 0.2949s, grad.norm=15.52896309
 27030: 20 [  490/ 1327], train_loss/perplexity = 3.66179848/38.9312973 secs/batch = 0.2957s, grad.norm=16.60248375
 27035: 20 [  495/ 1327], train_loss/perplexity = 3.76175976/43.0240707 secs/batch = 0.3000s, grad.norm=15.70598698
 27040: 20 [  500/ 1327], train_loss/perplexity = 3.96534252/52.7383308 secs/batch = 0.2959s, grad.norm=16.10607719
 27045: 20 [  505/ 1327], train_loss/perplexity = 4.04741096/57.2490463 secs/batch = 0.2948s, grad.norm=15.12787533
 27050: 20 [  510/ 1327], train_loss/perplexity = 4.35226631/77.6542511 secs/batch = 0.2956s, grad.norm=15.68166828
 27055: 20 [  515/ 1327], train_loss/perplexity = 4.05694437/57.7974319 secs/batch = 0.2951s, grad.norm=15.32197189
 27060: 20 [  520/ 1327], train_loss/perplexity = 4.05305386/57.5730095 secs/batch = 0.2947s, grad.norm=15.33965111
 27065: 20 [  525/ 1327], train_loss/perplexity = 3.77549338/43.6190224 secs/batch = 0.2943s, grad.norm=15.20444202
 27070: 20 [  530/ 1327], train_loss/perplexity = 3.76433086/43.1348343 secs/batch = 0.2928s, grad.norm=15.78231907
 27075: 20 [  535/ 1327], train_loss/perplexity = 3.89081287/48.9506607 secs/batch = 0.2948s, grad.norm=16.69631767
 27080: 20 [  540/ 1327], train_loss/perplexity = 3.96761608/52.8583717 secs/batch = 0.2952s, grad.norm=15.58988762
 27085: 20 [  545/ 1327], train_loss/perplexity = 3.91302943/50.0503464 secs/batch = 0.2936s, grad.norm=15.85582733
 27090: 20 [  550/ 1327], train_loss/perplexity = 3.86207700/47.5640411 secs/batch = 0.2947s, grad.norm=15.42337799
 27095: 20 [  555/ 1327], train_loss/perplexity = 3.78571463/44.0671501 secs/batch = 0.2931s, grad.norm=15.10848522
 27100: 20 [  560/ 1327], train_loss/perplexity = 3.90569973/49.6848335 secs/batch = 0.3019s, grad.norm=16.38607597
 27105: 20 [  565/ 1327], train_loss/perplexity = 3.76343179/43.0960693 secs/batch = 0.3001s, grad.norm=16.54557800
 27110: 20 [  570/ 1327], train_loss/perplexity = 3.71558976/41.0828094 secs/batch = 0.2955s, grad.norm=16.08656120
 27115: 20 [  575/ 1327], train_loss/perplexity = 3.51451683/33.5996895 secs/batch = 0.3012s, grad.norm=15.97307682
 27120: 20 [  580/ 1327], train_loss/perplexity = 3.94637656/51.7475243 secs/batch = 0.2948s, grad.norm=16.56107330
 27125: 20 [  585/ 1327], train_loss/perplexity = 3.58072853/35.8996849 secs/batch = 0.2956s, grad.norm=15.12332630
 27130: 20 [  590/ 1327], train_loss/perplexity = 3.87664604/48.2620735 secs/batch = 0.2990s, grad.norm=15.53692722
 27135: 20 [  595/ 1327], train_loss/perplexity = 3.89924932/49.3653755 secs/batch = 0.2992s, grad.norm=16.52550888
 27140: 20 [  600/ 1327], train_loss/perplexity = 4.07318163/58.7435646 secs/batch = 0.2942s, grad.norm=15.35030079
 27145: 20 [  605/ 1327], train_loss/perplexity = 3.98235059/53.6429787 secs/batch = 0.2955s, grad.norm=15.10359097
 27150: 20 [  610/ 1327], train_loss/perplexity = 4.19076920/66.0735931 secs/batch = 0.3001s, grad.norm=16.00822258
 27155: 20 [  615/ 1327], train_loss/perplexity = 3.75133991/42.5780945 secs/batch = 0.2937s, grad.norm=15.09648132
 27160: 20 [  620/ 1327], train_loss/perplexity = 4.10418367/60.5932617 secs/batch = 0.2957s, grad.norm=15.62936687
 27165: 20 [  625/ 1327], train_loss/perplexity = 4.02180719/55.8018608 secs/batch = 0.2932s, grad.norm=15.44156170
 27170: 20 [  630/ 1327], train_loss/perplexity = 4.16755104/64.5571594 secs/batch = 0.2953s, grad.norm=15.40910721
 27175: 20 [  635/ 1327], train_loss/perplexity = 3.83134890/46.1247139 secs/batch = 0.2964s, grad.norm=15.53265476
 27180: 20 [  640/ 1327], train_loss/perplexity = 3.81705475/45.4700890 secs/batch = 0.2955s, grad.norm=15.56136608
 27185: 20 [  645/ 1327], train_loss/perplexity = 4.11867380/61.4776573 secs/batch = 0.2954s, grad.norm=16.50801849
 27190: 20 [  650/ 1327], train_loss/perplexity = 3.62162256/37.3982010 secs/batch = 0.2944s, grad.norm=15.16184330
 27195: 20 [  655/ 1327], train_loss/perplexity = 3.79320669/44.3985443 secs/batch = 0.2988s, grad.norm=15.59944439
 27200: 20 [  660/ 1327], train_loss/perplexity = 3.74462080/42.2929649 secs/batch = 0.2933s, grad.norm=15.39486980
 27205: 20 [  665/ 1327], train_loss/perplexity = 3.81216192/45.2481575 secs/batch = 0.2990s, grad.norm=15.55156136
 27210: 20 [  670/ 1327], train_loss/perplexity = 3.77802515/43.7295952 secs/batch = 0.2955s, grad.norm=15.80827999
 27215: 20 [  675/ 1327], train_loss/perplexity = 3.63260484/37.8111801 secs/batch = 0.2914s, grad.norm=15.86774158
 27220: 20 [  680/ 1327], train_loss/perplexity = 3.85234261/47.1032791 secs/batch = 0.2942s, grad.norm=16.44629860
 27225: 20 [  685/ 1327], train_loss/perplexity = 3.61374283/37.1046715 secs/batch = 0.3004s, grad.norm=15.09471607
 27230: 20 [  690/ 1327], train_loss/perplexity = 4.04680586/57.2144127 secs/batch = 0.2950s, grad.norm=15.62414455
 27235: 20 [  695/ 1327], train_loss/perplexity = 3.88905907/48.8648872 secs/batch = 0.2938s, grad.norm=15.43625736
 27240: 20 [  700/ 1327], train_loss/perplexity = 4.10476923/60.6287498 secs/batch = 0.3007s, grad.norm=15.79002857
 27245: 20 [  705/ 1327], train_loss/perplexity = 3.81924248/45.5696754 secs/batch = 0.2981s, grad.norm=14.91778660
 27250: 20 [  710/ 1327], train_loss/perplexity = 3.70201063/40.5287094 secs/batch = 0.2957s, grad.norm=16.02458954
 27255: 20 [  715/ 1327], train_loss/perplexity = 3.54006743/34.4692421 secs/batch = 0.2961s, grad.norm=15.83582687
 27260: 20 [  720/ 1327], train_loss/perplexity = 3.59130406/36.2813568 secs/batch = 0.2980s, grad.norm=16.11378288
 27265: 20 [  725/ 1327], train_loss/perplexity = 3.70268011/40.5558548 secs/batch = 0.2947s, grad.norm=15.78312588
 27270: 20 [  730/ 1327], train_loss/perplexity = 3.83789539/46.4276581 secs/batch = 0.2924s, grad.norm=16.08763313
 27275: 20 [  735/ 1327], train_loss/perplexity = 3.85801697/47.3713188 secs/batch = 0.2994s, grad.norm=16.70338440
 27280: 20 [  740/ 1327], train_loss/perplexity = 3.38774419/29.5991077 secs/batch = 0.2953s, grad.norm=14.89355564
 27285: 20 [  745/ 1327], train_loss/perplexity = 3.83491063/46.2892914 secs/batch = 0.2992s, grad.norm=15.89884281
 27290: 20 [  750/ 1327], train_loss/perplexity = 3.72646213/41.5319138 secs/batch = 0.2946s, grad.norm=15.94052982
 27295: 20 [  755/ 1327], train_loss/perplexity = 3.60138249/36.6488647 secs/batch = 0.2984s, grad.norm=15.26040459
 27300: 20 [  760/ 1327], train_loss/perplexity = 3.45607734/31.6924133 secs/batch = 0.3007s, grad.norm=14.73284531
 27305: 20 [  765/ 1327], train_loss/perplexity = 3.58936262/36.2109871 secs/batch = 0.2956s, grad.norm=14.91041470
 27310: 20 [  770/ 1327], train_loss/perplexity = 3.56465054/35.3271065 secs/batch = 0.2998s, grad.norm=15.46358013
 27315: 20 [  775/ 1327], train_loss/perplexity = 3.59069872/36.2594032 secs/batch = 0.3009s, grad.norm=16.04858589
 27320: 20 [  780/ 1327], train_loss/perplexity = 3.93608046/51.2174568 secs/batch = 0.2980s, grad.norm=16.11596680
 27325: 20 [  785/ 1327], train_loss/perplexity = 3.84880042/46.9367256 secs/batch = 0.2951s, grad.norm=16.27760887
 27330: 20 [  790/ 1327], train_loss/perplexity = 3.57874441/35.8285255 secs/batch = 0.3023s, grad.norm=15.54587650
 27335: 20 [  795/ 1327], train_loss/perplexity = 3.99794006/54.4857979 secs/batch = 0.2950s, grad.norm=15.64362240
 27340: 20 [  800/ 1327], train_loss/perplexity = 3.80852127/45.0837212 secs/batch = 0.2937s, grad.norm=15.91511059
 27345: 20 [  805/ 1327], train_loss/perplexity = 4.15959406/64.0455170 secs/batch = 0.2994s, grad.norm=15.79235744
 27350: 20 [  810/ 1327], train_loss/perplexity = 3.79671192/44.5544434 secs/batch = 0.2982s, grad.norm=15.24059105
 27355: 20 [  815/ 1327], train_loss/perplexity = 3.63890243/38.0500526 secs/batch = 0.2948s, grad.norm=15.08851433
 27360: 20 [  820/ 1327], train_loss/perplexity = 3.61825013/37.2722893 secs/batch = 0.2944s, grad.norm=14.79435921
 27365: 20 [  825/ 1327], train_loss/perplexity = 3.78098607/43.8592682 secs/batch = 0.2953s, grad.norm=15.04491043
 27370: 20 [  830/ 1327], train_loss/perplexity = 3.48927712/32.7622566 secs/batch = 0.2947s, grad.norm=15.84847069
 27375: 20 [  835/ 1327], train_loss/perplexity = 3.73330784/41.8172035 secs/batch = 0.2936s, grad.norm=15.75145817
 27380: 20 [  840/ 1327], train_loss/perplexity = 3.87026858/47.9552650 secs/batch = 0.2998s, grad.norm=15.78889561
 27385: 20 [  845/ 1327], train_loss/perplexity = 3.68567014/39.8718338 secs/batch = 0.2933s, grad.norm=15.91109943
 27390: 20 [  850/ 1327], train_loss/perplexity = 3.79742837/44.5863762 secs/batch = 0.2949s, grad.norm=15.34746838
 27395: 20 [  855/ 1327], train_loss/perplexity = 3.79400635/44.4340630 secs/batch = 0.2965s, grad.norm=16.06521034
 27400: 20 [  860/ 1327], train_loss/perplexity = 3.49698973/33.0159149 secs/batch = 0.2996s, grad.norm=15.20191193
 27405: 20 [  865/ 1327], train_loss/perplexity = 4.01565027/55.4593468 secs/batch = 0.2925s, grad.norm=16.00087357
 27410: 20 [  870/ 1327], train_loss/perplexity = 3.77056789/43.4047089 secs/batch = 0.2976s, grad.norm=16.31761551
 27415: 20 [  875/ 1327], train_loss/perplexity = 3.40686417/30.1704865 secs/batch = 0.2991s, grad.norm=14.91350365
 27420: 20 [  880/ 1327], train_loss/perplexity = 3.63132715/37.7629013 secs/batch = 0.2989s, grad.norm=15.26283360
 27425: 20 [  885/ 1327], train_loss/perplexity = 3.83677530/46.3756866 secs/batch = 0.2981s, grad.norm=15.04825211
 27430: 20 [  890/ 1327], train_loss/perplexity = 3.98278999/53.6665535 secs/batch = 0.2946s, grad.norm=15.44577503
 27435: 20 [  895/ 1327], train_loss/perplexity = 3.87541342/48.2026215 secs/batch = 0.2948s, grad.norm=15.12300873
 27440: 20 [  900/ 1327], train_loss/perplexity = 3.74175620/42.1719894 secs/batch = 0.2942s, grad.norm=14.84416294
 27445: 20 [  905/ 1327], train_loss/perplexity = 3.63813853/38.0209961 secs/batch = 0.2985s, grad.norm=14.31274033
 27450: 20 [  910/ 1327], train_loss/perplexity = 3.69702363/40.3270988 secs/batch = 0.2982s, grad.norm=14.27726746
 27455: 20 [  915/ 1327], train_loss/perplexity = 3.89370751/49.0925598 secs/batch = 0.2995s, grad.norm=14.79342651
 27460: 20 [  920/ 1327], train_loss/perplexity = 4.05329895/57.5871201 secs/batch = 0.3009s, grad.norm=15.98255539
 27465: 20 [  925/ 1327], train_loss/perplexity = 3.84675360/46.8407516 secs/batch = 0.2927s, grad.norm=15.09376144
 27470: 20 [  930/ 1327], train_loss/perplexity = 3.96367168/52.6502876 secs/batch = 0.2948s, grad.norm=15.48815346
 27475: 20 [  935/ 1327], train_loss/perplexity = 4.02176523/55.7995186 secs/batch = 0.2933s, grad.norm=15.46769905
 27480: 20 [  940/ 1327], train_loss/perplexity = 3.93831491/51.3320312 secs/batch = 0.2965s, grad.norm=15.25541306
 27485: 20 [  945/ 1327], train_loss/perplexity = 4.04605579/57.1715164 secs/batch = 0.2919s, grad.norm=15.48908997
 27490: 20 [  950/ 1327], train_loss/perplexity = 3.90131569/49.4674911 secs/batch = 0.2993s, grad.norm=15.44188023
 27495: 20 [  955/ 1327], train_loss/perplexity = 3.86513734/47.7098236 secs/batch = 0.2947s, grad.norm=15.54150105
 27500: 20 [  960/ 1327], train_loss/perplexity = 4.15651178/63.8484154 secs/batch = 0.2936s, grad.norm=16.01626015
 27505: 20 [  965/ 1327], train_loss/perplexity = 3.94728255/51.7944260 secs/batch = 0.2937s, grad.norm=15.75742054
 27510: 20 [  970/ 1327], train_loss/perplexity = 4.11232901/61.0888290 secs/batch = 0.2949s, grad.norm=15.74347782
 27515: 20 [  975/ 1327], train_loss/perplexity = 3.75882816/42.8981285 secs/batch = 0.2937s, grad.norm=16.02489853
 27520: 20 [  980/ 1327], train_loss/perplexity = 3.59580231/36.4449272 secs/batch = 0.2978s, grad.norm=15.41315460
 27525: 20 [  985/ 1327], train_loss/perplexity = 3.74539995/42.3259315 secs/batch = 0.2944s, grad.norm=15.94104290
 27530: 20 [  990/ 1327], train_loss/perplexity = 3.99038482/54.0756950 secs/batch = 0.2943s, grad.norm=16.23495293
 27535: 20 [  995/ 1327], train_loss/perplexity = 4.00456190/54.8477898 secs/batch = 0.2988s, grad.norm=15.71727943
 27540: 20 [ 1000/ 1327], train_loss/perplexity = 3.55457497/34.9729538 secs/batch = 0.2956s, grad.norm=15.42019558
 27545: 20 [ 1005/ 1327], train_loss/perplexity = 4.02260637/55.8464737 secs/batch = 0.2995s, grad.norm=15.84725666
 27550: 20 [ 1010/ 1327], train_loss/perplexity = 3.50458145/33.2675171 secs/batch = 0.2994s, grad.norm=14.51377773
 27555: 20 [ 1015/ 1327], train_loss/perplexity = 4.09550333/60.0695648 secs/batch = 0.2939s, grad.norm=15.94372082
 27560: 20 [ 1020/ 1327], train_loss/perplexity = 4.15060472/63.4723701 secs/batch = 0.2935s, grad.norm=14.99193096
 27565: 20 [ 1025/ 1327], train_loss/perplexity = 4.06657887/58.3569717 secs/batch = 0.2955s, grad.norm=15.32970428
 27570: 20 [ 1030/ 1327], train_loss/perplexity = 3.79270482/44.3762665 secs/batch = 0.2950s, grad.norm=14.84543896
 27575: 20 [ 1035/ 1327], train_loss/perplexity = 3.77010393/43.3845749 secs/batch = 0.2937s, grad.norm=15.10679436
 27580: 20 [ 1040/ 1327], train_loss/perplexity = 3.97897673/53.4622993 secs/batch = 0.2950s, grad.norm=15.87064552
 27585: 20 [ 1045/ 1327], train_loss/perplexity = 3.51482582/33.6100731 secs/batch = 0.3007s, grad.norm=15.09461403
 27590: 20 [ 1050/ 1327], train_loss/perplexity = 3.71130443/40.9071312 secs/batch = 0.2950s, grad.norm=15.24281788
 27595: 20 [ 1055/ 1327], train_loss/perplexity = 3.74289727/42.2201347 secs/batch = 0.2993s, grad.norm=15.59663010
 27600: 20 [ 1060/ 1327], train_loss/perplexity = 3.25322270/25.8735886 secs/batch = 0.2945s, grad.norm=15.65532112
 27605: 20 [ 1065/ 1327], train_loss/perplexity = 3.44553423/31.3600330 secs/batch = 0.2969s, grad.norm=15.75364304
 27610: 20 [ 1070/ 1327], train_loss/perplexity = 3.68975830/40.0351677 secs/batch = 0.2929s, grad.norm=16.14015770
 27615: 20 [ 1075/ 1327], train_loss/perplexity = 3.50493050/33.2791290 secs/batch = 0.2927s, grad.norm=15.12133598
 27620: 20 [ 1080/ 1327], train_loss/perplexity = 3.50862885/33.4024353 secs/batch = 0.2968s, grad.norm=15.23384190
 27625: 20 [ 1085/ 1327], train_loss/perplexity = 3.39174533/29.7177734 secs/batch = 0.2950s, grad.norm=15.42320728
 27630: 20 [ 1090/ 1327], train_loss/perplexity = 3.66528511/39.0672722 secs/batch = 0.2941s, grad.norm=16.11731720
 27635: 20 [ 1095/ 1327], train_loss/perplexity = 3.82773113/45.9581451 secs/batch = 0.2948s, grad.norm=16.19231415
 27640: 20 [ 1100/ 1327], train_loss/perplexity = 3.42052889/30.5855865 secs/batch = 0.2957s, grad.norm=16.38482857
 27645: 20 [ 1105/ 1327], train_loss/perplexity = 3.45687151/31.7175922 secs/batch = 0.2985s, grad.norm=15.72312641
 27650: 20 [ 1110/ 1327], train_loss/perplexity = 3.78497362/44.0345078 secs/batch = 0.2954s, grad.norm=16.50252914
 27655: 20 [ 1115/ 1327], train_loss/perplexity = 3.62764668/37.6241722 secs/batch = 0.2921s, grad.norm=14.90591240
 27660: 20 [ 1120/ 1327], train_loss/perplexity = 3.85311294/47.1395760 secs/batch = 0.2948s, grad.norm=15.26466656
 27665: 20 [ 1125/ 1327], train_loss/perplexity = 3.95465851/52.1778717 secs/batch = 0.2984s, grad.norm=16.21047020
 27670: 20 [ 1130/ 1327], train_loss/perplexity = 3.63734531/37.9908485 secs/batch = 0.2987s, grad.norm=15.67896080
 27675: 20 [ 1135/ 1327], train_loss/perplexity = 3.65141821/38.5292702 secs/batch = 0.2947s, grad.norm=15.22757626
 27680: 20 [ 1140/ 1327], train_loss/perplexity = 3.92132092/50.4670639 secs/batch = 0.2942s, grad.norm=15.70453548
 27685: 20 [ 1145/ 1327], train_loss/perplexity = 3.71304035/40.9782066 secs/batch = 0.2995s, grad.norm=15.10427380
 27690: 20 [ 1150/ 1327], train_loss/perplexity = 3.67877173/39.5977287 secs/batch = 0.2953s, grad.norm=14.92172337
 27695: 20 [ 1155/ 1327], train_loss/perplexity = 3.77348661/43.5315781 secs/batch = 0.2957s, grad.norm=15.75766659
 27700: 20 [ 1160/ 1327], train_loss/perplexity = 3.69775462/40.3565865 secs/batch = 0.2984s, grad.norm=15.73135948
 27705: 20 [ 1165/ 1327], train_loss/perplexity = 3.81533527/45.3919716 secs/batch = 0.2923s, grad.norm=15.54166126
 27710: 20 [ 1170/ 1327], train_loss/perplexity = 3.63038540/37.7273521 secs/batch = 0.2930s, grad.norm=15.42762756
 27715: 20 [ 1175/ 1327], train_loss/perplexity = 3.48382640/32.5841637 secs/batch = 0.2997s, grad.norm=15.61526585
 27720: 20 [ 1180/ 1327], train_loss/perplexity = 3.50860047/33.4014893 secs/batch = 0.2934s, grad.norm=15.66070271
 27725: 20 [ 1185/ 1327], train_loss/perplexity = 3.67128682/39.3024483 secs/batch = 0.2914s, grad.norm=15.38618565
 27730: 20 [ 1190/ 1327], train_loss/perplexity = 3.80273342/44.8235397 secs/batch = 0.2957s, grad.norm=16.23871231
 27735: 20 [ 1195/ 1327], train_loss/perplexity = 3.54969573/34.8027267 secs/batch = 0.2929s, grad.norm=15.17684555
 27740: 20 [ 1200/ 1327], train_loss/perplexity = 3.52420139/33.9266701 secs/batch = 0.2928s, grad.norm=15.45226574
 27745: 20 [ 1205/ 1327], train_loss/perplexity = 3.59214902/36.3120270 secs/batch = 0.2957s, grad.norm=15.86023808
 27750: 20 [ 1210/ 1327], train_loss/perplexity = 3.10562515/22.3231697 secs/batch = 0.2944s, grad.norm=15.39739513
 27755: 20 [ 1215/ 1327], train_loss/perplexity = 3.33537245/28.0888424 secs/batch = 0.2938s, grad.norm=14.76933479
 27760: 20 [ 1220/ 1327], train_loss/perplexity = 3.53758717/34.3838577 secs/batch = 0.2929s, grad.norm=15.47505474
 27765: 20 [ 1225/ 1327], train_loss/perplexity = 3.25429177/25.9012642 secs/batch = 0.2954s, grad.norm=15.91190720
 27770: 20 [ 1230/ 1327], train_loss/perplexity = 3.55201340/34.8834801 secs/batch = 0.2950s, grad.norm=15.04026127
 27775: 20 [ 1235/ 1327], train_loss/perplexity = 3.44768047/31.4274101 secs/batch = 0.3005s, grad.norm=15.08256340
 27780: 20 [ 1240/ 1327], train_loss/perplexity = 3.74940515/42.4957962 secs/batch = 0.2947s, grad.norm=16.28497314
 27785: 20 [ 1245/ 1327], train_loss/perplexity = 3.70062447/40.4725685 secs/batch = 0.2940s, grad.norm=15.35307312
 27790: 20 [ 1250/ 1327], train_loss/perplexity = 3.81496954/45.3753738 secs/batch = 0.2993s, grad.norm=15.38304520
 27795: 20 [ 1255/ 1327], train_loss/perplexity = 3.83040905/46.0813828 secs/batch = 0.2931s, grad.norm=15.81964493
 27800: 20 [ 1260/ 1327], train_loss/perplexity = 3.57776117/35.7933159 secs/batch = 0.2983s, grad.norm=16.07305527
 27805: 20 [ 1265/ 1327], train_loss/perplexity = 3.74912024/42.4836884 secs/batch = 0.2973s, grad.norm=15.80269432
 27810: 20 [ 1270/ 1327], train_loss/perplexity = 3.48173642/32.5161362 secs/batch = 0.2937s, grad.norm=15.66593742
 27815: 20 [ 1275/ 1327], train_loss/perplexity = 3.73322296/41.8136559 secs/batch = 0.2934s, grad.norm=15.78849411
 27820: 20 [ 1280/ 1327], train_loss/perplexity = 3.55557561/35.0079651 secs/batch = 0.2933s, grad.norm=15.70374870
 27825: 20 [ 1285/ 1327], train_loss/perplexity = 3.48098564/32.4917297 secs/batch = 0.2942s, grad.norm=15.51904202
 27830: 20 [ 1290/ 1327], train_loss/perplexity = 3.73170829/41.7503700 secs/batch = 0.2939s, grad.norm=15.35189915
 27835: 20 [ 1295/ 1327], train_loss/perplexity = 3.66694498/39.1321716 secs/batch = 0.2940s, grad.norm=15.11966133
 27840: 20 [ 1300/ 1327], train_loss/perplexity = 3.85202932/47.0885239 secs/batch = 0.2983s, grad.norm=14.82101154
 27845: 20 [ 1305/ 1327], train_loss/perplexity = 3.95659494/52.2790108 secs/batch = 0.2925s, grad.norm=15.94540882
 27850: 20 [ 1310/ 1327], train_loss/perplexity = 4.19409990/66.2940369 secs/batch = 0.2928s, grad.norm=16.43611717
 27855: 20 [ 1315/ 1327], train_loss/perplexity = 3.93564606/51.1952133 secs/batch = 0.2995s, grad.norm=15.81366348
 27860: 20 [ 1320/ 1327], train_loss/perplexity = 3.97736192/53.3760376 secs/batch = 0.2946s, grad.norm=15.94566631
 27865: 20 [ 1325/ 1327], train_loss/perplexity = 3.85733604/47.3390732 secs/batch = 0.2983s, grad.norm=15.90416431
Epoch training time: 393.15855503082275
	> validation loss = 4.56718731, perplexity = 96.27294159
	> validation loss = 4.51370049, perplexity = 91.25889587
	> validation loss = 4.50489235, perplexity = 90.45860291
	> validation loss = 4.48100567, perplexity = 88.32345581
	> validation loss = 4.63923979, perplexity = 103.46566010
	> validation loss = 4.59377813, perplexity = 98.86725616
	> validation loss = 4.53099871, perplexity = 92.85124969
	> validation loss = 4.36816883, perplexity = 78.89902496
	> validation loss = 4.15695286, perplexity = 63.87658310
	> validation loss = 4.29440403, perplexity = 73.28852081
	> validation loss = 4.50490665, perplexity = 90.45989990
	> validation loss = 4.45669699, perplexity = 86.20230865
	> validation loss = 4.39907408, perplexity = 81.37548828
	> validation loss = 4.13723850, perplexity = 62.62963104
	> validation loss = 4.13407803, perplexity = 62.43200302
	> validation loss = 4.13362217, perplexity = 62.40354919
	> validation loss = 4.58001232, perplexity = 97.51559448
	> validation loss = 4.03557396, perplexity = 56.57538223
	> validation loss = 4.55134821, perplexity = 94.76007843
	> validation loss = 4.51189613, perplexity = 91.09438324
	> validation loss = 4.21763134, perplexity = 67.87252808
at the end of epoch: 20
train loss = 3.82925178, perplexity = 46.02808605
validation loss = 4.40224824, perplexity = 81.63419610
Saved model cv/epoch020_4.4022.model
 27872: 21 [    5/ 1327], train_loss/perplexity = 4.05611706/57.7496376 secs/batch = 0.2930s, grad.norm=16.17859650
 27877: 21 [   10/ 1327], train_loss/perplexity = 3.68288398/39.7608986 secs/batch = 0.2924s, grad.norm=15.05149174
 27882: 21 [   15/ 1327], train_loss/perplexity = 4.01573229/55.4638977 secs/batch = 0.2916s, grad.norm=14.97941685
 27887: 21 [   20/ 1327], train_loss/perplexity = 4.22028494/68.0528717 secs/batch = 0.2974s, grad.norm=15.15867043
 27892: 21 [   25/ 1327], train_loss/perplexity = 3.99197316/54.1616554 secs/batch = 0.2925s, grad.norm=15.96938133
 27897: 21 [   30/ 1327], train_loss/perplexity = 4.11254168/61.1018219 secs/batch = 0.2925s, grad.norm=16.24089241
 27902: 21 [   35/ 1327], train_loss/perplexity = 3.81920767/45.5680885 secs/batch = 0.2937s, grad.norm=15.24826336
 27907: 21 [   40/ 1327], train_loss/perplexity = 3.87553596/48.2085304 secs/batch = 0.2931s, grad.norm=15.72360229
 27912: 21 [   45/ 1327], train_loss/perplexity = 3.57185841/35.5826607 secs/batch = 0.2955s, grad.norm=14.71214485
 27917: 21 [   50/ 1327], train_loss/perplexity = 3.88696742/48.7627869 secs/batch = 0.2915s, grad.norm=15.72848415
 27922: 21 [   55/ 1327], train_loss/perplexity = 3.85052490/47.0177383 secs/batch = 0.2927s, grad.norm=16.31320381
 27927: 21 [   60/ 1327], train_loss/perplexity = 4.11279154/61.1170921 secs/batch = 0.2931s, grad.norm=16.25730705
 27932: 21 [   65/ 1327], train_loss/perplexity = 3.70392418/40.6063385 secs/batch = 0.2979s, grad.norm=15.10339069
 27937: 21 [   70/ 1327], train_loss/perplexity = 3.48580670/32.6487541 secs/batch = 0.2928s, grad.norm=15.69936943
 27942: 21 [   75/ 1327], train_loss/perplexity = 3.36358142/28.8924828 secs/batch = 0.2935s, grad.norm=14.38562870
 27947: 21 [   80/ 1327], train_loss/perplexity = 3.77986312/43.8100433 secs/batch = 0.2991s, grad.norm=16.10995483
 27952: 21 [   85/ 1327], train_loss/perplexity = 3.73367810/41.8326912 secs/batch = 0.2976s, grad.norm=16.37512589
 27957: 21 [   90/ 1327], train_loss/perplexity = 3.83363533/46.2302971 secs/batch = 0.2959s, grad.norm=16.34918404
 27962: 21 [   95/ 1327], train_loss/perplexity = 3.77333903/43.5251541 secs/batch = 0.2960s, grad.norm=15.92207146
 27967: 21 [  100/ 1327], train_loss/perplexity = 4.01572227/55.4633408 secs/batch = 0.2940s, grad.norm=15.94251823
 27972: 21 [  105/ 1327], train_loss/perplexity = 3.69924641/40.4168358 secs/batch = 0.2936s, grad.norm=16.27292442
 27977: 21 [  110/ 1327], train_loss/perplexity = 3.78149724/43.8816948 secs/batch = 0.2971s, grad.norm=15.98925018
 27982: 21 [  115/ 1327], train_loss/perplexity = 3.75128913/42.5759315 secs/batch = 0.3003s, grad.norm=16.17679405
 27987: 21 [  120/ 1327], train_loss/perplexity = 3.73634505/41.9444046 secs/batch = 0.2927s, grad.norm=16.32856369
 27992: 21 [  125/ 1327], train_loss/perplexity = 3.81248045/45.2625694 secs/batch = 0.2988s, grad.norm=16.40557671
 27997: 21 [  130/ 1327], train_loss/perplexity = 3.75113082/42.5691910 secs/batch = 0.2960s, grad.norm=16.80210304
 28002: 21 [  135/ 1327], train_loss/perplexity = 3.82898760/46.0159264 secs/batch = 0.2943s, grad.norm=15.74617577
 28007: 21 [  140/ 1327], train_loss/perplexity = 4.08420992/59.3949928 secs/batch = 0.2954s, grad.norm=16.37281799
 28012: 21 [  145/ 1327], train_loss/perplexity = 3.92491293/50.6486664 secs/batch = 0.2993s, grad.norm=16.78147697
 28017: 21 [  150/ 1327], train_loss/perplexity = 3.89511490/49.1617012 secs/batch = 0.2945s, grad.norm=16.32106781
 28022: 21 [  155/ 1327], train_loss/perplexity = 4.23689175/69.1924515 secs/batch = 0.2974s, grad.norm=16.10039520
 28027: 21 [  160/ 1327], train_loss/perplexity = 3.90226722/49.5145836 secs/batch = 0.2995s, grad.norm=15.14135742
 28032: 21 [  165/ 1327], train_loss/perplexity = 4.00482845/54.8624115 secs/batch = 0.2991s, grad.norm=16.08033752
 28037: 21 [  170/ 1327], train_loss/perplexity = 3.77461386/43.5806770 secs/batch = 0.3008s, grad.norm=15.38089657
 28042: 21 [  175/ 1327], train_loss/perplexity = 4.15225506/63.5772095 secs/batch = 0.2946s, grad.norm=15.85101414
 28047: 21 [  180/ 1327], train_loss/perplexity = 3.95104837/51.9898415 secs/batch = 0.2936s, grad.norm=15.83854389
 28052: 21 [  185/ 1327], train_loss/perplexity = 4.29032803/72.9904099 secs/batch = 0.2918s, grad.norm=16.60720062
 28057: 21 [  190/ 1327], train_loss/perplexity = 3.75663424/42.8041153 secs/batch = 0.2943s, grad.norm=15.02587414
 28062: 21 [  195/ 1327], train_loss/perplexity = 4.13238525/62.3264084 secs/batch = 0.2997s, grad.norm=15.33366299
 28067: 21 [  200/ 1327], train_loss/perplexity = 3.88103509/48.4743652 secs/batch = 0.2953s, grad.norm=16.20117569
 28072: 21 [  205/ 1327], train_loss/perplexity = 4.17981863/65.3539963 secs/batch = 0.2952s, grad.norm=15.87752438
 28077: 21 [  210/ 1327], train_loss/perplexity = 4.01760435/55.5678253 secs/batch = 0.2954s, grad.norm=15.27109909
 28082: 21 [  215/ 1327], train_loss/perplexity = 4.15449810/63.7199745 secs/batch = 0.2957s, grad.norm=15.19703007
 28087: 21 [  220/ 1327], train_loss/perplexity = 4.00212002/54.7140236 secs/batch = 0.2951s, grad.norm=15.45768070
 28092: 21 [  225/ 1327], train_loss/perplexity = 4.19228792/66.1740189 secs/batch = 0.2924s, grad.norm=16.11120415
 28097: 21 [  230/ 1327], train_loss/perplexity = 4.07391787/58.7868309 secs/batch = 0.2948s, grad.norm=16.95627213
 28102: 21 [  235/ 1327], train_loss/perplexity = 3.97766495/53.3922157 secs/batch = 0.3009s, grad.norm=15.98076916
 28107: 21 [  240/ 1327], train_loss/perplexity = 3.65499568/38.6673546 secs/batch = 0.3009s, grad.norm=16.02854156
 28112: 21 [  245/ 1327], train_loss/perplexity = 3.96692753/52.8219872 secs/batch = 0.2993s, grad.norm=16.04833031
 28117: 21 [  250/ 1327], train_loss/perplexity = 3.84256530/46.6449776 secs/batch = 0.2990s, grad.norm=15.75099468
 28122: 21 [  255/ 1327], train_loss/perplexity = 3.81176782/45.2303276 secs/batch = 0.2936s, grad.norm=15.78668213
 28127: 21 [  260/ 1327], train_loss/perplexity = 4.03999233/56.8259087 secs/batch = 0.2990s, grad.norm=16.11250877
 28132: 21 [  265/ 1327], train_loss/perplexity = 4.26947498/71.4840927 secs/batch = 0.2982s, grad.norm=15.42490196
 28137: 21 [  270/ 1327], train_loss/perplexity = 4.23325825/68.9414978 secs/batch = 0.2958s, grad.norm=15.94440269
 28142: 21 [  275/ 1327], train_loss/perplexity = 4.17683220/65.1591110 secs/batch = 0.2939s, grad.norm=15.89796448
 28147: 21 [  280/ 1327], train_loss/perplexity = 3.98009038/53.5218697 secs/batch = 0.2954s, grad.norm=15.63354683
 28152: 21 [  285/ 1327], train_loss/perplexity = 4.31141329/74.5457687 secs/batch = 0.2939s, grad.norm=16.08518410
 28157: 21 [  290/ 1327], train_loss/perplexity = 3.98362494/53.7113838 secs/batch = 0.2946s, grad.norm=16.37072754
 28162: 21 [  295/ 1327], train_loss/perplexity = 3.75059223/42.5462723 secs/batch = 0.2943s, grad.norm=15.48532391
 28167: 21 [  300/ 1327], train_loss/perplexity = 3.30310392/27.1969261 secs/batch = 0.2947s, grad.norm=14.67190170
 28172: 21 [  305/ 1327], train_loss/perplexity = 3.85101795/47.0409241 secs/batch = 0.2909s, grad.norm=15.28477764
 28177: 21 [  310/ 1327], train_loss/perplexity = 3.86714125/47.8055267 secs/batch = 0.2967s, grad.norm=15.69543171
 28182: 21 [  315/ 1327], train_loss/perplexity = 3.36882496/29.0443783 secs/batch = 0.2992s, grad.norm=14.61009502
 28187: 21 [  320/ 1327], train_loss/perplexity = 3.27667642/26.4875927 secs/batch = 0.2952s, grad.norm=15.89852810
 28192: 21 [  325/ 1327], train_loss/perplexity = 3.26637197/26.2160530 secs/batch = 0.2942s, grad.norm=14.10531425
 28197: 21 [  330/ 1327], train_loss/perplexity = 3.90053558/49.4289169 secs/batch = 0.2951s, grad.norm=15.23350334
 28202: 21 [  335/ 1327], train_loss/perplexity = 3.40388179/30.0806408 secs/batch = 0.2953s, grad.norm=15.10567188
 28207: 21 [  340/ 1327], train_loss/perplexity = 4.08419991/59.3943977 secs/batch = 0.3002s, grad.norm=15.42475986
 28212: 21 [  345/ 1327], train_loss/perplexity = 3.91629910/50.2142639 secs/batch = 0.2993s, grad.norm=14.84516525
 28217: 21 [  350/ 1327], train_loss/perplexity = 3.85309935/47.1389351 secs/batch = 0.2927s, grad.norm=15.95945072
 28222: 21 [  355/ 1327], train_loss/perplexity = 3.90011692/49.4082260 secs/batch = 0.2993s, grad.norm=15.77428436
 28227: 21 [  360/ 1327], train_loss/perplexity = 3.96969151/52.9681892 secs/batch = 0.3004s, grad.norm=16.94052887
 28232: 21 [  365/ 1327], train_loss/perplexity = 3.98145890/53.5951691 secs/batch = 0.2934s, grad.norm=15.47648335
 28237: 21 [  370/ 1327], train_loss/perplexity = 4.10960531/60.9226685 secs/batch = 0.2922s, grad.norm=16.10114098
 28242: 21 [  375/ 1327], train_loss/perplexity = 3.43770242/31.1153851 secs/batch = 0.3000s, grad.norm=15.37276363
 28247: 21 [  380/ 1327], train_loss/perplexity = 3.56752872/35.4289284 secs/batch = 0.2942s, grad.norm=15.56115818
 28252: 21 [  385/ 1327], train_loss/perplexity = 3.75597453/42.7758865 secs/batch = 0.2941s, grad.norm=16.27836609
 28257: 21 [  390/ 1327], train_loss/perplexity = 3.92647696/50.7279472 secs/batch = 0.2949s, grad.norm=15.54773808
 28262: 21 [  395/ 1327], train_loss/perplexity = 3.93137312/50.9769287 secs/batch = 0.2952s, grad.norm=15.41516018
 28267: 21 [  400/ 1327], train_loss/perplexity = 3.87151480/48.0150642 secs/batch = 0.3003s, grad.norm=15.09128761
 28272: 21 [  405/ 1327], train_loss/perplexity = 4.17758894/65.2084427 secs/batch = 0.2946s, grad.norm=15.86104012
 28277: 21 [  410/ 1327], train_loss/perplexity = 3.78016615/43.8233223 secs/batch = 0.2904s, grad.norm=15.26439190
 28282: 21 [  415/ 1327], train_loss/perplexity = 3.72402215/41.4306984 secs/batch = 0.2995s, grad.norm=15.19051075
 28287: 21 [  420/ 1327], train_loss/perplexity = 3.40144491/30.0074272 secs/batch = 0.2952s, grad.norm=15.20974827
 28292: 21 [  425/ 1327], train_loss/perplexity = 3.70685935/40.7257004 secs/batch = 0.2991s, grad.norm=15.84045792
 28297: 21 [  430/ 1327], train_loss/perplexity = 3.93862295/51.3478432 secs/batch = 0.2992s, grad.norm=16.10663223
 28302: 21 [  435/ 1327], train_loss/perplexity = 3.97031450/53.0011978 secs/batch = 0.2960s, grad.norm=16.22112274
 28307: 21 [  440/ 1327], train_loss/perplexity = 3.45227647/31.5721836 secs/batch = 0.3005s, grad.norm=15.18235874
 28312: 21 [  445/ 1327], train_loss/perplexity = 3.92566109/50.6865768 secs/batch = 0.3000s, grad.norm=16.48915672
 28317: 21 [  450/ 1327], train_loss/perplexity = 3.81691694/45.4638252 secs/batch = 0.2953s, grad.norm=15.53803444
 28322: 21 [  455/ 1327], train_loss/perplexity = 3.79105687/44.3031998 secs/batch = 0.2935s, grad.norm=15.53290939
 28327: 21 [  460/ 1327], train_loss/perplexity = 3.77063942/43.4078102 secs/batch = 0.3010s, grad.norm=16.40355873
 28332: 21 [  465/ 1327], train_loss/perplexity = 3.48943686/32.7674904 secs/batch = 0.2956s, grad.norm=16.37071609
 28337: 21 [  470/ 1327], train_loss/perplexity = 4.21787500/67.8890686 secs/batch = 0.2940s, grad.norm=15.92839336
 28342: 21 [  475/ 1327], train_loss/perplexity = 3.61800504/37.2631569 secs/batch = 0.2927s, grad.norm=15.67615509
 28347: 21 [  480/ 1327], train_loss/perplexity = 3.72201204/41.3475037 secs/batch = 0.2952s, grad.norm=16.01784706
 28352: 21 [  485/ 1327], train_loss/perplexity = 3.76185989/43.0283813 secs/batch = 0.2948s, grad.norm=15.75823498
 28357: 21 [  490/ 1327], train_loss/perplexity = 3.59701729/36.4892349 secs/batch = 0.2942s, grad.norm=16.60447311
 28362: 21 [  495/ 1327], train_loss/perplexity = 3.76945543/43.3564491 secs/batch = 0.2926s, grad.norm=15.64317513
 28367: 21 [  500/ 1327], train_loss/perplexity = 3.86545944/47.7251930 secs/batch = 0.2947s, grad.norm=15.84508228
 28372: 21 [  505/ 1327], train_loss/perplexity = 4.03048038/56.2879448 secs/batch = 0.2971s, grad.norm=15.15521622
 28377: 21 [  510/ 1327], train_loss/perplexity = 4.30553198/74.1086273 secs/batch = 0.2942s, grad.norm=15.62017155
 28382: 21 [  515/ 1327], train_loss/perplexity = 3.94922185/51.8949699 secs/batch = 0.2945s, grad.norm=15.40122414
 28387: 21 [  520/ 1327], train_loss/perplexity = 4.10398293/60.5810966 secs/batch = 0.2981s, grad.norm=15.48663235
 28392: 21 [  525/ 1327], train_loss/perplexity = 3.73261476/41.7882309 secs/batch = 0.2947s, grad.norm=15.80910206
 28397: 21 [  530/ 1327], train_loss/perplexity = 3.73453021/41.8683510 secs/batch = 0.2963s, grad.norm=15.91320992
 28402: 21 [  535/ 1327], train_loss/perplexity = 3.85845137/47.3919029 secs/batch = 0.2985s, grad.norm=15.56686115
 28407: 21 [  540/ 1327], train_loss/perplexity = 3.98860979/53.9797935 secs/batch = 0.2982s, grad.norm=15.54567528
 28412: 21 [  545/ 1327], train_loss/perplexity = 3.91827106/50.3133812 secs/batch = 0.2977s, grad.norm=15.87393856
 28417: 21 [  550/ 1327], train_loss/perplexity = 3.87756634/48.3065109 secs/batch = 0.3004s, grad.norm=15.63846111
 28422: 21 [  555/ 1327], train_loss/perplexity = 3.78155708/43.8843193 secs/batch = 0.2998s, grad.norm=14.94604492
 28427: 21 [  560/ 1327], train_loss/perplexity = 3.88281441/48.5606918 secs/batch = 0.2940s, grad.norm=16.44812584
 28432: 21 [  565/ 1327], train_loss/perplexity = 3.71245670/40.9542961 secs/batch = 0.3002s, grad.norm=16.90529633
 28437: 21 [  570/ 1327], train_loss/perplexity = 3.78286862/43.9419136 secs/batch = 0.2985s, grad.norm=16.11747169
 28442: 21 [  575/ 1327], train_loss/perplexity = 3.57248688/35.6050301 secs/batch = 0.2971s, grad.norm=15.85355473
 28447: 21 [  580/ 1327], train_loss/perplexity = 3.92321324/50.5626526 secs/batch = 0.2947s, grad.norm=16.29173279
 28452: 21 [  585/ 1327], train_loss/perplexity = 3.56120276/35.2055168 secs/batch = 0.2933s, grad.norm=15.18185043
 28457: 21 [  590/ 1327], train_loss/perplexity = 3.94190693/51.5167465 secs/batch = 0.2958s, grad.norm=16.05317497
 28462: 21 [  595/ 1327], train_loss/perplexity = 3.89987516/49.3962822 secs/batch = 0.2949s, grad.norm=16.50300598
 28467: 21 [  600/ 1327], train_loss/perplexity = 4.08487129/59.4342880 secs/batch = 0.2963s, grad.norm=15.54094696
 28472: 21 [  605/ 1327], train_loss/perplexity = 3.99217319/54.1724892 secs/batch = 0.2933s, grad.norm=15.40767097
 28477: 21 [  610/ 1327], train_loss/perplexity = 4.20390177/66.9470367 secs/batch = 0.2946s, grad.norm=15.80950832
 28482: 21 [  615/ 1327], train_loss/perplexity = 3.75447893/42.7119598 secs/batch = 0.3017s, grad.norm=15.09612846
 28487: 21 [  620/ 1327], train_loss/perplexity = 4.06763458/58.4186134 secs/batch = 0.2943s, grad.norm=15.76965618
 28492: 21 [  625/ 1327], train_loss/perplexity = 4.06966400/58.5372925 secs/batch = 0.2928s, grad.norm=15.78699207
 28497: 21 [  630/ 1327], train_loss/perplexity = 4.12589598/61.9232674 secs/batch = 0.2949s, grad.norm=15.67295265
 28502: 21 [  635/ 1327], train_loss/perplexity = 3.85043645/47.0135765 secs/batch = 0.2967s, grad.norm=15.52390957
 28507: 21 [  640/ 1327], train_loss/perplexity = 3.84129357/46.5856972 secs/batch = 0.2958s, grad.norm=15.39476490
 28512: 21 [  645/ 1327], train_loss/perplexity = 4.10490799/60.6371651 secs/batch = 0.2959s, grad.norm=16.36949921
 28517: 21 [  650/ 1327], train_loss/perplexity = 3.63655233/37.9607353 secs/batch = 0.2948s, grad.norm=15.72225571
 28522: 21 [  655/ 1327], train_loss/perplexity = 3.77958512/43.7978668 secs/batch = 0.2952s, grad.norm=15.68849087
 28527: 21 [  660/ 1327], train_loss/perplexity = 3.75974464/42.9374619 secs/batch = 0.2939s, grad.norm=16.01901436
 28532: 21 [  665/ 1327], train_loss/perplexity = 3.83487582/46.2876778 secs/batch = 0.2992s, grad.norm=15.59397793
 28537: 21 [  670/ 1327], train_loss/perplexity = 3.71741104/41.1576996 secs/batch = 0.2920s, grad.norm=15.55661488
 28542: 21 [  675/ 1327], train_loss/perplexity = 3.58722854/36.1337929 secs/batch = 0.3000s, grad.norm=16.16813469
 28547: 21 [  680/ 1327], train_loss/perplexity = 3.80064249/44.7299156 secs/batch = 0.3006s, grad.norm=16.52104950
 28552: 21 [  685/ 1327], train_loss/perplexity = 3.53599930/34.3293037 secs/batch = 0.2932s, grad.norm=15.40039253
 28557: 21 [  690/ 1327], train_loss/perplexity = 4.00858927/55.0691261 secs/batch = 0.2924s, grad.norm=15.58222771
 28562: 21 [  695/ 1327], train_loss/perplexity = 3.85220957/47.0970116 secs/batch = 0.2958s, grad.norm=15.55252075
 28567: 21 [  700/ 1327], train_loss/perplexity = 4.09722233/60.1729164 secs/batch = 0.2985s, grad.norm=16.07413101
 28572: 21 [  705/ 1327], train_loss/perplexity = 3.83655095/46.3652840 secs/batch = 0.2931s, grad.norm=15.01604176
 28577: 21 [  710/ 1327], train_loss/perplexity = 3.69594836/40.2837563 secs/batch = 0.2921s, grad.norm=15.86364174
 28582: 21 [  715/ 1327], train_loss/perplexity = 3.58465290/36.0408440 secs/batch = 0.2985s, grad.norm=15.50916386
 28587: 21 [  720/ 1327], train_loss/perplexity = 3.58396244/36.0159683 secs/batch = 0.2981s, grad.norm=16.22594070
 28592: 21 [  725/ 1327], train_loss/perplexity = 3.70052075/40.4683723 secs/batch = 0.3001s, grad.norm=15.61783218
 28597: 21 [  730/ 1327], train_loss/perplexity = 3.71452761/41.0391960 secs/batch = 0.2986s, grad.norm=16.00337410
 28602: 21 [  735/ 1327], train_loss/perplexity = 3.91208792/50.0032463 secs/batch = 0.2936s, grad.norm=16.61592865
 28607: 21 [  740/ 1327], train_loss/perplexity = 3.40019464/29.9699326 secs/batch = 0.3020s, grad.norm=15.13820362
 28612: 21 [  745/ 1327], train_loss/perplexity = 3.86361361/47.6371841 secs/batch = 0.3019s, grad.norm=16.03487396
 28617: 21 [  750/ 1327], train_loss/perplexity = 3.74880695/42.4703827 secs/batch = 0.2981s, grad.norm=15.94480801
 28622: 21 [  755/ 1327], train_loss/perplexity = 3.59251952/36.3254852 secs/batch = 0.2915s, grad.norm=15.13910198
 28627: 21 [  760/ 1327], train_loss/perplexity = 3.49679542/33.0095024 secs/batch = 0.2947s, grad.norm=14.73396969
 28632: 21 [  765/ 1327], train_loss/perplexity = 3.63788342/38.0112991 secs/batch = 0.2942s, grad.norm=15.07361221
 28637: 21 [  770/ 1327], train_loss/perplexity = 3.52937245/34.1025581 secs/batch = 0.2944s, grad.norm=15.35968208
 28642: 21 [  775/ 1327], train_loss/perplexity = 3.62679625/37.5921860 secs/batch = 0.2924s, grad.norm=15.58610344
 28647: 21 [  780/ 1327], train_loss/perplexity = 3.88312054/48.5755615 secs/batch = 0.2980s, grad.norm=15.96660042
 28652: 21 [  785/ 1327], train_loss/perplexity = 3.92575550/50.6913605 secs/batch = 0.3001s, grad.norm=16.09188843
 28657: 21 [  790/ 1327], train_loss/perplexity = 3.57706642/35.7684555 secs/batch = 0.2936s, grad.norm=15.48571682
 28662: 21 [  795/ 1327], train_loss/perplexity = 3.95421934/52.1549644 secs/batch = 0.2981s, grad.norm=15.85569763
 28667: 21 [  800/ 1327], train_loss/perplexity = 3.79307747/44.3928070 secs/batch = 0.2982s, grad.norm=15.58393574
 28672: 21 [  805/ 1327], train_loss/perplexity = 4.17461205/65.0146103 secs/batch = 0.2957s, grad.norm=15.78540516
 28677: 21 [  810/ 1327], train_loss/perplexity = 3.78613186/44.0855408 secs/batch = 0.2941s, grad.norm=15.30091476
 28682: 21 [  815/ 1327], train_loss/perplexity = 3.70956421/40.8360062 secs/batch = 0.2936s, grad.norm=15.32805538
 28687: 21 [  820/ 1327], train_loss/perplexity = 3.51068997/33.4713554 secs/batch = 0.2998s, grad.norm=14.98239994
 28692: 21 [  825/ 1327], train_loss/perplexity = 3.80185795/44.7843132 secs/batch = 0.2952s, grad.norm=15.15245247
 28697: 21 [  830/ 1327], train_loss/perplexity = 3.54087019/34.4969254 secs/batch = 0.2954s, grad.norm=15.85213184
 28702: 21 [  835/ 1327], train_loss/perplexity = 3.79428911/44.4466286 secs/batch = 0.2931s, grad.norm=15.69530582
 28707: 21 [  840/ 1327], train_loss/perplexity = 3.85046864/47.0150909 secs/batch = 0.2955s, grad.norm=15.88308239
 28712: 21 [  845/ 1327], train_loss/perplexity = 3.65891790/38.8193130 secs/batch = 0.2959s, grad.norm=15.59229946
 28717: 21 [  850/ 1327], train_loss/perplexity = 3.75999880/42.9483757 secs/batch = 0.3009s, grad.norm=15.43394947
 28722: 21 [  855/ 1327], train_loss/perplexity = 3.77874279/43.7609901 secs/batch = 0.2945s, grad.norm=15.44800949
 28727: 21 [  860/ 1327], train_loss/perplexity = 3.59573174/36.4423561 secs/batch = 0.3017s, grad.norm=15.40543365
 28732: 21 [  865/ 1327], train_loss/perplexity = 3.93520522/51.1726494 secs/batch = 0.2951s, grad.norm=15.61472988
 28737: 21 [  870/ 1327], train_loss/perplexity = 3.85787177/47.3644409 secs/batch = 0.2955s, grad.norm=16.47120857
 28742: 21 [  875/ 1327], train_loss/perplexity = 3.39519477/29.8204613 secs/batch = 0.2944s, grad.norm=15.06188011
 28747: 21 [  880/ 1327], train_loss/perplexity = 3.62005472/37.3396111 secs/batch = 0.2953s, grad.norm=14.82430077
 28752: 21 [  885/ 1327], train_loss/perplexity = 3.83201838/46.1556053 secs/batch = 0.2931s, grad.norm=15.29871464
 28757: 21 [  890/ 1327], train_loss/perplexity = 3.92755580/50.7827034 secs/batch = 0.2949s, grad.norm=15.62030029
 28762: 21 [  895/ 1327], train_loss/perplexity = 3.86630583/47.7656059 secs/batch = 0.2946s, grad.norm=14.81428528
 28767: 21 [  900/ 1327], train_loss/perplexity = 3.72384977/41.4235573 secs/batch = 0.2936s, grad.norm=14.95503426
 28772: 21 [  905/ 1327], train_loss/perplexity = 3.60619473/36.8256531 secs/batch = 0.2923s, grad.norm=14.84463787
 28777: 21 [  910/ 1327], train_loss/perplexity = 3.66101122/38.9006615 secs/batch = 0.2984s, grad.norm=14.19418049
 28782: 21 [  915/ 1327], train_loss/perplexity = 3.97522950/53.2623405 secs/batch = 0.2941s, grad.norm=14.97083282
 28787: 21 [  920/ 1327], train_loss/perplexity = 4.02758884/56.1254196 secs/batch = 0.2985s, grad.norm=16.21239281
 28792: 21 [  925/ 1327], train_loss/perplexity = 3.86729050/47.8126602 secs/batch = 0.2948s, grad.norm=14.95554066
 28797: 21 [  930/ 1327], train_loss/perplexity = 3.94379425/51.6140671 secs/batch = 0.2945s, grad.norm=15.61410046
 28802: 21 [  935/ 1327], train_loss/perplexity = 3.97441387/53.2189140 secs/batch = 0.2941s, grad.norm=15.26786613
 28807: 21 [  940/ 1327], train_loss/perplexity = 3.90309262/49.5554695 secs/batch = 0.2995s, grad.norm=14.71772671
 28812: 21 [  945/ 1327], train_loss/perplexity = 4.09712601/60.1671181 secs/batch = 0.2987s, grad.norm=15.19277477
 28817: 21 [  950/ 1327], train_loss/perplexity = 3.88510489/48.6720467 secs/batch = 0.2946s, grad.norm=15.71363354
 28822: 21 [  955/ 1327], train_loss/perplexity = 3.85640645/47.2950897 secs/batch = 0.2944s, grad.norm=15.80986309
 28827: 21 [  960/ 1327], train_loss/perplexity = 4.10813427/60.8331146 secs/batch = 0.2940s, grad.norm=15.80305386
 28832: 21 [  965/ 1327], train_loss/perplexity = 3.92524910/50.6656990 secs/batch = 0.2984s, grad.norm=15.57468605
 28837: 21 [  970/ 1327], train_loss/perplexity = 4.08317661/59.3336525 secs/batch = 0.2955s, grad.norm=15.68364716
 28842: 21 [  975/ 1327], train_loss/perplexity = 3.73947072/42.0757141 secs/batch = 0.2943s, grad.norm=16.29202843
 28847: 21 [  980/ 1327], train_loss/perplexity = 3.59905815/36.5637817 secs/batch = 0.2947s, grad.norm=15.27904987
 28852: 21 [  985/ 1327], train_loss/perplexity = 3.79067826/44.2864265 secs/batch = 0.2941s, grad.norm=16.02411652
 28857: 21 [  990/ 1327], train_loss/perplexity = 3.95466518/52.1782227 secs/batch = 0.2932s, grad.norm=16.14434814
 28862: 21 [  995/ 1327], train_loss/perplexity = 4.01255512/55.2879562 secs/batch = 0.2943s, grad.norm=15.84399033
 28867: 21 [ 1000/ 1327], train_loss/perplexity = 3.63753700/37.9981308 secs/batch = 0.2935s, grad.norm=15.71848106
 28872: 21 [ 1005/ 1327], train_loss/perplexity = 3.99114990/54.1170845 secs/batch = 0.2945s, grad.norm=15.54256248
 28877: 21 [ 1010/ 1327], train_loss/perplexity = 3.59298038/36.3422279 secs/batch = 0.2931s, grad.norm=14.64005280
 28882: 21 [ 1015/ 1327], train_loss/perplexity = 4.07385969/58.7834129 secs/batch = 0.2943s, grad.norm=15.68261433
 28887: 21 [ 1020/ 1327], train_loss/perplexity = 4.11081219/60.9962387 secs/batch = 0.2981s, grad.norm=15.22088623
 28892: 21 [ 1025/ 1327], train_loss/perplexity = 4.06693554/58.3777924 secs/batch = 0.2952s, grad.norm=15.41081810
 28897: 21 [ 1030/ 1327], train_loss/perplexity = 3.78032160/43.8301353 secs/batch = 0.2947s, grad.norm=15.05358791
 28902: 21 [ 1035/ 1327], train_loss/perplexity = 3.77161694/43.4502640 secs/batch = 0.2985s, grad.norm=15.04692364
 28907: 21 [ 1040/ 1327], train_loss/perplexity = 3.92563415/50.6852112 secs/batch = 0.2932s, grad.norm=15.89387894
 28912: 21 [ 1045/ 1327], train_loss/perplexity = 3.51913834/33.7553291 secs/batch = 0.2951s, grad.norm=14.42908001
 28917: 21 [ 1050/ 1327], train_loss/perplexity = 3.61014485/36.9714088 secs/batch = 0.3011s, grad.norm=15.12476921
 28922: 21 [ 1055/ 1327], train_loss/perplexity = 3.66870165/39.2009773 secs/batch = 0.2961s, grad.norm=15.85118675
 28927: 21 [ 1060/ 1327], train_loss/perplexity = 3.30900478/27.3578854 secs/batch = 0.2984s, grad.norm=15.67336082
 28932: 21 [ 1065/ 1327], train_loss/perplexity = 3.44831514/31.4473629 secs/batch = 0.2932s, grad.norm=15.32185173
 28937: 21 [ 1070/ 1327], train_loss/perplexity = 3.66518831/39.0634918 secs/batch = 0.2954s, grad.norm=15.94428253
 28942: 21 [ 1075/ 1327], train_loss/perplexity = 3.50514102/33.2861366 secs/batch = 0.2940s, grad.norm=15.42041683
 28947: 21 [ 1080/ 1327], train_loss/perplexity = 3.52567434/33.9766769 secs/batch = 0.2942s, grad.norm=15.27227783
 28952: 21 [ 1085/ 1327], train_loss/perplexity = 3.43070602/30.8984509 secs/batch = 0.2918s, grad.norm=15.20246315
 28957: 21 [ 1090/ 1327], train_loss/perplexity = 3.63161230/37.7736702 secs/batch = 0.2951s, grad.norm=16.24349022
 28962: 21 [ 1095/ 1327], train_loss/perplexity = 3.77877140/43.7622414 secs/batch = 0.2957s, grad.norm=15.80545235
 28967: 21 [ 1100/ 1327], train_loss/perplexity = 3.46286774/31.9083500 secs/batch = 0.2929s, grad.norm=16.59338760
 28972: 21 [ 1105/ 1327], train_loss/perplexity = 3.42124271/30.6074276 secs/batch = 0.2911s, grad.norm=15.49983120
 28977: 21 [ 1110/ 1327], train_loss/perplexity = 3.70821881/40.7811012 secs/batch = 0.2949s, grad.norm=16.30661392
 28982: 21 [ 1115/ 1327], train_loss/perplexity = 3.57616544/35.7362442 secs/batch = 0.2951s, grad.norm=15.45444107
 28987: 21 [ 1120/ 1327], train_loss/perplexity = 3.83274031/46.1889381 secs/batch = 0.2927s, grad.norm=15.18457413
 28992: 21 [ 1125/ 1327], train_loss/perplexity = 4.02210522/55.8184929 secs/batch = 0.2930s, grad.norm=16.04191017
 28997: 21 [ 1130/ 1327], train_loss/perplexity = 3.66343975/38.9952469 secs/batch = 0.2947s, grad.norm=15.86930656
 29002: 21 [ 1135/ 1327], train_loss/perplexity = 3.60398364/36.7443199 secs/batch = 0.2939s, grad.norm=15.15011311
 29007: 21 [ 1140/ 1327], train_loss/perplexity = 3.90206671/49.5046539 secs/batch = 0.2967s, grad.norm=15.99233532
 29012: 21 [ 1145/ 1327], train_loss/perplexity = 3.75391769/42.6879921 secs/batch = 0.2934s, grad.norm=15.17077160
 29017: 21 [ 1150/ 1327], train_loss/perplexity = 3.70627475/40.7019005 secs/batch = 0.2986s, grad.norm=14.99293804
 29022: 21 [ 1155/ 1327], train_loss/perplexity = 3.78112793/43.8654900 secs/batch = 0.2997s, grad.norm=15.84174919
 29027: 21 [ 1160/ 1327], train_loss/perplexity = 3.74998045/42.5202522 secs/batch = 0.2941s, grad.norm=15.89890480
 29032: 21 [ 1165/ 1327], train_loss/perplexity = 3.84299231/46.6649017 secs/batch = 0.2936s, grad.norm=15.89007092
 29037: 21 [ 1170/ 1327], train_loss/perplexity = 3.70054936/40.4695320 secs/batch = 0.2961s, grad.norm=15.84231663
 29042: 21 [ 1175/ 1327], train_loss/perplexity = 3.46979141/32.1300392 secs/batch = 0.3013s, grad.norm=15.35646534
 29047: 21 [ 1180/ 1327], train_loss/perplexity = 3.47989297/32.4562492 secs/batch = 0.2939s, grad.norm=15.99601460
 29052: 21 [ 1185/ 1327], train_loss/perplexity = 3.62108946/37.3782692 secs/batch = 0.2997s, grad.norm=15.39893818
 29057: 21 [ 1190/ 1327], train_loss/perplexity = 3.74195886/42.1805344 secs/batch = 0.2992s, grad.norm=15.76229572
 29062: 21 [ 1195/ 1327], train_loss/perplexity = 3.52802539/34.0566521 secs/batch = 0.2950s, grad.norm=15.04817867
 29067: 21 [ 1200/ 1327], train_loss/perplexity = 3.44099355/31.2179585 secs/batch = 0.3013s, grad.norm=15.24372196
 29072: 21 [ 1205/ 1327], train_loss/perplexity = 3.50268555/33.2045059 secs/batch = 0.2932s, grad.norm=15.60275364
 29077: 21 [ 1210/ 1327], train_loss/perplexity = 3.16829967/23.7670383 secs/batch = 0.2973s, grad.norm=15.35907269
 29082: 21 [ 1215/ 1327], train_loss/perplexity = 3.38332725/29.4686584 secs/batch = 0.2987s, grad.norm=14.91166306
 29087: 21 [ 1220/ 1327], train_loss/perplexity = 3.50164247/33.1698875 secs/batch = 0.2943s, grad.norm=15.55066013
 29092: 21 [ 1225/ 1327], train_loss/perplexity = 3.22723508/25.2098579 secs/batch = 0.2984s, grad.norm=15.94787693
 29097: 21 [ 1230/ 1327], train_loss/perplexity = 3.52931356/34.1005516 secs/batch = 0.2961s, grad.norm=15.32829666
 29102: 21 [ 1235/ 1327], train_loss/perplexity = 3.46226692/31.8891850 secs/batch = 0.2986s, grad.norm=15.20384026
 29107: 21 [ 1240/ 1327], train_loss/perplexity = 3.75070071/42.5508881 secs/batch = 0.2940s, grad.norm=16.25106430
 29112: 21 [ 1245/ 1327], train_loss/perplexity = 3.69266605/40.1517525 secs/batch = 0.2925s, grad.norm=15.33306885
 29117: 21 [ 1250/ 1327], train_loss/perplexity = 3.79362106/44.4169464 secs/batch = 0.2928s, grad.norm=15.11332512
 29122: 21 [ 1255/ 1327], train_loss/perplexity = 3.78596020/44.0779724 secs/batch = 0.2944s, grad.norm=14.93068123
 29127: 21 [ 1260/ 1327], train_loss/perplexity = 3.57497525/35.6937370 secs/batch = 0.2951s, grad.norm=15.99621487
 29132: 21 [ 1265/ 1327], train_loss/perplexity = 3.77873135/43.7604904 secs/batch = 0.2923s, grad.norm=15.86941528
 29137: 21 [ 1270/ 1327], train_loss/perplexity = 3.53195143/34.1906242 secs/batch = 0.2937s, grad.norm=16.14056396
 29142: 21 [ 1275/ 1327], train_loss/perplexity = 3.65625858/38.7162170 secs/batch = 0.3002s, grad.norm=16.09272003
 29147: 21 [ 1280/ 1327], train_loss/perplexity = 3.56074882/35.1895370 secs/batch = 0.2946s, grad.norm=15.91240597
 29152: 21 [ 1285/ 1327], train_loss/perplexity = 3.57516050/35.7003517 secs/batch = 0.2912s, grad.norm=15.81020641
 29157: 21 [ 1290/ 1327], train_loss/perplexity = 3.75196314/42.6046371 secs/batch = 0.2959s, grad.norm=15.48971653
 29162: 21 [ 1295/ 1327], train_loss/perplexity = 3.66873646/39.2023392 secs/batch = 0.2994s, grad.norm=15.32331371
 29167: 21 [ 1300/ 1327], train_loss/perplexity = 3.88580370/48.7060699 secs/batch = 0.2954s, grad.norm=15.30799103
 29172: 21 [ 1305/ 1327], train_loss/perplexity = 3.88413668/48.6249466 secs/batch = 0.2988s, grad.norm=15.64208508
 29177: 21 [ 1310/ 1327], train_loss/perplexity = 4.19308329/66.2266693 secs/batch = 0.2953s, grad.norm=16.25025177
 29182: 21 [ 1315/ 1327], train_loss/perplexity = 3.97240925/53.1123390 secs/batch = 0.2942s, grad.norm=15.88594055
 29187: 21 [ 1320/ 1327], train_loss/perplexity = 4.04305124/57.0000000 secs/batch = 0.2936s, grad.norm=15.99103165
 29192: 21 [ 1325/ 1327], train_loss/perplexity = 3.85323477/47.1453209 secs/batch = 0.2961s, grad.norm=15.72149372
Epoch training time: 392.7782678604126
	> validation loss = 4.56452990, perplexity = 96.01744843
	> validation loss = 4.51143837, perplexity = 91.05268860
	> validation loss = 4.50222921, perplexity = 90.21802521
	> validation loss = 4.48134518, perplexity = 88.35344696
	> validation loss = 4.63692713, perplexity = 103.22665405
	> validation loss = 4.59325361, perplexity = 98.81541443
	> validation loss = 4.53096247, perplexity = 92.84787750
	> validation loss = 4.36965132, perplexity = 79.01607513
	> validation loss = 4.15494013, perplexity = 63.74814606
	> validation loss = 4.29625463, perplexity = 73.42427826
	> validation loss = 4.50342131, perplexity = 90.32563782
	> validation loss = 4.45730543, perplexity = 86.25477600
	> validation loss = 4.40107727, perplexity = 81.53865814
	> validation loss = 4.13266134, perplexity = 62.34362030
	> validation loss = 4.13439655, perplexity = 62.45189285
	> validation loss = 4.13286495, perplexity = 62.35631561
	> validation loss = 4.57961798, perplexity = 97.47714996
	> validation loss = 4.03653812, perplexity = 56.62995529
	> validation loss = 4.54840088, perplexity = 94.48120117
	> validation loss = 4.51383686, perplexity = 91.27133942
	> validation loss = 4.21648550, perplexity = 67.79479980
at the end of epoch: 21
train loss = 3.83683668, perplexity = 46.37853220
validation loss = 4.40118895, perplexity = 81.54776692
Saved model cv/epoch021_4.4012.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.015625
new learning rate is: 0.0078125
 29199: 22 [    5/ 1327], train_loss/perplexity = 4.03859901/56.7467842 secs/batch = 0.2945s, grad.norm=15.91086864
 29204: 22 [   10/ 1327], train_loss/perplexity = 3.64215565/38.1740379 secs/batch = 0.2981s, grad.norm=15.08682919
 29209: 22 [   15/ 1327], train_loss/perplexity = 3.98869085/53.9841690 secs/batch = 0.2988s, grad.norm=14.94830132
 29214: 22 [   20/ 1327], train_loss/perplexity = 4.15235853/63.5837898 secs/batch = 0.2929s, grad.norm=15.00455856
 29219: 22 [   25/ 1327], train_loss/perplexity = 4.00154972/54.6828270 secs/batch = 0.2925s, grad.norm=15.94926834
 29224: 22 [   30/ 1327], train_loss/perplexity = 4.04849195/57.3109627 secs/batch = 0.2989s, grad.norm=16.27339935
 29229: 22 [   35/ 1327], train_loss/perplexity = 3.81197762/45.2398186 secs/batch = 0.2993s, grad.norm=15.48992920
 29234: 22 [   40/ 1327], train_loss/perplexity = 3.84225845/46.6306686 secs/batch = 0.2954s, grad.norm=15.72326946
 29239: 22 [   45/ 1327], train_loss/perplexity = 3.71957946/41.2470436 secs/batch = 0.2950s, grad.norm=15.09281063
 29244: 22 [   50/ 1327], train_loss/perplexity = 3.85438347/47.1995087 secs/batch = 0.2942s, grad.norm=16.07338333
 29249: 22 [   55/ 1327], train_loss/perplexity = 3.76781583/43.2854195 secs/batch = 0.2977s, grad.norm=15.94963169
 29254: 22 [   60/ 1327], train_loss/perplexity = 4.00865030/55.0724907 secs/batch = 0.2924s, grad.norm=15.76730156
 29259: 22 [   65/ 1327], train_loss/perplexity = 3.65799475/38.7834930 secs/batch = 0.2951s, grad.norm=15.28729916
 29264: 22 [   70/ 1327], train_loss/perplexity = 3.45367479/31.6163635 secs/batch = 0.2948s, grad.norm=15.73165417
 29269: 22 [   75/ 1327], train_loss/perplexity = 3.34173059/28.2680054 secs/batch = 0.2942s, grad.norm=14.84049225
 29274: 22 [   80/ 1327], train_loss/perplexity = 3.78528428/44.0481911 secs/batch = 0.2929s, grad.norm=15.54886246
 29279: 22 [   85/ 1327], train_loss/perplexity = 3.78181458/43.8956223 secs/batch = 0.2947s, grad.norm=16.37405014
 29284: 22 [   90/ 1327], train_loss/perplexity = 3.88764858/48.7960129 secs/batch = 0.2920s, grad.norm=15.94489956
 29289: 22 [   95/ 1327], train_loss/perplexity = 3.75710464/42.8242531 secs/batch = 0.2931s, grad.norm=15.78075314
 29294: 22 [  100/ 1327], train_loss/perplexity = 3.99345732/54.2420959 secs/batch = 0.2981s, grad.norm=16.61052895
 29299: 22 [  105/ 1327], train_loss/perplexity = 3.67328501/39.3810616 secs/batch = 0.3004s, grad.norm=16.25513649
 29304: 22 [  110/ 1327], train_loss/perplexity = 3.72076869/41.2961273 secs/batch = 0.2955s, grad.norm=16.04170227
 29309: 22 [  115/ 1327], train_loss/perplexity = 3.75073123/42.5521851 secs/batch = 0.2940s, grad.norm=16.19742966
 29314: 22 [  120/ 1327], train_loss/perplexity = 3.68804669/39.9667053 secs/batch = 0.2944s, grad.norm=16.25092888
 29319: 22 [  125/ 1327], train_loss/perplexity = 3.71551847/41.0798798 secs/batch = 0.3000s, grad.norm=16.20550156
 29324: 22 [  130/ 1327], train_loss/perplexity = 3.74386311/42.2609329 secs/batch = 0.2987s, grad.norm=16.99401283
 29329: 22 [  135/ 1327], train_loss/perplexity = 3.78845930/44.1882668 secs/batch = 0.3012s, grad.norm=15.81123257
 29334: 22 [  140/ 1327], train_loss/perplexity = 3.99712729/54.4415321 secs/batch = 0.2925s, grad.norm=16.35185242
 29339: 22 [  145/ 1327], train_loss/perplexity = 3.89140940/48.9798698 secs/batch = 0.2981s, grad.norm=17.13433266
 29344: 22 [  150/ 1327], train_loss/perplexity = 3.93368959/51.0951500 secs/batch = 0.2958s, grad.norm=16.23228836
 29349: 22 [  155/ 1327], train_loss/perplexity = 4.15080547/63.4851151 secs/batch = 0.2956s, grad.norm=16.28404045
 29354: 22 [  160/ 1327], train_loss/perplexity = 3.89898944/49.3525505 secs/batch = 0.2926s, grad.norm=15.40177059
 29359: 22 [  165/ 1327], train_loss/perplexity = 3.99466634/54.3077164 secs/batch = 0.2982s, grad.norm=16.17419052
 29364: 22 [  170/ 1327], train_loss/perplexity = 3.83968496/46.5108185 secs/batch = 0.2948s, grad.norm=15.50480843
 29369: 22 [  175/ 1327], train_loss/perplexity = 4.11086130/60.9992332 secs/batch = 0.2994s, grad.norm=16.13860321
 29374: 22 [  180/ 1327], train_loss/perplexity = 3.93318868/51.0695610 secs/batch = 0.2985s, grad.norm=15.75573635
 29379: 22 [  185/ 1327], train_loss/perplexity = 4.27485132/71.8694534 secs/batch = 0.2955s, grad.norm=16.61140823
 29384: 22 [  190/ 1327], train_loss/perplexity = 3.82649565/45.9014015 secs/batch = 0.2955s, grad.norm=15.29745388
 29389: 22 [  195/ 1327], train_loss/perplexity = 4.10170221/60.4430885 secs/batch = 0.2997s, grad.norm=15.54017639
 29394: 22 [  200/ 1327], train_loss/perplexity = 3.91250944/50.0243263 secs/batch = 0.2947s, grad.norm=16.16305733
 29399: 22 [  205/ 1327], train_loss/perplexity = 4.14714289/63.2530212 secs/batch = 0.2948s, grad.norm=16.25347710
 29404: 22 [  210/ 1327], train_loss/perplexity = 3.96992350/52.9804764 secs/batch = 0.2951s, grad.norm=15.63385963
 29409: 22 [  215/ 1327], train_loss/perplexity = 4.12942171/62.1419754 secs/batch = 0.2996s, grad.norm=15.40315151
 29414: 22 [  220/ 1327], train_loss/perplexity = 3.97932911/53.4811440 secs/batch = 0.2942s, grad.norm=15.43333912
 29419: 22 [  225/ 1327], train_loss/perplexity = 4.18969631/66.0027466 secs/batch = 0.2927s, grad.norm=15.86546898
 29424: 22 [  230/ 1327], train_loss/perplexity = 4.04894638/57.3370132 secs/batch = 0.2943s, grad.norm=16.61352539
 29429: 22 [  235/ 1327], train_loss/perplexity = 3.94681120/51.7700195 secs/batch = 0.2948s, grad.norm=16.14731026
 29434: 22 [  240/ 1327], train_loss/perplexity = 3.66503310/39.0574303 secs/batch = 0.2940s, grad.norm=15.63326454
 29439: 22 [  245/ 1327], train_loss/perplexity = 3.90622497/49.7109375 secs/batch = 0.2992s, grad.norm=15.85601044
 29444: 22 [  250/ 1327], train_loss/perplexity = 3.89126134/48.9726181 secs/batch = 0.2942s, grad.norm=15.65249443
 29449: 22 [  255/ 1327], train_loss/perplexity = 3.78479624/44.0266991 secs/batch = 0.2989s, grad.norm=15.95841122
 29454: 22 [  260/ 1327], train_loss/perplexity = 3.99851990/54.5173988 secs/batch = 0.2978s, grad.norm=16.60722351
 29459: 22 [  265/ 1327], train_loss/perplexity = 4.25037766/70.1318893 secs/batch = 0.2948s, grad.norm=15.57853985
 29464: 22 [  270/ 1327], train_loss/perplexity = 4.22520447/68.3884888 secs/batch = 0.2991s, grad.norm=16.20377731
 29469: 22 [  275/ 1327], train_loss/perplexity = 4.12747097/62.0208702 secs/batch = 0.3002s, grad.norm=16.04129410
 29474: 22 [  280/ 1327], train_loss/perplexity = 3.98083925/53.5619659 secs/batch = 0.2925s, grad.norm=15.97915649
 29479: 22 [  285/ 1327], train_loss/perplexity = 4.31881571/75.0996323 secs/batch = 0.3003s, grad.norm=15.75194073
 29484: 22 [  290/ 1327], train_loss/perplexity = 3.96185589/52.5547714 secs/batch = 0.2948s, grad.norm=16.24361801
 29489: 22 [  295/ 1327], train_loss/perplexity = 3.75441980/42.7094345 secs/batch = 0.2943s, grad.norm=15.81252289
 29494: 22 [  300/ 1327], train_loss/perplexity = 3.26906443/26.2867355 secs/batch = 0.2917s, grad.norm=14.95982361
 29499: 22 [  305/ 1327], train_loss/perplexity = 3.83125210/46.1202507 secs/batch = 0.2961s, grad.norm=15.46723938
 29504: 22 [  310/ 1327], train_loss/perplexity = 3.84167457/46.6034508 secs/batch = 0.2937s, grad.norm=15.91728115
 29509: 22 [  315/ 1327], train_loss/perplexity = 3.34230113/28.2841377 secs/batch = 0.2942s, grad.norm=14.68712616
 29514: 22 [  320/ 1327], train_loss/perplexity = 3.27102947/26.3384399 secs/batch = 0.2943s, grad.norm=16.75345612
 29519: 22 [  325/ 1327], train_loss/perplexity = 3.29013872/26.8465881 secs/batch = 0.2986s, grad.norm=14.98786545
 29524: 22 [  330/ 1327], train_loss/perplexity = 3.93198395/51.0080757 secs/batch = 0.2956s, grad.norm=16.14027977
 29529: 22 [  335/ 1327], train_loss/perplexity = 3.40622616/30.1512432 secs/batch = 0.2993s, grad.norm=14.52893257
 29534: 22 [  340/ 1327], train_loss/perplexity = 4.04494858/57.1082497 secs/batch = 0.2939s, grad.norm=15.55376911
 29539: 22 [  345/ 1327], train_loss/perplexity = 3.91286540/50.0421371 secs/batch = 0.2941s, grad.norm=15.35090637
 29544: 22 [  350/ 1327], train_loss/perplexity = 3.84421968/46.7222137 secs/batch = 0.2993s, grad.norm=15.80048180
 29549: 22 [  355/ 1327], train_loss/perplexity = 3.88908148/48.8659821 secs/batch = 0.2944s, grad.norm=15.68163109
 29554: 22 [  360/ 1327], train_loss/perplexity = 3.97488570/53.2440300 secs/batch = 0.2940s, grad.norm=16.78920364
 29559: 22 [  365/ 1327], train_loss/perplexity = 4.02278662/55.8565407 secs/batch = 0.2930s, grad.norm=15.71000099
 29564: 22 [  370/ 1327], train_loss/perplexity = 4.09756374/60.1934624 secs/batch = 0.2939s, grad.norm=16.07557869
 29569: 22 [  375/ 1327], train_loss/perplexity = 3.49036264/32.7978401 secs/batch = 0.2956s, grad.norm=15.61545277
 29574: 22 [  380/ 1327], train_loss/perplexity = 3.50426078/33.2568512 secs/batch = 0.2948s, grad.norm=15.77253914
 29579: 22 [  385/ 1327], train_loss/perplexity = 3.75051951/42.5431786 secs/batch = 0.2927s, grad.norm=16.16047287
 29584: 22 [  390/ 1327], train_loss/perplexity = 3.89733267/49.2708511 secs/batch = 0.2954s, grad.norm=15.51368713
 29589: 22 [  395/ 1327], train_loss/perplexity = 3.94607544/51.7319412 secs/batch = 0.2989s, grad.norm=16.24707794
 29594: 22 [  400/ 1327], train_loss/perplexity = 3.86432266/47.6709709 secs/batch = 0.2990s, grad.norm=15.80594254
 29599: 22 [  405/ 1327], train_loss/perplexity = 4.19381428/66.2751007 secs/batch = 0.2998s, grad.norm=16.14907074
 29604: 22 [  410/ 1327], train_loss/perplexity = 3.77737713/43.7012672 secs/batch = 0.2952s, grad.norm=15.56009483
 29609: 22 [  415/ 1327], train_loss/perplexity = 3.82577467/45.8683205 secs/batch = 0.2946s, grad.norm=15.67491722
 29614: 22 [  420/ 1327], train_loss/perplexity = 3.37619877/29.2593384 secs/batch = 0.2934s, grad.norm=15.28087807
 29619: 22 [  425/ 1327], train_loss/perplexity = 3.75836134/42.8781052 secs/batch = 0.2982s, grad.norm=16.00554466
 29624: 22 [  430/ 1327], train_loss/perplexity = 3.90196490/49.4996147 secs/batch = 0.2946s, grad.norm=16.15624428
 29629: 22 [  435/ 1327], train_loss/perplexity = 3.94494414/51.6734505 secs/batch = 0.3000s, grad.norm=15.90644360
 29634: 22 [  440/ 1327], train_loss/perplexity = 3.49660802/33.0033150 secs/batch = 0.3003s, grad.norm=15.53377724
 29639: 22 [  445/ 1327], train_loss/perplexity = 3.89054894/48.9377441 secs/batch = 0.2938s, grad.norm=16.46852493
 29644: 22 [  450/ 1327], train_loss/perplexity = 3.78769588/44.1545448 secs/batch = 0.2920s, grad.norm=15.68568707
 29649: 22 [  455/ 1327], train_loss/perplexity = 3.78167319/43.8894157 secs/batch = 0.2951s, grad.norm=15.73748112
 29654: 22 [  460/ 1327], train_loss/perplexity = 3.80777740/45.0501976 secs/batch = 0.2950s, grad.norm=16.13540649
 29659: 22 [  465/ 1327], train_loss/perplexity = 3.50014067/33.1201096 secs/batch = 0.2946s, grad.norm=16.63340569
 29664: 22 [  470/ 1327], train_loss/perplexity = 4.16084862/64.1259155 secs/batch = 0.2928s, grad.norm=15.67244339
 29669: 22 [  475/ 1327], train_loss/perplexity = 3.61482120/37.1447029 secs/batch = 0.2946s, grad.norm=15.67614746
 29674: 22 [  480/ 1327], train_loss/perplexity = 3.71878076/41.2141151 secs/batch = 0.2947s, grad.norm=16.01965141
 29679: 22 [  485/ 1327], train_loss/perplexity = 3.87342119/48.1066856 secs/batch = 0.2985s, grad.norm=16.45129776
 29684: 22 [  490/ 1327], train_loss/perplexity = 3.67409301/39.4128952 secs/batch = 0.2922s, grad.norm=17.12772179
 29689: 22 [  495/ 1327], train_loss/perplexity = 3.73174238/41.7517929 secs/batch = 0.2942s, grad.norm=15.66028690
 29694: 22 [  500/ 1327], train_loss/perplexity = 3.87256622/48.0655746 secs/batch = 0.2961s, grad.norm=16.09222603
 29699: 22 [  505/ 1327], train_loss/perplexity = 3.94851613/51.8583603 secs/batch = 0.2993s, grad.norm=14.78805923
 29704: 22 [  510/ 1327], train_loss/perplexity = 4.37265253/79.2535782 secs/batch = 0.2929s, grad.norm=15.67565441
 29709: 22 [  515/ 1327], train_loss/perplexity = 3.97250891/53.1176300 secs/batch = 0.2947s, grad.norm=15.07395649
 29714: 22 [  520/ 1327], train_loss/perplexity = 4.08431864/59.4014511 secs/batch = 0.2952s, grad.norm=15.76072121
 29719: 22 [  525/ 1327], train_loss/perplexity = 3.79452229/44.4569931 secs/batch = 0.2990s, grad.norm=15.58939457
 29724: 22 [  530/ 1327], train_loss/perplexity = 3.71521544/41.0674324 secs/batch = 0.2939s, grad.norm=15.90453243
 29729: 22 [  535/ 1327], train_loss/perplexity = 3.90096974/49.4503784 secs/batch = 0.3014s, grad.norm=15.65215969
 29734: 22 [  540/ 1327], train_loss/perplexity = 3.97272182/53.1289406 secs/batch = 0.2936s, grad.norm=15.93954086
 29739: 22 [  545/ 1327], train_loss/perplexity = 3.87159276/48.0188065 secs/batch = 0.2982s, grad.norm=15.94627857
 29744: 22 [  550/ 1327], train_loss/perplexity = 3.88058686/48.4526405 secs/batch = 0.2943s, grad.norm=15.78708839
 29749: 22 [  555/ 1327], train_loss/perplexity = 3.70514703/40.6560249 secs/batch = 0.2987s, grad.norm=15.13782501
 29754: 22 [  560/ 1327], train_loss/perplexity = 3.95420742/52.1543427 secs/batch = 0.2940s, grad.norm=16.89771080
 29759: 22 [  565/ 1327], train_loss/perplexity = 3.75724387/42.8302193 secs/batch = 0.3011s, grad.norm=16.67554855
 29764: 22 [  570/ 1327], train_loss/perplexity = 3.69950485/40.4272804 secs/batch = 0.2929s, grad.norm=16.34537888
 29769: 22 [  575/ 1327], train_loss/perplexity = 3.58722448/36.1336479 secs/batch = 0.2956s, grad.norm=16.02536392
 29774: 22 [  580/ 1327], train_loss/perplexity = 3.97750735/53.3838005 secs/batch = 0.2988s, grad.norm=16.60670471
 29779: 22 [  585/ 1327], train_loss/perplexity = 3.57706308/35.7683372 secs/batch = 0.3000s, grad.norm=15.43767929
 29784: 22 [  590/ 1327], train_loss/perplexity = 3.93506908/51.1656837 secs/batch = 0.3005s, grad.norm=16.28323555
 29789: 22 [  595/ 1327], train_loss/perplexity = 3.90339589/49.5704994 secs/batch = 0.2951s, grad.norm=16.61964035
 29794: 22 [  600/ 1327], train_loss/perplexity = 4.06483364/58.2552147 secs/batch = 0.2951s, grad.norm=15.34842587
 29799: 22 [  605/ 1327], train_loss/perplexity = 3.97106123/53.0407906 secs/batch = 0.2971s, grad.norm=15.36190605
 29804: 22 [  610/ 1327], train_loss/perplexity = 4.18238449/65.5219040 secs/batch = 0.2981s, grad.norm=16.27362442
 29809: 22 [  615/ 1327], train_loss/perplexity = 3.76348162/43.0982170 secs/batch = 0.2989s, grad.norm=14.92842197
 29814: 22 [  620/ 1327], train_loss/perplexity = 4.10541439/60.6678810 secs/batch = 0.2939s, grad.norm=15.78673935
 29819: 22 [  625/ 1327], train_loss/perplexity = 4.03103495/56.3191681 secs/batch = 0.2987s, grad.norm=15.39196396
 29824: 22 [  630/ 1327], train_loss/perplexity = 4.10733747/60.7846603 secs/batch = 0.2931s, grad.norm=15.43414307
 29829: 22 [  635/ 1327], train_loss/perplexity = 3.88539076/48.6859627 secs/batch = 0.2946s, grad.norm=15.75558662
 29834: 22 [  640/ 1327], train_loss/perplexity = 3.82651210/45.9021568 secs/batch = 0.2924s, grad.norm=15.70742607
 29839: 22 [  645/ 1327], train_loss/perplexity = 4.05228043/57.5284958 secs/batch = 0.2929s, grad.norm=16.50864792
 29844: 22 [  650/ 1327], train_loss/perplexity = 3.62244296/37.4288940 secs/batch = 0.2943s, grad.norm=15.29440117
 29849: 22 [  655/ 1327], train_loss/perplexity = 3.75794005/42.8600464 secs/batch = 0.2927s, grad.norm=15.48906803
 29854: 22 [  660/ 1327], train_loss/perplexity = 3.74562931/42.3356400 secs/batch = 0.2947s, grad.norm=15.77410221
 29859: 22 [  665/ 1327], train_loss/perplexity = 3.87230086/48.0528221 secs/batch = 0.2921s, grad.norm=15.88086224
 29864: 22 [  670/ 1327], train_loss/perplexity = 3.75803661/42.8641853 secs/batch = 0.2967s, grad.norm=15.67325974
 29869: 22 [  675/ 1327], train_loss/perplexity = 3.66064692/38.8864899 secs/batch = 0.2952s, grad.norm=16.32580948
 29874: 22 [  680/ 1327], train_loss/perplexity = 3.79296994/44.3880348 secs/batch = 0.2984s, grad.norm=16.26914024
 29879: 22 [  685/ 1327], train_loss/perplexity = 3.60220599/36.6790581 secs/batch = 0.2985s, grad.norm=15.51255512
 29884: 22 [  690/ 1327], train_loss/perplexity = 4.02357197/55.9004250 secs/batch = 0.2989s, grad.norm=15.72387218
 29889: 22 [  695/ 1327], train_loss/perplexity = 3.87755680/48.3060493 secs/batch = 0.2961s, grad.norm=15.56501293
 29894: 22 [  700/ 1327], train_loss/perplexity = 4.06263876/58.1274948 secs/batch = 0.2935s, grad.norm=16.45031548
 29899: 22 [  705/ 1327], train_loss/perplexity = 3.80038404/44.7183533 secs/batch = 0.2921s, grad.norm=14.98017502
 29904: 22 [  710/ 1327], train_loss/perplexity = 3.72403193/41.4311066 secs/batch = 0.2964s, grad.norm=16.23069191
 29909: 22 [  715/ 1327], train_loss/perplexity = 3.60490513/36.7781944 secs/batch = 0.2947s, grad.norm=15.54393196
 29914: 22 [  720/ 1327], train_loss/perplexity = 3.56985378/35.5114021 secs/batch = 0.2997s, grad.norm=16.06741714
 29919: 22 [  725/ 1327], train_loss/perplexity = 3.70572352/40.6794701 secs/batch = 0.2959s, grad.norm=15.65529537
 29924: 22 [  730/ 1327], train_loss/perplexity = 3.78863621/44.1960831 secs/batch = 0.2947s, grad.norm=16.64310646
 29929: 22 [  735/ 1327], train_loss/perplexity = 3.89971614/49.3884277 secs/batch = 0.2957s, grad.norm=16.39246750
 29934: 22 [  740/ 1327], train_loss/perplexity = 3.42518520/30.7283344 secs/batch = 0.2941s, grad.norm=15.36585808
 29939: 22 [  745/ 1327], train_loss/perplexity = 3.83804226/46.4344788 secs/batch = 0.3003s, grad.norm=16.22764206
 29944: 22 [  750/ 1327], train_loss/perplexity = 3.74825525/42.4469566 secs/batch = 0.2962s, grad.norm=15.69421196
 29949: 22 [  755/ 1327], train_loss/perplexity = 3.57622528/35.7383842 secs/batch = 0.2948s, grad.norm=14.89026833
 29954: 22 [  760/ 1327], train_loss/perplexity = 3.46569991/31.9988480 secs/batch = 0.2989s, grad.norm=15.11900139
 29959: 22 [  765/ 1327], train_loss/perplexity = 3.59984183/36.5924454 secs/batch = 0.2934s, grad.norm=15.13902760
 29964: 22 [  770/ 1327], train_loss/perplexity = 3.54413056/34.6095810 secs/batch = 0.2949s, grad.norm=15.82075405
 29969: 22 [  775/ 1327], train_loss/perplexity = 3.64042664/38.1080933 secs/batch = 0.2953s, grad.norm=16.05788994
 29974: 22 [  780/ 1327], train_loss/perplexity = 3.96417451/52.6767654 secs/batch = 0.2946s, grad.norm=15.70386600
 29979: 22 [  785/ 1327], train_loss/perplexity = 3.85167193/47.0716972 secs/batch = 0.2960s, grad.norm=15.69714832
 29984: 22 [  790/ 1327], train_loss/perplexity = 3.56451774/35.3224144 secs/batch = 0.2948s, grad.norm=15.93796062
 29989: 22 [  795/ 1327], train_loss/perplexity = 3.93174076/50.9956703 secs/batch = 0.3005s, grad.norm=15.72197247
 29994: 22 [  800/ 1327], train_loss/perplexity = 3.78807259/44.1711807 secs/batch = 0.2925s, grad.norm=15.75429821
 29999: 22 [  805/ 1327], train_loss/perplexity = 4.22116375/68.1127014 secs/batch = 0.2931s, grad.norm=15.63441181
 30004: 22 [  810/ 1327], train_loss/perplexity = 3.79205966/44.3476486 secs/batch = 0.2990s, grad.norm=15.21953964
 30009: 22 [  815/ 1327], train_loss/perplexity = 3.63823247/38.0245667 secs/batch = 0.2998s, grad.norm=15.29178143
 30014: 22 [  820/ 1327], train_loss/perplexity = 3.59463215/36.4023056 secs/batch = 0.2955s, grad.norm=14.67800045
 30019: 22 [  825/ 1327], train_loss/perplexity = 3.77813721/43.7344971 secs/batch = 0.2990s, grad.norm=15.45313740
 30024: 22 [  830/ 1327], train_loss/perplexity = 3.54357195/34.5902519 secs/batch = 0.2957s, grad.norm=15.69757366
 30029: 22 [  835/ 1327], train_loss/perplexity = 3.79739070/44.5846977 secs/batch = 0.3004s, grad.norm=15.86516571
 30034: 22 [  840/ 1327], train_loss/perplexity = 3.80879235/45.0959473 secs/batch = 0.3009s, grad.norm=15.92453575
 30039: 22 [  845/ 1327], train_loss/perplexity = 3.62146521/37.3923149 secs/batch = 0.2930s, grad.norm=16.09600067
 30044: 22 [  850/ 1327], train_loss/perplexity = 3.73082972/41.7137032 secs/batch = 0.2948s, grad.norm=15.79882145
 30049: 22 [  855/ 1327], train_loss/perplexity = 3.82504177/45.8347130 secs/batch = 0.2956s, grad.norm=16.02181053
 30054: 22 [  860/ 1327], train_loss/perplexity = 3.52679753/34.0148621 secs/batch = 0.2931s, grad.norm=15.46875668
 30059: 22 [  865/ 1327], train_loss/perplexity = 4.00673676/54.9672050 secs/batch = 0.2992s, grad.norm=16.12930298
 30064: 22 [  870/ 1327], train_loss/perplexity = 3.77627754/43.6532402 secs/batch = 0.2956s, grad.norm=16.27836227
 30069: 22 [  875/ 1327], train_loss/perplexity = 3.41654921/30.4641075 secs/batch = 0.2992s, grad.norm=15.07112312
 30074: 22 [  880/ 1327], train_loss/perplexity = 3.66324234/38.9875488 secs/batch = 0.2945s, grad.norm=14.94925117
 30079: 22 [  885/ 1327], train_loss/perplexity = 3.83524227/46.3046455 secs/batch = 0.2976s, grad.norm=15.45584106
 30084: 22 [  890/ 1327], train_loss/perplexity = 3.94906592/51.8868790 secs/batch = 0.2952s, grad.norm=15.75787830
 30089: 22 [  895/ 1327], train_loss/perplexity = 3.89295053/49.0554123 secs/batch = 0.2962s, grad.norm=15.02749062
 30094: 22 [  900/ 1327], train_loss/perplexity = 3.77784395/43.7216759 secs/batch = 0.2980s, grad.norm=15.42126465
 30099: 22 [  905/ 1327], train_loss/perplexity = 3.59250641/36.3250084 secs/batch = 0.2953s, grad.norm=14.71388626
 30104: 22 [  910/ 1327], train_loss/perplexity = 3.67823505/39.5764809 secs/batch = 0.2919s, grad.norm=14.31052780
 30109: 22 [  915/ 1327], train_loss/perplexity = 3.90244341/49.5233078 secs/batch = 0.2955s, grad.norm=15.09525585
 30114: 22 [  920/ 1327], train_loss/perplexity = 4.09675407/60.1447449 secs/batch = 0.2993s, grad.norm=16.14268875
 30119: 22 [  925/ 1327], train_loss/perplexity = 3.90104508/49.4541054 secs/batch = 0.2946s, grad.norm=15.50195789
 30124: 22 [  930/ 1327], train_loss/perplexity = 3.95240831/52.0605927 secs/batch = 0.2929s, grad.norm=15.60960007
 30129: 22 [  935/ 1327], train_loss/perplexity = 3.96910477/52.9371185 secs/batch = 0.2951s, grad.norm=15.17784691
 30134: 22 [  940/ 1327], train_loss/perplexity = 3.88970470/48.8964462 secs/batch = 0.2932s, grad.norm=15.35136604
 30139: 22 [  945/ 1327], train_loss/perplexity = 4.06423521/58.2203636 secs/batch = 0.2937s, grad.norm=15.08338928
 30144: 22 [  950/ 1327], train_loss/perplexity = 3.86050224/47.4891968 secs/batch = 0.2928s, grad.norm=15.53285694
 30149: 22 [  955/ 1327], train_loss/perplexity = 3.82711172/45.9296875 secs/batch = 0.2955s, grad.norm=15.08594894
 30154: 22 [  960/ 1327], train_loss/perplexity = 4.14555883/63.1529045 secs/batch = 0.2959s, grad.norm=15.89254189
 30159: 22 [  965/ 1327], train_loss/perplexity = 3.92890882/50.8514595 secs/batch = 0.2957s, grad.norm=15.73861599
 30164: 22 [  970/ 1327], train_loss/perplexity = 4.07671070/58.9512405 secs/batch = 0.2985s, grad.norm=15.62046146
 30169: 22 [  975/ 1327], train_loss/perplexity = 3.78322029/43.9573708 secs/batch = 0.2947s, grad.norm=16.56918335
 30174: 22 [  980/ 1327], train_loss/perplexity = 3.64011621/38.0962639 secs/batch = 0.2947s, grad.norm=14.96541405
 30179: 22 [  985/ 1327], train_loss/perplexity = 3.84505415/46.7612152 secs/batch = 0.3007s, grad.norm=16.06536293
 30184: 22 [  990/ 1327], train_loss/perplexity = 3.93737841/51.2839775 secs/batch = 0.2950s, grad.norm=16.04559898
 30189: 22 [  995/ 1327], train_loss/perplexity = 3.95529652/52.2111740 secs/batch = 0.2992s, grad.norm=15.71929550
 30194: 22 [ 1000/ 1327], train_loss/perplexity = 3.47607255/32.3324890 secs/batch = 0.3002s, grad.norm=15.12839031
 30199: 22 [ 1005/ 1327], train_loss/perplexity = 3.94572425/51.7137794 secs/batch = 0.2950s, grad.norm=15.46868134
 30204: 22 [ 1010/ 1327], train_loss/perplexity = 3.56508017/35.3422852 secs/batch = 0.2939s, grad.norm=14.66015911
 30209: 22 [ 1015/ 1327], train_loss/perplexity = 4.00880051/55.0807610 secs/batch = 0.2939s, grad.norm=15.40663147
 30214: 22 [ 1020/ 1327], train_loss/perplexity = 4.14205742/62.9321671 secs/batch = 0.2966s, grad.norm=15.71709156
 30219: 22 [ 1025/ 1327], train_loss/perplexity = 4.01061344/55.1807098 secs/batch = 0.2948s, grad.norm=15.34658241
 30224: 22 [ 1030/ 1327], train_loss/perplexity = 3.84491611/46.7547607 secs/batch = 0.2967s, grad.norm=15.19226074
 30229: 22 [ 1035/ 1327], train_loss/perplexity = 3.78607059/44.0828400 secs/batch = 0.2930s, grad.norm=15.13239098
 30234: 22 [ 1040/ 1327], train_loss/perplexity = 3.94610262/51.7333488 secs/batch = 0.2996s, grad.norm=15.94894314
 30239: 22 [ 1045/ 1327], train_loss/perplexity = 3.51430726/33.5926476 secs/batch = 0.2949s, grad.norm=15.03857517
 30244: 22 [ 1050/ 1327], train_loss/perplexity = 3.61620092/37.1959877 secs/batch = 0.2997s, grad.norm=15.23483086
 30249: 22 [ 1055/ 1327], train_loss/perplexity = 3.67592716/39.4852486 secs/batch = 0.3006s, grad.norm=15.83702469
 30254: 22 [ 1060/ 1327], train_loss/perplexity = 3.26268721/26.1196308 secs/batch = 0.2952s, grad.norm=15.74044418
 30259: 22 [ 1065/ 1327], train_loss/perplexity = 3.45962596/31.8050785 secs/batch = 0.2950s, grad.norm=15.94357967
 30264: 22 [ 1070/ 1327], train_loss/perplexity = 3.69932342/40.4199486 secs/batch = 0.2948s, grad.norm=15.79742432
 30269: 22 [ 1075/ 1327], train_loss/perplexity = 3.47450638/32.2818909 secs/batch = 0.2950s, grad.norm=15.24202442
 30274: 22 [ 1080/ 1327], train_loss/perplexity = 3.50316572/33.2204514 secs/batch = 0.3012s, grad.norm=15.51512241
 30279: 22 [ 1085/ 1327], train_loss/perplexity = 3.43174267/30.9304981 secs/batch = 0.2939s, grad.norm=15.66332531
 30284: 22 [ 1090/ 1327], train_loss/perplexity = 3.63101768/37.7512169 secs/batch = 0.2939s, grad.norm=15.91725826
 30289: 22 [ 1095/ 1327], train_loss/perplexity = 3.73823524/42.0237617 secs/batch = 0.2992s, grad.norm=16.01063728
 30294: 22 [ 1100/ 1327], train_loss/perplexity = 3.47018385/32.1426506 secs/batch = 0.2954s, grad.norm=16.36710548
 30299: 22 [ 1105/ 1327], train_loss/perplexity = 3.43523645/31.0387516 secs/batch = 0.2947s, grad.norm=15.40553570
 30304: 22 [ 1110/ 1327], train_loss/perplexity = 3.72628045/41.5243683 secs/batch = 0.2935s, grad.norm=16.26419830
 30309: 22 [ 1115/ 1327], train_loss/perplexity = 3.60598063/36.8177719 secs/batch = 0.2949s, grad.norm=15.08367729
 30314: 22 [ 1120/ 1327], train_loss/perplexity = 3.84341860/46.6847992 secs/batch = 0.2980s, grad.norm=15.51785946
 30319: 22 [ 1125/ 1327], train_loss/perplexity = 4.03402519/56.4878273 secs/batch = 0.2952s, grad.norm=16.40277672
 30324: 22 [ 1130/ 1327], train_loss/perplexity = 3.59820771/36.5326996 secs/batch = 0.2951s, grad.norm=15.76627159
 30329: 22 [ 1135/ 1327], train_loss/perplexity = 3.62556982/37.5461121 secs/batch = 0.2943s, grad.norm=15.48721695
 30334: 22 [ 1140/ 1327], train_loss/perplexity = 3.88867426/48.8460846 secs/batch = 0.2949s, grad.norm=15.95681858
 30339: 22 [ 1145/ 1327], train_loss/perplexity = 3.75132179/42.5773239 secs/batch = 0.2955s, grad.norm=15.23739624
 30344: 22 [ 1150/ 1327], train_loss/perplexity = 3.68472910/39.8343315 secs/batch = 0.2942s, grad.norm=15.30760670
 30349: 22 [ 1155/ 1327], train_loss/perplexity = 3.77977586/43.8062210 secs/batch = 0.2949s, grad.norm=16.33164597
 30354: 22 [ 1160/ 1327], train_loss/perplexity = 3.76016426/42.9554825 secs/batch = 0.2944s, grad.norm=15.90559959
 30359: 22 [ 1165/ 1327], train_loss/perplexity = 3.83164048/46.1381645 secs/batch = 0.2962s, grad.norm=15.73316383
 30364: 22 [ 1170/ 1327], train_loss/perplexity = 3.67014027/39.2574120 secs/batch = 0.2935s, grad.norm=15.41020489
 30369: 22 [ 1175/ 1327], train_loss/perplexity = 3.42071843/30.5913849 secs/batch = 0.2934s, grad.norm=14.88516521
 30374: 22 [ 1180/ 1327], train_loss/perplexity = 3.48761749/32.7079277 secs/batch = 0.2970s, grad.norm=15.75082207
 30379: 22 [ 1185/ 1327], train_loss/perplexity = 3.63702536/37.9786949 secs/batch = 0.2942s, grad.norm=15.32666874
 30384: 22 [ 1190/ 1327], train_loss/perplexity = 3.68219280/39.7334251 secs/batch = 0.2999s, grad.norm=16.03350830
 30389: 22 [ 1195/ 1327], train_loss/perplexity = 3.52879763/34.0829620 secs/batch = 0.2954s, grad.norm=15.08598614
 30394: 22 [ 1200/ 1327], train_loss/perplexity = 3.48654175/32.6727600 secs/batch = 0.2951s, grad.norm=15.60108852
 30399: 22 [ 1205/ 1327], train_loss/perplexity = 3.52964067/34.1117096 secs/batch = 0.2950s, grad.norm=15.80335522
 30404: 22 [ 1210/ 1327], train_loss/perplexity = 3.11293626/22.4869747 secs/batch = 0.2958s, grad.norm=15.31336498
 30409: 22 [ 1215/ 1327], train_loss/perplexity = 3.40959096/30.2528667 secs/batch = 0.2954s, grad.norm=14.72808933
 30414: 22 [ 1220/ 1327], train_loss/perplexity = 3.50395060/33.2465363 secs/batch = 0.2976s, grad.norm=15.42742252
 30419: 22 [ 1225/ 1327], train_loss/perplexity = 3.19981027/24.5278759 secs/batch = 0.2948s, grad.norm=16.37088203
 30424: 22 [ 1230/ 1327], train_loss/perplexity = 3.54383850/34.5994759 secs/batch = 0.2974s, grad.norm=15.45076656
 30429: 22 [ 1235/ 1327], train_loss/perplexity = 3.47266865/32.2226181 secs/batch = 0.3005s, grad.norm=15.15994167
 30434: 22 [ 1240/ 1327], train_loss/perplexity = 3.74525785/42.3199196 secs/batch = 0.2959s, grad.norm=15.96298981
 30439: 22 [ 1245/ 1327], train_loss/perplexity = 3.70585752/40.6849213 secs/batch = 0.2955s, grad.norm=15.38836575
 30444: 22 [ 1250/ 1327], train_loss/perplexity = 3.87085605/47.9834442 secs/batch = 0.2959s, grad.norm=14.65045738
 30449: 22 [ 1255/ 1327], train_loss/perplexity = 3.85066462/47.0243073 secs/batch = 0.2951s, grad.norm=15.20421124
 30454: 22 [ 1260/ 1327], train_loss/perplexity = 3.55608010/35.0256310 secs/batch = 0.2941s, grad.norm=16.18286705
 30459: 22 [ 1265/ 1327], train_loss/perplexity = 3.79501414/44.4788666 secs/batch = 0.2947s, grad.norm=15.60350609
 30464: 22 [ 1270/ 1327], train_loss/perplexity = 3.46450067/31.9604969 secs/batch = 0.2946s, grad.norm=15.82238007
 30469: 22 [ 1275/ 1327], train_loss/perplexity = 3.71673846/41.1300278 secs/batch = 0.2944s, grad.norm=16.00812721
 30474: 22 [ 1280/ 1327], train_loss/perplexity = 3.59215546/36.3122597 secs/batch = 0.2943s, grad.norm=15.74753857
 30479: 22 [ 1285/ 1327], train_loss/perplexity = 3.50487661/33.2773361 secs/batch = 0.3011s, grad.norm=15.75697708
 30484: 22 [ 1290/ 1327], train_loss/perplexity = 3.67247510/39.3491783 secs/batch = 0.2941s, grad.norm=15.21717739
 30489: 22 [ 1295/ 1327], train_loss/perplexity = 3.69687867/40.3212509 secs/batch = 0.2956s, grad.norm=15.22619152
 30494: 22 [ 1300/ 1327], train_loss/perplexity = 3.89659357/49.2344513 secs/batch = 0.2954s, grad.norm=15.01407242
 30499: 22 [ 1305/ 1327], train_loss/perplexity = 3.89654970/49.2322884 secs/batch = 0.2953s, grad.norm=15.77991486
 30504: 22 [ 1310/ 1327], train_loss/perplexity = 4.19068909/66.0682983 secs/batch = 0.2949s, grad.norm=16.37472153
 30509: 22 [ 1315/ 1327], train_loss/perplexity = 3.96325016/52.6280975 secs/batch = 0.3027s, grad.norm=15.97375774
 30514: 22 [ 1320/ 1327], train_loss/perplexity = 3.95089102/51.9816628 secs/batch = 0.2962s, grad.norm=16.11268425
 30519: 22 [ 1325/ 1327], train_loss/perplexity = 3.85455990/47.2078362 secs/batch = 0.2939s, grad.norm=16.10569572
Epoch training time: 392.85507583618164
	> validation loss = 4.56393814, perplexity = 95.96063995
	> validation loss = 4.50854397, perplexity = 90.78952789
	> validation loss = 4.50164318, perplexity = 90.16516876
	> validation loss = 4.47883844, perplexity = 88.13224030
	> validation loss = 4.63672876, perplexity = 103.20618439
	> validation loss = 4.59298420, perplexity = 98.78879547
	> validation loss = 4.53079605, perplexity = 92.83242798
	> validation loss = 4.36896896, perplexity = 78.96217346
	> validation loss = 4.15279293, perplexity = 63.61141586
	> validation loss = 4.29331017, perplexity = 73.20839691
	> validation loss = 4.50219059, perplexity = 90.21453857
	> validation loss = 4.45487404, perplexity = 86.04531097
	> validation loss = 4.40041399, perplexity = 81.48459625
	> validation loss = 4.13185596, perplexity = 62.29343033
	> validation loss = 4.13178921, perplexity = 62.28927231
	> validation loss = 4.13244438, perplexity = 62.33009720
	> validation loss = 4.57946730, perplexity = 97.46246338
	> validation loss = 4.03391695, perplexity = 56.48171616
	> validation loss = 4.54569340, perplexity = 94.22573853
	> validation loss = 4.51104212, perplexity = 91.01661682
	> validation loss = 4.21537209, perplexity = 67.71936035
at the end of epoch: 22
train loss = 3.82359343, perplexity = 45.76837885
validation loss = 4.39982271, perplexity = 81.43642931
Saved model cv/epoch022_4.3998.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.0078125
new learning rate is: 0.00390625
 30526: 23 [    5/ 1327], train_loss/perplexity = 4.12065220/61.5994034 secs/batch = 0.2950s, grad.norm=16.11230850
 30531: 23 [   10/ 1327], train_loss/perplexity = 3.68903065/40.0060463 secs/batch = 0.2981s, grad.norm=15.15763664
 30536: 23 [   15/ 1327], train_loss/perplexity = 4.04300833/56.9975510 secs/batch = 0.2946s, grad.norm=15.00278473
 30541: 23 [   20/ 1327], train_loss/perplexity = 4.14832211/63.3276558 secs/batch = 0.2928s, grad.norm=14.83535862
 30546: 23 [   25/ 1327], train_loss/perplexity = 3.95792246/52.3484573 secs/batch = 0.2931s, grad.norm=16.04609299
 30551: 23 [   30/ 1327], train_loss/perplexity = 4.03040886/56.2839203 secs/batch = 0.2924s, grad.norm=16.27279663
 30556: 23 [   35/ 1327], train_loss/perplexity = 3.83117962/46.1169052 secs/batch = 0.2935s, grad.norm=15.36504555
 30561: 23 [   40/ 1327], train_loss/perplexity = 3.82379436/45.7775764 secs/batch = 0.2945s, grad.norm=16.00520706
 30566: 23 [   45/ 1327], train_loss/perplexity = 3.72311211/41.3930130 secs/batch = 0.2998s, grad.norm=15.02055836
 30571: 23 [   50/ 1327], train_loss/perplexity = 3.92512918/50.6596222 secs/batch = 0.2994s, grad.norm=15.91568661
 30576: 23 [   55/ 1327], train_loss/perplexity = 3.80093074/44.7428093 secs/batch = 0.2986s, grad.norm=16.41905212
 30581: 23 [   60/ 1327], train_loss/perplexity = 3.99180317/54.1524467 secs/batch = 0.2952s, grad.norm=16.52396774
 30586: 23 [   65/ 1327], train_loss/perplexity = 3.62038708/37.3520241 secs/batch = 0.2948s, grad.norm=15.19580269
 30591: 23 [   70/ 1327], train_loss/perplexity = 3.52109146/33.8213234 secs/batch = 0.2941s, grad.norm=15.54632187
 30596: 23 [   75/ 1327], train_loss/perplexity = 3.35386705/28.6131687 secs/batch = 0.2981s, grad.norm=14.63350964
 30601: 23 [   80/ 1327], train_loss/perplexity = 3.78646421/44.1001968 secs/batch = 0.2925s, grad.norm=16.01728630
 30606: 23 [   85/ 1327], train_loss/perplexity = 3.78967595/44.2420616 secs/batch = 0.2997s, grad.norm=15.87257195
 30611: 23 [   90/ 1327], train_loss/perplexity = 3.88413024/48.6246338 secs/batch = 0.3008s, grad.norm=16.35712433
 30616: 23 [   95/ 1327], train_loss/perplexity = 3.72979450/41.6705437 secs/batch = 0.2954s, grad.norm=16.06146812
 30621: 23 [  100/ 1327], train_loss/perplexity = 3.94293714/51.5698471 secs/batch = 0.2941s, grad.norm=16.26488113
 30626: 23 [  105/ 1327], train_loss/perplexity = 3.70606828/40.6934967 secs/batch = 0.2934s, grad.norm=16.33733177
 30631: 23 [  110/ 1327], train_loss/perplexity = 3.71267962/40.9634247 secs/batch = 0.2993s, grad.norm=16.02569199
 30636: 23 [  115/ 1327], train_loss/perplexity = 3.72364664/41.4151459 secs/batch = 0.2955s, grad.norm=16.48086166
 30641: 23 [  120/ 1327], train_loss/perplexity = 3.79676342/44.5567398 secs/batch = 0.2959s, grad.norm=16.19568443
 30646: 23 [  125/ 1327], train_loss/perplexity = 3.76356339/43.1017418 secs/batch = 0.3022s, grad.norm=16.29294968
 30651: 23 [  130/ 1327], train_loss/perplexity = 3.75311661/42.6538086 secs/batch = 0.2945s, grad.norm=16.72930527
 30656: 23 [  135/ 1327], train_loss/perplexity = 3.76048422/42.9692268 secs/batch = 0.2973s, grad.norm=15.39802456
 30661: 23 [  140/ 1327], train_loss/perplexity = 4.10435534/60.6036644 secs/batch = 0.2952s, grad.norm=16.76549149
 30666: 23 [  145/ 1327], train_loss/perplexity = 3.90289164/49.5455093 secs/batch = 0.2951s, grad.norm=16.94009399
 30671: 23 [  150/ 1327], train_loss/perplexity = 3.96619415/52.7832642 secs/batch = 0.2957s, grad.norm=16.75828362
 30676: 23 [  155/ 1327], train_loss/perplexity = 4.13638592/62.5762558 secs/batch = 0.2933s, grad.norm=16.20564079
 30681: 23 [  160/ 1327], train_loss/perplexity = 3.92020059/50.4105568 secs/batch = 0.3013s, grad.norm=15.33580685
 30686: 23 [  165/ 1327], train_loss/perplexity = 3.99748921/54.4612389 secs/batch = 0.3012s, grad.norm=16.03915024
 30691: 23 [  170/ 1327], train_loss/perplexity = 3.76686978/43.2444878 secs/batch = 0.3024s, grad.norm=15.47572517
 30696: 23 [  175/ 1327], train_loss/perplexity = 4.08161354/59.2409821 secs/batch = 0.2958s, grad.norm=16.11744881
 30701: 23 [  180/ 1327], train_loss/perplexity = 3.89597225/49.2038689 secs/batch = 0.2948s, grad.norm=15.88762283
 30706: 23 [  185/ 1327], train_loss/perplexity = 4.27762747/72.0692520 secs/batch = 0.2999s, grad.norm=16.33535576
 30711: 23 [  190/ 1327], train_loss/perplexity = 3.78984880/44.2497101 secs/batch = 0.2983s, grad.norm=15.32863998
 30716: 23 [  195/ 1327], train_loss/perplexity = 4.04236221/56.9607391 secs/batch = 0.3022s, grad.norm=15.36657906
 30721: 23 [  200/ 1327], train_loss/perplexity = 3.90449595/49.6250610 secs/batch = 0.2952s, grad.norm=16.41970444
 30726: 23 [  205/ 1327], train_loss/perplexity = 4.16663933/64.4983292 secs/batch = 0.3028s, grad.norm=16.48900414
 30731: 23 [  210/ 1327], train_loss/perplexity = 3.96316957/52.6238556 secs/batch = 0.2990s, grad.norm=15.57868385
 30736: 23 [  215/ 1327], train_loss/perplexity = 4.10075712/60.3859901 secs/batch = 0.2993s, grad.norm=15.25948143
 30741: 23 [  220/ 1327], train_loss/perplexity = 4.02380466/55.9134331 secs/batch = 0.3027s, grad.norm=15.90900993
 30746: 23 [  225/ 1327], train_loss/perplexity = 4.22501755/68.3757019 secs/batch = 0.2955s, grad.norm=16.24103546
 30751: 23 [  230/ 1327], train_loss/perplexity = 4.07432270/58.8106346 secs/batch = 0.2944s, grad.norm=16.96976471
 30756: 23 [  235/ 1327], train_loss/perplexity = 3.91383410/50.0906372 secs/batch = 0.2993s, grad.norm=15.81249237
 30761: 23 [  240/ 1327], train_loss/perplexity = 3.64016843/38.0982513 secs/batch = 0.2954s, grad.norm=16.23289108
 30766: 23 [  245/ 1327], train_loss/perplexity = 4.00593472/54.9231377 secs/batch = 0.2991s, grad.norm=16.41654396
 30771: 23 [  250/ 1327], train_loss/perplexity = 3.85473824/47.2162552 secs/batch = 0.3009s, grad.norm=15.42527008
 30776: 23 [  255/ 1327], train_loss/perplexity = 3.77187252/43.4613724 secs/batch = 0.2952s, grad.norm=15.81918144
 30781: 23 [  260/ 1327], train_loss/perplexity = 4.02757263/56.1245117 secs/batch = 0.2940s, grad.norm=16.64094353
 30786: 23 [  265/ 1327], train_loss/perplexity = 4.19731665/66.5076294 secs/batch = 0.2948s, grad.norm=15.27138710
 30791: 23 [  270/ 1327], train_loss/perplexity = 4.25172091/70.2261581 secs/batch = 0.3021s, grad.norm=16.15327072
 30796: 23 [  275/ 1327], train_loss/perplexity = 4.14078999/62.8524551 secs/batch = 0.2954s, grad.norm=15.89536190
 30801: 23 [  280/ 1327], train_loss/perplexity = 3.93821907/51.3271103 secs/batch = 0.2934s, grad.norm=15.73195267
 30806: 23 [  285/ 1327], train_loss/perplexity = 4.32428789/75.5117188 secs/batch = 0.2971s, grad.norm=16.07553482
 30811: 23 [  290/ 1327], train_loss/perplexity = 4.00491285/54.8670425 secs/batch = 0.2943s, grad.norm=16.17144775
 30816: 23 [  295/ 1327], train_loss/perplexity = 3.76413321/43.1263084 secs/batch = 0.2985s, grad.norm=15.84009266
 30821: 23 [  300/ 1327], train_loss/perplexity = 3.31229711/27.4481049 secs/batch = 0.2925s, grad.norm=14.63123512
 30826: 23 [  305/ 1327], train_loss/perplexity = 3.84447098/46.7339554 secs/batch = 0.2996s, grad.norm=15.81079102
 30831: 23 [  310/ 1327], train_loss/perplexity = 3.83160591/46.1365700 secs/batch = 0.2936s, grad.norm=15.86718655
 30836: 23 [  315/ 1327], train_loss/perplexity = 3.34697247/28.4165707 secs/batch = 0.2941s, grad.norm=14.40620995
 30841: 23 [  320/ 1327], train_loss/perplexity = 3.35338140/28.5992756 secs/batch = 0.2978s, grad.norm=16.09380913
 30846: 23 [  325/ 1327], train_loss/perplexity = 3.31617284/27.5546913 secs/batch = 0.3005s, grad.norm=14.75685310
 30851: 23 [  330/ 1327], train_loss/perplexity = 3.95102215/51.9884796 secs/batch = 0.2974s, grad.norm=15.79598522
 30856: 23 [  335/ 1327], train_loss/perplexity = 3.35854101/28.7472191 secs/batch = 0.2934s, grad.norm=14.39494133
 30861: 23 [  340/ 1327], train_loss/perplexity = 4.04670572/57.2086868 secs/batch = 0.2984s, grad.norm=15.41351414
 30866: 23 [  345/ 1327], train_loss/perplexity = 3.93392015/51.1069336 secs/batch = 0.2946s, grad.norm=15.56789303
 30871: 23 [  350/ 1327], train_loss/perplexity = 3.83985710/46.5188255 secs/batch = 0.2996s, grad.norm=16.11504555
 30876: 23 [  355/ 1327], train_loss/perplexity = 3.81237316/45.2577171 secs/batch = 0.2983s, grad.norm=15.86493206
 30881: 23 [  360/ 1327], train_loss/perplexity = 3.84805250/46.9016342 secs/batch = 0.2924s, grad.norm=16.66883659
 30886: 23 [  365/ 1327], train_loss/perplexity = 4.00582361/54.9170380 secs/batch = 0.2953s, grad.norm=15.82551575
 30891: 23 [  370/ 1327], train_loss/perplexity = 4.05798006/57.8573265 secs/batch = 0.2942s, grad.norm=15.86616039
 30896: 23 [  375/ 1327], train_loss/perplexity = 3.47764516/32.3833733 secs/batch = 0.3006s, grad.norm=15.56136322
 30901: 23 [  380/ 1327], train_loss/perplexity = 3.53737783/34.3766594 secs/batch = 0.2928s, grad.norm=16.07294655
 30906: 23 [  385/ 1327], train_loss/perplexity = 3.75630426/42.7899933 secs/batch = 0.2963s, grad.norm=16.05022430
 30911: 23 [  390/ 1327], train_loss/perplexity = 3.87001276/47.9429970 secs/batch = 0.2955s, grad.norm=15.56427574
 30916: 23 [  395/ 1327], train_loss/perplexity = 3.91549778/50.1740417 secs/batch = 0.2946s, grad.norm=15.73618031
 30921: 23 [  400/ 1327], train_loss/perplexity = 3.89781213/49.2944794 secs/batch = 0.2941s, grad.norm=15.66982937
 30926: 23 [  405/ 1327], train_loss/perplexity = 4.19713497/66.4955444 secs/batch = 0.2964s, grad.norm=15.73535252
 30931: 23 [  410/ 1327], train_loss/perplexity = 3.79050970/44.2789650 secs/batch = 0.3011s, grad.norm=15.62365913
 30936: 23 [  415/ 1327], train_loss/perplexity = 3.74883604/42.4716187 secs/batch = 0.2943s, grad.norm=15.60187912
 30941: 23 [  420/ 1327], train_loss/perplexity = 3.37244940/29.1498394 secs/batch = 0.2944s, grad.norm=15.35173798
 30946: 23 [  425/ 1327], train_loss/perplexity = 3.71583533/41.0928993 secs/batch = 0.2994s, grad.norm=16.11115646
 30951: 23 [  430/ 1327], train_loss/perplexity = 3.90140510/49.4719124 secs/batch = 0.2939s, grad.norm=15.96685505
 30956: 23 [  435/ 1327], train_loss/perplexity = 3.95717478/52.3093300 secs/batch = 0.2941s, grad.norm=16.50187492
 30961: 23 [  440/ 1327], train_loss/perplexity = 3.53114796/34.1631622 secs/batch = 0.2945s, grad.norm=15.68879795
 30966: 23 [  445/ 1327], train_loss/perplexity = 3.88954949/48.8888550 secs/batch = 0.2929s, grad.norm=16.28904724
 30971: 23 [  450/ 1327], train_loss/perplexity = 3.86338592/47.6263390 secs/batch = 0.2963s, grad.norm=16.10464287
 30976: 23 [  455/ 1327], train_loss/perplexity = 3.73612022/41.9349747 secs/batch = 0.2967s, grad.norm=15.39760876
 30981: 23 [  460/ 1327], train_loss/perplexity = 3.85314131/47.1409149 secs/batch = 0.2953s, grad.norm=16.63896561
 30986: 23 [  465/ 1327], train_loss/perplexity = 3.48757315/32.7064781 secs/batch = 0.2937s, grad.norm=16.18769264
 30991: 23 [  470/ 1327], train_loss/perplexity = 4.20491791/67.0150986 secs/batch = 0.2963s, grad.norm=15.80555248
 30996: 23 [  475/ 1327], train_loss/perplexity = 3.64359617/38.2290688 secs/batch = 0.2944s, grad.norm=15.98684788
 31001: 23 [  480/ 1327], train_loss/perplexity = 3.77305126/43.5126305 secs/batch = 0.2988s, grad.norm=16.00785828
 31006: 23 [  485/ 1327], train_loss/perplexity = 3.75690079/42.8155251 secs/batch = 0.2944s, grad.norm=15.97752094
 31011: 23 [  490/ 1327], train_loss/perplexity = 3.65437317/38.6432915 secs/batch = 0.2993s, grad.norm=17.02119064
 31016: 23 [  495/ 1327], train_loss/perplexity = 3.76314306/43.0836296 secs/batch = 0.2971s, grad.norm=15.49245167
 31021: 23 [  500/ 1327], train_loss/perplexity = 3.82487106/45.8268929 secs/batch = 0.3005s, grad.norm=15.94581604
 31026: 23 [  505/ 1327], train_loss/perplexity = 4.02186584/55.8051338 secs/batch = 0.2954s, grad.norm=15.36020756
 31031: 23 [  510/ 1327], train_loss/perplexity = 4.34030771/76.7311478 secs/batch = 0.2993s, grad.norm=15.85720539
 31036: 23 [  515/ 1327], train_loss/perplexity = 4.01841879/55.6131020 secs/batch = 0.2987s, grad.norm=15.91002274
 31041: 23 [  520/ 1327], train_loss/perplexity = 4.03308201/56.4345741 secs/batch = 0.2955s, grad.norm=15.65139675
 31046: 23 [  525/ 1327], train_loss/perplexity = 3.76174664/43.0235062 secs/batch = 0.2944s, grad.norm=15.54036999
 31051: 23 [  530/ 1327], train_loss/perplexity = 3.77005577/43.3824844 secs/batch = 0.2940s, grad.norm=16.18488693
 31056: 23 [  535/ 1327], train_loss/perplexity = 3.86788654/47.8411674 secs/batch = 0.2960s, grad.norm=16.03455925
 31061: 23 [  540/ 1327], train_loss/perplexity = 3.95834446/52.3705521 secs/batch = 0.3011s, grad.norm=15.70107174
 31066: 23 [  545/ 1327], train_loss/perplexity = 3.91176319/49.9870110 secs/batch = 0.3001s, grad.norm=15.70815659
 31071: 23 [  550/ 1327], train_loss/perplexity = 3.90099978/49.4518661 secs/batch = 0.2928s, grad.norm=15.94576073
 31076: 23 [  555/ 1327], train_loss/perplexity = 3.78671813/44.1113930 secs/batch = 0.2956s, grad.norm=15.01479912
 31081: 23 [  560/ 1327], train_loss/perplexity = 3.86547899/47.7261276 secs/batch = 0.2946s, grad.norm=16.57379532
 31086: 23 [  565/ 1327], train_loss/perplexity = 3.69383264/40.1986198 secs/batch = 0.2947s, grad.norm=16.39868736
 31091: 23 [  570/ 1327], train_loss/perplexity = 3.64385271/38.2388763 secs/batch = 0.2937s, grad.norm=16.10053444
 31096: 23 [  575/ 1327], train_loss/perplexity = 3.52032638/33.7954559 secs/batch = 0.2951s, grad.norm=16.08761787
 31101: 23 [  580/ 1327], train_loss/perplexity = 3.93516755/51.1707230 secs/batch = 0.3007s, grad.norm=16.59475327
 31106: 23 [  585/ 1327], train_loss/perplexity = 3.54532385/34.6509056 secs/batch = 0.2954s, grad.norm=15.32807159
 31111: 23 [  590/ 1327], train_loss/perplexity = 3.87196422/48.0366478 secs/batch = 0.2943s, grad.norm=15.62967491
 31116: 23 [  595/ 1327], train_loss/perplexity = 3.88706589/48.7675858 secs/batch = 0.2979s, grad.norm=15.99301434
 31121: 23 [  600/ 1327], train_loss/perplexity = 4.10785723/60.8162613 secs/batch = 0.2945s, grad.norm=15.33552551
 31126: 23 [  605/ 1327], train_loss/perplexity = 3.98468876/53.7685509 secs/batch = 0.2931s, grad.norm=15.34139633
 31131: 23 [  610/ 1327], train_loss/perplexity = 4.17006350/64.7195587 secs/batch = 0.3021s, grad.norm=16.00238228
 31136: 23 [  615/ 1327], train_loss/perplexity = 3.76881671/43.3287659 secs/batch = 0.3001s, grad.norm=14.99004459
 31141: 23 [  620/ 1327], train_loss/perplexity = 4.12135363/61.6426277 secs/batch = 0.3013s, grad.norm=15.82312298
 31146: 23 [  625/ 1327], train_loss/perplexity = 4.00129938/54.6691399 secs/batch = 0.2951s, grad.norm=15.81031227
 31151: 23 [  630/ 1327], train_loss/perplexity = 4.09672260/60.1428528 secs/batch = 0.2950s, grad.norm=15.65186214
 31156: 23 [  635/ 1327], train_loss/perplexity = 3.80251074/44.8135567 secs/batch = 0.2950s, grad.norm=15.56988430
 31161: 23 [  640/ 1327], train_loss/perplexity = 3.77789998/43.7241249 secs/batch = 0.2947s, grad.norm=15.58135223
 31166: 23 [  645/ 1327], train_loss/perplexity = 4.11181450/61.0574074 secs/batch = 0.2960s, grad.norm=16.20466614
 31171: 23 [  650/ 1327], train_loss/perplexity = 3.57638168/35.7439728 secs/batch = 0.3023s, grad.norm=15.41985512
 31176: 23 [  655/ 1327], train_loss/perplexity = 3.82516456/45.8403435 secs/batch = 0.2958s, grad.norm=16.26427841
 31181: 23 [  660/ 1327], train_loss/perplexity = 3.73749352/41.9926033 secs/batch = 0.2960s, grad.norm=16.11190987
 31186: 23 [  665/ 1327], train_loss/perplexity = 3.90802097/49.8002968 secs/batch = 0.3011s, grad.norm=15.77747154
 31191: 23 [  670/ 1327], train_loss/perplexity = 3.77718830/43.6930161 secs/batch = 0.3020s, grad.norm=15.71793270
 31196: 23 [  675/ 1327], train_loss/perplexity = 3.58184958/35.9399529 secs/batch = 0.2944s, grad.norm=15.98105049
 31201: 23 [  680/ 1327], train_loss/perplexity = 3.81582284/45.4141083 secs/batch = 0.2981s, grad.norm=16.43998146
 31206: 23 [  685/ 1327], train_loss/perplexity = 3.59096146/36.2689285 secs/batch = 0.2944s, grad.norm=15.50456238
 31211: 23 [  690/ 1327], train_loss/perplexity = 3.95065880/51.9695930 secs/batch = 0.3012s, grad.norm=15.52462864
 31216: 23 [  695/ 1327], train_loss/perplexity = 3.86339426/47.6267357 secs/batch = 0.2950s, grad.norm=15.60366344
 31221: 23 [  700/ 1327], train_loss/perplexity = 4.00635767/54.9463730 secs/batch = 0.3005s, grad.norm=15.88757324
 31226: 23 [  705/ 1327], train_loss/perplexity = 3.77423716/43.5642624 secs/batch = 0.2945s, grad.norm=15.23601246
 31231: 23 [  710/ 1327], train_loss/perplexity = 3.68547440/39.8640289 secs/batch = 0.2985s, grad.norm=15.63395119
 31236: 23 [  715/ 1327], train_loss/perplexity = 3.59229064/36.3171692 secs/batch = 0.2934s, grad.norm=15.72511768
 31241: 23 [  720/ 1327], train_loss/perplexity = 3.65259361/38.5745850 secs/batch = 0.3002s, grad.norm=16.11803818
 31246: 23 [  725/ 1327], train_loss/perplexity = 3.64126015/38.1398697 secs/batch = 0.2943s, grad.norm=15.70784950
 31251: 23 [  730/ 1327], train_loss/perplexity = 3.79160690/44.3275719 secs/batch = 0.2923s, grad.norm=16.28664589
 31256: 23 [  735/ 1327], train_loss/perplexity = 3.93351293/51.0861244 secs/batch = 0.2996s, grad.norm=16.74182510
 31261: 23 [  740/ 1327], train_loss/perplexity = 3.37120104/29.1134720 secs/batch = 0.3008s, grad.norm=15.02553177
 31266: 23 [  745/ 1327], train_loss/perplexity = 3.83134031/46.1243172 secs/batch = 0.2940s, grad.norm=16.28648567
 31271: 23 [  750/ 1327], train_loss/perplexity = 3.74612093/42.3564606 secs/batch = 0.2964s, grad.norm=16.02307892
 31276: 23 [  755/ 1327], train_loss/perplexity = 3.54357266/34.5902786 secs/batch = 0.2928s, grad.norm=15.16796684
 31281: 23 [  760/ 1327], train_loss/perplexity = 3.47542119/32.3114357 secs/batch = 0.3006s, grad.norm=15.08332253
 31286: 23 [  765/ 1327], train_loss/perplexity = 3.58008003/35.8764114 secs/batch = 0.3006s, grad.norm=14.85133076
 31291: 23 [  770/ 1327], train_loss/perplexity = 3.57484961/35.6892548 secs/batch = 0.3015s, grad.norm=15.31282139
 31296: 23 [  775/ 1327], train_loss/perplexity = 3.57690287/35.7626076 secs/batch = 0.3008s, grad.norm=16.22560692
 31301: 23 [  780/ 1327], train_loss/perplexity = 3.91294909/50.0463257 secs/batch = 0.2945s, grad.norm=15.97179604
 31306: 23 [  785/ 1327], train_loss/perplexity = 3.88394356/48.6155548 secs/batch = 0.2947s, grad.norm=15.65053463
 31311: 23 [  790/ 1327], train_loss/perplexity = 3.61894488/37.2981949 secs/batch = 0.2933s, grad.norm=15.97180843
 31316: 23 [  795/ 1327], train_loss/perplexity = 3.97631931/53.3204155 secs/batch = 0.2966s, grad.norm=16.04884911
 31321: 23 [  800/ 1327], train_loss/perplexity = 3.80169749/44.7771301 secs/batch = 0.2950s, grad.norm=16.10478401
 31326: 23 [  805/ 1327], train_loss/perplexity = 4.09704161/60.1620407 secs/batch = 0.2962s, grad.norm=15.88018036
 31331: 23 [  810/ 1327], train_loss/perplexity = 3.80761385/45.0428314 secs/batch = 0.3009s, grad.norm=15.35006905
 31336: 23 [  815/ 1327], train_loss/perplexity = 3.67662954/39.5129929 secs/batch = 0.3013s, grad.norm=15.12362766
 31341: 23 [  820/ 1327], train_loss/perplexity = 3.54333401/34.5820236 secs/batch = 0.2942s, grad.norm=14.92181683
 31346: 23 [  825/ 1327], train_loss/perplexity = 3.77664423/43.6692505 secs/batch = 0.2966s, grad.norm=15.27212524
 31351: 23 [  830/ 1327], train_loss/perplexity = 3.47855926/32.4129906 secs/batch = 0.2967s, grad.norm=15.67795753
 31356: 23 [  835/ 1327], train_loss/perplexity = 3.72671270/41.5423203 secs/batch = 0.2947s, grad.norm=15.67570210
 31361: 23 [  840/ 1327], train_loss/perplexity = 3.85230112/47.1013260 secs/batch = 0.2946s, grad.norm=15.78496933
 31366: 23 [  845/ 1327], train_loss/perplexity = 3.64288998/38.2020798 secs/batch = 0.2973s, grad.norm=15.77667522
 31371: 23 [  850/ 1327], train_loss/perplexity = 3.72671771/41.5425301 secs/batch = 0.2953s, grad.norm=15.25203991
 31376: 23 [  855/ 1327], train_loss/perplexity = 3.73993778/42.0953712 secs/batch = 0.2996s, grad.norm=15.70908356
 31381: 23 [  860/ 1327], train_loss/perplexity = 3.53095722/34.1566467 secs/batch = 0.3000s, grad.norm=15.58690071
 31386: 23 [  865/ 1327], train_loss/perplexity = 3.96617031/52.7820053 secs/batch = 0.2933s, grad.norm=16.03339767
 31391: 23 [  870/ 1327], train_loss/perplexity = 3.86586165/47.7443924 secs/batch = 0.2935s, grad.norm=16.26089287
 31396: 23 [  875/ 1327], train_loss/perplexity = 3.37873268/29.3335724 secs/batch = 0.2961s, grad.norm=14.90823650
 31401: 23 [  880/ 1327], train_loss/perplexity = 3.68464327/39.8309097 secs/batch = 0.2940s, grad.norm=15.44562244
 31406: 23 [  885/ 1327], train_loss/perplexity = 3.86499548/47.7030563 secs/batch = 0.2992s, grad.norm=15.21938229
 31411: 23 [  890/ 1327], train_loss/perplexity = 3.93296742/51.0582657 secs/batch = 0.2943s, grad.norm=15.51901722
 31416: 23 [  895/ 1327], train_loss/perplexity = 3.91849351/50.3245735 secs/batch = 0.2942s, grad.norm=15.04839897
 31421: 23 [  900/ 1327], train_loss/perplexity = 3.78104544/43.8618736 secs/batch = 0.2953s, grad.norm=15.40914822
 31426: 23 [  905/ 1327], train_loss/perplexity = 3.63132286/37.7627373 secs/batch = 0.3005s, grad.norm=14.75752735
 31431: 23 [  910/ 1327], train_loss/perplexity = 3.71631384/41.1125679 secs/batch = 0.2999s, grad.norm=14.37809086
 31436: 23 [  915/ 1327], train_loss/perplexity = 3.90676689/49.7378845 secs/batch = 0.2938s, grad.norm=15.28666496
 31441: 23 [  920/ 1327], train_loss/perplexity = 4.02276897/55.8555527 secs/batch = 0.2933s, grad.norm=16.26469421
 31446: 23 [  925/ 1327], train_loss/perplexity = 3.83203936/46.1565704 secs/batch = 0.2937s, grad.norm=15.02653980
 31451: 23 [  930/ 1327], train_loss/perplexity = 3.94880581/51.8733826 secs/batch = 0.3009s, grad.norm=15.53505611
 31456: 23 [  935/ 1327], train_loss/perplexity = 3.95184064/52.0310478 secs/batch = 0.2937s, grad.norm=15.39181232
 31461: 23 [  940/ 1327], train_loss/perplexity = 3.86824894/47.8585091 secs/batch = 0.2962s, grad.norm=14.86267185
 31466: 23 [  945/ 1327], train_loss/perplexity = 4.10381842/60.5711327 secs/batch = 0.2948s, grad.norm=15.37365246
 31471: 23 [  950/ 1327], train_loss/perplexity = 3.84947538/46.9684181 secs/batch = 0.2938s, grad.norm=15.72438526
 31476: 23 [  955/ 1327], train_loss/perplexity = 3.83231616/46.1693497 secs/batch = 0.2949s, grad.norm=15.38063335
 31481: 23 [  960/ 1327], train_loss/perplexity = 4.14381266/63.0427246 secs/batch = 0.2942s, grad.norm=16.17886734
 31486: 23 [  965/ 1327], train_loss/perplexity = 3.88112783/48.4788589 secs/batch = 0.2957s, grad.norm=15.66040039
 31491: 23 [  970/ 1327], train_loss/perplexity = 4.10789156/60.8183517 secs/batch = 0.2943s, grad.norm=15.53129292
 31496: 23 [  975/ 1327], train_loss/perplexity = 3.69006062/40.0472755 secs/batch = 0.2956s, grad.norm=16.47286797
 31501: 23 [  980/ 1327], train_loss/perplexity = 3.62460828/37.5100250 secs/batch = 0.2928s, grad.norm=15.06377506
 31506: 23 [  985/ 1327], train_loss/perplexity = 3.76744723/43.2694664 secs/batch = 0.2933s, grad.norm=15.93632603
 31511: 23 [  990/ 1327], train_loss/perplexity = 3.96434164/52.6855736 secs/batch = 0.2962s, grad.norm=15.96005630
 31516: 23 [  995/ 1327], train_loss/perplexity = 3.95812535/52.3590775 secs/batch = 0.2952s, grad.norm=15.35186958
 31521: 23 [ 1000/ 1327], train_loss/perplexity = 3.49210525/32.8550415 secs/batch = 0.2952s, grad.norm=15.44988346
 31526: 23 [ 1005/ 1327], train_loss/perplexity = 3.98939276/54.0220757 secs/batch = 0.2945s, grad.norm=15.59635258
 31531: 23 [ 1010/ 1327], train_loss/perplexity = 3.58072114/35.8994217 secs/batch = 0.2946s, grad.norm=14.85988903
 31536: 23 [ 1015/ 1327], train_loss/perplexity = 4.07131910/58.6342545 secs/batch = 0.2953s, grad.norm=15.74731541
 31541: 23 [ 1020/ 1327], train_loss/perplexity = 4.07318306/58.7436485 secs/batch = 0.2929s, grad.norm=15.38966656
 31546: 23 [ 1025/ 1327], train_loss/perplexity = 4.03399658/56.4862137 secs/batch = 0.3020s, grad.norm=15.42243195
 31551: 23 [ 1030/ 1327], train_loss/perplexity = 3.80265570/44.8200569 secs/batch = 0.2952s, grad.norm=15.26113319
 31556: 23 [ 1035/ 1327], train_loss/perplexity = 3.76734638/43.2651024 secs/batch = 0.2951s, grad.norm=15.45882225
 31561: 23 [ 1040/ 1327], train_loss/perplexity = 3.94535732/51.6948051 secs/batch = 0.2996s, grad.norm=16.09956741
 31566: 23 [ 1045/ 1327], train_loss/perplexity = 3.54407454/34.6076431 secs/batch = 0.2996s, grad.norm=14.97890091
 31571: 23 [ 1050/ 1327], train_loss/perplexity = 3.64439726/38.2597046 secs/batch = 0.2956s, grad.norm=15.13514614
 31576: 23 [ 1055/ 1327], train_loss/perplexity = 3.62890339/37.6714821 secs/batch = 0.2934s, grad.norm=15.89295769
 31581: 23 [ 1060/ 1327], train_loss/perplexity = 3.31706548/27.5792999 secs/batch = 0.3011s, grad.norm=16.23550987
 31586: 23 [ 1065/ 1327], train_loss/perplexity = 3.46046066/31.8316364 secs/batch = 0.3009s, grad.norm=15.64420509
 31591: 23 [ 1070/ 1327], train_loss/perplexity = 3.77012157/43.3853378 secs/batch = 0.3002s, grad.norm=16.40928841
 31596: 23 [ 1075/ 1327], train_loss/perplexity = 3.54826760/34.7530594 secs/batch = 0.2976s, grad.norm=15.60518837
 31601: 23 [ 1080/ 1327], train_loss/perplexity = 3.53556871/34.3145256 secs/batch = 0.3015s, grad.norm=15.80767822
 31606: 23 [ 1085/ 1327], train_loss/perplexity = 3.47760677/32.3821297 secs/batch = 0.2954s, grad.norm=15.56121540
 31611: 23 [ 1090/ 1327], train_loss/perplexity = 3.64134026/38.1429253 secs/batch = 0.2990s, grad.norm=15.88473701
 31616: 23 [ 1095/ 1327], train_loss/perplexity = 3.73068428/41.7076378 secs/batch = 0.2985s, grad.norm=15.66448307
 31621: 23 [ 1100/ 1327], train_loss/perplexity = 3.42107654/30.6023426 secs/batch = 0.2962s, grad.norm=16.31024933
 31626: 23 [ 1105/ 1327], train_loss/perplexity = 3.47119880/32.1752892 secs/batch = 0.2949s, grad.norm=15.45849514
 31631: 23 [ 1110/ 1327], train_loss/perplexity = 3.75074029/42.5525703 secs/batch = 0.2953s, grad.norm=16.68690109
 31636: 23 [ 1115/ 1327], train_loss/perplexity = 3.65055943/38.4961967 secs/batch = 0.2980s, grad.norm=15.12887192
 31641: 23 [ 1120/ 1327], train_loss/perplexity = 3.80928636/45.1182289 secs/batch = 0.2950s, grad.norm=15.47201443
 31646: 23 [ 1125/ 1327], train_loss/perplexity = 4.01619577/55.4896088 secs/batch = 0.2949s, grad.norm=16.29710960
 31651: 23 [ 1130/ 1327], train_loss/perplexity = 3.66071916/38.8893013 secs/batch = 0.2956s, grad.norm=15.54767704
 31656: 23 [ 1135/ 1327], train_loss/perplexity = 3.56215143/35.2389297 secs/batch = 0.2926s, grad.norm=15.16337967
 31661: 23 [ 1140/ 1327], train_loss/perplexity = 3.85714579/47.3300667 secs/batch = 0.2953s, grad.norm=15.80611324
 31666: 23 [ 1145/ 1327], train_loss/perplexity = 3.73470449/41.8756485 secs/batch = 0.2951s, grad.norm=15.07007408
 31671: 23 [ 1150/ 1327], train_loss/perplexity = 3.67585945/39.4825745 secs/batch = 0.2938s, grad.norm=15.32320881
 31676: 23 [ 1155/ 1327], train_loss/perplexity = 3.78860164/44.1945572 secs/batch = 0.2963s, grad.norm=15.76160526
 31681: 23 [ 1160/ 1327], train_loss/perplexity = 3.73657227/41.9539375 secs/batch = 0.2956s, grad.norm=16.06065750
 31686: 23 [ 1165/ 1327], train_loss/perplexity = 3.79865360/44.6410408 secs/batch = 0.2952s, grad.norm=15.58685398
 31691: 23 [ 1170/ 1327], train_loss/perplexity = 3.66174126/38.9290695 secs/batch = 0.2952s, grad.norm=15.47070503
 31696: 23 [ 1175/ 1327], train_loss/perplexity = 3.45239878/31.5760460 secs/batch = 0.3013s, grad.norm=15.38319492
 31701: 23 [ 1180/ 1327], train_loss/perplexity = 3.39437151/29.7959213 secs/batch = 0.2993s, grad.norm=15.66859436
 31706: 23 [ 1185/ 1327], train_loss/perplexity = 3.66388822/39.0127373 secs/batch = 0.2947s, grad.norm=15.63620281
 31711: 23 [ 1190/ 1327], train_loss/perplexity = 3.74368620/42.2534599 secs/batch = 0.2939s, grad.norm=15.90991402
 31716: 23 [ 1195/ 1327], train_loss/perplexity = 3.50433683/33.2593803 secs/batch = 0.2946s, grad.norm=14.84406567
 31721: 23 [ 1200/ 1327], train_loss/perplexity = 3.53931522/34.4433250 secs/batch = 0.3000s, grad.norm=15.34862995
 31726: 23 [ 1205/ 1327], train_loss/perplexity = 3.53664637/34.3515244 secs/batch = 0.3005s, grad.norm=15.80629253
 31731: 23 [ 1210/ 1327], train_loss/perplexity = 3.17057204/23.8211079 secs/batch = 0.2974s, grad.norm=15.56241512
 31736: 23 [ 1215/ 1327], train_loss/perplexity = 3.34622550/28.3953533 secs/batch = 0.3022s, grad.norm=14.50712872
 31741: 23 [ 1220/ 1327], train_loss/perplexity = 3.49070024/32.8089142 secs/batch = 0.2956s, grad.norm=15.94616127
 31746: 23 [ 1225/ 1327], train_loss/perplexity = 3.16670680/23.7292099 secs/batch = 0.2947s, grad.norm=16.31136703
 31751: 23 [ 1230/ 1327], train_loss/perplexity = 3.62388277/37.4828224 secs/batch = 0.2957s, grad.norm=15.36266994
 31756: 23 [ 1235/ 1327], train_loss/perplexity = 3.42229390/30.6396179 secs/batch = 0.2961s, grad.norm=15.41426945
 31761: 23 [ 1240/ 1327], train_loss/perplexity = 3.76747561/43.2706947 secs/batch = 0.2950s, grad.norm=16.28142738
 31766: 23 [ 1245/ 1327], train_loss/perplexity = 3.63434839/37.8771629 secs/batch = 0.3019s, grad.norm=15.25412369
 31771: 23 [ 1250/ 1327], train_loss/perplexity = 3.76079321/42.9825058 secs/batch = 0.2943s, grad.norm=14.92740250
 31776: 23 [ 1255/ 1327], train_loss/perplexity = 3.81856823/45.5389595 secs/batch = 0.2932s, grad.norm=15.14927197
 31781: 23 [ 1260/ 1327], train_loss/perplexity = 3.60033250/36.6104050 secs/batch = 0.3005s, grad.norm=16.21468163
 31786: 23 [ 1265/ 1327], train_loss/perplexity = 3.68778563/39.9562721 secs/batch = 0.2944s, grad.norm=15.43393135
 31791: 23 [ 1270/ 1327], train_loss/perplexity = 3.54353428/34.5889511 secs/batch = 0.3011s, grad.norm=16.04081535
 31796: 23 [ 1275/ 1327], train_loss/perplexity = 3.78303361/43.9491653 secs/batch = 0.3010s, grad.norm=16.29771805
 31801: 23 [ 1280/ 1327], train_loss/perplexity = 3.54301047/34.5708389 secs/batch = 0.2989s, grad.norm=16.13984871
 31806: 23 [ 1285/ 1327], train_loss/perplexity = 3.54765511/34.7317810 secs/batch = 0.2949s, grad.norm=15.43560314
 31811: 23 [ 1290/ 1327], train_loss/perplexity = 3.70936131/40.8277206 secs/batch = 0.2957s, grad.norm=15.42207527
 31816: 23 [ 1295/ 1327], train_loss/perplexity = 3.69135451/40.0991249 secs/batch = 0.2950s, grad.norm=15.27007198
 31821: 23 [ 1300/ 1327], train_loss/perplexity = 3.87317371/48.0947838 secs/batch = 0.2955s, grad.norm=15.06459618
 31826: 23 [ 1305/ 1327], train_loss/perplexity = 3.96526575/52.7342834 secs/batch = 0.2952s, grad.norm=16.03601074
 31831: 23 [ 1310/ 1327], train_loss/perplexity = 4.16653347/64.4915009 secs/batch = 0.2953s, grad.norm=16.44532013
 31836: 23 [ 1315/ 1327], train_loss/perplexity = 3.93062901/50.9390106 secs/batch = 0.2951s, grad.norm=16.06987762
 31841: 23 [ 1320/ 1327], train_loss/perplexity = 4.03587484/56.5924072 secs/batch = 0.2927s, grad.norm=15.82811260
 31846: 23 [ 1325/ 1327], train_loss/perplexity = 3.90959692/49.8788414 secs/batch = 0.2951s, grad.norm=16.02439308
Epoch training time: 393.5291938781738
	> validation loss = 4.56454420, perplexity = 96.01882172
	> validation loss = 4.50716019, perplexity = 90.66398621
	> validation loss = 4.50098133, perplexity = 90.10551453
	> validation loss = 4.47833347, perplexity = 88.08775330
	> validation loss = 4.63701630, perplexity = 103.23586273
	> validation loss = 4.59321165, perplexity = 98.81127167
	> validation loss = 4.52936602, perplexity = 92.69977570
	> validation loss = 4.36722136, perplexity = 78.82430267
	> validation loss = 4.15277338, perplexity = 63.61017227
	> validation loss = 4.29293299, perplexity = 73.18079376
	> validation loss = 4.50059700, perplexity = 90.07088470
	> validation loss = 4.45469952, perplexity = 86.03029633
	> validation loss = 4.40160179, perplexity = 81.58144379
	> validation loss = 4.13055897, perplexity = 62.21268845
	> validation loss = 4.13122559, perplexity = 62.25417328
	> validation loss = 4.13215399, perplexity = 62.31199646
	> validation loss = 4.57860374, perplexity = 97.37833405
	> validation loss = 4.03277063, perplexity = 56.41700745
	> validation loss = 4.54613829, perplexity = 94.26766968
	> validation loss = 4.51080799, perplexity = 90.99531555
	> validation loss = 4.21467257, perplexity = 67.67200470
at the end of epoch: 23
train loss = 3.82151661, perplexity = 45.67342464
validation loss = 4.39929580, perplexity = 81.39353098
Saved model cv/epoch023_4.3993.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.00390625
new learning rate is: 0.001953125
 31853: 24 [    5/ 1327], train_loss/perplexity = 4.02879667/56.1932526 secs/batch = 0.3025s, grad.norm=15.90005302
 31858: 24 [   10/ 1327], train_loss/perplexity = 3.64605284/38.3231010 secs/batch = 0.2953s, grad.norm=15.29846382
 31863: 24 [   15/ 1327], train_loss/perplexity = 3.97459245/53.2284203 secs/batch = 0.2936s, grad.norm=14.99493217
 31868: 24 [   20/ 1327], train_loss/perplexity = 4.16660404/64.4960556 secs/batch = 0.2951s, grad.norm=14.97870636
 31873: 24 [   25/ 1327], train_loss/perplexity = 4.00047541/54.6241112 secs/batch = 0.2992s, grad.norm=16.14506531
 31878: 24 [   30/ 1327], train_loss/perplexity = 4.10922146/60.8992882 secs/batch = 0.2943s, grad.norm=16.18055344
 31883: 24 [   35/ 1327], train_loss/perplexity = 3.79532743/44.4928017 secs/batch = 0.2992s, grad.norm=15.46860790
 31888: 24 [   40/ 1327], train_loss/perplexity = 3.83877611/46.4685669 secs/batch = 0.2999s, grad.norm=15.56010818
 31893: 24 [   45/ 1327], train_loss/perplexity = 3.67748189/39.5466843 secs/batch = 0.2946s, grad.norm=14.61742592
 31898: 24 [   50/ 1327], train_loss/perplexity = 3.82343626/45.7611847 secs/batch = 0.2949s, grad.norm=15.48525333
 31903: 24 [   55/ 1327], train_loss/perplexity = 3.75673556/42.8084526 secs/batch = 0.2942s, grad.norm=15.84648514
 31908: 24 [   60/ 1327], train_loss/perplexity = 4.01840115/55.6121178 secs/batch = 0.2931s, grad.norm=16.14234352
 31913: 24 [   65/ 1327], train_loss/perplexity = 3.72029471/41.2765579 secs/batch = 0.2990s, grad.norm=15.50181866
 31918: 24 [   70/ 1327], train_loss/perplexity = 3.41687226/30.4739513 secs/batch = 0.2951s, grad.norm=15.49812508
 31923: 24 [   75/ 1327], train_loss/perplexity = 3.36720610/28.9973984 secs/batch = 0.2952s, grad.norm=14.79118919
 31928: 24 [   80/ 1327], train_loss/perplexity = 3.81203413/45.2423744 secs/batch = 0.2957s, grad.norm=15.98815727
 31933: 24 [   85/ 1327], train_loss/perplexity = 3.71935558/41.2378120 secs/batch = 0.2948s, grad.norm=16.01737595
 31938: 24 [   90/ 1327], train_loss/perplexity = 3.82309461/45.7455559 secs/batch = 0.2946s, grad.norm=15.90519047
 31943: 24 [   95/ 1327], train_loss/perplexity = 3.72575879/41.5027122 secs/batch = 0.2943s, grad.norm=15.86818123
 31948: 24 [  100/ 1327], train_loss/perplexity = 4.06121778/58.0449562 secs/batch = 0.2929s, grad.norm=16.24126053
 31953: 24 [  105/ 1327], train_loss/perplexity = 3.71433616/41.0313416 secs/batch = 0.2944s, grad.norm=16.51178169
 31958: 24 [  110/ 1327], train_loss/perplexity = 3.71393514/41.0148888 secs/batch = 0.2953s, grad.norm=16.16922188
 31963: 24 [  115/ 1327], train_loss/perplexity = 3.75399232/42.6911774 secs/batch = 0.2947s, grad.norm=16.04141426
 31968: 24 [  120/ 1327], train_loss/perplexity = 3.77505875/43.6000710 secs/batch = 0.2944s, grad.norm=16.03139877
 31973: 24 [  125/ 1327], train_loss/perplexity = 3.78093505/43.8570328 secs/batch = 0.2983s, grad.norm=16.22142982
 31978: 24 [  130/ 1327], train_loss/perplexity = 3.73393416/41.8434029 secs/batch = 0.2948s, grad.norm=16.82012558
 31983: 24 [  135/ 1327], train_loss/perplexity = 3.84407854/46.7156181 secs/batch = 0.2939s, grad.norm=16.13638878
 31988: 24 [  140/ 1327], train_loss/perplexity = 4.06248522/58.1185684 secs/batch = 0.2998s, grad.norm=16.69670296
 31993: 24 [  145/ 1327], train_loss/perplexity = 3.88761616/48.7944298 secs/batch = 0.2949s, grad.norm=16.83300018
 31998: 24 [  150/ 1327], train_loss/perplexity = 3.99579906/54.3692665 secs/batch = 0.2963s, grad.norm=16.63089180
 32003: 24 [  155/ 1327], train_loss/perplexity = 4.18429518/65.6472168 secs/batch = 0.2942s, grad.norm=16.13885117
 32008: 24 [  160/ 1327], train_loss/perplexity = 3.94166374/51.5042191 secs/batch = 0.2956s, grad.norm=15.43312740
 32013: 24 [  165/ 1327], train_loss/perplexity = 3.99082232/54.0993576 secs/batch = 0.2942s, grad.norm=16.27164650
 32018: 24 [  170/ 1327], train_loss/perplexity = 3.82229066/45.7087936 secs/batch = 0.2935s, grad.norm=15.34913826
 32023: 24 [  175/ 1327], train_loss/perplexity = 4.06702328/58.3829155 secs/batch = 0.2937s, grad.norm=15.98424721
 32028: 24 [  180/ 1327], train_loss/perplexity = 3.94419718/51.6348686 secs/batch = 0.2951s, grad.norm=15.85435677
 32033: 24 [  185/ 1327], train_loss/perplexity = 4.24019003/69.4210434 secs/batch = 0.2989s, grad.norm=16.55723953
 32038: 24 [  190/ 1327], train_loss/perplexity = 3.81410336/45.3360901 secs/batch = 0.3013s, grad.norm=15.54649448
 32043: 24 [  195/ 1327], train_loss/perplexity = 4.09918547/60.2911568 secs/batch = 0.2982s, grad.norm=15.63337612
 32048: 24 [  200/ 1327], train_loss/perplexity = 3.96501088/52.7208443 secs/batch = 0.2971s, grad.norm=15.90041924
 32053: 24 [  205/ 1327], train_loss/perplexity = 4.15690041/63.8732338 secs/batch = 0.2959s, grad.norm=16.45375824
 32058: 24 [  210/ 1327], train_loss/perplexity = 3.94877958/51.8720245 secs/batch = 0.3002s, grad.norm=15.58821201
 32063: 24 [  215/ 1327], train_loss/perplexity = 4.08718777/59.5721245 secs/batch = 0.2963s, grad.norm=15.48252964
 32068: 24 [  220/ 1327], train_loss/perplexity = 3.97888565/53.4574318 secs/batch = 0.2953s, grad.norm=15.52674484
 32073: 24 [  225/ 1327], train_loss/perplexity = 4.20566559/67.0652237 secs/batch = 0.2957s, grad.norm=16.12602806
 32078: 24 [  230/ 1327], train_loss/perplexity = 4.06414223/58.2149506 secs/batch = 0.2943s, grad.norm=17.16604233
 32083: 24 [  235/ 1327], train_loss/perplexity = 3.96382713/52.6584702 secs/batch = 0.3002s, grad.norm=15.70008755
 32088: 24 [  240/ 1327], train_loss/perplexity = 3.59922409/36.5698471 secs/batch = 0.2942s, grad.norm=15.77500439
 32093: 24 [  245/ 1327], train_loss/perplexity = 4.02461624/55.9588318 secs/batch = 0.2953s, grad.norm=16.32555008
 32098: 24 [  250/ 1327], train_loss/perplexity = 3.89341521/49.0782127 secs/batch = 0.2944s, grad.norm=15.50525093
 32103: 24 [  255/ 1327], train_loss/perplexity = 3.77222228/43.4765739 secs/batch = 0.2999s, grad.norm=15.64124966
 32108: 24 [  260/ 1327], train_loss/perplexity = 3.97904205/53.4657936 secs/batch = 0.2940s, grad.norm=16.30126190
 32113: 24 [  265/ 1327], train_loss/perplexity = 4.19625330/66.4369431 secs/batch = 0.2980s, grad.norm=15.63251495
 32118: 24 [  270/ 1327], train_loss/perplexity = 4.24227762/69.5661163 secs/batch = 0.2956s, grad.norm=16.11477852
 32123: 24 [  275/ 1327], train_loss/perplexity = 4.16162205/64.1755371 secs/batch = 0.2959s, grad.norm=15.90180016
 32128: 24 [  280/ 1327], train_loss/perplexity = 3.97405839/53.2000008 secs/batch = 0.2954s, grad.norm=15.98826313
 32133: 24 [  285/ 1327], train_loss/perplexity = 4.33576202/76.3831406 secs/batch = 0.2947s, grad.norm=15.86888695
 32138: 24 [  290/ 1327], train_loss/perplexity = 4.05125904/57.4697685 secs/batch = 0.2984s, grad.norm=16.55710220
 32143: 24 [  295/ 1327], train_loss/perplexity = 3.76291656/43.0738716 secs/batch = 0.2954s, grad.norm=15.94340134
 32148: 24 [  300/ 1327], train_loss/perplexity = 3.29797316/27.0577412 secs/batch = 0.2987s, grad.norm=14.84150124
 32153: 24 [  305/ 1327], train_loss/perplexity = 3.90493536/49.6468697 secs/batch = 0.2939s, grad.norm=15.48972607
 32158: 24 [  310/ 1327], train_loss/perplexity = 3.86282277/47.5995255 secs/batch = 0.2941s, grad.norm=15.89010239
 32163: 24 [  315/ 1327], train_loss/perplexity = 3.37123394/29.1144295 secs/batch = 0.2966s, grad.norm=14.51125526
 32168: 24 [  320/ 1327], train_loss/perplexity = 3.34964323/28.4925671 secs/batch = 0.3015s, grad.norm=16.67619514
 32173: 24 [  325/ 1327], train_loss/perplexity = 3.37575936/29.2464848 secs/batch = 0.2948s, grad.norm=14.50400162
 32178: 24 [  330/ 1327], train_loss/perplexity = 4.05042267/57.4217224 secs/batch = 0.2994s, grad.norm=16.01871681
 32183: 24 [  335/ 1327], train_loss/perplexity = 3.37868237/29.3320961 secs/batch = 0.2957s, grad.norm=14.90283871
 32188: 24 [  340/ 1327], train_loss/perplexity = 4.05385447/57.6191216 secs/batch = 0.2944s, grad.norm=15.80374241
 32193: 24 [  345/ 1327], train_loss/perplexity = 3.79878664/44.6469803 secs/batch = 0.2957s, grad.norm=15.44476700
 32198: 24 [  350/ 1327], train_loss/perplexity = 3.90530753/49.6653519 secs/batch = 0.2997s, grad.norm=16.58726501
 32203: 24 [  355/ 1327], train_loss/perplexity = 3.86061096/47.4943581 secs/batch = 0.2924s, grad.norm=15.96810627
 32208: 24 [  360/ 1327], train_loss/perplexity = 3.86597800/47.7499504 secs/batch = 0.2957s, grad.norm=16.93391991
 32213: 24 [  365/ 1327], train_loss/perplexity = 4.05341339/57.5937119 secs/batch = 0.3011s, grad.norm=15.92385674
 32218: 24 [  370/ 1327], train_loss/perplexity = 4.09529209/60.0568771 secs/batch = 0.3003s, grad.norm=16.08974838
 32223: 24 [  375/ 1327], train_loss/perplexity = 3.49062204/32.8063469 secs/batch = 0.2983s, grad.norm=15.76039886
 32228: 24 [  380/ 1327], train_loss/perplexity = 3.55074453/34.8392487 secs/batch = 0.2943s, grad.norm=15.72614193
 32233: 24 [  385/ 1327], train_loss/perplexity = 3.78624153/44.0903778 secs/batch = 0.2949s, grad.norm=17.00685883
 32238: 24 [  390/ 1327], train_loss/perplexity = 3.91089964/49.9438629 secs/batch = 0.2937s, grad.norm=15.83247471
 32243: 24 [  395/ 1327], train_loss/perplexity = 3.94760752/51.8112602 secs/batch = 0.2997s, grad.norm=15.92746067
 32248: 24 [  400/ 1327], train_loss/perplexity = 3.85049462/47.0163116 secs/batch = 0.2998s, grad.norm=15.73039055
 32253: 24 [  405/ 1327], train_loss/perplexity = 4.12426472/61.8223343 secs/batch = 0.2962s, grad.norm=15.81667995
 32258: 24 [  410/ 1327], train_loss/perplexity = 3.76594210/43.2043877 secs/batch = 0.3010s, grad.norm=15.42015934
 32263: 24 [  415/ 1327], train_loss/perplexity = 3.78593802/44.0769958 secs/batch = 0.2973s, grad.norm=15.51307201
 32268: 24 [  420/ 1327], train_loss/perplexity = 3.37090158/29.1047554 secs/batch = 0.2954s, grad.norm=15.48898888
 32273: 24 [  425/ 1327], train_loss/perplexity = 3.66914177/39.2182350 secs/batch = 0.2925s, grad.norm=16.48496819
 32278: 24 [  430/ 1327], train_loss/perplexity = 3.82827878/45.9833221 secs/batch = 0.2988s, grad.norm=15.82944107
 32283: 24 [  435/ 1327], train_loss/perplexity = 3.95332074/52.1081161 secs/batch = 0.2945s, grad.norm=16.33720016
 32288: 24 [  440/ 1327], train_loss/perplexity = 3.47702789/32.3633919 secs/batch = 0.2954s, grad.norm=15.34481716
 32293: 24 [  445/ 1327], train_loss/perplexity = 3.86907244/47.8979378 secs/batch = 0.2963s, grad.norm=16.36888885
 32298: 24 [  450/ 1327], train_loss/perplexity = 3.83511829/46.2989044 secs/batch = 0.2984s, grad.norm=16.28249741
 32303: 24 [  455/ 1327], train_loss/perplexity = 3.79055262/44.2808647 secs/batch = 0.2950s, grad.norm=15.51014042
 32308: 24 [  460/ 1327], train_loss/perplexity = 3.74011326/42.1027565 secs/batch = 0.2955s, grad.norm=16.47803307
 32313: 24 [  465/ 1327], train_loss/perplexity = 3.46276021/31.9049187 secs/batch = 0.3007s, grad.norm=16.26781082
 32318: 24 [  470/ 1327], train_loss/perplexity = 4.21441126/67.6543198 secs/batch = 0.2949s, grad.norm=16.07210732
 32323: 24 [  475/ 1327], train_loss/perplexity = 3.68699265/39.9245987 secs/batch = 0.2951s, grad.norm=15.77748775
 32328: 24 [  480/ 1327], train_loss/perplexity = 3.70842910/40.7896805 secs/batch = 0.2990s, grad.norm=15.84114933
 32333: 24 [  485/ 1327], train_loss/perplexity = 3.76198077/43.0335808 secs/batch = 0.3015s, grad.norm=15.97890949
 32338: 24 [  490/ 1327], train_loss/perplexity = 3.61375833/37.1052437 secs/batch = 0.2945s, grad.norm=16.48892784
 32343: 24 [  495/ 1327], train_loss/perplexity = 3.68108225/39.6893234 secs/batch = 0.2946s, grad.norm=15.56228828
 32348: 24 [  500/ 1327], train_loss/perplexity = 3.82582712/45.8707237 secs/batch = 0.2954s, grad.norm=15.87464428
 32353: 24 [  505/ 1327], train_loss/perplexity = 3.98942089/54.0235939 secs/batch = 0.3011s, grad.norm=15.27675056
 32358: 24 [  510/ 1327], train_loss/perplexity = 4.32389593/75.4821320 secs/batch = 0.2943s, grad.norm=15.36641216
 32363: 24 [  515/ 1327], train_loss/perplexity = 3.97773695/53.3960609 secs/batch = 0.3000s, grad.norm=15.74810028
 32368: 24 [  520/ 1327], train_loss/perplexity = 4.05815411/57.8673935 secs/batch = 0.2955s, grad.norm=15.79466057
 32373: 24 [  525/ 1327], train_loss/perplexity = 3.77824616/43.7392616 secs/batch = 0.2957s, grad.norm=15.81169224
 32378: 24 [  530/ 1327], train_loss/perplexity = 3.71566391/41.0858536 secs/batch = 0.2952s, grad.norm=16.25470924
 32383: 24 [  535/ 1327], train_loss/perplexity = 3.94750309/51.8058510 secs/batch = 0.2937s, grad.norm=16.01812172
 32388: 24 [  540/ 1327], train_loss/perplexity = 3.96529984/52.7360802 secs/batch = 0.2997s, grad.norm=15.72069836
 32393: 24 [  545/ 1327], train_loss/perplexity = 3.85632324/47.2911530 secs/batch = 0.2968s, grad.norm=15.86592197
 32398: 24 [  550/ 1327], train_loss/perplexity = 3.92069674/50.4355736 secs/batch = 0.2968s, grad.norm=15.81745148
 32403: 24 [  555/ 1327], train_loss/perplexity = 3.79000187/44.2564812 secs/batch = 0.2941s, grad.norm=15.52509117
 32408: 24 [  560/ 1327], train_loss/perplexity = 3.88048697/48.4478035 secs/batch = 0.2941s, grad.norm=16.63811874
 32413: 24 [  565/ 1327], train_loss/perplexity = 3.74322605/42.2340202 secs/batch = 0.2956s, grad.norm=16.60862923
 32418: 24 [  570/ 1327], train_loss/perplexity = 3.63527489/37.9122734 secs/batch = 0.2954s, grad.norm=16.22794151
 32423: 24 [  575/ 1327], train_loss/perplexity = 3.56769919/35.4349709 secs/batch = 0.2957s, grad.norm=16.38070107
 32428: 24 [  580/ 1327], train_loss/perplexity = 3.91884232/50.3421326 secs/batch = 0.2945s, grad.norm=16.75288391
 32433: 24 [  585/ 1327], train_loss/perplexity = 3.55715895/35.0634384 secs/batch = 0.2992s, grad.norm=15.41724968
 32438: 24 [  590/ 1327], train_loss/perplexity = 3.97444630/53.2206421 secs/batch = 0.2955s, grad.norm=15.91045666
 32443: 24 [  595/ 1327], train_loss/perplexity = 3.91812825/50.3061943 secs/batch = 0.3012s, grad.norm=16.36071396
 32448: 24 [  600/ 1327], train_loss/perplexity = 4.07295036/58.7299805 secs/batch = 0.2943s, grad.norm=15.32547665
 32453: 24 [  605/ 1327], train_loss/perplexity = 3.97817945/53.4196930 secs/batch = 0.2995s, grad.norm=15.64214802
 32458: 24 [  610/ 1327], train_loss/perplexity = 4.18370962/65.6087875 secs/batch = 0.2959s, grad.norm=16.11787415
 32463: 24 [  615/ 1327], train_loss/perplexity = 3.74785018/42.4297676 secs/batch = 0.2955s, grad.norm=15.17786312
 32468: 24 [  620/ 1327], train_loss/perplexity = 4.10858059/60.8602715 secs/batch = 0.2960s, grad.norm=15.70778847
 32473: 24 [  625/ 1327], train_loss/perplexity = 4.00271845/54.7467728 secs/batch = 0.2943s, grad.norm=15.54717636
 32478: 24 [  630/ 1327], train_loss/perplexity = 4.13195515/62.2996101 secs/batch = 0.2981s, grad.norm=15.46121311
 32483: 24 [  635/ 1327], train_loss/perplexity = 3.81706977/45.4707718 secs/batch = 0.2962s, grad.norm=15.66699314
 32488: 24 [  640/ 1327], train_loss/perplexity = 3.82307148/45.7444954 secs/batch = 0.2988s, grad.norm=15.50851345
 32493: 24 [  645/ 1327], train_loss/perplexity = 4.00921297/55.1034851 secs/batch = 0.2948s, grad.norm=16.61297035
 32498: 24 [  650/ 1327], train_loss/perplexity = 3.58909082/36.2011490 secs/batch = 0.2937s, grad.norm=15.87517071
 32503: 24 [  655/ 1327], train_loss/perplexity = 3.74480557/42.3007812 secs/batch = 0.2944s, grad.norm=16.32522583
 32508: 24 [  660/ 1327], train_loss/perplexity = 3.72272062/41.3768120 secs/batch = 0.3006s, grad.norm=15.98565197
 32513: 24 [  665/ 1327], train_loss/perplexity = 3.82316089/45.7485847 secs/batch = 0.3008s, grad.norm=15.88826656
 32518: 24 [  670/ 1327], train_loss/perplexity = 3.77936006/43.7880096 secs/batch = 0.2950s, grad.norm=16.17019272
 32523: 24 [  675/ 1327], train_loss/perplexity = 3.59467936/36.4040260 secs/batch = 0.2964s, grad.norm=15.70597935
 32528: 24 [  680/ 1327], train_loss/perplexity = 3.87168407/48.0231934 secs/batch = 0.2969s, grad.norm=16.51520157
 32533: 24 [  685/ 1327], train_loss/perplexity = 3.59528565/36.4261055 secs/batch = 0.2944s, grad.norm=15.44459343
 32538: 24 [  690/ 1327], train_loss/perplexity = 3.99759650/54.4670830 secs/batch = 0.2938s, grad.norm=15.64389229
 32543: 24 [  695/ 1327], train_loss/perplexity = 3.80039501/44.7188454 secs/batch = 0.2942s, grad.norm=15.56717014
 32548: 24 [  700/ 1327], train_loss/perplexity = 4.04972649/57.3817596 secs/batch = 0.2961s, grad.norm=16.25946808
 32553: 24 [  705/ 1327], train_loss/perplexity = 3.85419226/47.1904831 secs/batch = 0.2979s, grad.norm=15.31632996
 32558: 24 [  710/ 1327], train_loss/perplexity = 3.71715999/41.1473694 secs/batch = 0.2965s, grad.norm=15.78985977
 32563: 24 [  715/ 1327], train_loss/perplexity = 3.57431722/35.6702576 secs/batch = 0.2954s, grad.norm=15.36800575
 32568: 24 [  720/ 1327], train_loss/perplexity = 3.63458610/37.8861694 secs/batch = 0.2941s, grad.norm=16.06992722
 32573: 24 [  725/ 1327], train_loss/perplexity = 3.64968586/38.4625816 secs/batch = 0.2947s, grad.norm=15.89268494
 32578: 24 [  730/ 1327], train_loss/perplexity = 3.75504184/42.7360077 secs/batch = 0.2954s, grad.norm=16.49001694
 32583: 24 [  735/ 1327], train_loss/perplexity = 3.84878087/46.9358063 secs/batch = 0.3014s, grad.norm=16.57556915
 32588: 24 [  740/ 1327], train_loss/perplexity = 3.41230822/30.3351841 secs/batch = 0.3005s, grad.norm=15.31290627
 32593: 24 [  745/ 1327], train_loss/perplexity = 3.93364358/51.0928001 secs/batch = 0.2942s, grad.norm=16.18540955
 32598: 24 [  750/ 1327], train_loss/perplexity = 3.83599472/46.3395004 secs/batch = 0.2972s, grad.norm=15.72681332
 32603: 24 [  755/ 1327], train_loss/perplexity = 3.60196924/36.6703758 secs/batch = 0.2952s, grad.norm=15.16084862
 32608: 24 [  760/ 1327], train_loss/perplexity = 3.52910233/34.0933495 secs/batch = 0.2948s, grad.norm=14.99323940
 32613: 24 [  765/ 1327], train_loss/perplexity = 3.61873722/37.2904472 secs/batch = 0.2945s, grad.norm=14.88910198
 32618: 24 [  770/ 1327], train_loss/perplexity = 3.53857183/34.4177284 secs/batch = 0.2943s, grad.norm=15.35416317
 32623: 24 [  775/ 1327], train_loss/perplexity = 3.55180621/34.8762550 secs/batch = 0.2942s, grad.norm=15.75953197
 32628: 24 [  780/ 1327], train_loss/perplexity = 3.91789174/50.2943001 secs/batch = 0.2969s, grad.norm=16.19767952
 32633: 24 [  785/ 1327], train_loss/perplexity = 3.83440304/46.2658005 secs/batch = 0.2968s, grad.norm=15.97959709
 32638: 24 [  790/ 1327], train_loss/perplexity = 3.54561615/34.6610336 secs/batch = 0.2959s, grad.norm=15.42589188
 32643: 24 [  795/ 1327], train_loss/perplexity = 3.93078065/50.9467354 secs/batch = 0.2954s, grad.norm=15.76001453
 32648: 24 [  800/ 1327], train_loss/perplexity = 3.81311750/45.2914162 secs/batch = 0.2940s, grad.norm=16.05733109
 32653: 24 [  805/ 1327], train_loss/perplexity = 4.16571236/64.4385681 secs/batch = 0.2943s, grad.norm=15.93081760
 32658: 24 [  810/ 1327], train_loss/perplexity = 3.77739573/43.7020798 secs/batch = 0.2988s, grad.norm=14.74039268
 32663: 24 [  815/ 1327], train_loss/perplexity = 3.68127823/39.6971054 secs/batch = 0.2955s, grad.norm=15.29124260
 32668: 24 [  820/ 1327], train_loss/perplexity = 3.54326606/34.5796738 secs/batch = 0.2950s, grad.norm=14.70917416
 32673: 24 [  825/ 1327], train_loss/perplexity = 3.76050234/42.9700050 secs/batch = 0.2950s, grad.norm=15.38062954
 32678: 24 [  830/ 1327], train_loss/perplexity = 3.48807931/32.7230377 secs/batch = 0.2938s, grad.norm=15.83366966
 32683: 24 [  835/ 1327], train_loss/perplexity = 3.77921700/43.7817459 secs/batch = 0.2962s, grad.norm=16.08131599
 32688: 24 [  840/ 1327], train_loss/perplexity = 3.94133663/51.4873734 secs/batch = 0.3012s, grad.norm=16.16790962
 32693: 24 [  845/ 1327], train_loss/perplexity = 3.57543850/35.7102776 secs/batch = 0.3015s, grad.norm=15.90028381
 32698: 24 [  850/ 1327], train_loss/perplexity = 3.79465127/44.4627266 secs/batch = 0.2938s, grad.norm=15.50839710
 32703: 24 [  855/ 1327], train_loss/perplexity = 3.79647684/44.5439720 secs/batch = 0.2952s, grad.norm=16.47494125
 32708: 24 [  860/ 1327], train_loss/perplexity = 3.54044843/34.4823799 secs/batch = 0.2990s, grad.norm=15.36634064
 32713: 24 [  865/ 1327], train_loss/perplexity = 3.95223475/52.0515594 secs/batch = 0.3010s, grad.norm=15.73278522
 32718: 24 [  870/ 1327], train_loss/perplexity = 3.86756945/47.8260002 secs/batch = 0.2942s, grad.norm=16.38279724
 32723: 24 [  875/ 1327], train_loss/perplexity = 3.42104006/30.6012249 secs/batch = 0.2952s, grad.norm=14.95282745
 32728: 24 [  880/ 1327], train_loss/perplexity = 3.64085007/38.1242294 secs/batch = 0.2959s, grad.norm=15.09903336
 32733: 24 [  885/ 1327], train_loss/perplexity = 3.79022622/44.2664146 secs/batch = 0.2951s, grad.norm=15.15486145
 32738: 24 [  890/ 1327], train_loss/perplexity = 3.91946793/50.3736343 secs/batch = 0.3010s, grad.norm=15.68976402
 32743: 24 [  895/ 1327], train_loss/perplexity = 3.90092278/49.4480591 secs/batch = 0.2951s, grad.norm=15.52534485
 32748: 24 [  900/ 1327], train_loss/perplexity = 3.77852035/43.7512589 secs/batch = 0.2934s, grad.norm=15.30598450
 32753: 24 [  905/ 1327], train_loss/perplexity = 3.63465643/37.8888321 secs/batch = 0.3009s, grad.norm=14.57738590
 32758: 24 [  910/ 1327], train_loss/perplexity = 3.65357423/38.6124306 secs/batch = 0.2950s, grad.norm=14.14057255
 32763: 24 [  915/ 1327], train_loss/perplexity = 3.89386582/49.1003342 secs/batch = 0.3020s, grad.norm=15.08631992
 32768: 24 [  920/ 1327], train_loss/perplexity = 4.03500128/56.5429916 secs/batch = 0.3008s, grad.norm=15.81261826
 32773: 24 [  925/ 1327], train_loss/perplexity = 3.87818766/48.3365326 secs/batch = 0.3017s, grad.norm=15.29271507
 32778: 24 [  930/ 1327], train_loss/perplexity = 3.96871734/52.9166145 secs/batch = 0.2933s, grad.norm=15.73491287
 32783: 24 [  935/ 1327], train_loss/perplexity = 3.97777915/53.3983116 secs/batch = 0.2953s, grad.norm=15.39280415
 32788: 24 [  940/ 1327], train_loss/perplexity = 3.85689831/47.3183556 secs/batch = 0.2959s, grad.norm=15.33831501
 32793: 24 [  945/ 1327], train_loss/perplexity = 4.10599327/60.7030106 secs/batch = 0.2938s, grad.norm=15.20520878
 32798: 24 [  950/ 1327], train_loss/perplexity = 3.80975509/45.1393814 secs/batch = 0.2947s, grad.norm=15.65623569
 32803: 24 [  955/ 1327], train_loss/perplexity = 3.80990934/45.1463470 secs/batch = 0.2966s, grad.norm=15.31299019
 32808: 24 [  960/ 1327], train_loss/perplexity = 4.12279177/61.7313423 secs/batch = 0.2962s, grad.norm=15.91616249
 32813: 24 [  965/ 1327], train_loss/perplexity = 3.89381719/49.0979462 secs/batch = 0.2956s, grad.norm=15.70393372
 32818: 24 [  970/ 1327], train_loss/perplexity = 4.12526131/61.8839760 secs/batch = 0.2931s, grad.norm=15.73321247
 32823: 24 [  975/ 1327], train_loss/perplexity = 3.71635723/41.1143494 secs/batch = 0.3028s, grad.norm=16.88056183
 32828: 24 [  980/ 1327], train_loss/perplexity = 3.59518027/36.4222641 secs/batch = 0.3011s, grad.norm=15.16840744
 32833: 24 [  985/ 1327], train_loss/perplexity = 3.77741241/43.7028122 secs/batch = 0.2946s, grad.norm=15.88882160
 32838: 24 [  990/ 1327], train_loss/perplexity = 3.92319322/50.5616417 secs/batch = 0.2952s, grad.norm=15.99888134
 32843: 24 [  995/ 1327], train_loss/perplexity = 3.97950149/53.4903603 secs/batch = 0.2999s, grad.norm=15.72688389
 32848: 24 [ 1000/ 1327], train_loss/perplexity = 3.52358675/33.9058228 secs/batch = 0.2910s, grad.norm=15.35055542
 32853: 24 [ 1005/ 1327], train_loss/perplexity = 4.01308632/55.3173332 secs/batch = 0.2959s, grad.norm=15.77233696
 32858: 24 [ 1010/ 1327], train_loss/perplexity = 3.52125883/33.8269844 secs/batch = 0.3003s, grad.norm=14.99910927
 32863: 24 [ 1015/ 1327], train_loss/perplexity = 4.04205608/56.9433022 secs/batch = 0.2958s, grad.norm=15.61927509
 32868: 24 [ 1020/ 1327], train_loss/perplexity = 4.07608700/58.9144859 secs/batch = 0.2959s, grad.norm=15.51207161
 32873: 24 [ 1025/ 1327], train_loss/perplexity = 4.00761414/55.0154533 secs/batch = 0.3013s, grad.norm=15.29741478
 32878: 24 [ 1030/ 1327], train_loss/perplexity = 3.83036280/46.0792542 secs/batch = 0.2968s, grad.norm=15.32422447
 32883: 24 [ 1035/ 1327], train_loss/perplexity = 3.79505229/44.4805641 secs/batch = 0.2990s, grad.norm=15.18761253
 32888: 24 [ 1040/ 1327], train_loss/perplexity = 3.94350863/51.5993271 secs/batch = 0.2955s, grad.norm=15.88698387
 32893: 24 [ 1045/ 1327], train_loss/perplexity = 3.50899839/33.4147835 secs/batch = 0.2949s, grad.norm=14.71365643
 32898: 24 [ 1050/ 1327], train_loss/perplexity = 3.66277075/38.9691658 secs/batch = 0.2954s, grad.norm=15.68663120
 32903: 24 [ 1055/ 1327], train_loss/perplexity = 3.65920782/38.8305702 secs/batch = 0.2944s, grad.norm=15.74059391
 32908: 24 [ 1060/ 1327], train_loss/perplexity = 3.25126648/25.8230228 secs/batch = 0.2961s, grad.norm=16.16019249
 32913: 24 [ 1065/ 1327], train_loss/perplexity = 3.44022751/31.1940536 secs/batch = 0.2963s, grad.norm=15.77700043
 32918: 24 [ 1070/ 1327], train_loss/perplexity = 3.65702295/38.7458229 secs/batch = 0.2941s, grad.norm=15.68789959
 32923: 24 [ 1075/ 1327], train_loss/perplexity = 3.51980925/33.7779846 secs/batch = 0.2940s, grad.norm=15.74200535
 32928: 24 [ 1080/ 1327], train_loss/perplexity = 3.49427772/32.9264984 secs/batch = 0.2932s, grad.norm=15.49813175
 32933: 24 [ 1085/ 1327], train_loss/perplexity = 3.42289495/30.6580410 secs/batch = 0.2962s, grad.norm=15.69342613
 32938: 24 [ 1090/ 1327], train_loss/perplexity = 3.58534551/36.0658150 secs/batch = 0.2920s, grad.norm=15.80991650
 32943: 24 [ 1095/ 1327], train_loss/perplexity = 3.70031452/40.4600296 secs/batch = 0.3017s, grad.norm=15.90229034
 32948: 24 [ 1100/ 1327], train_loss/perplexity = 3.43738604/31.1055431 secs/batch = 0.2944s, grad.norm=16.57558250
 32953: 24 [ 1105/ 1327], train_loss/perplexity = 3.50571322/33.3051910 secs/batch = 0.3000s, grad.norm=16.04376411
 32958: 24 [ 1110/ 1327], train_loss/perplexity = 3.73243761/41.7808304 secs/batch = 0.2948s, grad.norm=16.26521873
 32963: 24 [ 1115/ 1327], train_loss/perplexity = 3.54843068/34.7587280 secs/batch = 0.2996s, grad.norm=14.72623348
 32968: 24 [ 1120/ 1327], train_loss/perplexity = 3.82965040/46.0464363 secs/batch = 0.2939s, grad.norm=15.53194809
 32973: 24 [ 1125/ 1327], train_loss/perplexity = 3.99238920/54.1841927 secs/batch = 0.2985s, grad.norm=16.63891792
 32978: 24 [ 1130/ 1327], train_loss/perplexity = 3.63714433/37.9832153 secs/batch = 0.2971s, grad.norm=15.73936272
 32983: 24 [ 1135/ 1327], train_loss/perplexity = 3.64582777/38.3144760 secs/batch = 0.2952s, grad.norm=15.38503551
 32988: 24 [ 1140/ 1327], train_loss/perplexity = 3.90166354/49.4846992 secs/batch = 0.2936s, grad.norm=15.68320465
 32993: 24 [ 1145/ 1327], train_loss/perplexity = 3.71711469/41.1455040 secs/batch = 0.2956s, grad.norm=15.25403976
 32998: 24 [ 1150/ 1327], train_loss/perplexity = 3.70045137/40.4655647 secs/batch = 0.2945s, grad.norm=15.21358490
 33003: 24 [ 1155/ 1327], train_loss/perplexity = 3.77679157/43.6756859 secs/batch = 0.2950s, grad.norm=15.91863537
 33008: 24 [ 1160/ 1327], train_loss/perplexity = 3.73483992/41.8813210 secs/batch = 0.2958s, grad.norm=16.02333450
 33013: 24 [ 1165/ 1327], train_loss/perplexity = 3.79133749/44.3156319 secs/batch = 0.3010s, grad.norm=15.65202332
 33018: 24 [ 1170/ 1327], train_loss/perplexity = 3.68257976/39.7488060 secs/batch = 0.2982s, grad.norm=15.99539661
 33023: 24 [ 1175/ 1327], train_loss/perplexity = 3.41903663/30.5399799 secs/batch = 0.2957s, grad.norm=15.22096157
 33028: 24 [ 1180/ 1327], train_loss/perplexity = 3.46354699/31.9300308 secs/batch = 0.2951s, grad.norm=15.56869221
 33033: 24 [ 1185/ 1327], train_loss/perplexity = 3.63466167/37.8890305 secs/batch = 0.2963s, grad.norm=15.49384689
 33038: 24 [ 1190/ 1327], train_loss/perplexity = 3.68696451/39.9234772 secs/batch = 0.2961s, grad.norm=15.57760811
 33043: 24 [ 1195/ 1327], train_loss/perplexity = 3.57452059/35.6775131 secs/batch = 0.3017s, grad.norm=15.12548161
 33048: 24 [ 1200/ 1327], train_loss/perplexity = 3.54491425/34.6367149 secs/batch = 0.2955s, grad.norm=15.66941738
 33053: 24 [ 1205/ 1327], train_loss/perplexity = 3.53344965/34.2418861 secs/batch = 0.2947s, grad.norm=15.49614906
 33058: 24 [ 1210/ 1327], train_loss/perplexity = 3.09580231/22.1049671 secs/batch = 0.2990s, grad.norm=15.40085030
 33063: 24 [ 1215/ 1327], train_loss/perplexity = 3.38439679/29.5001926 secs/batch = 0.2961s, grad.norm=15.01716709
 33068: 24 [ 1220/ 1327], train_loss/perplexity = 3.46017361/31.8225002 secs/batch = 0.3010s, grad.norm=15.57623577
 33073: 24 [ 1225/ 1327], train_loss/perplexity = 3.25503492/25.9205189 secs/batch = 0.3004s, grad.norm=16.15393257
 33078: 24 [ 1230/ 1327], train_loss/perplexity = 3.57160473/35.5736351 secs/batch = 0.2999s, grad.norm=15.14149857
 33083: 24 [ 1235/ 1327], train_loss/perplexity = 3.50320315/33.2216949 secs/batch = 0.2952s, grad.norm=15.30995941
 33088: 24 [ 1240/ 1327], train_loss/perplexity = 3.74028468/42.1099777 secs/batch = 0.3007s, grad.norm=16.30500603
 33093: 24 [ 1245/ 1327], train_loss/perplexity = 3.61774564/37.2534904 secs/batch = 0.3002s, grad.norm=14.89347267
 33098: 24 [ 1250/ 1327], train_loss/perplexity = 3.79285336/44.3828583 secs/batch = 0.3001s, grad.norm=15.09765816
 33103: 24 [ 1255/ 1327], train_loss/perplexity = 3.79278588/44.3798637 secs/batch = 0.2966s, grad.norm=15.03812218
 33108: 24 [ 1260/ 1327], train_loss/perplexity = 3.57812428/35.8063164 secs/batch = 0.2947s, grad.norm=16.29207039
 33113: 24 [ 1265/ 1327], train_loss/perplexity = 3.81922388/45.5688286 secs/batch = 0.2983s, grad.norm=15.79900074
 33118: 24 [ 1270/ 1327], train_loss/perplexity = 3.50394058/33.2462044 secs/batch = 0.3021s, grad.norm=15.76365852
 33123: 24 [ 1275/ 1327], train_loss/perplexity = 3.66685867/39.1287956 secs/batch = 0.2955s, grad.norm=15.77798176
 33128: 24 [ 1280/ 1327], train_loss/perplexity = 3.62969279/37.7012329 secs/batch = 0.2948s, grad.norm=15.91444683
 33133: 24 [ 1285/ 1327], train_loss/perplexity = 3.51743984/33.6980438 secs/batch = 0.2943s, grad.norm=15.58971786
 33138: 24 [ 1290/ 1327], train_loss/perplexity = 3.72452140/41.4513893 secs/batch = 0.2986s, grad.norm=15.35506248
 33143: 24 [ 1295/ 1327], train_loss/perplexity = 3.69116640/40.0915833 secs/batch = 0.2945s, grad.norm=15.10902977
 33148: 24 [ 1300/ 1327], train_loss/perplexity = 3.91054797/49.9263039 secs/batch = 0.2955s, grad.norm=15.09986210
 33153: 24 [ 1305/ 1327], train_loss/perplexity = 3.93687558/51.2581978 secs/batch = 0.2980s, grad.norm=15.86560440
 33158: 24 [ 1310/ 1327], train_loss/perplexity = 4.28261709/72.4297485 secs/batch = 0.2984s, grad.norm=16.26398849
 33163: 24 [ 1315/ 1327], train_loss/perplexity = 3.98266625/53.6599159 secs/batch = 0.2946s, grad.norm=15.68108749
 33168: 24 [ 1320/ 1327], train_loss/perplexity = 3.94694710/51.7770538 secs/batch = 0.3002s, grad.norm=15.61003208
 33173: 24 [ 1325/ 1327], train_loss/perplexity = 3.86797667/47.8454819 secs/batch = 0.3007s, grad.norm=16.21513748
Epoch training time: 393.73092436790466
	> validation loss = 4.56384945, perplexity = 95.95213318
	> validation loss = 4.50647449, perplexity = 90.60183716
	> validation loss = 4.50031042, perplexity = 90.04508209
	> validation loss = 4.47823763, perplexity = 88.07930756
	> validation loss = 4.63701677, perplexity = 103.23590851
	> validation loss = 4.59302425, perplexity = 98.79275513
	> validation loss = 4.52859068, perplexity = 92.62792969
	> validation loss = 4.36699629, perplexity = 78.80656433
	> validation loss = 4.15289116, perplexity = 63.61766434
	> validation loss = 4.29251528, perplexity = 73.15023041
	> validation loss = 4.50009537, perplexity = 90.02571869
	> validation loss = 4.45429516, perplexity = 85.99551392
	> validation loss = 4.40151072, perplexity = 81.57401276
	> validation loss = 4.12987328, perplexity = 62.17004395
	> validation loss = 4.13100195, perplexity = 62.24025345
	> validation loss = 4.13195515, perplexity = 62.29961014
	> validation loss = 4.57838392, perplexity = 97.35693359
	> validation loss = 4.03231192, perplexity = 56.39113235
	> validation loss = 4.54637384, perplexity = 94.28987885
	> validation loss = 4.50960255, perplexity = 90.88568878
	> validation loss = 4.21402359, perplexity = 67.62809753
at the end of epoch: 24
train loss = 3.82871073, perplexity = 46.00318955
validation loss = 4.39894798, perplexity = 81.36522571
Saved model cv/epoch024_4.3989.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.00195312
new learning rate is: 0.0009765625
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.7.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.7.5 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:910] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:1e.0
Total memory: 11.17GiB
Free memory: 11.11GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0)
reading train
reading valid
reading test

actual longest token length is: 21
size of word vocabulary: 10000
size of char vocabulary: 51
number of tokens in train: 929589
number of tokens in valid: 73760
number of tokens in test: 82430
initialized all dataset readers
Created and initialized fresh model. Size: 10178344
     5: 0 [    5/ 1327], train_loss/perplexity = 8.16208458/3505.4865723 secs/batch = 0.1995s, grad.norm=39.03876495
    10: 0 [   10/ 1327], train_loss/perplexity = 7.54329681/1888.0443115 secs/batch = 0.1990s, grad.norm=9.47731972
    15: 0 [   15/ 1327], train_loss/perplexity = 7.43823624/1699.7496338 secs/batch = 0.1997s, grad.norm=7.27399874
    20: 0 [   20/ 1327], train_loss/perplexity = 7.44970942/1719.3634033 secs/batch = 0.2000s, grad.norm=6.26370764
    25: 0 [   25/ 1327], train_loss/perplexity = 7.39333344/1625.1142578 secs/batch = 0.1996s, grad.norm=10.90227127
    30: 0 [   30/ 1327], train_loss/perplexity = 6.95811176/1051.6458740 secs/batch = 0.1967s, grad.norm=6.19800472
    35: 0 [   35/ 1327], train_loss/perplexity = 6.95462513/1047.9855957 secs/batch = 0.1996s, grad.norm=8.06376743
    40: 0 [   40/ 1327], train_loss/perplexity = 6.96051455/1054.1757812 secs/batch = 0.2004s, grad.norm=5.69570637
    45: 0 [   45/ 1327], train_loss/perplexity = 6.89072847/983.1173096 secs/batch = 0.2012s, grad.norm=5.28553104
    50: 0 [   50/ 1327], train_loss/perplexity = 7.09793091/1209.4620361 secs/batch = 0.1991s, grad.norm=4.91061974
    55: 0 [   55/ 1327], train_loss/perplexity = 7.09743643/1208.8641357 secs/batch = 0.1935s, grad.norm=5.20550108
    60: 0 [   60/ 1327], train_loss/perplexity = 6.95526600/1048.6574707 secs/batch = 0.1995s, grad.norm=4.09632635
    65: 0 [   65/ 1327], train_loss/perplexity = 7.00643158/1103.7089844 secs/batch = 0.2000s, grad.norm=14.98110962
    70: 0 [   70/ 1327], train_loss/perplexity = 6.90184927/994.1113892 secs/batch = 0.1999s, grad.norm=5.06612110
    75: 0 [   75/ 1327], train_loss/perplexity = 6.91847658/1010.7789917 secs/batch = 0.1989s, grad.norm=12.15030861
    80: 0 [   80/ 1327], train_loss/perplexity = 6.88587761/978.3599243 secs/batch = 0.1990s, grad.norm=10.79444218
    85: 0 [   85/ 1327], train_loss/perplexity = 7.21368313/1357.8842773 secs/batch = 0.1925s, grad.norm=20.96580887
    90: 0 [   90/ 1327], train_loss/perplexity = 6.89255857/984.9182129 secs/batch = 0.1964s, grad.norm=6.37804222
    95: 0 [   95/ 1327], train_loss/perplexity = 6.78397369/883.5728149 secs/batch = 0.1963s, grad.norm=4.06388950
   100: 0 [  100/ 1327], train_loss/perplexity = 6.91955233/1011.8668823 secs/batch = 0.1951s, grad.norm=4.36675882
   105: 0 [  105/ 1327], train_loss/perplexity = 7.05979252/1164.2036133 secs/batch = 0.2008s, grad.norm=7.89655161
   110: 0 [  110/ 1327], train_loss/perplexity = 6.68737841/802.2164307 secs/batch = 0.1999s, grad.norm=5.41932631
   115: 0 [  115/ 1327], train_loss/perplexity = 6.52611685/682.7418823 secs/batch = 0.2002s, grad.norm=4.83156300
   120: 0 [  120/ 1327], train_loss/perplexity = 6.89629173/988.6019287 secs/batch = 0.1993s, grad.norm=4.07505035
   125: 0 [  125/ 1327], train_loss/perplexity = 6.84860039/942.5607910 secs/batch = 0.1996s, grad.norm=3.29378152
   130: 0 [  130/ 1327], train_loss/perplexity = 6.86685371/959.9236450 secs/batch = 0.1998s, grad.norm=6.46418715
   135: 0 [  135/ 1327], train_loss/perplexity = 6.76334286/865.5307007 secs/batch = 0.2003s, grad.norm=3.98746109
   140: 0 [  140/ 1327], train_loss/perplexity = 6.81732798/913.5407715 secs/batch = 0.1993s, grad.norm=3.07809091
   145: 0 [  145/ 1327], train_loss/perplexity = 6.92191744/1014.2629395 secs/batch = 0.1996s, grad.norm=4.95808601
   150: 0 [  150/ 1327], train_loss/perplexity = 6.67966175/796.0498047 secs/batch = 0.1995s, grad.norm=2.67097139
   155: 0 [  155/ 1327], train_loss/perplexity = 6.78733110/886.5443115 secs/batch = 0.2001s, grad.norm=4.51471424
   160: 0 [  160/ 1327], train_loss/perplexity = 6.60845423/741.3361816 secs/batch = 0.2001s, grad.norm=8.30886459
   165: 0 [  165/ 1327], train_loss/perplexity = 6.79388952/892.3777466 secs/batch = 0.1999s, grad.norm=6.30701542
   170: 0 [  170/ 1327], train_loss/perplexity = 6.77346992/874.3405151 secs/batch = 0.2008s, grad.norm=5.20951843
   175: 0 [  175/ 1327], train_loss/perplexity = 6.86730623/960.3580933 secs/batch = 0.1980s, grad.norm=3.47314382
   180: 0 [  180/ 1327], train_loss/perplexity = 6.76241684/864.7296143 secs/batch = 0.2002s, grad.norm=2.96127129
   185: 0 [  185/ 1327], train_loss/perplexity = 6.94833612/1041.4155273 secs/batch = 0.2002s, grad.norm=4.58066320
   190: 0 [  190/ 1327], train_loss/perplexity = 6.75543880/858.7164917 secs/batch = 0.1992s, grad.norm=3.52286720
   195: 0 [  195/ 1327], train_loss/perplexity = 6.70938206/820.0637207 secs/batch = 0.1951s, grad.norm=4.30887985
   200: 0 [  200/ 1327], train_loss/perplexity = 6.80876684/905.7531738 secs/batch = 0.1924s, grad.norm=4.43780708
   205: 0 [  205/ 1327], train_loss/perplexity = 6.85371494/947.3939209 secs/batch = 0.1996s, grad.norm=13.48111439
   210: 0 [  210/ 1327], train_loss/perplexity = 6.58009815/720.6100464 secs/batch = 0.1998s, grad.norm=4.44138384
   215: 0 [  215/ 1327], train_loss/perplexity = 6.73498201/841.3283691 secs/batch = 0.1935s, grad.norm=4.67292452
   220: 0 [  220/ 1327], train_loss/perplexity = 6.68587828/801.0138550 secs/batch = 0.1997s, grad.norm=3.73314881
   225: 0 [  225/ 1327], train_loss/perplexity = 6.78783321/886.9895630 secs/batch = 0.1996s, grad.norm=4.53892374
   230: 0 [  230/ 1327], train_loss/perplexity = 6.62591362/754.3931274 secs/batch = 0.2014s, grad.norm=3.98652220
   235: 0 [  235/ 1327], train_loss/perplexity = 6.84168434/936.0644531 secs/batch = 0.1947s, grad.norm=11.04157925
   240: 0 [  240/ 1327], train_loss/perplexity = 6.57816839/719.2207642 secs/batch = 0.1999s, grad.norm=5.59043694
   245: 0 [  245/ 1327], train_loss/perplexity = 6.77114630/872.3112793 secs/batch = 0.1991s, grad.norm=3.85905027
   250: 0 [  250/ 1327], train_loss/perplexity = 6.51559544/675.5961304 secs/batch = 0.2003s, grad.norm=4.87419367
   255: 0 [  255/ 1327], train_loss/perplexity = 6.66566181/784.9827881 secs/batch = 0.2000s, grad.norm=3.90953088
   260: 0 [  260/ 1327], train_loss/perplexity = 6.81222153/908.8876953 secs/batch = 0.1994s, grad.norm=4.08014584
   265: 0 [  265/ 1327], train_loss/perplexity = 6.64125824/766.0582886 secs/batch = 0.1991s, grad.norm=6.58506441
   270: 0 [  270/ 1327], train_loss/perplexity = 6.61750650/748.0774536 secs/batch = 0.1937s, grad.norm=5.48550177
   275: 0 [  275/ 1327], train_loss/perplexity = 6.84759617/941.6146851 secs/batch = 0.2002s, grad.norm=4.10916615
   280: 0 [  280/ 1327], train_loss/perplexity = 6.54598761/696.4441528 secs/batch = 0.1993s, grad.norm=4.25876665
   285: 0 [  285/ 1327], train_loss/perplexity = 6.65687370/778.1145020 secs/batch = 0.1993s, grad.norm=3.60040545
   290: 0 [  290/ 1327], train_loss/perplexity = 6.70595169/817.2554321 secs/batch = 0.1966s, grad.norm=4.52937078
   295: 0 [  295/ 1327], train_loss/perplexity = 6.53331375/687.6732178 secs/batch = 0.1997s, grad.norm=5.02602577
   300: 0 [  300/ 1327], train_loss/perplexity = 6.32249975/556.9635010 secs/batch = 0.1994s, grad.norm=5.02262115
   305: 0 [  305/ 1327], train_loss/perplexity = 6.70195103/813.9923706 secs/batch = 0.2003s, grad.norm=17.59013939
   310: 0 [  310/ 1327], train_loss/perplexity = 6.67305183/790.8053589 secs/batch = 0.1998s, grad.norm=7.92907667
   315: 0 [  315/ 1327], train_loss/perplexity = 6.39529514/599.0200806 secs/batch = 0.1927s, grad.norm=5.29178286
   320: 0 [  320/ 1327], train_loss/perplexity = 6.63862038/764.0401611 secs/batch = 0.2001s, grad.norm=5.43829298
   325: 0 [  325/ 1327], train_loss/perplexity = 6.33403683/563.4264526 secs/batch = 0.1994s, grad.norm=15.82923222
   330: 0 [  330/ 1327], train_loss/perplexity = 6.55605841/703.4933472 secs/batch = 0.1954s, grad.norm=3.62145877
   335: 0 [  335/ 1327], train_loss/perplexity = 5.81611824/335.6665344 secs/batch = 0.1984s, grad.norm=5.23685932
   340: 0 [  340/ 1327], train_loss/perplexity = 6.62155962/751.1156616 secs/batch = 0.2001s, grad.norm=7.22115040
   345: 0 [  345/ 1327], train_loss/perplexity = 6.35175610/573.4989624 secs/batch = 0.2005s, grad.norm=3.75767040
   350: 0 [  350/ 1327], train_loss/perplexity = 6.39976931/601.7062378 secs/batch = 0.1999s, grad.norm=3.98932433
   355: 0 [  355/ 1327], train_loss/perplexity = 6.64429188/768.3857422 secs/batch = 0.1988s, grad.norm=9.20577526
   360: 0 [  360/ 1327], train_loss/perplexity = 6.53021622/685.5464478 secs/batch = 0.1987s, grad.norm=3.80691409
   365: 0 [  365/ 1327], train_loss/perplexity = 6.42583179/617.5942993 secs/batch = 0.1995s, grad.norm=3.35233021
   370: 0 [  370/ 1327], train_loss/perplexity = 6.56800365/711.9471436 secs/batch = 0.1999s, grad.norm=5.63955879
   375: 0 [  375/ 1327], train_loss/perplexity = 6.08276272/438.2382507 secs/batch = 0.1967s, grad.norm=3.79499006
   380: 0 [  380/ 1327], train_loss/perplexity = 6.36750984/582.6052246 secs/batch = 0.2005s, grad.norm=4.30298853
   385: 0 [  385/ 1327], train_loss/perplexity = 6.43975639/626.2542114 secs/batch = 0.1996s, grad.norm=5.64718723
   390: 0 [  390/ 1327], train_loss/perplexity = 6.44193840/627.6221924 secs/batch = 0.1982s, grad.norm=7.25208998
   395: 0 [  395/ 1327], train_loss/perplexity = 6.58371592/723.2218018 secs/batch = 0.1950s, grad.norm=5.09309816
   400: 0 [  400/ 1327], train_loss/perplexity = 6.25204659/519.0740967 secs/batch = 0.1994s, grad.norm=6.19641018
   405: 0 [  405/ 1327], train_loss/perplexity = 6.41837358/613.0053101 secs/batch = 0.1988s, grad.norm=4.38304424
   410: 0 [  410/ 1327], train_loss/perplexity = 6.30652428/548.1364746 secs/batch = 0.1990s, grad.norm=4.79624557
   415: 0 [  415/ 1327], train_loss/perplexity = 6.25315285/519.6486206 secs/batch = 0.1994s, grad.norm=4.92551184
   420: 0 [  420/ 1327], train_loss/perplexity = 6.17298985/479.6179504 secs/batch = 0.1995s, grad.norm=5.35549021
   425: 0 [  425/ 1327], train_loss/perplexity = 6.53635502/689.7678223 secs/batch = 0.2008s, grad.norm=4.99194479
   430: 0 [  430/ 1327], train_loss/perplexity = 6.38786364/594.5849609 secs/batch = 0.1998s, grad.norm=5.69925547
   435: 0 [  435/ 1327], train_loss/perplexity = 6.46313143/641.0653687 secs/batch = 0.1992s, grad.norm=7.16843557
   440: 0 [  440/ 1327], train_loss/perplexity = 6.36118841/578.9339600 secs/batch = 0.1997s, grad.norm=10.80729961
   445: 0 [  445/ 1327], train_loss/perplexity = 6.32837105/560.2432251 secs/batch = 0.1991s, grad.norm=8.86659431
   450: 0 [  450/ 1327], train_loss/perplexity = 6.18877792/487.2502747 secs/batch = 0.1992s, grad.norm=6.06418228
   455: 0 [  455/ 1327], train_loss/perplexity = 5.88013029/357.8558655 secs/batch = 0.2000s, grad.norm=4.71921158
   460: 0 [  460/ 1327], train_loss/perplexity = 6.15426254/470.7195740 secs/batch = 0.1996s, grad.norm=5.24128246
   465: 0 [  465/ 1327], train_loss/perplexity = 6.14105320/464.5425720 secs/batch = 0.1994s, grad.norm=8.16402149
   470: 0 [  470/ 1327], train_loss/perplexity = 6.42824841/619.0886230 secs/batch = 0.1996s, grad.norm=7.78731728
   475: 0 [  475/ 1327], train_loss/perplexity = 6.34060049/567.1367798 secs/batch = 0.1936s, grad.norm=4.72081566
   480: 0 [  480/ 1327], train_loss/perplexity = 6.27165222/529.3512573 secs/batch = 0.1993s, grad.norm=5.61742687
   485: 0 [  485/ 1327], train_loss/perplexity = 6.01851845/410.9692688 secs/batch = 0.1996s, grad.norm=4.68291616
   490: 0 [  490/ 1327], train_loss/perplexity = 6.20416307/494.8046570 secs/batch = 0.1992s, grad.norm=6.63497639
   495: 0 [  495/ 1327], train_loss/perplexity = 5.90643644/367.3945923 secs/batch = 0.2007s, grad.norm=9.17120171
   500: 0 [  500/ 1327], train_loss/perplexity = 6.33095694/561.6938477 secs/batch = 0.1932s, grad.norm=7.28437567
   505: 0 [  505/ 1327], train_loss/perplexity = 6.13030052/459.5742493 secs/batch = 0.1955s, grad.norm=7.47810221
   510: 0 [  510/ 1327], train_loss/perplexity = 6.34903622/571.9412231 secs/batch = 0.1994s, grad.norm=6.25024223
   515: 0 [  515/ 1327], train_loss/perplexity = 6.04683161/422.7713928 secs/batch = 0.1987s, grad.norm=5.74517632
   520: 0 [  520/ 1327], train_loss/perplexity = 6.36556864/581.4754028 secs/batch = 0.2005s, grad.norm=8.25802135
   525: 0 [  525/ 1327], train_loss/perplexity = 6.10502148/448.1022644 secs/batch = 0.1987s, grad.norm=5.55146027
   530: 0 [  530/ 1327], train_loss/perplexity = 6.00859356/406.9106140 secs/batch = 0.1997s, grad.norm=5.82498693
   535: 0 [  535/ 1327], train_loss/perplexity = 6.13153839/460.1434937 secs/batch = 0.1991s, grad.norm=6.00982666
   540: 0 [  540/ 1327], train_loss/perplexity = 6.04997921/424.1042175 secs/batch = 0.1991s, grad.norm=5.15321255
   545: 0 [  545/ 1327], train_loss/perplexity = 6.21858120/501.9905090 secs/batch = 0.1992s, grad.norm=6.80997896
   550: 0 [  550/ 1327], train_loss/perplexity = 6.11261797/451.5192261 secs/batch = 0.1999s, grad.norm=6.54548883
   555: 0 [  555/ 1327], train_loss/perplexity = 6.06219912/429.3185120 secs/batch = 0.2000s, grad.norm=6.40332508
   560: 0 [  560/ 1327], train_loss/perplexity = 6.08029747/437.1592102 secs/batch = 0.1995s, grad.norm=6.70685482
   565: 0 [  565/ 1327], train_loss/perplexity = 6.09813547/445.0272217 secs/batch = 0.1993s, grad.norm=6.10411739
   570: 0 [  570/ 1327], train_loss/perplexity = 5.94196701/380.6830139 secs/batch = 0.1993s, grad.norm=7.17697287
   575: 0 [  575/ 1327], train_loss/perplexity = 5.94992208/383.7234497 secs/batch = 0.2005s, grad.norm=6.70040655
   580: 0 [  580/ 1327], train_loss/perplexity = 6.04178238/420.6421204 secs/batch = 0.2000s, grad.norm=6.11177158
   585: 0 [  585/ 1327], train_loss/perplexity = 5.75953960/317.2022705 secs/batch = 0.2001s, grad.norm=6.09596682
   590: 0 [  590/ 1327], train_loss/perplexity = 6.10594654/448.5169678 secs/batch = 0.1973s, grad.norm=5.64919949
   595: 0 [  595/ 1327], train_loss/perplexity = 5.98022032/395.5274963 secs/batch = 0.1996s, grad.norm=6.26102304
   600: 0 [  600/ 1327], train_loss/perplexity = 6.20861912/497.0144653 secs/batch = 0.1982s, grad.norm=6.67423630
   605: 0 [  605/ 1327], train_loss/perplexity = 6.13310146/460.8633118 secs/batch = 0.1947s, grad.norm=5.80543804
   610: 0 [  610/ 1327], train_loss/perplexity = 6.22647858/505.9706116 secs/batch = 0.1966s, grad.norm=5.72998095
   615: 0 [  615/ 1327], train_loss/perplexity = 5.57995176/265.0588074 secs/batch = 0.1990s, grad.norm=5.91396761
   620: 0 [  620/ 1327], train_loss/perplexity = 5.88611221/360.0029602 secs/batch = 0.2000s, grad.norm=6.67314005
   625: 0 [  625/ 1327], train_loss/perplexity = 6.07504559/434.8693237 secs/batch = 0.1995s, grad.norm=5.67966413
   630: 0 [  630/ 1327], train_loss/perplexity = 6.03193378/416.5197144 secs/batch = 0.1952s, grad.norm=5.71587229
   635: 0 [  635/ 1327], train_loss/perplexity = 5.87861395/357.3136292 secs/batch = 0.1962s, grad.norm=6.90874577
   640: 0 [  640/ 1327], train_loss/perplexity = 6.13339520/460.9986877 secs/batch = 0.1935s, grad.norm=10.41341400
   645: 0 [  645/ 1327], train_loss/perplexity = 6.15481997/470.9820557 secs/batch = 0.1991s, grad.norm=8.43676281
   650: 0 [  650/ 1327], train_loss/perplexity = 5.86276579/351.6955261 secs/batch = 0.1993s, grad.norm=7.31160355
   655: 0 [  655/ 1327], train_loss/perplexity = 5.77751970/322.9571533 secs/batch = 0.2000s, grad.norm=6.80492973
   660: 0 [  660/ 1327], train_loss/perplexity = 5.83406353/341.7445374 secs/batch = 0.1988s, grad.norm=7.54266119
   665: 0 [  665/ 1327], train_loss/perplexity = 5.95119095/384.2106323 secs/batch = 0.1988s, grad.norm=6.25972605
   670: 0 [  670/ 1327], train_loss/perplexity = 5.85785961/349.9742737 secs/batch = 0.1983s, grad.norm=7.49013090
   675: 0 [  675/ 1327], train_loss/perplexity = 5.60643911/272.1733398 secs/batch = 0.1998s, grad.norm=7.49205399
   680: 0 [  680/ 1327], train_loss/perplexity = 5.93821716/379.2581787 secs/batch = 0.1987s, grad.norm=6.54874849
   685: 0 [  685/ 1327], train_loss/perplexity = 5.90624952/367.3259277 secs/batch = 0.1981s, grad.norm=6.59796810
   690: 0 [  690/ 1327], train_loss/perplexity = 6.00130272/403.9546814 secs/batch = 0.1990s, grad.norm=6.90642214
   695: 0 [  695/ 1327], train_loss/perplexity = 5.74554825/312.7950745 secs/batch = 0.1975s, grad.norm=6.75284290
   700: 0 [  700/ 1327], train_loss/perplexity = 5.97322893/392.7718506 secs/batch = 0.2004s, grad.norm=5.92963743
   705: 0 [  705/ 1327], train_loss/perplexity = 5.72293091/305.7998962 secs/batch = 0.2002s, grad.norm=6.77077246
   710: 0 [  710/ 1327], train_loss/perplexity = 5.87005091/354.2670288 secs/batch = 0.2015s, grad.norm=7.36131144
   715: 0 [  715/ 1327], train_loss/perplexity = 5.72722721/307.1165161 secs/batch = 0.1985s, grad.norm=6.82969856
   720: 0 [  720/ 1327], train_loss/perplexity = 5.79229927/327.7657776 secs/batch = 0.1993s, grad.norm=7.23947811
   725: 0 [  725/ 1327], train_loss/perplexity = 5.54801416/256.7272339 secs/batch = 0.1999s, grad.norm=8.25407696
   730: 0 [  730/ 1327], train_loss/perplexity = 5.74212742/311.7268677 secs/batch = 0.2003s, grad.norm=6.99940157
   735: 0 [  735/ 1327], train_loss/perplexity = 5.81889772/336.6008301 secs/batch = 0.1990s, grad.norm=6.66076946
   740: 0 [  740/ 1327], train_loss/perplexity = 5.31337595/203.0345001 secs/batch = 0.1996s, grad.norm=7.27627277
   745: 0 [  745/ 1327], train_loss/perplexity = 5.77976847/323.6842346 secs/batch = 0.1981s, grad.norm=6.53191090
   750: 0 [  750/ 1327], train_loss/perplexity = 5.62912703/278.4189453 secs/batch = 0.1988s, grad.norm=7.29590511
   755: 0 [  755/ 1327], train_loss/perplexity = 5.59501266/269.0810547 secs/batch = 0.2007s, grad.norm=6.25600195
   760: 0 [  760/ 1327], train_loss/perplexity = 5.57903910/264.8170166 secs/batch = 0.2003s, grad.norm=6.87158346
   765: 0 [  765/ 1327], train_loss/perplexity = 5.56356525/260.7508240 secs/batch = 0.2005s, grad.norm=7.55596495
   770: 0 [  770/ 1327], train_loss/perplexity = 5.68768024/295.2080078 secs/batch = 0.1997s, grad.norm=7.09534740
   775: 0 [  775/ 1327], train_loss/perplexity = 5.70903540/301.5800171 secs/batch = 0.2007s, grad.norm=6.84724522
   780: 0 [  780/ 1327], train_loss/perplexity = 5.88174868/358.4354858 secs/batch = 0.1999s, grad.norm=7.16147757
   785: 0 [  785/ 1327], train_loss/perplexity = 5.72715998/307.0958557 secs/batch = 0.2004s, grad.norm=6.94310188
   790: 0 [  790/ 1327], train_loss/perplexity = 5.52331781/250.4646606 secs/batch = 0.1996s, grad.norm=7.57451773
   795: 0 [  795/ 1327], train_loss/perplexity = 5.75556278/315.9432983 secs/batch = 0.2002s, grad.norm=6.92718554
   800: 0 [  800/ 1327], train_loss/perplexity = 5.81639338/335.7589111 secs/batch = 0.1980s, grad.norm=8.02951050
   805: 0 [  805/ 1327], train_loss/perplexity = 6.08121204/437.5592041 secs/batch = 0.1974s, grad.norm=7.51932669
   810: 0 [  810/ 1327], train_loss/perplexity = 5.81521702/335.3641663 secs/batch = 0.1992s, grad.norm=7.51880741
   815: 0 [  815/ 1327], train_loss/perplexity = 5.63572836/280.2629700 secs/batch = 0.1991s, grad.norm=6.51709366
   820: 0 [  820/ 1327], train_loss/perplexity = 5.21260214/183.5711212 secs/batch = 0.2000s, grad.norm=6.82135677
   825: 0 [  825/ 1327], train_loss/perplexity = 5.39944839/221.2843170 secs/batch = 0.2001s, grad.norm=6.58143997
   830: 0 [  830/ 1327], train_loss/perplexity = 5.29427958/199.1940765 secs/batch = 0.1996s, grad.norm=7.88470459
   835: 0 [  835/ 1327], train_loss/perplexity = 5.60540581/271.8922424 secs/batch = 0.2004s, grad.norm=6.98775578
   840: 0 [  840/ 1327], train_loss/perplexity = 5.69579077/297.6120300 secs/batch = 0.2003s, grad.norm=6.65947199
   845: 0 [  845/ 1327], train_loss/perplexity = 5.52365112/250.5481567 secs/batch = 0.1996s, grad.norm=6.73484612
   850: 0 [  850/ 1327], train_loss/perplexity = 5.59707880/269.6376038 secs/batch = 0.1957s, grad.norm=7.58403826
   855: 0 [  855/ 1327], train_loss/perplexity = 5.55242062/257.8609924 secs/batch = 0.1962s, grad.norm=7.65059614
   860: 0 [  860/ 1327], train_loss/perplexity = 5.31085777/202.5238800 secs/batch = 0.1997s, grad.norm=7.41594219
   865: 0 [  865/ 1327], train_loss/perplexity = 5.74000168/311.0649414 secs/batch = 0.1993s, grad.norm=7.17544842
   870: 0 [  870/ 1327], train_loss/perplexity = 5.82767391/339.5679016 secs/batch = 0.1997s, grad.norm=7.73329782
   875: 0 [  875/ 1327], train_loss/perplexity = 5.34501410/209.5608368 secs/batch = 0.1997s, grad.norm=7.39279509
   880: 0 [  880/ 1327], train_loss/perplexity = 5.42190742/226.3103790 secs/batch = 0.2005s, grad.norm=7.11140299
   885: 0 [  885/ 1327], train_loss/perplexity = 5.50061131/244.8415527 secs/batch = 0.1988s, grad.norm=8.81413460
   890: 0 [  890/ 1327], train_loss/perplexity = 5.72431564/306.2236328 secs/batch = 0.1990s, grad.norm=7.57423878
   895: 0 [  895/ 1327], train_loss/perplexity = 5.80514097/332.0019836 secs/batch = 0.1991s, grad.norm=10.59333324
   900: 0 [  900/ 1327], train_loss/perplexity = 5.64747143/283.5735168 secs/batch = 0.1984s, grad.norm=7.57527065
   905: 0 [  905/ 1327], train_loss/perplexity = 5.45553589/234.0502625 secs/batch = 0.1992s, grad.norm=6.87781477
   910: 0 [  910/ 1327], train_loss/perplexity = 5.61946344/275.7413940 secs/batch = 0.1998s, grad.norm=8.93544292
   915: 0 [  915/ 1327], train_loss/perplexity = 5.84314346/344.8616943 secs/batch = 0.1996s, grad.norm=6.70366859
   920: 0 [  920/ 1327], train_loss/perplexity = 5.90210676/365.8073120 secs/batch = 0.1993s, grad.norm=7.82641268
   925: 0 [  925/ 1327], train_loss/perplexity = 5.63458252/279.9420166 secs/batch = 0.1951s, grad.norm=7.81401730
   930: 0 [  930/ 1327], train_loss/perplexity = 5.56861210/262.0701294 secs/batch = 0.1991s, grad.norm=7.81742620
   935: 0 [  935/ 1327], train_loss/perplexity = 5.59537363/269.1781921 secs/batch = 0.1993s, grad.norm=7.13947630
   940: 0 [  940/ 1327], train_loss/perplexity = 5.56569242/261.3060608 secs/batch = 0.1992s, grad.norm=8.35393715
   945: 0 [  945/ 1327], train_loss/perplexity = 5.79648781/329.1415100 secs/batch = 0.1979s, grad.norm=7.55709743
   950: 0 [  950/ 1327], train_loss/perplexity = 5.49710083/243.9835510 secs/batch = 0.1993s, grad.norm=7.51941347
   955: 0 [  955/ 1327], train_loss/perplexity = 5.69939518/298.6867065 secs/batch = 0.1988s, grad.norm=7.41972208
   960: 0 [  960/ 1327], train_loss/perplexity = 5.84575987/345.7651672 secs/batch = 0.1938s, grad.norm=7.07052088
   965: 0 [  965/ 1327], train_loss/perplexity = 5.58110762/265.3653564 secs/batch = 0.1997s, grad.norm=7.46676302
   970: 0 [  970/ 1327], train_loss/perplexity = 5.79357958/328.1856995 secs/batch = 0.1986s, grad.norm=6.64825201
   975: 0 [  975/ 1327], train_loss/perplexity = 5.56969166/262.3531799 secs/batch = 0.1998s, grad.norm=7.31400204
   980: 0 [  980/ 1327], train_loss/perplexity = 5.35666656/212.0170288 secs/batch = 0.1959s, grad.norm=7.57309628
   985: 0 [  985/ 1327], train_loss/perplexity = 5.58606672/266.6846008 secs/batch = 0.1998s, grad.norm=8.00732613
   990: 0 [  990/ 1327], train_loss/perplexity = 5.74049664/311.2189331 secs/batch = 0.1993s, grad.norm=7.32943249
   995: 0 [  995/ 1327], train_loss/perplexity = 5.70919323/301.6276245 secs/batch = 0.1991s, grad.norm=7.67590189
  1000: 0 [ 1000/ 1327], train_loss/perplexity = 5.14862537/172.1946259 secs/batch = 0.1991s, grad.norm=7.69565964
  1005: 0 [ 1005/ 1327], train_loss/perplexity = 5.69073868/296.1122742 secs/batch = 0.2001s, grad.norm=8.47617626
  1010: 0 [ 1010/ 1327], train_loss/perplexity = 5.21719265/184.4157410 secs/batch = 0.2000s, grad.norm=7.57059622
  1015: 0 [ 1015/ 1327], train_loss/perplexity = 5.62969398/278.5768433 secs/batch = 0.1993s, grad.norm=7.37786674
  1020: 0 [ 1020/ 1327], train_loss/perplexity = 5.89352942/362.6830750 secs/batch = 0.1991s, grad.norm=7.82153082
  1025: 0 [ 1025/ 1327], train_loss/perplexity = 5.66618872/288.9312439 secs/batch = 0.1982s, grad.norm=7.37043762
  1030: 0 [ 1030/ 1327], train_loss/perplexity = 5.51951265/249.5134125 secs/batch = 0.2002s, grad.norm=7.65149403
  1035: 0 [ 1035/ 1327], train_loss/perplexity = 5.42188549/226.3054199 secs/batch = 0.1992s, grad.norm=7.54992437
  1040: 0 [ 1040/ 1327], train_loss/perplexity = 5.66878843/289.6833496 secs/batch = 0.1998s, grad.norm=7.08559370
  1045: 0 [ 1045/ 1327], train_loss/perplexity = 5.28494453/197.3432312 secs/batch = 0.1997s, grad.norm=7.70153379
  1050: 0 [ 1050/ 1327], train_loss/perplexity = 5.33102798/206.6502991 secs/batch = 0.1999s, grad.norm=8.79002857
  1055: 0 [ 1055/ 1327], train_loss/perplexity = 5.54340029/255.5454559 secs/batch = 0.1991s, grad.norm=8.28565216
  1060: 0 [ 1060/ 1327], train_loss/perplexity = 5.21821117/184.6036682 secs/batch = 0.1995s, grad.norm=8.54090786
  1065: 0 [ 1065/ 1327], train_loss/perplexity = 5.27857542/196.0903320 secs/batch = 0.1921s, grad.norm=7.96417141
  1070: 0 [ 1070/ 1327], train_loss/perplexity = 5.66891623/289.7203674 secs/batch = 0.2001s, grad.norm=7.81719255
  1075: 0 [ 1075/ 1327], train_loss/perplexity = 5.44874716/232.4667358 secs/batch = 0.1992s, grad.norm=8.63961887
  1080: 0 [ 1080/ 1327], train_loss/perplexity = 5.25064945/190.6900787 secs/batch = 0.1991s, grad.norm=7.72455072
  1085: 0 [ 1085/ 1327], train_loss/perplexity = 5.17682457/177.1194916 secs/batch = 0.1975s, grad.norm=7.88854170
  1090: 0 [ 1090/ 1327], train_loss/perplexity = 5.43082619/228.3378143 secs/batch = 0.1991s, grad.norm=8.56716442
  1095: 0 [ 1095/ 1327], train_loss/perplexity = 5.52248240/250.2554932 secs/batch = 0.1994s, grad.norm=8.59450817
  1100: 0 [ 1100/ 1327], train_loss/perplexity = 5.51163340/247.5551453 secs/batch = 0.1992s, grad.norm=8.28496456
  1105: 0 [ 1105/ 1327], train_loss/perplexity = 5.25529861/191.5786896 secs/batch = 0.1997s, grad.norm=8.65474224
  1110: 0 [ 1110/ 1327], train_loss/perplexity = 5.85705948/349.6943359 secs/batch = 0.1989s, grad.norm=7.52022362
  1115: 0 [ 1115/ 1327], train_loss/perplexity = 5.27343464/195.0848541 secs/batch = 0.2000s, grad.norm=7.94132280
  1120: 0 [ 1120/ 1327], train_loss/perplexity = 5.48872280/241.9479980 secs/batch = 0.1998s, grad.norm=7.80145454
  1125: 0 [ 1125/ 1327], train_loss/perplexity = 5.71373415/303.0003967 secs/batch = 0.1947s, grad.norm=7.96553421
  1130: 0 [ 1130/ 1327], train_loss/perplexity = 5.41328526/224.3674774 secs/batch = 0.1986s, grad.norm=7.53956223
  1135: 0 [ 1135/ 1327], train_loss/perplexity = 5.45641136/234.2552643 secs/batch = 0.2005s, grad.norm=8.63227367
  1140: 0 [ 1140/ 1327], train_loss/perplexity = 5.64847231/283.8574829 secs/batch = 0.1934s, grad.norm=7.76449299
  1145: 0 [ 1145/ 1327], train_loss/perplexity = 5.37912941/216.8334198 secs/batch = 0.1986s, grad.norm=8.42222786
  1150: 0 [ 1150/ 1327], train_loss/perplexity = 5.31596947/203.5617676 secs/batch = 0.1996s, grad.norm=7.92323637
  1155: 0 [ 1155/ 1327], train_loss/perplexity = 5.51507664/248.4090118 secs/batch = 0.1998s, grad.norm=7.59702778
  1160: 0 [ 1160/ 1327], train_loss/perplexity = 5.53403854/253.1642609 secs/batch = 0.1999s, grad.norm=8.15350914
  1165: 0 [ 1165/ 1327], train_loss/perplexity = 5.59439182/268.9140625 secs/batch = 0.1990s, grad.norm=7.86293030
  1170: 0 [ 1170/ 1327], train_loss/perplexity = 5.48372364/240.7414703 secs/batch = 0.2008s, grad.norm=8.78412437
  1175: 0 [ 1175/ 1327], train_loss/perplexity = 5.21827984/184.6163483 secs/batch = 0.2005s, grad.norm=8.33004284
  1180: 0 [ 1180/ 1327], train_loss/perplexity = 5.16869640/175.6856689 secs/batch = 0.1996s, grad.norm=8.86057758
  1185: 0 [ 1185/ 1327], train_loss/perplexity = 5.38101625/217.2429352 secs/batch = 0.2005s, grad.norm=8.35665703
  1190: 0 [ 1190/ 1327], train_loss/perplexity = 5.40514374/222.5482025 secs/batch = 0.1992s, grad.norm=8.49069500
  1195: 0 [ 1195/ 1327], train_loss/perplexity = 5.26492023/193.4308777 secs/batch = 0.1993s, grad.norm=8.20860577
  1200: 0 [ 1200/ 1327], train_loss/perplexity = 5.11694002/166.8241119 secs/batch = 0.1992s, grad.norm=8.19620705
  1205: 0 [ 1205/ 1327], train_loss/perplexity = 5.26738787/193.9087830 secs/batch = 0.1997s, grad.norm=8.65904617
  1210: 0 [ 1210/ 1327], train_loss/perplexity = 5.00989246/149.8886108 secs/batch = 0.1988s, grad.norm=8.45117664
  1215: 0 [ 1215/ 1327], train_loss/perplexity = 5.03741407/154.0710754 secs/batch = 0.1996s, grad.norm=8.39423275
  1220: 0 [ 1220/ 1327], train_loss/perplexity = 5.25857639/192.2076721 secs/batch = 0.1958s, grad.norm=9.02623177
  1225: 0 [ 1225/ 1327], train_loss/perplexity = 5.21754932/184.4815216 secs/batch = 0.1990s, grad.norm=9.53572559
  1230: 0 [ 1230/ 1327], train_loss/perplexity = 5.26074076/192.6241302 secs/batch = 0.1999s, grad.norm=8.37743568
  1235: 0 [ 1235/ 1327], train_loss/perplexity = 5.31183434/202.7217407 secs/batch = 0.1919s, grad.norm=8.60500336
  1240: 0 [ 1240/ 1327], train_loss/perplexity = 5.38681364/218.5060425 secs/batch = 0.2000s, grad.norm=7.94802427
  1245: 0 [ 1245/ 1327], train_loss/perplexity = 5.22903204/186.6120758 secs/batch = 0.1997s, grad.norm=9.15631199
  1250: 0 [ 1250/ 1327], train_loss/perplexity = 5.42646551/227.3442841 secs/batch = 0.1993s, grad.norm=8.54681778
  1255: 0 [ 1255/ 1327], train_loss/perplexity = 5.35270119/211.1779633 secs/batch = 0.1987s, grad.norm=8.44635868
  1260: 0 [ 1260/ 1327], train_loss/perplexity = 5.37971783/216.9610443 secs/batch = 0.1941s, grad.norm=9.29726410
  1265: 0 [ 1265/ 1327], train_loss/perplexity = 5.41615295/225.0118256 secs/batch = 0.2000s, grad.norm=8.23213959
  1270: 0 [ 1270/ 1327], train_loss/perplexity = 5.20289707/181.7981567 secs/batch = 0.1986s, grad.norm=8.18999195
  1275: 0 [ 1275/ 1327], train_loss/perplexity = 5.52782536/251.5961761 secs/batch = 0.1998s, grad.norm=8.64507771
  1280: 0 [ 1280/ 1327], train_loss/perplexity = 5.21626282/184.2443390 secs/batch = 0.1961s, grad.norm=8.70888710
  1285: 0 [ 1285/ 1327], train_loss/perplexity = 5.25190496/190.9296417 secs/batch = 0.1991s, grad.norm=8.23304176
  1290: 0 [ 1290/ 1327], train_loss/perplexity = 5.36211300/213.1749115 secs/batch = 0.2002s, grad.norm=8.20923996
  1295: 0 [ 1295/ 1327], train_loss/perplexity = 5.44286346/231.1029968 secs/batch = 0.1988s, grad.norm=8.55881405
  1300: 0 [ 1300/ 1327], train_loss/perplexity = 5.49184132/242.7036896 secs/batch = 0.1992s, grad.norm=8.22818756
  1305: 0 [ 1305/ 1327], train_loss/perplexity = 5.64057159/281.6236572 secs/batch = 0.1986s, grad.norm=8.73430443
  1310: 0 [ 1310/ 1327], train_loss/perplexity = 5.82323885/338.0652161 secs/batch = 0.2005s, grad.norm=7.82084513
  1315: 0 [ 1315/ 1327], train_loss/perplexity = 5.68142080/293.3659363 secs/batch = 0.1995s, grad.norm=8.48326683
  1320: 0 [ 1320/ 1327], train_loss/perplexity = 5.68236637/293.6434631 secs/batch = 0.1958s, grad.norm=8.24250507
  1325: 0 [ 1325/ 1327], train_loss/perplexity = 5.48057079/239.9836426 secs/batch = 0.1986s, grad.norm=8.51194572
Epoch training time: 265.54278230667114
	> validation loss = 5.55145550, perplexity = 257.61224365
	> validation loss = 5.45809031, perplexity = 234.64889526
	> validation loss = 5.37114429, perplexity = 215.10887146
	> validation loss = 5.45234632, perplexity = 233.30493164
	> validation loss = 5.65770197, perplexity = 286.48953247
	> validation loss = 5.46032953, perplexity = 235.17491150
	> validation loss = 5.38006926, perplexity = 217.03730774
	> validation loss = 5.38093281, perplexity = 217.22480774
	> validation loss = 5.59366417, perplexity = 268.71844482
	> validation loss = 5.35846424, perplexity = 212.39849854
	> validation loss = 5.45162487, perplexity = 233.13667297
	> validation loss = 5.49970531, perplexity = 244.61984253
	> validation loss = 5.36004496, perplexity = 212.73451233
	> validation loss = 5.33442211, perplexity = 207.35289001
	> validation loss = 5.09273148, perplexity = 162.83403015
	> validation loss = 5.14086485, perplexity = 170.86347961
	> validation loss = 5.45892429, perplexity = 234.84466553
	> validation loss = 5.12661028, perplexity = 168.44515991
	> validation loss = 5.52816963, perplexity = 251.68281555
	> validation loss = 5.47461796, perplexity = 238.55931091
	> validation loss = 5.30789614, perplexity = 201.92495728
at the end of epoch: 0
train loss = 5.48755793, perplexity = 241.66632028
validation loss = 5.38173436, perplexity = 217.39899624
Saved model cv/epoch000_5.3817.model
  1332: 1 [    5/ 1327], train_loss/perplexity = 5.67033720/290.1323547 secs/batch = 0.1991s, grad.norm=8.28468895
  1337: 1 [   10/ 1327], train_loss/perplexity = 5.30353355/201.0459595 secs/batch = 0.1928s, grad.norm=10.85934544
  1342: 1 [   15/ 1327], train_loss/perplexity = 5.30114222/200.5657654 secs/batch = 0.1995s, grad.norm=8.03460979
  1347: 1 [   20/ 1327], train_loss/perplexity = 5.72402620/306.1350098 secs/batch = 0.1998s, grad.norm=8.65402794
  1352: 1 [   25/ 1327], train_loss/perplexity = 5.51116991/247.4404449 secs/batch = 0.1947s, grad.norm=8.34520245
  1357: 1 [   30/ 1327], train_loss/perplexity = 5.37823629/216.6398468 secs/batch = 0.2006s, grad.norm=8.42480087
  1362: 1 [   35/ 1327], train_loss/perplexity = 5.21158075/183.3837128 secs/batch = 0.1999s, grad.norm=8.19559574
  1367: 1 [   40/ 1327], train_loss/perplexity = 5.35177612/210.9826965 secs/batch = 0.1991s, grad.norm=9.02347088
  1372: 1 [   45/ 1327], train_loss/perplexity = 4.98475742/146.1681213 secs/batch = 0.1988s, grad.norm=8.40948868
  1377: 1 [   50/ 1327], train_loss/perplexity = 5.46123028/235.3868408 secs/batch = 0.2005s, grad.norm=8.83602810
  1382: 1 [   55/ 1327], train_loss/perplexity = 5.31278181/202.9139099 secs/batch = 0.1993s, grad.norm=8.52425575
  1387: 1 [   60/ 1327], train_loss/perplexity = 5.58068371/265.2528992 secs/batch = 0.1996s, grad.norm=9.82909489
  1392: 1 [   65/ 1327], train_loss/perplexity = 5.10222387/164.3870697 secs/batch = 0.1989s, grad.norm=8.61859608
  1397: 1 [   70/ 1327], train_loss/perplexity = 4.93442726/138.9935150 secs/batch = 0.1992s, grad.norm=9.01395226
  1402: 1 [   75/ 1327], train_loss/perplexity = 4.91508818/136.3313293 secs/batch = 0.1989s, grad.norm=8.53965950
  1407: 1 [   80/ 1327], train_loss/perplexity = 5.28850269/198.0466614 secs/batch = 0.1999s, grad.norm=8.07541084
  1412: 1 [   85/ 1327], train_loss/perplexity = 5.35072803/210.7616882 secs/batch = 0.2004s, grad.norm=8.60088348
  1417: 1 [   90/ 1327], train_loss/perplexity = 5.33307076/207.0728760 secs/batch = 0.1988s, grad.norm=9.02014828
  1422: 1 [   95/ 1327], train_loss/perplexity = 5.12431240/168.0585480 secs/batch = 0.1987s, grad.norm=8.66904640
  1427: 1 [  100/ 1327], train_loss/perplexity = 5.43116856/228.4160004 secs/batch = 0.1992s, grad.norm=8.55038738
  1432: 1 [  105/ 1327], train_loss/perplexity = 5.47750282/239.2485199 secs/batch = 0.1990s, grad.norm=9.59891129
  1437: 1 [  110/ 1327], train_loss/perplexity = 5.19900990/181.0928497 secs/batch = 0.1994s, grad.norm=8.23144054
  1442: 1 [  115/ 1327], train_loss/perplexity = 5.08252478/161.1804810 secs/batch = 0.1997s, grad.norm=8.96848106
  1447: 1 [  120/ 1327], train_loss/perplexity = 5.26484203/193.4157562 secs/batch = 0.1986s, grad.norm=9.26006317
  1452: 1 [  125/ 1327], train_loss/perplexity = 5.37723112/216.4221954 secs/batch = 0.1980s, grad.norm=8.62730789
  1457: 1 [  130/ 1327], train_loss/perplexity = 5.23288345/187.3321838 secs/batch = 0.1925s, grad.norm=9.22803974
  1462: 1 [  135/ 1327], train_loss/perplexity = 5.26177549/192.8235474 secs/batch = 0.1977s, grad.norm=8.75724506
  1467: 1 [  140/ 1327], train_loss/perplexity = 5.53426266/253.2210083 secs/batch = 0.2001s, grad.norm=8.22911930
  1472: 1 [  145/ 1327], train_loss/perplexity = 5.57908487/264.8291321 secs/batch = 0.1996s, grad.norm=9.50668621
  1477: 1 [  150/ 1327], train_loss/perplexity = 5.38669157/218.4793701 secs/batch = 0.1995s, grad.norm=8.57665825
  1482: 1 [  155/ 1327], train_loss/perplexity = 5.63550425/280.2001648 secs/batch = 0.1951s, grad.norm=9.01571178
  1487: 1 [  160/ 1327], train_loss/perplexity = 5.28813553/197.9739685 secs/batch = 0.1989s, grad.norm=8.93743706
  1492: 1 [  165/ 1327], train_loss/perplexity = 5.47438431/238.5035706 secs/batch = 0.2003s, grad.norm=8.56640244
  1497: 1 [  170/ 1327], train_loss/perplexity = 5.34053898/208.6251221 secs/batch = 0.1984s, grad.norm=8.61026382
  1502: 1 [  175/ 1327], train_loss/perplexity = 5.51911831/249.4150391 secs/batch = 0.1982s, grad.norm=8.25324631
  1507: 1 [  180/ 1327], train_loss/perplexity = 5.41150856/223.9692078 secs/batch = 0.1988s, grad.norm=9.24047565
  1512: 1 [  185/ 1327], train_loss/perplexity = 5.67804861/292.3783264 secs/batch = 0.1990s, grad.norm=8.46253681
  1517: 1 [  190/ 1327], train_loss/perplexity = 5.15657520/173.5690002 secs/batch = 0.1994s, grad.norm=10.03524303
  1522: 1 [  195/ 1327], train_loss/perplexity = 5.39590168/220.5008850 secs/batch = 0.1994s, grad.norm=8.50675964
  1527: 1 [  200/ 1327], train_loss/perplexity = 5.33342171/207.1455536 secs/batch = 0.1986s, grad.norm=8.95770454
  1532: 1 [  205/ 1327], train_loss/perplexity = 5.43666506/229.6749573 secs/batch = 0.1992s, grad.norm=8.48108578
  1537: 1 [  210/ 1327], train_loss/perplexity = 5.28607845/197.5671387 secs/batch = 0.2000s, grad.norm=8.22606373
  1542: 1 [  215/ 1327], train_loss/perplexity = 5.40062046/221.5438385 secs/batch = 0.1980s, grad.norm=8.31155968
  1547: 1 [  220/ 1327], train_loss/perplexity = 5.49869156/244.3719788 secs/batch = 0.1990s, grad.norm=8.84433842
  1552: 1 [  225/ 1327], train_loss/perplexity = 5.64845991/283.8539734 secs/batch = 0.1990s, grad.norm=8.84703445
  1557: 1 [  230/ 1327], train_loss/perplexity = 5.42092705/226.0886230 secs/batch = 0.2002s, grad.norm=8.71760178
  1562: 1 [  235/ 1327], train_loss/perplexity = 5.32184029/204.7603607 secs/batch = 0.1941s, grad.norm=8.24916553
  1567: 1 [  240/ 1327], train_loss/perplexity = 5.11397362/166.3299713 secs/batch = 0.1976s, grad.norm=9.39017391
  1572: 1 [  245/ 1327], train_loss/perplexity = 5.41492558/224.7358246 secs/batch = 0.1993s, grad.norm=8.14166737
  1577: 1 [  250/ 1327], train_loss/perplexity = 5.11680698/166.8019104 secs/batch = 0.2002s, grad.norm=8.49093151
  1582: 1 [  255/ 1327], train_loss/perplexity = 5.20939970/182.9841766 secs/batch = 0.1998s, grad.norm=8.28890705
  1587: 1 [  260/ 1327], train_loss/perplexity = 5.55707979/259.0652161 secs/batch = 0.2001s, grad.norm=9.32897282
  1592: 1 [  265/ 1327], train_loss/perplexity = 5.47074223/237.6365051 secs/batch = 0.1993s, grad.norm=8.35852718
  1597: 1 [  270/ 1327], train_loss/perplexity = 5.57055759/262.5804749 secs/batch = 0.1997s, grad.norm=8.40055466
  1602: 1 [  275/ 1327], train_loss/perplexity = 5.70527124/300.4469604 secs/batch = 0.2000s, grad.norm=8.72766590
  1607: 1 [  280/ 1327], train_loss/perplexity = 5.35631943/211.9434357 secs/batch = 0.1991s, grad.norm=8.84835339
  1612: 1 [  285/ 1327], train_loss/perplexity = 5.64381504/282.5385742 secs/batch = 0.1943s, grad.norm=9.26713848
  1617: 1 [  290/ 1327], train_loss/perplexity = 5.46145344/235.4393768 secs/batch = 0.1996s, grad.norm=9.01565933
  1622: 1 [  295/ 1327], train_loss/perplexity = 5.23867273/188.4198456 secs/batch = 0.1986s, grad.norm=8.54179955
  1627: 1 [  300/ 1327], train_loss/perplexity = 4.77715111/118.7655182 secs/batch = 0.1996s, grad.norm=8.69493675
  1632: 1 [  305/ 1327], train_loss/perplexity = 5.28075933/196.5190430 secs/batch = 0.1996s, grad.norm=8.83346081
  1637: 1 [  310/ 1327], train_loss/perplexity = 5.31620646/203.6100159 secs/batch = 0.2001s, grad.norm=9.11505985
  1642: 1 [  315/ 1327], train_loss/perplexity = 4.97800255/145.1840973 secs/batch = 0.1992s, grad.norm=8.37989902
  1647: 1 [  320/ 1327], train_loss/perplexity = 5.15505314/173.3050232 secs/batch = 0.1997s, grad.norm=10.52653885
  1652: 1 [  325/ 1327], train_loss/perplexity = 4.91218615/135.9362640 secs/batch = 0.1996s, grad.norm=8.81269169
  1657: 1 [  330/ 1327], train_loss/perplexity = 5.28883410/198.1123047 secs/batch = 0.1997s, grad.norm=9.16390514
  1662: 1 [  335/ 1327], train_loss/perplexity = 4.67206764/106.9185867 secs/batch = 0.1994s, grad.norm=9.08409882
  1667: 1 [  340/ 1327], train_loss/perplexity = 5.42026567/225.9391479 secs/batch = 0.2000s, grad.norm=8.61131382
  1672: 1 [  345/ 1327], train_loss/perplexity = 5.26221228/192.9077911 secs/batch = 0.1996s, grad.norm=8.49727345
  1677: 1 [  350/ 1327], train_loss/perplexity = 5.38249874/217.5652313 secs/batch = 0.1950s, grad.norm=9.31233978
  1682: 1 [  355/ 1327], train_loss/perplexity = 5.50265884/245.3433990 secs/batch = 0.1929s, grad.norm=9.07011223
  1687: 1 [  360/ 1327], train_loss/perplexity = 5.55228901/257.8270569 secs/batch = 0.2006s, grad.norm=8.94226933
  1692: 1 [  365/ 1327], train_loss/perplexity = 5.43659782/229.6595154 secs/batch = 0.1919s, grad.norm=8.40178108
  1697: 1 [  370/ 1327], train_loss/perplexity = 5.44799566/232.2921143 secs/batch = 0.1994s, grad.norm=9.11888695
  1702: 1 [  375/ 1327], train_loss/perplexity = 4.81230783/123.0151901 secs/batch = 0.1979s, grad.norm=9.74151039
  1707: 1 [  380/ 1327], train_loss/perplexity = 5.09506559/163.2145538 secs/batch = 0.1995s, grad.norm=9.51725292
  1712: 1 [  385/ 1327], train_loss/perplexity = 5.28082848/196.5326233 secs/batch = 0.1940s, grad.norm=8.95896149
  1717: 1 [  390/ 1327], train_loss/perplexity = 5.28988791/198.3211975 secs/batch = 0.1994s, grad.norm=8.79743671
  1722: 1 [  395/ 1327], train_loss/perplexity = 5.52187967/250.1047058 secs/batch = 0.2002s, grad.norm=8.83589840
  1727: 1 [  400/ 1327], train_loss/perplexity = 5.23705292/188.1148987 secs/batch = 0.1998s, grad.norm=10.02163124
  1732: 1 [  405/ 1327], train_loss/perplexity = 5.55541992/258.6355591 secs/batch = 0.1940s, grad.norm=9.08004284
  1737: 1 [  410/ 1327], train_loss/perplexity = 5.28321934/197.0030823 secs/batch = 0.1980s, grad.norm=8.78463459
  1742: 1 [  415/ 1327], train_loss/perplexity = 5.10036945/164.0825195 secs/batch = 0.1995s, grad.norm=9.73872280
  1747: 1 [  420/ 1327], train_loss/perplexity = 5.01816750/151.1340942 secs/batch = 0.1991s, grad.norm=10.35822487
  1752: 1 [  425/ 1327], train_loss/perplexity = 5.29238415/198.8168640 secs/batch = 0.1993s, grad.norm=10.86419487
  1757: 1 [  430/ 1327], train_loss/perplexity = 5.40246153/221.9520874 secs/batch = 0.1994s, grad.norm=9.30621243
  1762: 1 [  435/ 1327], train_loss/perplexity = 5.43616247/229.5595551 secs/batch = 0.1934s, grad.norm=9.17606640
  1767: 1 [  440/ 1327], train_loss/perplexity = 5.19263506/179.9420929 secs/batch = 0.1988s, grad.norm=10.86941147
  1772: 1 [  445/ 1327], train_loss/perplexity = 5.33772945/208.0398102 secs/batch = 0.1996s, grad.norm=10.15591908
  1777: 1 [  450/ 1327], train_loss/perplexity = 5.21244907/183.5430145 secs/batch = 0.1996s, grad.norm=9.48243427
  1782: 1 [  455/ 1327], train_loss/perplexity = 4.99508333/147.6852570 secs/batch = 0.1991s, grad.norm=11.75353146
  1787: 1 [  460/ 1327], train_loss/perplexity = 5.26800823/194.0291138 secs/batch = 0.1982s, grad.norm=9.63816071
  1792: 1 [  465/ 1327], train_loss/perplexity = 5.09071970/162.5067749 secs/batch = 0.2002s, grad.norm=10.39708710
  1797: 1 [  470/ 1327], train_loss/perplexity = 5.50575256/246.1035919 secs/batch = 0.2010s, grad.norm=8.78372192
  1802: 1 [  475/ 1327], train_loss/perplexity = 5.08264875/161.2004700 secs/batch = 0.1995s, grad.norm=8.83237171
  1807: 1 [  480/ 1327], train_loss/perplexity = 5.25233030/191.0108643 secs/batch = 0.1999s, grad.norm=9.68942547
  1812: 1 [  485/ 1327], train_loss/perplexity = 5.15392876/173.1102600 secs/batch = 0.1988s, grad.norm=8.90693665
  1817: 1 [  490/ 1327], train_loss/perplexity = 5.04569864/155.3527985 secs/batch = 0.1942s, grad.norm=9.98376846
  1822: 1 [  495/ 1327], train_loss/perplexity = 5.02980232/152.9027863 secs/batch = 0.1994s, grad.norm=9.59318542
  1827: 1 [  500/ 1327], train_loss/perplexity = 5.41943645/225.7518616 secs/batch = 0.2002s, grad.norm=9.15008354
  1832: 1 [  505/ 1327], train_loss/perplexity = 5.27007055/194.4296722 secs/batch = 0.2003s, grad.norm=9.45902061
  1837: 1 [  510/ 1327], train_loss/perplexity = 5.61548042/274.6452942 secs/batch = 0.1986s, grad.norm=8.57475853
  1842: 1 [  515/ 1327], train_loss/perplexity = 5.32100344/204.5890656 secs/batch = 0.2005s, grad.norm=8.99172211
  1847: 1 [  520/ 1327], train_loss/perplexity = 5.52803040/251.6477814 secs/batch = 0.1996s, grad.norm=9.53005314
  1852: 1 [  525/ 1327], train_loss/perplexity = 5.06974411/159.1336060 secs/batch = 0.1996s, grad.norm=9.03161240
  1857: 1 [  530/ 1327], train_loss/perplexity = 5.13266993/169.4689789 secs/batch = 0.1991s, grad.norm=9.70477962
  1862: 1 [  535/ 1327], train_loss/perplexity = 5.18134594/177.9221191 secs/batch = 0.1986s, grad.norm=9.05106544
  1867: 1 [  540/ 1327], train_loss/perplexity = 5.30230808/200.7997437 secs/batch = 0.1987s, grad.norm=8.59398842
  1872: 1 [  545/ 1327], train_loss/perplexity = 5.42662382/227.3802643 secs/batch = 0.1986s, grad.norm=8.99105549
  1877: 1 [  550/ 1327], train_loss/perplexity = 5.27844143/196.0640564 secs/batch = 0.1999s, grad.norm=8.70063877
  1882: 1 [  555/ 1327], train_loss/perplexity = 5.16132355/174.3951263 secs/batch = 0.1994s, grad.norm=8.94280148
  1887: 1 [  560/ 1327], train_loss/perplexity = 5.22937298/186.6757202 secs/batch = 0.2000s, grad.norm=10.21093941
  1892: 1 [  565/ 1327], train_loss/perplexity = 5.23875093/188.4345856 secs/batch = 0.2001s, grad.norm=10.00713730
  1897: 1 [  570/ 1327], train_loss/perplexity = 5.14267778/171.1735229 secs/batch = 0.1983s, grad.norm=9.93949795
  1902: 1 [  575/ 1327], train_loss/perplexity = 5.07717657/160.3207550 secs/batch = 0.1951s, grad.norm=9.55455208
  1907: 1 [  580/ 1327], train_loss/perplexity = 5.38924885/219.0387878 secs/batch = 0.1995s, grad.norm=10.37617207
  1912: 1 [  585/ 1327], train_loss/perplexity = 4.96156931/142.8177490 secs/batch = 0.2000s, grad.norm=9.46971512
  1917: 1 [  590/ 1327], train_loss/perplexity = 5.20078659/181.4148865 secs/batch = 0.2005s, grad.norm=8.99966812
  1922: 1 [  595/ 1327], train_loss/perplexity = 5.15017271/172.4612732 secs/batch = 0.1993s, grad.norm=10.35696983
  1927: 1 [  600/ 1327], train_loss/perplexity = 5.46091747/235.3132172 secs/batch = 0.1991s, grad.norm=9.08037567
  1932: 1 [  605/ 1327], train_loss/perplexity = 5.37275362/215.4553375 secs/batch = 0.1990s, grad.norm=9.42858315
  1937: 1 [  610/ 1327], train_loss/perplexity = 5.50208807/245.2033997 secs/batch = 0.1990s, grad.norm=9.92917156
  1942: 1 [  615/ 1327], train_loss/perplexity = 4.91568899/136.4132690 secs/batch = 0.1999s, grad.norm=9.42879963
  1947: 1 [  620/ 1327], train_loss/perplexity = 5.29872513/200.0815735 secs/batch = 0.1994s, grad.norm=9.57392120
  1952: 1 [  625/ 1327], train_loss/perplexity = 5.38526678/218.1683044 secs/batch = 0.2002s, grad.norm=9.15998650
  1957: 1 [  630/ 1327], train_loss/perplexity = 5.46498203/236.2716064 secs/batch = 0.1949s, grad.norm=9.48332787
  1962: 1 [  635/ 1327], train_loss/perplexity = 5.12260437/167.7717438 secs/batch = 0.2002s, grad.norm=9.05559254
  1967: 1 [  640/ 1327], train_loss/perplexity = 5.24533176/189.6787262 secs/batch = 0.1997s, grad.norm=9.33390236
  1972: 1 [  645/ 1327], train_loss/perplexity = 5.43161964/228.5190582 secs/batch = 0.1992s, grad.norm=9.20545959
  1977: 1 [  650/ 1327], train_loss/perplexity = 5.11822319/167.0383148 secs/batch = 0.1994s, grad.norm=9.99300003
  1982: 1 [  655/ 1327], train_loss/perplexity = 5.17901134/177.5072327 secs/batch = 0.1999s, grad.norm=9.46810627
  1987: 1 [  660/ 1327], train_loss/perplexity = 5.05027819/156.0658722 secs/batch = 0.2001s, grad.norm=9.86464405
  1992: 1 [  665/ 1327], train_loss/perplexity = 5.29469538/199.2769165 secs/batch = 0.1992s, grad.norm=9.16024685
  1997: 1 [  670/ 1327], train_loss/perplexity = 5.13150692/169.2720032 secs/batch = 0.1994s, grad.norm=9.30678082
  2002: 1 [  675/ 1327], train_loss/perplexity = 4.93169975/138.6149292 secs/batch = 0.2005s, grad.norm=10.04125977
  2007: 1 [  680/ 1327], train_loss/perplexity = 5.22914553/186.6332550 secs/batch = 0.1974s, grad.norm=9.65108204
  2012: 1 [  685/ 1327], train_loss/perplexity = 5.14139748/170.9545135 secs/batch = 0.1989s, grad.norm=9.24948692
  2017: 1 [  690/ 1327], train_loss/perplexity = 5.36814690/214.4650726 secs/batch = 0.2002s, grad.norm=9.21206188
  2022: 1 [  695/ 1327], train_loss/perplexity = 5.17166281/176.2075958 secs/batch = 0.1989s, grad.norm=9.39951324
  2027: 1 [  700/ 1327], train_loss/perplexity = 5.34280157/209.0976868 secs/batch = 0.1997s, grad.norm=9.25529575
  2032: 1 [  705/ 1327], train_loss/perplexity = 5.07435131/159.8684540 secs/batch = 0.1991s, grad.norm=9.14933109
  2037: 1 [  710/ 1327], train_loss/perplexity = 5.09104538/162.5597076 secs/batch = 0.2002s, grad.norm=9.59347153
  2042: 1 [  715/ 1327], train_loss/perplexity = 5.06906176/159.0250549 secs/batch = 0.1980s, grad.norm=9.30695152
  2047: 1 [  720/ 1327], train_loss/perplexity = 5.17226696/176.3140869 secs/batch = 0.1987s, grad.norm=9.68762016
  2052: 1 [  725/ 1327], train_loss/perplexity = 4.98372602/146.0174408 secs/batch = 0.1995s, grad.norm=9.90365887
  2057: 1 [  730/ 1327], train_loss/perplexity = 5.14910364/172.2769928 secs/batch = 0.1998s, grad.norm=9.60556126
  2062: 1 [  735/ 1327], train_loss/perplexity = 5.26013088/192.5066833 secs/batch = 0.1997s, grad.norm=9.37758350
  2067: 1 [  740/ 1327], train_loss/perplexity = 4.64741373/104.3148499 secs/batch = 0.1994s, grad.norm=9.48445320
  2072: 1 [  745/ 1327], train_loss/perplexity = 5.18819523/179.1449432 secs/batch = 0.1992s, grad.norm=9.09924889
  2077: 1 [  750/ 1327], train_loss/perplexity = 5.02165747/151.6624756 secs/batch = 0.1942s, grad.norm=10.35673428
  2082: 1 [  755/ 1327], train_loss/perplexity = 5.01227331/150.2459106 secs/batch = 0.1990s, grad.norm=9.33407116
  2087: 1 [  760/ 1327], train_loss/perplexity = 4.90492010/134.9521332 secs/batch = 0.1995s, grad.norm=9.53607082
  2092: 1 [  765/ 1327], train_loss/perplexity = 5.00813150/149.6249084 secs/batch = 0.1989s, grad.norm=9.44180298
  2097: 1 [  770/ 1327], train_loss/perplexity = 4.88870335/132.7812958 secs/batch = 0.1980s, grad.norm=9.75089359
  2102: 1 [  775/ 1327], train_loss/perplexity = 5.07549620/160.0515900 secs/batch = 0.1995s, grad.norm=9.62390137
  2107: 1 [  780/ 1327], train_loss/perplexity = 5.34495831/209.5491486 secs/batch = 0.1999s, grad.norm=9.42764759
  2112: 1 [  785/ 1327], train_loss/perplexity = 5.12935257/168.9077301 secs/batch = 0.1999s, grad.norm=10.05031872
  2117: 1 [  790/ 1327], train_loss/perplexity = 4.94107103/139.9200287 secs/batch = 0.1974s, grad.norm=9.58440208
  2122: 1 [  795/ 1327], train_loss/perplexity = 5.30911970/202.1721802 secs/batch = 0.1987s, grad.norm=9.56554794
  2127: 1 [  800/ 1327], train_loss/perplexity = 5.18312073/178.2381744 secs/batch = 0.1996s, grad.norm=9.76958084
  2132: 1 [  805/ 1327], train_loss/perplexity = 5.53969860/254.6012573 secs/batch = 0.1991s, grad.norm=10.10693932
  2137: 1 [  810/ 1327], train_loss/perplexity = 5.16832161/175.6198273 secs/batch = 0.1993s, grad.norm=9.43757725
  2142: 1 [  815/ 1327], train_loss/perplexity = 5.05659246/157.0544281 secs/batch = 0.1994s, grad.norm=9.17282104
  2147: 1 [  820/ 1327], train_loss/perplexity = 4.71909714/112.0670242 secs/batch = 0.1993s, grad.norm=8.74154758
  2152: 1 [  825/ 1327], train_loss/perplexity = 4.90796661/135.3638916 secs/batch = 0.1988s, grad.norm=9.22591496
  2157: 1 [  830/ 1327], train_loss/perplexity = 4.81737328/123.6398926 secs/batch = 0.1985s, grad.norm=10.12878227
  2162: 1 [  835/ 1327], train_loss/perplexity = 5.04675913/155.5176392 secs/batch = 0.2007s, grad.norm=9.87629414
  2167: 1 [  840/ 1327], train_loss/perplexity = 5.18099022/177.8588409 secs/batch = 0.1976s, grad.norm=10.06807995
  2172: 1 [  845/ 1327], train_loss/perplexity = 5.00607347/149.3172913 secs/batch = 0.1992s, grad.norm=10.71146488
  2177: 1 [  850/ 1327], train_loss/perplexity = 5.02023172/151.4463959 secs/batch = 0.2002s, grad.norm=9.48957443
  2182: 1 [  855/ 1327], train_loss/perplexity = 4.99312401/147.3961792 secs/batch = 0.1999s, grad.norm=10.21811390
  2187: 1 [  860/ 1327], train_loss/perplexity = 4.74140549/114.5951538 secs/batch = 0.1983s, grad.norm=9.71409988
  2192: 1 [  865/ 1327], train_loss/perplexity = 5.26280308/193.0217896 secs/batch = 0.2001s, grad.norm=9.44661999
  2197: 1 [  870/ 1327], train_loss/perplexity = 5.25357914/191.2495575 secs/batch = 0.1998s, grad.norm=9.53665447
  2202: 1 [  875/ 1327], train_loss/perplexity = 4.74523497/115.0348282 secs/batch = 0.2006s, grad.norm=10.56853199
  2207: 1 [  880/ 1327], train_loss/perplexity = 4.93148518/138.5851898 secs/batch = 0.1990s, grad.norm=10.14524841
  2212: 1 [  885/ 1327], train_loss/perplexity = 5.01754570/151.0401459 secs/batch = 0.1994s, grad.norm=9.79510117
  2217: 1 [  890/ 1327], train_loss/perplexity = 5.25289106/191.1180115 secs/batch = 0.2008s, grad.norm=9.32026100
  2222: 1 [  895/ 1327], train_loss/perplexity = 5.31298304/202.9547424 secs/batch = 0.2005s, grad.norm=9.44414043
  2227: 1 [  900/ 1327], train_loss/perplexity = 5.11305714/166.1776123 secs/batch = 0.1994s, grad.norm=9.62941647
  2232: 1 [  905/ 1327], train_loss/perplexity = 4.94046259/139.8349152 secs/batch = 0.1993s, grad.norm=9.70084381
  2237: 1 [  910/ 1327], train_loss/perplexity = 5.11084175/165.8098602 secs/batch = 0.1995s, grad.norm=9.33417892
  2242: 1 [  915/ 1327], train_loss/perplexity = 5.28017902/196.4050293 secs/batch = 0.2000s, grad.norm=9.60474873
  2247: 1 [  920/ 1327], train_loss/perplexity = 5.44588900/231.8032532 secs/batch = 0.2003s, grad.norm=9.71617508
  2252: 1 [  925/ 1327], train_loss/perplexity = 5.25462389/191.4494629 secs/batch = 0.2005s, grad.norm=14.51980686
  2257: 1 [  930/ 1327], train_loss/perplexity = 5.10640907/165.0765076 secs/batch = 0.1964s, grad.norm=9.72787189
  2262: 1 [  935/ 1327], train_loss/perplexity = 5.14350033/171.3143768 secs/batch = 0.1994s, grad.norm=9.37864590
  2267: 1 [  940/ 1327], train_loss/perplexity = 5.18019152/177.7168427 secs/batch = 0.1999s, grad.norm=9.77818012
  2272: 1 [  945/ 1327], train_loss/perplexity = 5.32629395/205.6743164 secs/batch = 0.1997s, grad.norm=9.45311165
  2277: 1 [  950/ 1327], train_loss/perplexity = 5.09817600/163.7230072 secs/batch = 0.1998s, grad.norm=9.93069363
  2282: 1 [  955/ 1327], train_loss/perplexity = 5.17427015/176.6676331 secs/batch = 0.1983s, grad.norm=9.67268467
  2287: 1 [  960/ 1327], train_loss/perplexity = 5.41770458/225.3612366 secs/batch = 0.1994s, grad.norm=9.49726105
  2292: 1 [  965/ 1327], train_loss/perplexity = 5.12507582/168.1868896 secs/batch = 0.1940s, grad.norm=9.02293301
  2297: 1 [  970/ 1327], train_loss/perplexity = 5.38610411/218.3510590 secs/batch = 0.1998s, grad.norm=9.29941559
  2302: 1 [  975/ 1327], train_loss/perplexity = 5.12746906/168.5898895 secs/batch = 0.2001s, grad.norm=9.91970730
  2307: 1 [  980/ 1327], train_loss/perplexity = 4.91881609/136.8405151 secs/batch = 0.1998s, grad.norm=9.34845734
  2312: 1 [  985/ 1327], train_loss/perplexity = 5.09245682/162.7893219 secs/batch = 0.1995s, grad.norm=10.17100143
  2317: 1 [  990/ 1327], train_loss/perplexity = 5.22120476/185.1571198 secs/batch = 0.2002s, grad.norm=9.48015308
  2322: 1 [  995/ 1327], train_loss/perplexity = 5.26654959/193.7463074 secs/batch = 0.2000s, grad.norm=9.37313652
  2327: 1 [ 1000/ 1327], train_loss/perplexity = 4.68325472/108.1214066 secs/batch = 0.1957s, grad.norm=9.37062454
  2332: 1 [ 1005/ 1327], train_loss/perplexity = 5.22629547/186.1021118 secs/batch = 0.1990s, grad.norm=10.93242741
  2337: 1 [ 1010/ 1327], train_loss/perplexity = 4.78955936/120.2483749 secs/batch = 0.1992s, grad.norm=9.37518311
  2342: 1 [ 1015/ 1327], train_loss/perplexity = 5.24783516/190.1541748 secs/batch = 0.1990s, grad.norm=9.51932907
  2347: 1 [ 1020/ 1327], train_loss/perplexity = 5.48291874/240.5477753 secs/batch = 0.1986s, grad.norm=9.31440353
  2352: 1 [ 1025/ 1327], train_loss/perplexity = 5.25203991/190.9553986 secs/batch = 0.1999s, grad.norm=9.32171917
  2357: 1 [ 1030/ 1327], train_loss/perplexity = 5.05438185/156.7076263 secs/batch = 0.1998s, grad.norm=9.48317814
  2362: 1 [ 1035/ 1327], train_loss/perplexity = 4.92700958/137.9663239 secs/batch = 0.1984s, grad.norm=9.40461349
  2367: 1 [ 1040/ 1327], train_loss/perplexity = 5.25620127/191.7516937 secs/batch = 0.2003s, grad.norm=9.26612091
  2372: 1 [ 1045/ 1327], train_loss/perplexity = 4.88937759/132.8708496 secs/batch = 0.1990s, grad.norm=9.73340893
  2377: 1 [ 1050/ 1327], train_loss/perplexity = 4.86907053/130.1998444 secs/batch = 0.1994s, grad.norm=10.02006912
  2382: 1 [ 1055/ 1327], train_loss/perplexity = 5.06981659/159.1451416 secs/batch = 0.1995s, grad.norm=10.05889130
  2387: 1 [ 1060/ 1327], train_loss/perplexity = 4.71241093/111.3202209 secs/batch = 0.1997s, grad.norm=10.46692753
  2392: 1 [ 1065/ 1327], train_loss/perplexity = 4.83655167/126.0339966 secs/batch = 0.1989s, grad.norm=9.72250080
  2397: 1 [ 1070/ 1327], train_loss/perplexity = 5.20149374/181.5432129 secs/batch = 0.1967s, grad.norm=9.70681286
  2402: 1 [ 1075/ 1327], train_loss/perplexity = 4.92502022/137.6921234 secs/batch = 0.1995s, grad.norm=9.85835266
  2407: 1 [ 1080/ 1327], train_loss/perplexity = 4.81714010/123.6110687 secs/batch = 0.1989s, grad.norm=11.09356403
  2412: 1 [ 1085/ 1327], train_loss/perplexity = 4.68102646/107.8807526 secs/batch = 0.1941s, grad.norm=9.84905910
  2417: 1 [ 1090/ 1327], train_loss/perplexity = 4.94622564/140.6431274 secs/batch = 0.2003s, grad.norm=10.47784710
  2422: 1 [ 1095/ 1327], train_loss/perplexity = 5.04429960/155.1356049 secs/batch = 0.1991s, grad.norm=10.64519024
  2427: 1 [ 1100/ 1327], train_loss/perplexity = 5.01338673/150.4132843 secs/batch = 0.1957s, grad.norm=10.98247910
  2432: 1 [ 1105/ 1327], train_loss/perplexity = 4.80364370/121.9539719 secs/batch = 0.1982s, grad.norm=10.01122570
  2437: 1 [ 1110/ 1327], train_loss/perplexity = 5.39009476/219.2241516 secs/batch = 0.1994s, grad.norm=10.41897774
  2442: 1 [ 1115/ 1327], train_loss/perplexity = 4.88125277/131.7956696 secs/batch = 0.1977s, grad.norm=10.08753204
  2447: 1 [ 1120/ 1327], train_loss/perplexity = 5.04155970/154.7111359 secs/batch = 0.2008s, grad.norm=10.68156815
  2452: 1 [ 1125/ 1327], train_loss/perplexity = 5.24880600/190.3388672 secs/batch = 0.1945s, grad.norm=9.84635067
  2457: 1 [ 1130/ 1327], train_loss/perplexity = 4.99787903/148.0987091 secs/batch = 0.1989s, grad.norm=10.18781185
  2462: 1 [ 1135/ 1327], train_loss/perplexity = 4.97660017/144.9806366 secs/batch = 0.1992s, grad.norm=9.81714249
  2467: 1 [ 1140/ 1327], train_loss/perplexity = 5.24661398/189.9221039 secs/batch = 0.1981s, grad.norm=9.51869297
  2472: 1 [ 1145/ 1327], train_loss/perplexity = 4.98923874/146.8246155 secs/batch = 0.1999s, grad.norm=10.08265591
  2477: 1 [ 1150/ 1327], train_loss/perplexity = 4.97012568/144.0449829 secs/batch = 0.1997s, grad.norm=9.86782551
  2482: 1 [ 1155/ 1327], train_loss/perplexity = 5.09405804/163.0501862 secs/batch = 0.2002s, grad.norm=9.73327541
  2487: 1 [ 1160/ 1327], train_loss/perplexity = 5.09423876/163.0796509 secs/batch = 0.1995s, grad.norm=10.06052017
  2492: 1 [ 1165/ 1327], train_loss/perplexity = 5.11232805/166.0564880 secs/batch = 0.1993s, grad.norm=9.98414993
  2497: 1 [ 1170/ 1327], train_loss/perplexity = 5.01966381/151.3604126 secs/batch = 0.1944s, grad.norm=10.16504765
  2502: 1 [ 1175/ 1327], train_loss/perplexity = 4.79418087/120.8053894 secs/batch = 0.1994s, grad.norm=10.72354698
  2507: 1 [ 1180/ 1327], train_loss/perplexity = 4.78315258/119.4804306 secs/batch = 0.1990s, grad.norm=10.10099602
  2512: 1 [ 1185/ 1327], train_loss/perplexity = 4.97621155/144.9243011 secs/batch = 0.1988s, grad.norm=9.77863312
  2517: 1 [ 1190/ 1327], train_loss/perplexity = 4.94232798/140.0960083 secs/batch = 0.1988s, grad.norm=9.59969139
  2522: 1 [ 1195/ 1327], train_loss/perplexity = 4.85055780/127.8116608 secs/batch = 0.1950s, grad.norm=9.87615204
  2527: 1 [ 1200/ 1327], train_loss/perplexity = 4.74752283/115.2983170 secs/batch = 0.1998s, grad.norm=9.61838627
  2532: 1 [ 1205/ 1327], train_loss/perplexity = 4.83450699/125.7765579 secs/batch = 0.1983s, grad.norm=10.19589901
  2537: 1 [ 1210/ 1327], train_loss/perplexity = 4.59388351/98.8776779 secs/batch = 0.1997s, grad.norm=9.96676636
  2542: 1 [ 1215/ 1327], train_loss/perplexity = 4.66228056/105.8772659 secs/batch = 0.1995s, grad.norm=9.71140099
  2547: 1 [ 1220/ 1327], train_loss/perplexity = 4.88379669/132.1313782 secs/batch = 0.2010s, grad.norm=10.86059380
  2552: 1 [ 1225/ 1327], train_loss/perplexity = 4.70349407/110.3320084 secs/batch = 0.1947s, grad.norm=11.36229897
  2557: 1 [ 1230/ 1327], train_loss/perplexity = 4.85655975/128.5810852 secs/batch = 0.2000s, grad.norm=11.50798512
  2562: 1 [ 1235/ 1327], train_loss/perplexity = 4.89063120/133.0375214 secs/batch = 0.1994s, grad.norm=10.10032558
  2567: 1 [ 1240/ 1327], train_loss/perplexity = 4.96560097/143.3946991 secs/batch = 0.1998s, grad.norm=9.73001957
  2572: 1 [ 1245/ 1327], train_loss/perplexity = 4.94358492/140.2722168 secs/batch = 0.1993s, grad.norm=10.63879871
  2577: 1 [ 1250/ 1327], train_loss/perplexity = 5.05646610/157.0345917 secs/batch = 0.2013s, grad.norm=9.87651157
  2582: 1 [ 1255/ 1327], train_loss/perplexity = 5.04112625/154.6440887 secs/batch = 0.1947s, grad.norm=10.46419239
  2587: 1 [ 1260/ 1327], train_loss/perplexity = 4.88714838/132.5749817 secs/batch = 0.1958s, grad.norm=10.61395359
  2592: 1 [ 1265/ 1327], train_loss/perplexity = 5.09480381/163.1718292 secs/batch = 0.1996s, grad.norm=10.39925766
  2597: 1 [ 1270/ 1327], train_loss/perplexity = 4.79369926/120.7472229 secs/batch = 0.1988s, grad.norm=9.75174713
  2602: 1 [ 1275/ 1327], train_loss/perplexity = 5.07044888/159.2457886 secs/batch = 0.1996s, grad.norm=10.56429005
  2607: 1 [ 1280/ 1327], train_loss/perplexity = 4.85923719/128.9258118 secs/batch = 0.1988s, grad.norm=10.55041218
  2612: 1 [ 1285/ 1327], train_loss/perplexity = 4.88186741/131.8767090 secs/batch = 0.1996s, grad.norm=10.11189651
  2617: 1 [ 1290/ 1327], train_loss/perplexity = 5.01501513/150.6584167 secs/batch = 0.2003s, grad.norm=10.02262783
  2622: 1 [ 1295/ 1327], train_loss/perplexity = 5.07087088/159.3130035 secs/batch = 0.1989s, grad.norm=10.27065849
  2627: 1 [ 1300/ 1327], train_loss/perplexity = 5.16339684/174.7570648 secs/batch = 0.1991s, grad.norm=10.61240768
  2632: 1 [ 1305/ 1327], train_loss/perplexity = 5.32216549/204.8269501 secs/batch = 0.1996s, grad.norm=10.58009243
  2637: 1 [ 1310/ 1327], train_loss/perplexity = 5.48867607/241.9366913 secs/batch = 0.1950s, grad.norm=10.32309628
  2642: 1 [ 1315/ 1327], train_loss/perplexity = 5.31858730/204.0953522 secs/batch = 0.1988s, grad.norm=10.15176582
  2647: 1 [ 1320/ 1327], train_loss/perplexity = 5.29382801/199.1041412 secs/batch = 0.1987s, grad.norm=9.58880615
  2652: 1 [ 1325/ 1327], train_loss/perplexity = 5.18723488/178.9729919 secs/batch = 0.1994s, grad.norm=10.05094719
Epoch training time: 264.02854013442993
	> validation loss = 5.28105164, perplexity = 196.57649231
	> validation loss = 5.14053583, perplexity = 170.80726624
	> validation loss = 5.06137609, perplexity = 157.80752563
	> validation loss = 5.14898205, perplexity = 172.25605774
	> validation loss = 5.34332943, perplexity = 209.20809937
	> validation loss = 5.17098761, perplexity = 176.08865356
	> validation loss = 5.09692383, perplexity = 163.51812744
	> validation loss = 5.05978537, perplexity = 157.55670166
	> validation loss = 5.17104721, perplexity = 176.09915161
	> validation loss = 4.98749542, perplexity = 146.56886292
	> validation loss = 5.07915306, perplexity = 160.63795471
	> validation loss = 5.15208721, perplexity = 172.79176331
	> validation loss = 5.04959679, perplexity = 155.95956421
	> validation loss = 4.95762396, perplexity = 142.25538635
	> validation loss = 4.76293612, perplexity = 117.08921051
	> validation loss = 4.75240278, perplexity = 115.86234283
	> validation loss = 5.19287586, perplexity = 179.98542786
	> validation loss = 4.76353502, perplexity = 117.15935516
	> validation loss = 5.20711994, perplexity = 182.56748962
	> validation loss = 5.09946442, perplexity = 163.93408203
	> validation loss = 4.93125200, perplexity = 138.55287170
at the end of epoch: 1
train loss = 5.13452368, perplexity = 169.78342897
validation loss = 5.05876089, perplexity = 157.39536571
Saved model cv/epoch001_5.0588.model
  2659: 2 [    5/ 1327], train_loss/perplexity = 5.36646986/214.1057129 secs/batch = 0.1969s, grad.norm=10.23826218
  2664: 2 [   10/ 1327], train_loss/perplexity = 4.82777739/124.9329758 secs/batch = 0.1996s, grad.norm=9.84617996
  2669: 2 [   15/ 1327], train_loss/perplexity = 4.99218464/147.2577820 secs/batch = 0.1990s, grad.norm=9.16112328
  2674: 2 [   20/ 1327], train_loss/perplexity = 5.31977987/204.3388977 secs/batch = 0.1990s, grad.norm=11.11367130
  2679: 2 [   25/ 1327], train_loss/perplexity = 5.20152092/181.5481567 secs/batch = 0.1989s, grad.norm=10.36631870
  2684: 2 [   30/ 1327], train_loss/perplexity = 5.09109735/162.5681610 secs/batch = 0.1975s, grad.norm=10.60599899
  2689: 2 [   35/ 1327], train_loss/perplexity = 4.92877769/138.2104797 secs/batch = 0.2010s, grad.norm=9.70998478
  2694: 2 [   40/ 1327], train_loss/perplexity = 4.97172594/144.2756805 secs/batch = 0.1996s, grad.norm=10.56540394
  2699: 2 [   45/ 1327], train_loss/perplexity = 4.71168947/111.2399368 secs/batch = 0.2010s, grad.norm=9.39154339
  2704: 2 [   50/ 1327], train_loss/perplexity = 4.96604538/143.4584351 secs/batch = 0.1996s, grad.norm=10.12047768
  2709: 2 [   55/ 1327], train_loss/perplexity = 4.98754930/146.5767670 secs/batch = 0.1985s, grad.norm=10.57686806
  2714: 2 [   60/ 1327], train_loss/perplexity = 5.24416971/189.4584503 secs/batch = 0.1996s, grad.norm=10.71173286
  2719: 2 [   65/ 1327], train_loss/perplexity = 4.78476238/119.6729202 secs/batch = 0.1993s, grad.norm=9.90011883
  2724: 2 [   70/ 1327], train_loss/perplexity = 4.60350752/99.8338699 secs/batch = 0.1992s, grad.norm=10.51105118
  2729: 2 [   75/ 1327], train_loss/perplexity = 4.55039024/94.6693420 secs/batch = 0.2006s, grad.norm=10.87803650
  2734: 2 [   80/ 1327], train_loss/perplexity = 4.94374180/140.2942200 secs/batch = 0.1926s, grad.norm=10.67364883
  2739: 2 [   85/ 1327], train_loss/perplexity = 4.97736406/145.0914307 secs/batch = 0.1995s, grad.norm=10.35441399
  2744: 2 [   90/ 1327], train_loss/perplexity = 4.93652439/139.2853088 secs/batch = 0.2006s, grad.norm=10.31361961
  2749: 2 [   95/ 1327], train_loss/perplexity = 4.76922226/117.8275681 secs/batch = 0.1934s, grad.norm=10.04701138
  2754: 2 [  100/ 1327], train_loss/perplexity = 5.12902403/168.8522491 secs/batch = 0.1937s, grad.norm=10.30979729
  2759: 2 [  105/ 1327], train_loss/perplexity = 5.11634302/166.7245483 secs/batch = 0.2001s, grad.norm=10.83671188
  2764: 2 [  110/ 1327], train_loss/perplexity = 4.89884233/134.1343994 secs/batch = 0.2000s, grad.norm=12.65341377
  2769: 2 [  115/ 1327], train_loss/perplexity = 4.79978657/121.4844894 secs/batch = 0.1999s, grad.norm=11.50660419
  2774: 2 [  120/ 1327], train_loss/perplexity = 4.89348841/133.4181824 secs/batch = 0.1991s, grad.norm=10.94847965
  2779: 2 [  125/ 1327], train_loss/perplexity = 5.04124594/154.6625977 secs/batch = 0.1992s, grad.norm=10.26072407
  2784: 2 [  130/ 1327], train_loss/perplexity = 4.86799812/130.0602875 secs/batch = 0.1993s, grad.norm=11.01497078
  2789: 2 [  135/ 1327], train_loss/perplexity = 4.94895220/141.0271149 secs/batch = 0.1997s, grad.norm=10.16752148
  2794: 2 [  140/ 1327], train_loss/perplexity = 5.19582415/180.5168610 secs/batch = 0.1998s, grad.norm=10.28129005
  2799: 2 [  145/ 1327], train_loss/perplexity = 5.22180223/185.2677765 secs/batch = 0.1998s, grad.norm=10.90082073
  2804: 2 [  150/ 1327], train_loss/perplexity = 5.06517076/158.4074860 secs/batch = 0.2003s, grad.norm=10.67857456
  2809: 2 [  155/ 1327], train_loss/perplexity = 5.28378677/197.1148987 secs/batch = 0.1965s, grad.norm=9.52185822
  2814: 2 [  160/ 1327], train_loss/perplexity = 5.02439737/152.0785828 secs/batch = 0.2006s, grad.norm=10.22658348
  2819: 2 [  165/ 1327], train_loss/perplexity = 5.22292137/185.4752350 secs/batch = 0.1992s, grad.norm=9.80171967
  2824: 2 [  170/ 1327], train_loss/perplexity = 4.94539404/140.5262146 secs/batch = 0.1992s, grad.norm=10.12915325
  2829: 2 [  175/ 1327], train_loss/perplexity = 5.23039913/186.8673706 secs/batch = 0.2006s, grad.norm=9.74487400
  2834: 2 [  180/ 1327], train_loss/perplexity = 5.14821768/172.1244354 secs/batch = 0.1991s, grad.norm=10.81069660
  2839: 2 [  185/ 1327], train_loss/perplexity = 5.32521915/205.4533844 secs/batch = 0.1990s, grad.norm=10.22825432
  2844: 2 [  190/ 1327], train_loss/perplexity = 4.83834600/126.2603455 secs/batch = 0.1992s, grad.norm=9.87572765
  2849: 2 [  195/ 1327], train_loss/perplexity = 5.14851189/172.1750793 secs/batch = 0.1994s, grad.norm=9.56876373
  2854: 2 [  200/ 1327], train_loss/perplexity = 5.03747177/154.0799713 secs/batch = 0.1989s, grad.norm=9.87166786
  2859: 2 [  205/ 1327], train_loss/perplexity = 5.13461113/169.7982788 secs/batch = 0.1985s, grad.norm=9.93488598
  2864: 2 [  210/ 1327], train_loss/perplexity = 5.03169632/153.1926575 secs/batch = 0.1974s, grad.norm=10.11898041
  2869: 2 [  215/ 1327], train_loss/perplexity = 5.18239498/178.1088715 secs/batch = 0.2015s, grad.norm=10.07809734
  2874: 2 [  220/ 1327], train_loss/perplexity = 5.16461563/174.9701996 secs/batch = 0.2001s, grad.norm=10.14631844
  2879: 2 [  225/ 1327], train_loss/perplexity = 5.36491776/213.7736511 secs/batch = 0.1994s, grad.norm=10.37748814
  2884: 2 [  230/ 1327], train_loss/perplexity = 5.14036942/170.7788391 secs/batch = 0.2003s, grad.norm=10.48736095
  2889: 2 [  235/ 1327], train_loss/perplexity = 4.97512245/144.7665558 secs/batch = 0.1997s, grad.norm=10.16809177
  2894: 2 [  240/ 1327], train_loss/perplexity = 4.83997202/126.4658127 secs/batch = 0.1988s, grad.norm=10.52511978
  2899: 2 [  245/ 1327], train_loss/perplexity = 5.13592291/170.0211639 secs/batch = 0.1986s, grad.norm=9.75415897
  2904: 2 [  250/ 1327], train_loss/perplexity = 4.85327911/128.1599579 secs/batch = 0.1997s, grad.norm=10.20180702
  2909: 2 [  255/ 1327], train_loss/perplexity = 4.93002224/138.3825836 secs/batch = 0.1994s, grad.norm=9.83895206
  2914: 2 [  260/ 1327], train_loss/perplexity = 5.26654148/193.7447357 secs/batch = 0.2002s, grad.norm=10.25423622
  2919: 2 [  265/ 1327], train_loss/perplexity = 5.26555824/193.5543365 secs/batch = 0.1988s, grad.norm=9.94341755
  2924: 2 [  270/ 1327], train_loss/perplexity = 5.65627861/286.0820312 secs/batch = 0.1992s, grad.norm=36.51152039
  2929: 2 [  275/ 1327], train_loss/perplexity = 5.45851660/234.7489319 secs/batch = 0.1999s, grad.norm=9.79567719
  2934: 2 [  280/ 1327], train_loss/perplexity = 5.10131168/164.2371979 secs/batch = 0.1999s, grad.norm=9.95843315
  2939: 2 [  285/ 1327], train_loss/perplexity = 5.33207798/206.8674011 secs/batch = 0.1989s, grad.norm=10.33407211
  2944: 2 [  290/ 1327], train_loss/perplexity = 5.17746925/177.2337036 secs/batch = 0.1991s, grad.norm=10.51294899
  2949: 2 [  295/ 1327], train_loss/perplexity = 4.90626907/135.1342926 secs/batch = 0.1989s, grad.norm=9.74653816
  2954: 2 [  300/ 1327], train_loss/perplexity = 4.52132273/91.9571533 secs/batch = 0.1994s, grad.norm=9.97314548
  2959: 2 [  305/ 1327], train_loss/perplexity = 4.98682880/146.4711914 secs/batch = 0.1990s, grad.norm=9.99580669
  2964: 2 [  310/ 1327], train_loss/perplexity = 4.98523426/146.2378235 secs/batch = 0.1993s, grad.norm=10.25614262
  2969: 2 [  315/ 1327], train_loss/perplexity = 4.67174435/106.8840256 secs/batch = 0.1999s, grad.norm=10.21707630
  2974: 2 [  320/ 1327], train_loss/perplexity = 4.67701292/107.4486389 secs/batch = 0.1992s, grad.norm=12.33155060
  2979: 2 [  325/ 1327], train_loss/perplexity = 4.61075115/100.5596542 secs/batch = 0.2000s, grad.norm=10.26545811
  2984: 2 [  330/ 1327], train_loss/perplexity = 5.00155354/148.6439056 secs/batch = 0.1992s, grad.norm=10.85466290
  2989: 2 [  335/ 1327], train_loss/perplexity = 4.44983768/85.6130447 secs/batch = 0.2006s, grad.norm=10.57099819
  2994: 2 [  340/ 1327], train_loss/perplexity = 5.17562389/176.9069519 secs/batch = 0.1934s, grad.norm=9.72528648
  2999: 2 [  345/ 1327], train_loss/perplexity = 4.98103762/145.6254120 secs/batch = 0.1992s, grad.norm=9.72592068
  3004: 2 [  350/ 1327], train_loss/perplexity = 5.10372210/164.6335449 secs/batch = 0.1990s, grad.norm=10.51340675
  3009: 2 [  355/ 1327], train_loss/perplexity = 5.17006874/175.9269257 secs/batch = 0.2000s, grad.norm=10.05349541
  3014: 2 [  360/ 1327], train_loss/perplexity = 5.28137350/196.6397705 secs/batch = 0.2000s, grad.norm=10.28077984
  3019: 2 [  365/ 1327], train_loss/perplexity = 5.18045664/177.7639618 secs/batch = 0.2001s, grad.norm=9.95879173
  3024: 2 [  370/ 1327], train_loss/perplexity = 5.13368511/169.6411133 secs/batch = 0.1955s, grad.norm=10.18355751
  3029: 2 [  375/ 1327], train_loss/perplexity = 4.54529095/94.1878281 secs/batch = 0.1997s, grad.norm=10.23248768
  3034: 2 [  380/ 1327], train_loss/perplexity = 4.72290611/112.4946976 secs/batch = 0.1981s, grad.norm=10.86149693
  3039: 2 [  385/ 1327], train_loss/perplexity = 4.96290207/143.0082092 secs/batch = 0.1996s, grad.norm=10.53646755
  3044: 2 [  390/ 1327], train_loss/perplexity = 5.03209877/153.2543182 secs/batch = 0.2004s, grad.norm=9.68758583
  3049: 2 [  395/ 1327], train_loss/perplexity = 5.21682787/184.3484802 secs/batch = 0.1992s, grad.norm=10.62683105
  3054: 2 [  400/ 1327], train_loss/perplexity = 4.96094704/142.7288971 secs/batch = 0.1994s, grad.norm=9.84457111
  3059: 2 [  405/ 1327], train_loss/perplexity = 5.33588743/207.6569519 secs/batch = 0.1991s, grad.norm=10.03915215
  3064: 2 [  410/ 1327], train_loss/perplexity = 4.95552969/141.9577789 secs/batch = 0.2005s, grad.norm=10.33581638
  3069: 2 [  415/ 1327], train_loss/perplexity = 4.81908894/123.8522034 secs/batch = 0.1980s, grad.norm=10.31191444
  3074: 2 [  420/ 1327], train_loss/perplexity = 4.62864876/102.3756332 secs/batch = 0.1984s, grad.norm=11.22636986
  3079: 2 [  425/ 1327], train_loss/perplexity = 4.86565590/129.7560120 secs/batch = 0.1988s, grad.norm=11.50712872
  3084: 2 [  430/ 1327], train_loss/perplexity = 5.13081837/169.1554871 secs/batch = 0.1991s, grad.norm=10.51168728
  3089: 2 [  435/ 1327], train_loss/perplexity = 5.15070915/172.5538177 secs/batch = 0.1994s, grad.norm=10.37993240
  3094: 2 [  440/ 1327], train_loss/perplexity = 4.87354565/130.7838135 secs/batch = 0.2011s, grad.norm=11.63755894
  3099: 2 [  445/ 1327], train_loss/perplexity = 5.09741974/163.5992279 secs/batch = 0.1993s, grad.norm=10.98872375
  3104: 2 [  450/ 1327], train_loss/perplexity = 4.90380478/134.8016968 secs/batch = 0.1995s, grad.norm=10.59586239
  3109: 2 [  455/ 1327], train_loss/perplexity = 4.79345751/120.7180328 secs/batch = 0.1992s, grad.norm=10.29302406
  3114: 2 [  460/ 1327], train_loss/perplexity = 4.94774246/140.8566132 secs/batch = 0.2010s, grad.norm=11.37100029
  3119: 2 [  465/ 1327], train_loss/perplexity = 4.78054285/119.1690216 secs/batch = 0.2023s, grad.norm=11.77192402
  3124: 2 [  470/ 1327], train_loss/perplexity = 5.26784658/193.9977570 secs/batch = 0.1977s, grad.norm=10.36864090
  3129: 2 [  475/ 1327], train_loss/perplexity = 4.83676720/126.0611649 secs/batch = 0.2000s, grad.norm=10.32941723
  3134: 2 [  480/ 1327], train_loss/perplexity = 5.01999521/151.4105835 secs/batch = 0.1995s, grad.norm=11.14617348
  3139: 2 [  485/ 1327], train_loss/perplexity = 4.92029428/137.0429382 secs/batch = 0.1926s, grad.norm=11.06263542
  3144: 2 [  490/ 1327], train_loss/perplexity = 4.82379103/124.4359360 secs/batch = 0.2006s, grad.norm=11.49955177
  3149: 2 [  495/ 1327], train_loss/perplexity = 4.82129812/124.1261139 secs/batch = 0.1992s, grad.norm=10.77576065
  3154: 2 [  500/ 1327], train_loss/perplexity = 5.10155869/164.2777710 secs/batch = 0.1969s, grad.norm=10.75079727
  3159: 2 [  505/ 1327], train_loss/perplexity = 5.04671574/155.5108795 secs/batch = 0.1988s, grad.norm=9.73830318
  3164: 2 [  510/ 1327], train_loss/perplexity = 5.45567417/234.0826263 secs/batch = 0.1991s, grad.norm=10.27562618
  3169: 2 [  515/ 1327], train_loss/perplexity = 5.06783152/158.8295288 secs/batch = 0.1999s, grad.norm=10.34135437
  3174: 2 [  520/ 1327], train_loss/perplexity = 5.31218338/202.7925110 secs/batch = 0.1989s, grad.norm=10.23309898
  3179: 2 [  525/ 1327], train_loss/perplexity = 4.82346249/124.3950653 secs/batch = 0.1960s, grad.norm=10.39722252
  3184: 2 [  530/ 1327], train_loss/perplexity = 4.86111832/129.1685791 secs/batch = 0.1950s, grad.norm=10.97693253
  3189: 2 [  535/ 1327], train_loss/perplexity = 4.93559837/139.1563873 secs/batch = 0.1993s, grad.norm=10.27636719
  3194: 2 [  540/ 1327], train_loss/perplexity = 5.06084824/157.7242432 secs/batch = 0.1996s, grad.norm=10.29926872
  3199: 2 [  545/ 1327], train_loss/perplexity = 5.16440821/174.9338989 secs/batch = 0.1931s, grad.norm=10.92655182
  3204: 2 [  550/ 1327], train_loss/perplexity = 5.06613684/158.5605927 secs/batch = 0.1992s, grad.norm=10.78208256
  3209: 2 [  555/ 1327], train_loss/perplexity = 4.92397308/137.5480194 secs/batch = 0.1993s, grad.norm=10.82361221
  3214: 2 [  560/ 1327], train_loss/perplexity = 4.97927856/145.3694763 secs/batch = 0.1994s, grad.norm=11.25785732
  3219: 2 [  565/ 1327], train_loss/perplexity = 4.96117115/142.7608948 secs/batch = 0.1992s, grad.norm=11.50607777
  3224: 2 [  570/ 1327], train_loss/perplexity = 4.81834221/123.7597504 secs/batch = 0.2001s, grad.norm=11.26220798
  3229: 2 [  575/ 1327], train_loss/perplexity = 4.74767876/115.3162994 secs/batch = 0.1991s, grad.norm=10.99139404
  3234: 2 [  580/ 1327], train_loss/perplexity = 5.17018938/175.9481506 secs/batch = 0.1990s, grad.norm=11.04227543
  3239: 2 [  585/ 1327], train_loss/perplexity = 4.66903257/106.5945663 secs/batch = 0.1982s, grad.norm=10.80104351
  3244: 2 [  590/ 1327], train_loss/perplexity = 4.94093704/139.9012756 secs/batch = 0.1986s, grad.norm=10.37850094
  3249: 2 [  595/ 1327], train_loss/perplexity = 4.92655945/137.9042358 secs/batch = 0.1992s, grad.norm=10.81793308
  3254: 2 [  600/ 1327], train_loss/perplexity = 5.15466404/173.2375946 secs/batch = 0.1992s, grad.norm=10.07976532
  3259: 2 [  605/ 1327], train_loss/perplexity = 5.12803555/168.6854248 secs/batch = 0.1999s, grad.norm=11.20837498
  3264: 2 [  610/ 1327], train_loss/perplexity = 5.25839758/192.1733093 secs/batch = 0.1929s, grad.norm=11.18414783
  3269: 2 [  615/ 1327], train_loss/perplexity = 4.77297115/118.2701187 secs/batch = 0.1987s, grad.norm=10.37802124
  3274: 2 [  620/ 1327], train_loss/perplexity = 5.08261633/161.1952515 secs/batch = 0.1998s, grad.norm=10.20036411
  3279: 2 [  625/ 1327], train_loss/perplexity = 5.18701649/178.9338989 secs/batch = 0.1989s, grad.norm=9.76105785
  3284: 2 [  630/ 1327], train_loss/perplexity = 5.24022532/188.7126160 secs/batch = 0.2000s, grad.norm=10.00911045
  3289: 2 [  635/ 1327], train_loss/perplexity = 4.92519999/137.7168732 secs/batch = 0.1988s, grad.norm=10.37239552
  3294: 2 [  640/ 1327], train_loss/perplexity = 4.98549652/146.2761841 secs/batch = 0.1947s, grad.norm=10.20490551
  3299: 2 [  645/ 1327], train_loss/perplexity = 5.17900276/177.5057068 secs/batch = 0.1997s, grad.norm=10.50387859
  3304: 2 [  650/ 1327], train_loss/perplexity = 4.84485340/127.0846481 secs/batch = 0.2005s, grad.norm=11.03726673
  3309: 2 [  655/ 1327], train_loss/perplexity = 4.87499571/130.9735870 secs/batch = 0.2006s, grad.norm=10.20783138
  3314: 2 [  660/ 1327], train_loss/perplexity = 4.75443602/116.0981598 secs/batch = 0.2008s, grad.norm=10.61778069
  3319: 2 [  665/ 1327], train_loss/perplexity = 5.01519442/150.6854248 secs/batch = 0.1990s, grad.norm=10.53146744
  3324: 2 [  670/ 1327], train_loss/perplexity = 4.92274094/137.3786469 secs/batch = 0.2008s, grad.norm=10.62742901
  3329: 2 [  675/ 1327], train_loss/perplexity = 4.67275429/106.9920273 secs/batch = 0.2000s, grad.norm=11.24478626
  3334: 2 [  680/ 1327], train_loss/perplexity = 4.96201372/142.8812256 secs/batch = 0.1995s, grad.norm=11.30846882
  3339: 2 [  685/ 1327], train_loss/perplexity = 4.83111048/125.3500824 secs/batch = 0.1996s, grad.norm=10.90263081
  3344: 2 [  690/ 1327], train_loss/perplexity = 5.15128326/172.6529083 secs/batch = 0.1923s, grad.norm=10.50417137
  3349: 2 [  695/ 1327], train_loss/perplexity = 4.95235872/141.5083466 secs/batch = 0.2009s, grad.norm=10.59349346
  3354: 2 [  700/ 1327], train_loss/perplexity = 5.20383739/181.9691925 secs/batch = 0.1995s, grad.norm=10.62463760
  3359: 2 [  705/ 1327], train_loss/perplexity = 4.89586067/133.7350616 secs/batch = 0.1998s, grad.norm=10.04181671
  3364: 2 [  710/ 1327], train_loss/perplexity = 4.83867121/126.3014145 secs/batch = 0.1971s, grad.norm=10.51942444
  3369: 2 [  715/ 1327], train_loss/perplexity = 4.82650423/124.7740173 secs/batch = 0.1995s, grad.norm=10.23702812
  3374: 2 [  720/ 1327], train_loss/perplexity = 4.91804075/136.7344513 secs/batch = 0.2000s, grad.norm=10.87385273
  3379: 2 [  725/ 1327], train_loss/perplexity = 4.76247406/117.0351181 secs/batch = 0.1994s, grad.norm=10.68645573
  3384: 2 [  730/ 1327], train_loss/perplexity = 4.95853376/142.3848724 secs/batch = 0.1995s, grad.norm=10.13817120
  3389: 2 [  735/ 1327], train_loss/perplexity = 5.01245880/150.2737732 secs/batch = 0.1993s, grad.norm=10.73250389
  3394: 2 [  740/ 1327], train_loss/perplexity = 4.45772123/86.2906494 secs/batch = 0.1997s, grad.norm=10.04926205
  3399: 2 [  745/ 1327], train_loss/perplexity = 4.95951033/142.5239868 secs/batch = 0.2000s, grad.norm=10.03960133
  3404: 2 [  750/ 1327], train_loss/perplexity = 4.77618790/118.6511765 secs/batch = 0.1992s, grad.norm=10.69534302
  3409: 2 [  755/ 1327], train_loss/perplexity = 4.73541164/113.9103394 secs/batch = 0.1949s, grad.norm=10.70199966
  3414: 2 [  760/ 1327], train_loss/perplexity = 4.67492533/107.2245636 secs/batch = 0.1999s, grad.norm=11.04749107
  3419: 2 [  765/ 1327], train_loss/perplexity = 4.71704054/111.8367844 secs/batch = 0.1984s, grad.norm=10.35435390
  3424: 2 [  770/ 1327], train_loss/perplexity = 4.61950397/101.4437027 secs/batch = 0.1999s, grad.norm=10.88911915
  3429: 2 [  775/ 1327], train_loss/perplexity = 4.86819553/130.0859680 secs/batch = 0.1986s, grad.norm=10.98196030
  3434: 2 [  780/ 1327], train_loss/perplexity = 5.15673876/173.5973816 secs/batch = 0.1994s, grad.norm=10.34125137
  3439: 2 [  785/ 1327], train_loss/perplexity = 4.91729259/136.6321869 secs/batch = 0.1952s, grad.norm=11.40676785
  3444: 2 [  790/ 1327], train_loss/perplexity = 4.75597429/116.2768860 secs/batch = 0.1995s, grad.norm=11.06128883
  3449: 2 [  795/ 1327], train_loss/perplexity = 5.10676479/165.1352386 secs/batch = 0.1998s, grad.norm=10.89698982
  3454: 2 [  800/ 1327], train_loss/perplexity = 4.94960928/141.1198120 secs/batch = 0.1980s, grad.norm=10.16229439
  3459: 2 [  805/ 1327], train_loss/perplexity = 5.29150867/198.6428833 secs/batch = 0.2001s, grad.norm=10.48569393
  3464: 2 [  810/ 1327], train_loss/perplexity = 4.93052340/138.4519653 secs/batch = 0.1999s, grad.norm=10.28082752
  3469: 2 [  815/ 1327], train_loss/perplexity = 4.84232712/126.7639999 secs/batch = 0.2004s, grad.norm=10.49904537
  3474: 2 [  820/ 1327], train_loss/perplexity = 4.51444054/91.3264542 secs/batch = 0.1952s, grad.norm=9.77696228
  3479: 2 [  825/ 1327], train_loss/perplexity = 4.73204231/113.5271835 secs/batch = 0.1992s, grad.norm=10.07704163
  3484: 2 [  830/ 1327], train_loss/perplexity = 4.62570763/102.0749817 secs/batch = 0.1994s, grad.norm=11.01563263
  3489: 2 [  835/ 1327], train_loss/perplexity = 4.92048454/137.0690155 secs/batch = 0.1976s, grad.norm=11.18621635
  3494: 2 [  840/ 1327], train_loss/perplexity = 5.00913668/149.7753754 secs/batch = 0.1937s, grad.norm=10.75714970
  3499: 2 [  845/ 1327], train_loss/perplexity = 4.77863979/118.9424515 secs/batch = 0.2013s, grad.norm=10.57971191
  3504: 2 [  850/ 1327], train_loss/perplexity = 4.84045744/126.5272141 secs/batch = 0.1995s, grad.norm=10.60080147
  3509: 2 [  855/ 1327], train_loss/perplexity = 4.81747389/123.6523361 secs/batch = 0.1990s, grad.norm=10.80593204
  3514: 2 [  860/ 1327], train_loss/perplexity = 4.57146215/96.6853790 secs/batch = 0.1996s, grad.norm=10.67276764
  3519: 2 [  865/ 1327], train_loss/perplexity = 4.98686981/146.4772034 secs/batch = 0.1993s, grad.norm=10.64082623
  3524: 2 [  870/ 1327], train_loss/perplexity = 5.03869629/154.2687683 secs/batch = 0.1985s, grad.norm=11.13743114
  3529: 2 [  875/ 1327], train_loss/perplexity = 4.51773167/91.6275177 secs/batch = 0.1997s, grad.norm=10.18334961
  3534: 2 [  880/ 1327], train_loss/perplexity = 4.70676374/110.6933441 secs/batch = 0.1992s, grad.norm=9.96131229
  3539: 2 [  885/ 1327], train_loss/perplexity = 4.85109520/127.8803711 secs/batch = 0.1932s, grad.norm=10.36049080
  3544: 2 [  890/ 1327], train_loss/perplexity = 5.02734900/152.5281219 secs/batch = 0.1988s, grad.norm=10.43397236
  3549: 2 [  895/ 1327], train_loss/perplexity = 5.10140228/164.2520752 secs/batch = 0.2006s, grad.norm=10.16510773
  3554: 2 [  900/ 1327], train_loss/perplexity = 4.87974119/131.5966034 secs/batch = 0.1988s, grad.norm=10.86591530
  3559: 2 [  905/ 1327], train_loss/perplexity = 4.77063322/117.9939346 secs/batch = 0.2012s, grad.norm=11.61676979
  3564: 2 [  910/ 1327], train_loss/perplexity = 4.82891512/125.0751953 secs/batch = 0.1991s, grad.norm=11.25502205
  3569: 2 [  915/ 1327], train_loss/perplexity = 4.98974895/146.8995361 secs/batch = 0.1988s, grad.norm=10.65235806
  3574: 2 [  920/ 1327], train_loss/perplexity = 5.20006943/181.2848206 secs/batch = 0.1988s, grad.norm=10.33921909
  3579: 2 [  925/ 1327], train_loss/perplexity = 5.02496386/152.1647491 secs/batch = 0.1995s, grad.norm=10.56245041
  3584: 2 [  930/ 1327], train_loss/perplexity = 4.91773415/136.6925354 secs/batch = 0.1995s, grad.norm=10.49586678
  3589: 2 [  935/ 1327], train_loss/perplexity = 5.02932405/152.8296661 secs/batch = 0.1998s, grad.norm=10.34202290
  3594: 2 [  940/ 1327], train_loss/perplexity = 5.01384497/150.4822235 secs/batch = 0.1947s, grad.norm=10.55083752
  3599: 2 [  945/ 1327], train_loss/perplexity = 5.16484404/175.0101624 secs/batch = 0.1963s, grad.norm=10.68462753
  3604: 2 [  950/ 1327], train_loss/perplexity = 4.93731833/139.3959351 secs/batch = 0.2002s, grad.norm=10.56400585
  3609: 2 [  955/ 1327], train_loss/perplexity = 4.95866823/142.4040222 secs/batch = 0.1995s, grad.norm=10.05961704
  3614: 2 [  960/ 1327], train_loss/perplexity = 5.21355391/183.7459106 secs/batch = 0.1990s, grad.norm=10.52442741
  3619: 2 [  965/ 1327], train_loss/perplexity = 4.99386549/147.5055084 secs/batch = 0.1991s, grad.norm=10.44973469
  3624: 2 [  970/ 1327], train_loss/perplexity = 5.18511915/178.5947266 secs/batch = 0.1995s, grad.norm=10.32055473
  3629: 2 [  975/ 1327], train_loss/perplexity = 4.91582155/136.4313507 secs/batch = 0.1995s, grad.norm=10.67208004
  3634: 2 [  980/ 1327], train_loss/perplexity = 4.69810581/109.7391052 secs/batch = 0.2007s, grad.norm=10.05508995
  3639: 2 [  985/ 1327], train_loss/perplexity = 4.95204353/141.4637604 secs/batch = 0.1989s, grad.norm=11.77227402
  3644: 2 [  990/ 1327], train_loss/perplexity = 5.06600475/158.5396576 secs/batch = 0.2001s, grad.norm=10.38689423
  3649: 2 [  995/ 1327], train_loss/perplexity = 5.07590675/160.1173096 secs/batch = 0.1994s, grad.norm=10.10083771
  3654: 2 [ 1000/ 1327], train_loss/perplexity = 4.51356125/91.2461929 secs/batch = 0.1987s, grad.norm=10.21115303
  3659: 2 [ 1005/ 1327], train_loss/perplexity = 5.03432274/153.5955353 secs/batch = 0.2002s, grad.norm=10.67943287
  3664: 2 [ 1010/ 1327], train_loss/perplexity = 4.60579443/100.0624466 secs/batch = 0.1958s, grad.norm=9.58097553
  3669: 2 [ 1015/ 1327], train_loss/perplexity = 5.09347820/162.9556732 secs/batch = 0.1988s, grad.norm=10.01585960
  3674: 2 [ 1020/ 1327], train_loss/perplexity = 5.33751678/207.9955750 secs/batch = 0.2003s, grad.norm=10.03014946
  3679: 2 [ 1025/ 1327], train_loss/perplexity = 5.08212662/161.1163177 secs/batch = 0.2004s, grad.norm=9.92139244
  3684: 2 [ 1030/ 1327], train_loss/perplexity = 4.87320995/130.7399139 secs/batch = 0.1997s, grad.norm=10.09346485
  3689: 2 [ 1035/ 1327], train_loss/perplexity = 4.78575373/119.7916183 secs/batch = 0.1991s, grad.norm=10.79574394
  3694: 2 [ 1040/ 1327], train_loss/perplexity = 5.07209396/159.5079803 secs/batch = 0.2004s, grad.norm=10.58673096
  3699: 2 [ 1045/ 1327], train_loss/perplexity = 4.66710567/106.3893738 secs/batch = 0.2007s, grad.norm=10.48370171
  3704: 2 [ 1050/ 1327], train_loss/perplexity = 4.69913864/109.8525085 secs/batch = 0.1986s, grad.norm=11.14572239
  3709: 2 [ 1055/ 1327], train_loss/perplexity = 4.81274557/123.0690460 secs/batch = 0.1990s, grad.norm=11.73335171
  3714: 2 [ 1060/ 1327], train_loss/perplexity = 4.49695778/89.7436981 secs/batch = 0.1949s, grad.norm=11.31871605
  3719: 2 [ 1065/ 1327], train_loss/perplexity = 4.64820147/104.3970566 secs/batch = 0.1999s, grad.norm=10.67037964
  3724: 2 [ 1070/ 1327], train_loss/perplexity = 5.01352215/150.4336548 secs/batch = 0.1996s, grad.norm=10.59734154
  3729: 2 [ 1075/ 1327], train_loss/perplexity = 4.76856184/117.7497787 secs/batch = 0.2005s, grad.norm=10.78285122
  3734: 2 [ 1080/ 1327], train_loss/perplexity = 4.68982601/108.8342438 secs/batch = 0.1993s, grad.norm=10.94192028
  3739: 2 [ 1085/ 1327], train_loss/perplexity = 4.45613575/86.1539459 secs/batch = 0.1999s, grad.norm=10.53286362
  3744: 2 [ 1090/ 1327], train_loss/perplexity = 4.62136459/101.6326218 secs/batch = 0.1998s, grad.norm=11.23854637
  3749: 2 [ 1095/ 1327], train_loss/perplexity = 4.86470747/129.6330109 secs/batch = 0.2002s, grad.norm=11.59293461
  3754: 2 [ 1100/ 1327], train_loss/perplexity = 4.73388290/113.7363358 secs/batch = 0.2009s, grad.norm=12.24542999
  3759: 2 [ 1105/ 1327], train_loss/perplexity = 4.56130743/95.7085266 secs/batch = 0.2001s, grad.norm=10.87629032
  3764: 2 [ 1110/ 1327], train_loss/perplexity = 5.15243864/172.8525085 secs/batch = 0.1996s, grad.norm=12.47563171
  3769: 2 [ 1115/ 1327], train_loss/perplexity = 4.63864899/103.4045486 secs/batch = 0.2004s, grad.norm=10.38107014
  3774: 2 [ 1120/ 1327], train_loss/perplexity = 4.89387846/133.4702301 secs/batch = 0.1995s, grad.norm=10.75934696
  3779: 2 [ 1125/ 1327], train_loss/perplexity = 5.11543512/166.5732422 secs/batch = 0.1989s, grad.norm=11.14968681
  3784: 2 [ 1130/ 1327], train_loss/perplexity = 4.73680592/114.0692749 secs/batch = 0.1994s, grad.norm=10.65938568
  3789: 2 [ 1135/ 1327], train_loss/perplexity = 4.77842712/118.9171600 secs/batch = 0.1948s, grad.norm=10.78849125
  3794: 2 [ 1140/ 1327], train_loss/perplexity = 5.06983423/159.1479492 secs/batch = 0.1994s, grad.norm=11.13776398
  3799: 2 [ 1145/ 1327], train_loss/perplexity = 4.91886473/136.8471680 secs/batch = 0.1939s, grad.norm=11.50828075
  3804: 2 [ 1150/ 1327], train_loss/perplexity = 4.80159187/121.7040024 secs/batch = 0.1997s, grad.norm=11.22797871
  3809: 2 [ 1155/ 1327], train_loss/perplexity = 4.86220074/129.3084564 secs/batch = 0.1935s, grad.norm=10.47126198
  3814: 2 [ 1160/ 1327], train_loss/perplexity = 4.89587736/133.7372894 secs/batch = 0.2020s, grad.norm=10.88928509
  3819: 2 [ 1165/ 1327], train_loss/perplexity = 4.99205589/147.2388153 secs/batch = 0.1992s, grad.norm=11.28079510
  3824: 2 [ 1170/ 1327], train_loss/perplexity = 4.77216721/118.1750717 secs/batch = 0.1992s, grad.norm=10.71810722
  3829: 2 [ 1175/ 1327], train_loss/perplexity = 4.56539202/96.1002579 secs/batch = 0.2005s, grad.norm=10.86466789
  3834: 2 [ 1180/ 1327], train_loss/perplexity = 4.52972126/92.7327118 secs/batch = 0.1995s, grad.norm=11.19049168
  3839: 2 [ 1185/ 1327], train_loss/perplexity = 4.77507162/118.5188065 secs/batch = 0.1998s, grad.norm=10.50312424
  3844: 2 [ 1190/ 1327], train_loss/perplexity = 4.77293634/118.2659988 secs/batch = 0.1993s, grad.norm=10.59599876
  3849: 2 [ 1195/ 1327], train_loss/perplexity = 4.62592173/102.0968323 secs/batch = 0.1993s, grad.norm=10.60462666
  3854: 2 [ 1200/ 1327], train_loss/perplexity = 4.57024860/96.5681152 secs/batch = 0.1998s, grad.norm=10.67243671
  3859: 2 [ 1205/ 1327], train_loss/perplexity = 4.62241459/101.7393951 secs/batch = 0.1994s, grad.norm=11.13932133
  3864: 2 [ 1210/ 1327], train_loss/perplexity = 4.38406277/80.1630554 secs/batch = 0.1997s, grad.norm=11.58147526
  3869: 2 [ 1215/ 1327], train_loss/perplexity = 4.50774193/90.7167435 secs/batch = 0.1989s, grad.norm=11.89014244
  3874: 2 [ 1220/ 1327], train_loss/perplexity = 4.68393898/108.1954117 secs/batch = 0.1985s, grad.norm=11.25576115
  3879: 2 [ 1225/ 1327], train_loss/perplexity = 4.50369072/90.3499756 secs/batch = 0.1995s, grad.norm=12.05070877
  3884: 2 [ 1230/ 1327], train_loss/perplexity = 4.68501711/108.3121262 secs/batch = 0.1993s, grad.norm=10.63458061
  3889: 2 [ 1235/ 1327], train_loss/perplexity = 4.67456913/107.1863708 secs/batch = 0.1993s, grad.norm=10.72587299
  3894: 2 [ 1240/ 1327], train_loss/perplexity = 4.74598598/115.1212540 secs/batch = 0.1992s, grad.norm=10.73890114
  3899: 2 [ 1245/ 1327], train_loss/perplexity = 4.79853725/121.3328094 secs/batch = 0.1991s, grad.norm=10.76723194
  3904: 2 [ 1250/ 1327], train_loss/perplexity = 4.87610817/131.1193695 secs/batch = 0.1992s, grad.norm=10.45938301
  3909: 2 [ 1255/ 1327], train_loss/perplexity = 4.89387321/133.4695282 secs/batch = 0.1992s, grad.norm=10.44816113
  3914: 2 [ 1260/ 1327], train_loss/perplexity = 4.74572468/115.0911789 secs/batch = 0.1944s, grad.norm=11.12044621
  3919: 2 [ 1265/ 1327], train_loss/perplexity = 4.92301798/137.4167023 secs/batch = 0.1990s, grad.norm=10.57891083
  3924: 2 [ 1270/ 1327], train_loss/perplexity = 4.60454893/99.9378967 secs/batch = 0.2000s, grad.norm=10.80322647
  3929: 2 [ 1275/ 1327], train_loss/perplexity = 4.85097551/127.8650665 secs/batch = 0.1994s, grad.norm=11.46436501
  3934: 2 [ 1280/ 1327], train_loss/perplexity = 4.63898230/103.4390259 secs/batch = 0.1995s, grad.norm=10.99850368
  3939: 2 [ 1285/ 1327], train_loss/perplexity = 4.63892841/103.4334488 secs/batch = 0.1997s, grad.norm=11.02037144
  3944: 2 [ 1290/ 1327], train_loss/perplexity = 4.79265165/120.6207886 secs/batch = 0.1987s, grad.norm=10.53503418
  3949: 2 [ 1295/ 1327], train_loss/perplexity = 4.90071487/134.3858185 secs/batch = 0.1979s, grad.norm=11.19527626
  3954: 2 [ 1300/ 1327], train_loss/perplexity = 4.98084259/145.5970154 secs/batch = 0.2000s, grad.norm=11.04737663
  3959: 2 [ 1305/ 1327], train_loss/perplexity = 5.12777805/168.6419830 secs/batch = 0.1930s, grad.norm=11.18838787
  3964: 2 [ 1310/ 1327], train_loss/perplexity = 5.33360815/207.1841736 secs/batch = 0.2005s, grad.norm=10.65331554
  3969: 2 [ 1315/ 1327], train_loss/perplexity = 5.12656689/168.4378510 secs/batch = 0.1991s, grad.norm=10.89612865
  3974: 2 [ 1320/ 1327], train_loss/perplexity = 5.10343170/164.5857544 secs/batch = 0.1994s, grad.norm=10.78706741
  3979: 2 [ 1325/ 1327], train_loss/perplexity = 5.02007008/151.4219208 secs/batch = 0.1996s, grad.norm=10.70645905
Epoch training time: 264.21539759635925
	> validation loss = 5.14622211, perplexity = 171.78129578
	> validation loss = 4.99592209, perplexity = 147.80917358
	> validation loss = 4.95539665, perplexity = 141.93888855
	> validation loss = 5.00466299, perplexity = 149.10682678
	> validation loss = 5.20029020, perplexity = 181.32485962
	> validation loss = 5.05947161, perplexity = 157.50726318
	> validation loss = 4.97233486, perplexity = 144.36355591
	> validation loss = 4.87582445, perplexity = 131.08218384
	> validation loss = 4.90875101, perplexity = 135.47010803
	> validation loss = 4.88494110, perplexity = 132.28266907
	> validation loss = 4.87724590, perplexity = 131.26864624
	> validation loss = 4.99540091, perplexity = 147.73216248
	> validation loss = 4.87730980, perplexity = 131.27702332
	> validation loss = 4.80699825, perplexity = 122.36376190
	> validation loss = 4.65086079, perplexity = 104.67504883
	> validation loss = 4.60331345, perplexity = 99.81449890
	> validation loss = 5.05810452, perplexity = 157.29208374
	> validation loss = 4.69125700, perplexity = 108.99009705
	> validation loss = 5.05244350, perplexity = 156.40417480
	> validation loss = 4.94533730, perplexity = 140.51823425
	> validation loss = 4.79170227, perplexity = 120.50632477
at the end of epoch: 2
train loss = 4.96603331, perplexity = 143.45670829
validation loss = 4.92039280, perplexity = 137.05643892
Saved model cv/epoch002_4.9204.model
  3986: 3 [    5/ 1327], train_loss/perplexity = 5.18677044/178.8898773 secs/batch = 0.1993s, grad.norm=10.64424229
  3991: 3 [   10/ 1327], train_loss/perplexity = 4.66507578/106.1736298 secs/batch = 0.1999s, grad.norm=10.45824337
  3996: 3 [   15/ 1327], train_loss/perplexity = 4.85866880/128.8525543 secs/batch = 0.1959s, grad.norm=10.17548370
  4001: 3 [   20/ 1327], train_loss/perplexity = 5.11840725/167.0690613 secs/batch = 0.1991s, grad.norm=11.43050575
  4006: 3 [   25/ 1327], train_loss/perplexity = 4.99039125/146.9939270 secs/batch = 0.1984s, grad.norm=10.88400173
  4011: 3 [   30/ 1327], train_loss/perplexity = 4.98596048/146.3440704 secs/batch = 0.1989s, grad.norm=10.46691132
  4016: 3 [   35/ 1327], train_loss/perplexity = 4.78241920/119.3928375 secs/batch = 0.1984s, grad.norm=10.01191998
  4021: 3 [   40/ 1327], train_loss/perplexity = 4.81115437/122.8733749 secs/batch = 0.1984s, grad.norm=11.59984875
  4026: 3 [   45/ 1327], train_loss/perplexity = 4.56175518/95.7513962 secs/batch = 0.1997s, grad.norm=10.23664093
  4031: 3 [   50/ 1327], train_loss/perplexity = 4.76199722/116.9793243 secs/batch = 0.1947s, grad.norm=10.34922218
  4036: 3 [   55/ 1327], train_loss/perplexity = 4.80956650/122.6784210 secs/batch = 0.1990s, grad.norm=10.85195637
  4041: 3 [   60/ 1327], train_loss/perplexity = 5.09408665/163.0548553 secs/batch = 0.1988s, grad.norm=11.31390572
  4046: 3 [   65/ 1327], train_loss/perplexity = 4.64125061/103.6739197 secs/batch = 0.1947s, grad.norm=11.27123642
  4051: 3 [   70/ 1327], train_loss/perplexity = 4.48556614/88.7271652 secs/batch = 0.1997s, grad.norm=11.16213131
  4056: 3 [   75/ 1327], train_loss/perplexity = 4.38175154/79.9779968 secs/batch = 0.2008s, grad.norm=10.74592590
  4061: 3 [   80/ 1327], train_loss/perplexity = 4.78008938/119.1149979 secs/batch = 0.2001s, grad.norm=11.46754932
  4066: 3 [   85/ 1327], train_loss/perplexity = 4.74371910/114.8605881 secs/batch = 0.2000s, grad.norm=10.51800251
  4071: 3 [   90/ 1327], train_loss/perplexity = 4.81815815/123.7369766 secs/batch = 0.1998s, grad.norm=11.43381023
  4076: 3 [   95/ 1327], train_loss/perplexity = 4.62675190/102.1816254 secs/batch = 0.1998s, grad.norm=11.06056213
  4081: 3 [  100/ 1327], train_loss/perplexity = 4.98330784/145.9563904 secs/batch = 0.1988s, grad.norm=11.02067947
  4086: 3 [  105/ 1327], train_loss/perplexity = 4.91194916/135.9040527 secs/batch = 0.1965s, grad.norm=11.48666573
  4091: 3 [  110/ 1327], train_loss/perplexity = 4.70616579/110.6271744 secs/batch = 0.1963s, grad.norm=10.94039345
  4096: 3 [  115/ 1327], train_loss/perplexity = 4.66998863/106.6965332 secs/batch = 0.1989s, grad.norm=11.99724102
  4101: 3 [  120/ 1327], train_loss/perplexity = 4.78826761/120.0931396 secs/batch = 0.2000s, grad.norm=11.75069427
  4106: 3 [  125/ 1327], train_loss/perplexity = 4.90783882/135.3465881 secs/batch = 0.1950s, grad.norm=11.45681381
  4111: 3 [  130/ 1327], train_loss/perplexity = 4.70629597/110.6415787 secs/batch = 0.2007s, grad.norm=11.95112610
  4116: 3 [  135/ 1327], train_loss/perplexity = 4.77801800/118.8685150 secs/batch = 0.1998s, grad.norm=11.45638752
  4121: 3 [  140/ 1327], train_loss/perplexity = 5.10903549/165.5106354 secs/batch = 0.1990s, grad.norm=11.19935036
  4126: 3 [  145/ 1327], train_loss/perplexity = 4.95272446/141.5601196 secs/batch = 0.1951s, grad.norm=11.92008114
  4131: 3 [  150/ 1327], train_loss/perplexity = 4.91720057/136.6196136 secs/batch = 0.1996s, grad.norm=11.01572990
  4136: 3 [  155/ 1327], train_loss/perplexity = 5.12901926/168.8514404 secs/batch = 0.1984s, grad.norm=10.10954666
  4141: 3 [  160/ 1327], train_loss/perplexity = 4.85492706/128.3713226 secs/batch = 0.2019s, grad.norm=10.81211853
  4146: 3 [  165/ 1327], train_loss/perplexity = 5.00649452/149.3801727 secs/batch = 0.1999s, grad.norm=10.80814075
  4151: 3 [  170/ 1327], train_loss/perplexity = 4.78565073/119.7792816 secs/batch = 0.2002s, grad.norm=11.13226986
  4156: 3 [  175/ 1327], train_loss/perplexity = 5.05488443/156.7864075 secs/batch = 0.1988s, grad.norm=10.50044346
  4161: 3 [  180/ 1327], train_loss/perplexity = 4.95786333/142.2894440 secs/batch = 0.2005s, grad.norm=11.16270065
  4166: 3 [  185/ 1327], train_loss/perplexity = 5.23369980/187.4851837 secs/batch = 0.1993s, grad.norm=11.56462955
  4171: 3 [  190/ 1327], train_loss/perplexity = 4.67382336/107.1064682 secs/batch = 0.2003s, grad.norm=10.65238762
  4176: 3 [  195/ 1327], train_loss/perplexity = 5.00072145/148.5202637 secs/batch = 0.2003s, grad.norm=10.62723351
  4181: 3 [  200/ 1327], train_loss/perplexity = 4.89564133/133.7057343 secs/batch = 0.1996s, grad.norm=10.99627686
  4186: 3 [  205/ 1327], train_loss/perplexity = 5.03058624/153.0226898 secs/batch = 0.1999s, grad.norm=10.51009369
  4191: 3 [  210/ 1327], train_loss/perplexity = 4.88049984/131.6964722 secs/batch = 0.1997s, grad.norm=9.89529991
  4196: 3 [  215/ 1327], train_loss/perplexity = 5.06398296/158.2194519 secs/batch = 0.1986s, grad.norm=10.49377918
  4201: 3 [  220/ 1327], train_loss/perplexity = 5.03683329/153.9816284 secs/batch = 0.1987s, grad.norm=10.47060108
  4206: 3 [  225/ 1327], train_loss/perplexity = 5.17763901/177.2637939 secs/batch = 0.1986s, grad.norm=10.99100018
  4211: 3 [  230/ 1327], train_loss/perplexity = 5.01042128/149.9678955 secs/batch = 0.1988s, grad.norm=10.83022785
  4216: 3 [  235/ 1327], train_loss/perplexity = 4.82417393/124.4835968 secs/batch = 0.1985s, grad.norm=10.46758652
  4221: 3 [  240/ 1327], train_loss/perplexity = 4.66039610/105.6779327 secs/batch = 0.1982s, grad.norm=11.33209324
  4226: 3 [  245/ 1327], train_loss/perplexity = 4.95850372/142.3806000 secs/batch = 0.1987s, grad.norm=10.68360329
  4231: 3 [  250/ 1327], train_loss/perplexity = 4.74388742/114.8799210 secs/batch = 0.1971s, grad.norm=10.84793377
  4236: 3 [  255/ 1327], train_loss/perplexity = 4.77267599/118.2352142 secs/batch = 0.2001s, grad.norm=10.60317707
  4241: 3 [  260/ 1327], train_loss/perplexity = 5.07172775/159.4495850 secs/batch = 0.1998s, grad.norm=11.41081715
  4246: 3 [  265/ 1327], train_loss/perplexity = 5.12118053/167.5330353 secs/batch = 0.1990s, grad.norm=10.64969730
  4251: 3 [  270/ 1327], train_loss/perplexity = 5.23165178/187.1015930 secs/batch = 0.1989s, grad.norm=10.26794529
  4256: 3 [  275/ 1327], train_loss/perplexity = 5.29987192/200.3111572 secs/batch = 0.1990s, grad.norm=10.79528713
  4261: 3 [  280/ 1327], train_loss/perplexity = 4.96187830/142.8618774 secs/batch = 0.1983s, grad.norm=10.75643158
  4266: 3 [  285/ 1327], train_loss/perplexity = 5.18153858/177.9564056 secs/batch = 0.1958s, grad.norm=11.54640007
  4271: 3 [  290/ 1327], train_loss/perplexity = 5.02779150/152.5956268 secs/batch = 0.1988s, grad.norm=10.77311993
  4276: 3 [  295/ 1327], train_loss/perplexity = 4.69672728/109.5879364 secs/batch = 0.1947s, grad.norm=10.74865818
  4281: 3 [  300/ 1327], train_loss/perplexity = 4.36267567/78.4668045 secs/batch = 0.1995s, grad.norm=10.37085724
  4286: 3 [  305/ 1327], train_loss/perplexity = 4.88883877/132.7992706 secs/batch = 0.1952s, grad.norm=10.97547150
  4291: 3 [  310/ 1327], train_loss/perplexity = 4.80833960/122.5279999 secs/batch = 0.1934s, grad.norm=10.70031834
  4296: 3 [  315/ 1327], train_loss/perplexity = 4.46163082/86.6286697 secs/batch = 0.2006s, grad.norm=10.85363674
  4301: 3 [  320/ 1327], train_loss/perplexity = 4.45955944/86.4494171 secs/batch = 0.1998s, grad.norm=11.77652264
  4306: 3 [  325/ 1327], train_loss/perplexity = 4.37996483/79.8352280 secs/batch = 0.1993s, grad.norm=10.74890423
  4311: 3 [  330/ 1327], train_loss/perplexity = 4.86781788/130.0368500 secs/batch = 0.1993s, grad.norm=11.95556164
  4316: 3 [  335/ 1327], train_loss/perplexity = 4.28649902/72.7114639 secs/batch = 0.2004s, grad.norm=10.99410248
  4321: 3 [  340/ 1327], train_loss/perplexity = 5.07595062/160.1243439 secs/batch = 0.1999s, grad.norm=10.56049442
  4326: 3 [  345/ 1327], train_loss/perplexity = 4.85577297/128.4799652 secs/batch = 0.2004s, grad.norm=10.48593616
  4331: 3 [  350/ 1327], train_loss/perplexity = 4.96462679/143.2550812 secs/batch = 0.1998s, grad.norm=11.17855072
  4336: 3 [  355/ 1327], train_loss/perplexity = 4.98795128/146.6356964 secs/batch = 0.1988s, grad.norm=10.91589355
  4341: 3 [  360/ 1327], train_loss/perplexity = 5.12928009/168.8954773 secs/batch = 0.1991s, grad.norm=10.73217487
  4346: 3 [  365/ 1327], train_loss/perplexity = 5.00167322/148.6616974 secs/batch = 0.2014s, grad.norm=10.87976265
  4351: 3 [  370/ 1327], train_loss/perplexity = 5.00240326/148.7702637 secs/batch = 0.1999s, grad.norm=11.93046761
  4356: 3 [  375/ 1327], train_loss/perplexity = 4.42973709/83.9093552 secs/batch = 0.1934s, grad.norm=11.26276398
  4361: 3 [  380/ 1327], train_loss/perplexity = 4.58087158/97.5994263 secs/batch = 0.2002s, grad.norm=11.29989433
  4366: 3 [  385/ 1327], train_loss/perplexity = 4.81683493/123.5733490 secs/batch = 0.1997s, grad.norm=11.32001972
  4371: 3 [  390/ 1327], train_loss/perplexity = 4.85275555/128.0928650 secs/batch = 0.1990s, grad.norm=10.55842400
  4376: 3 [  395/ 1327], train_loss/perplexity = 4.99243355/147.2944336 secs/batch = 0.1993s, grad.norm=11.18504810
  4381: 3 [  400/ 1327], train_loss/perplexity = 4.83246994/125.5206070 secs/batch = 0.1990s, grad.norm=11.04065704
  4386: 3 [  405/ 1327], train_loss/perplexity = 5.12831354/168.7323151 secs/batch = 0.1997s, grad.norm=10.59056950
  4391: 3 [  410/ 1327], train_loss/perplexity = 4.82748699/124.8966980 secs/batch = 0.2008s, grad.norm=10.76041985
  4396: 3 [  415/ 1327], train_loss/perplexity = 4.68509674/108.3207474 secs/batch = 0.1991s, grad.norm=10.57166386
  4401: 3 [  420/ 1327], train_loss/perplexity = 4.51108456/91.0204849 secs/batch = 0.2000s, grad.norm=11.97953701
  4406: 3 [  425/ 1327], train_loss/perplexity = 4.70718527/110.7400208 secs/batch = 0.1997s, grad.norm=11.85417175
  4411: 3 [  430/ 1327], train_loss/perplexity = 4.98905039/146.7969513 secs/batch = 0.2006s, grad.norm=12.13013268
  4416: 3 [  435/ 1327], train_loss/perplexity = 4.98787785/146.6249390 secs/batch = 0.2005s, grad.norm=11.79418564
  4421: 3 [  440/ 1327], train_loss/perplexity = 4.66422606/106.0834503 secs/batch = 0.1988s, grad.norm=11.62994003
  4426: 3 [  445/ 1327], train_loss/perplexity = 4.91274881/136.0127716 secs/batch = 0.1979s, grad.norm=11.92822552
  4431: 3 [  450/ 1327], train_loss/perplexity = 4.78971386/120.2669525 secs/batch = 0.1920s, grad.norm=11.27457237
  4436: 3 [  455/ 1327], train_loss/perplexity = 4.70286798/110.2629547 secs/batch = 0.1989s, grad.norm=10.80636024
  4441: 3 [  460/ 1327], train_loss/perplexity = 4.85413647/128.2698822 secs/batch = 0.1947s, grad.norm=12.17814541
  4446: 3 [  465/ 1327], train_loss/perplexity = 4.56838942/96.3887405 secs/batch = 0.1935s, grad.norm=12.48922920
  4451: 3 [  470/ 1327], train_loss/perplexity = 5.14681339/171.8828888 secs/batch = 0.2010s, grad.norm=11.37488270
  4456: 3 [  475/ 1327], train_loss/perplexity = 4.68648911/108.4716797 secs/batch = 0.2001s, grad.norm=11.60102081
  4461: 3 [  480/ 1327], train_loss/perplexity = 4.81910181/123.8537979 secs/batch = 0.1994s, grad.norm=11.72054195
  4466: 3 [  485/ 1327], train_loss/perplexity = 4.77227306/118.1875839 secs/batch = 0.2004s, grad.norm=12.29071426
  4471: 3 [  490/ 1327], train_loss/perplexity = 4.70831871/110.8656082 secs/batch = 0.1958s, grad.norm=12.13348198
  4476: 3 [  495/ 1327], train_loss/perplexity = 4.63228703/102.7487869 secs/batch = 0.1997s, grad.norm=10.93725204
  4481: 3 [  500/ 1327], train_loss/perplexity = 4.98376799/146.0235596 secs/batch = 0.2002s, grad.norm=11.18748951
  4486: 3 [  505/ 1327], train_loss/perplexity = 4.95690727/142.1534729 secs/batch = 0.1998s, grad.norm=10.84659672
  4491: 3 [  510/ 1327], train_loss/perplexity = 5.28545523/197.4440460 secs/batch = 0.2002s, grad.norm=10.24150181
  4496: 3 [  515/ 1327], train_loss/perplexity = 4.98210526/145.7809601 secs/batch = 0.2008s, grad.norm=10.88095093
  4501: 3 [  520/ 1327], train_loss/perplexity = 5.15724850/173.6858978 secs/batch = 0.2000s, grad.norm=10.65248585
  4506: 3 [  525/ 1327], train_loss/perplexity = 4.66696072/106.3739471 secs/batch = 0.1956s, grad.norm=10.82993221
  4511: 3 [  530/ 1327], train_loss/perplexity = 4.74988174/115.5706177 secs/batch = 0.1948s, grad.norm=11.77454662
  4516: 3 [  535/ 1327], train_loss/perplexity = 4.84410048/126.9889984 secs/batch = 0.1998s, grad.norm=10.49809170
  4521: 3 [  540/ 1327], train_loss/perplexity = 4.89668369/133.8451691 secs/batch = 0.1985s, grad.norm=10.57372665
  4526: 3 [  545/ 1327], train_loss/perplexity = 5.02423763/152.0542908 secs/batch = 0.1993s, grad.norm=11.63037014
  4531: 3 [  550/ 1327], train_loss/perplexity = 4.93029594/138.4204712 secs/batch = 0.1994s, grad.norm=11.32347202
  4536: 3 [  555/ 1327], train_loss/perplexity = 4.70239925/110.2112808 secs/batch = 0.1990s, grad.norm=10.36461830
  4541: 3 [  560/ 1327], train_loss/perplexity = 4.84224510/126.7536087 secs/batch = 0.1999s, grad.norm=13.20895004
  4546: 3 [  565/ 1327], train_loss/perplexity = 4.81288147/123.0857773 secs/batch = 0.1994s, grad.norm=11.53938103
  4551: 3 [  570/ 1327], train_loss/perplexity = 4.73000002/113.2955627 secs/batch = 0.1932s, grad.norm=12.69692039
  4556: 3 [  575/ 1327], train_loss/perplexity = 4.60420418/99.9034424 secs/batch = 0.2013s, grad.norm=11.58279514
  4561: 3 [  580/ 1327], train_loss/perplexity = 5.02574253/152.2832947 secs/batch = 0.1995s, grad.norm=12.02308464
  4566: 3 [  585/ 1327], train_loss/perplexity = 4.45720673/86.2462616 secs/batch = 0.2008s, grad.norm=11.22965717
  4571: 3 [  590/ 1327], train_loss/perplexity = 4.74342394/114.8266907 secs/batch = 0.1991s, grad.norm=11.37656975
  4576: 3 [  595/ 1327], train_loss/perplexity = 4.78552675/119.7644348 secs/batch = 0.2005s, grad.norm=12.09823704
  4581: 3 [  600/ 1327], train_loss/perplexity = 4.95508051/141.8940277 secs/batch = 0.1991s, grad.norm=10.68437290
  4586: 3 [  605/ 1327], train_loss/perplexity = 4.96080637/142.7088318 secs/batch = 0.2003s, grad.norm=11.62134552
  4591: 3 [  610/ 1327], train_loss/perplexity = 5.11974764/167.2931519 secs/batch = 0.1995s, grad.norm=11.59422588
  4596: 3 [  615/ 1327], train_loss/perplexity = 4.55535507/95.1405334 secs/batch = 0.1996s, grad.norm=10.72002602
  4601: 3 [  620/ 1327], train_loss/perplexity = 4.94606924/140.6211243 secs/batch = 0.1988s, grad.norm=11.31076622
  4606: 3 [  625/ 1327], train_loss/perplexity = 5.04838562/155.7707825 secs/batch = 0.1999s, grad.norm=11.87168026
  4611: 3 [  630/ 1327], train_loss/perplexity = 5.09201860/162.7179871 secs/batch = 0.1988s, grad.norm=11.43137932
  4616: 3 [  635/ 1327], train_loss/perplexity = 4.79619455/121.0488968 secs/batch = 0.2006s, grad.norm=11.04112244
  4621: 3 [  640/ 1327], train_loss/perplexity = 4.83781767/126.1936569 secs/batch = 0.1992s, grad.norm=11.24067116
  4626: 3 [  645/ 1327], train_loss/perplexity = 5.11802292/167.0048676 secs/batch = 0.2000s, grad.norm=13.00263977
  4631: 3 [  650/ 1327], train_loss/perplexity = 4.75722170/116.4220200 secs/batch = 0.1930s, grad.norm=11.98232651
  4636: 3 [  655/ 1327], train_loss/perplexity = 4.76811695/117.6974030 secs/batch = 0.1995s, grad.norm=11.02690125
  4641: 3 [  660/ 1327], train_loss/perplexity = 4.69026947/108.8825150 secs/batch = 0.1995s, grad.norm=13.23334885
  4646: 3 [  665/ 1327], train_loss/perplexity = 4.92539406/137.7436066 secs/batch = 0.1997s, grad.norm=11.41820431
  4651: 3 [  670/ 1327], train_loss/perplexity = 4.77120686/118.0616379 secs/batch = 0.1999s, grad.norm=11.47561359
  4656: 3 [  675/ 1327], train_loss/perplexity = 4.62508059/102.0109940 secs/batch = 0.1990s, grad.norm=11.43405724
  4661: 3 [  680/ 1327], train_loss/perplexity = 4.83562803/125.9176407 secs/batch = 0.1939s, grad.norm=12.16255665
  4666: 3 [  685/ 1327], train_loss/perplexity = 4.69712257/109.6312637 secs/batch = 0.1953s, grad.norm=11.26891708
  4671: 3 [  690/ 1327], train_loss/perplexity = 5.12366343/167.9495087 secs/batch = 0.1993s, grad.norm=16.06036568
  4676: 3 [  695/ 1327], train_loss/perplexity = 4.82085276/124.0708466 secs/batch = 0.2003s, grad.norm=11.15910625
  4681: 3 [  700/ 1327], train_loss/perplexity = 5.07986069/160.7516632 secs/batch = 0.1989s, grad.norm=11.28446102
  4686: 3 [  705/ 1327], train_loss/perplexity = 4.75060368/115.6540833 secs/batch = 0.1987s, grad.norm=10.88691902
  4691: 3 [  710/ 1327], train_loss/perplexity = 4.74469137/114.9723129 secs/batch = 0.1990s, grad.norm=11.38414192
  4696: 3 [  715/ 1327], train_loss/perplexity = 4.66086864/105.7278824 secs/batch = 0.1933s, grad.norm=11.06092930
  4701: 3 [  720/ 1327], train_loss/perplexity = 4.74414444/114.9094543 secs/batch = 0.1977s, grad.norm=11.49032021
  4706: 3 [  725/ 1327], train_loss/perplexity = 4.62009144/101.5033112 secs/batch = 0.1965s, grad.norm=11.60677338
  4711: 3 [  730/ 1327], train_loss/perplexity = 4.83316135/125.6074219 secs/batch = 0.1943s, grad.norm=10.74705315
  4716: 3 [  735/ 1327], train_loss/perplexity = 4.90459776/134.9086304 secs/batch = 0.2002s, grad.norm=12.14039421
  4721: 3 [  740/ 1327], train_loss/perplexity = 4.30036354/73.7265930 secs/batch = 0.2004s, grad.norm=10.86280346
  4726: 3 [  745/ 1327], train_loss/perplexity = 4.77497149/118.5069351 secs/batch = 0.2004s, grad.norm=11.02430439
  4731: 3 [  750/ 1327], train_loss/perplexity = 4.61134195/100.6190872 secs/batch = 0.1984s, grad.norm=11.48362827
  4736: 3 [  755/ 1327], train_loss/perplexity = 4.67798948/107.5536194 secs/batch = 0.1988s, grad.norm=13.09493351
  4741: 3 [  760/ 1327], train_loss/perplexity = 4.46804333/87.1859589 secs/batch = 0.1996s, grad.norm=11.77537918
  4746: 3 [  765/ 1327], train_loss/perplexity = 4.63515854/103.0442505 secs/batch = 0.1997s, grad.norm=12.35919380
  4751: 3 [  770/ 1327], train_loss/perplexity = 4.50122547/90.1275101 secs/batch = 0.1989s, grad.norm=11.74636555
  4756: 3 [  775/ 1327], train_loss/perplexity = 4.70106220/110.0640182 secs/batch = 0.2001s, grad.norm=12.19463539
  4761: 3 [  780/ 1327], train_loss/perplexity = 5.03981638/154.4416504 secs/batch = 0.1988s, grad.norm=11.67148876
  4766: 3 [  785/ 1327], train_loss/perplexity = 4.76020670/116.7700577 secs/batch = 0.1988s, grad.norm=12.85325527
  4771: 3 [  790/ 1327], train_loss/perplexity = 4.64795208/104.3710251 secs/batch = 0.1987s, grad.norm=11.23676491
  4776: 3 [  795/ 1327], train_loss/perplexity = 4.99713278/147.9882355 secs/batch = 0.2005s, grad.norm=11.67031574
  4781: 3 [  800/ 1327], train_loss/perplexity = 4.81570387/123.4336624 secs/batch = 0.2002s, grad.norm=11.19519901
  4786: 3 [  805/ 1327], train_loss/perplexity = 5.22418451/185.7096710 secs/batch = 0.1999s, grad.norm=11.52797985
  4791: 3 [  810/ 1327], train_loss/perplexity = 4.84208250/126.7330017 secs/batch = 0.1988s, grad.norm=11.05977726
  4796: 3 [  815/ 1327], train_loss/perplexity = 4.72360802/112.5736923 secs/batch = 0.1992s, grad.norm=11.11145973
  4801: 3 [  820/ 1327], train_loss/perplexity = 4.41735315/82.8766327 secs/batch = 0.2003s, grad.norm=10.54132175
  4806: 3 [  825/ 1327], train_loss/perplexity = 4.67384195/107.1084595 secs/batch = 0.1995s, grad.norm=11.02696133
  4811: 3 [  830/ 1327], train_loss/perplexity = 4.43897820/84.6883621 secs/batch = 0.2003s, grad.norm=10.54755974
  4816: 3 [  835/ 1327], train_loss/perplexity = 4.81265497/123.0578995 secs/batch = 0.1978s, grad.norm=11.95599270
  4821: 3 [  840/ 1327], train_loss/perplexity = 4.88197184/131.8904724 secs/batch = 0.1999s, grad.norm=11.61039352
  4826: 3 [  845/ 1327], train_loss/perplexity = 4.72650623/112.9004211 secs/batch = 0.2008s, grad.norm=12.10579777
  4831: 3 [  850/ 1327], train_loss/perplexity = 4.72780991/113.0477066 secs/batch = 0.1999s, grad.norm=11.26745701
  4836: 3 [  855/ 1327], train_loss/perplexity = 4.70980072/111.0300293 secs/batch = 0.1994s, grad.norm=11.55195141
  4841: 3 [  860/ 1327], train_loss/perplexity = 4.44242859/84.9810791 secs/batch = 0.2005s, grad.norm=11.10920906
  4846: 3 [  865/ 1327], train_loss/perplexity = 4.89805889/134.0293579 secs/batch = 0.2003s, grad.norm=11.82499218
  4851: 3 [  870/ 1327], train_loss/perplexity = 4.89848566/134.0865784 secs/batch = 0.1993s, grad.norm=11.97033119
  4856: 3 [  875/ 1327], train_loss/perplexity = 4.37980318/79.8223190 secs/batch = 0.2000s, grad.norm=11.18873882
  4861: 3 [  880/ 1327], train_loss/perplexity = 4.58725929/98.2248535 secs/batch = 0.1998s, grad.norm=10.99874401
  4866: 3 [  885/ 1327], train_loss/perplexity = 4.73611355/113.9903183 secs/batch = 0.2005s, grad.norm=10.89961433
  4871: 3 [  890/ 1327], train_loss/perplexity = 4.89677382/133.8572388 secs/batch = 0.1992s, grad.norm=11.34774971
  4876: 3 [  895/ 1327], train_loss/perplexity = 5.01019239/149.9335785 secs/batch = 0.1987s, grad.norm=10.78178120
  4881: 3 [  900/ 1327], train_loss/perplexity = 4.78810692/120.0738449 secs/batch = 0.1994s, grad.norm=12.44393539
  4886: 3 [  905/ 1327], train_loss/perplexity = 4.54956770/94.5915070 secs/batch = 0.1995s, grad.norm=11.52966690
  4891: 3 [  910/ 1327], train_loss/perplexity = 4.69591904/109.4993973 secs/batch = 0.1999s, grad.norm=11.10244751
  4896: 3 [  915/ 1327], train_loss/perplexity = 4.89224291/133.2521057 secs/batch = 0.1993s, grad.norm=11.34738445
  4901: 3 [  920/ 1327], train_loss/perplexity = 5.07200813/159.4942932 secs/batch = 0.1991s, grad.norm=11.66651154
  4906: 3 [  925/ 1327], train_loss/perplexity = 4.88830614/132.7285614 secs/batch = 0.2007s, grad.norm=11.41011238
  4911: 3 [  930/ 1327], train_loss/perplexity = 4.83173990/125.4290085 secs/batch = 0.1957s, grad.norm=10.92475796
  4916: 3 [  935/ 1327], train_loss/perplexity = 4.91427994/136.2211914 secs/batch = 0.2001s, grad.norm=11.35549927
  4921: 3 [  940/ 1327], train_loss/perplexity = 4.87231827/130.6233826 secs/batch = 0.1996s, grad.norm=11.22775650
  4926: 3 [  945/ 1327], train_loss/perplexity = 5.10179996/164.3174133 secs/batch = 0.1994s, grad.norm=11.38258362
  4931: 3 [  950/ 1327], train_loss/perplexity = 4.79545069/120.9588852 secs/batch = 0.1997s, grad.norm=11.46712208
  4936: 3 [  955/ 1327], train_loss/perplexity = 4.80702639/122.3672028 secs/batch = 0.1971s, grad.norm=10.96616745
  4941: 3 [  960/ 1327], train_loss/perplexity = 5.10975933/165.6304932 secs/batch = 0.1991s, grad.norm=10.99397469
  4946: 3 [  965/ 1327], train_loss/perplexity = 4.88262081/131.9760895 secs/batch = 0.1993s, grad.norm=11.41838455
  4951: 3 [  970/ 1327], train_loss/perplexity = 5.09646463/163.4430542 secs/batch = 0.1994s, grad.norm=10.58353233
  4956: 3 [  975/ 1327], train_loss/perplexity = 4.78319120/119.4850464 secs/batch = 0.1987s, grad.norm=11.64610100
  4961: 3 [  980/ 1327], train_loss/perplexity = 4.57995987/97.5104828 secs/batch = 0.2003s, grad.norm=10.95080948
  4966: 3 [  985/ 1327], train_loss/perplexity = 4.72734022/112.9946213 secs/batch = 0.2002s, grad.norm=11.69467354
  4971: 3 [  990/ 1327], train_loss/perplexity = 4.93567610/139.1672058 secs/batch = 0.1989s, grad.norm=11.05647087
  4976: 3 [  995/ 1327], train_loss/perplexity = 4.96459961/143.2511749 secs/batch = 0.1991s, grad.norm=10.69542599
  4981: 3 [ 1000/ 1327], train_loss/perplexity = 4.39255762/80.8469315 secs/batch = 0.2005s, grad.norm=10.56915760
  4986: 3 [ 1005/ 1327], train_loss/perplexity = 4.94741392/140.8103485 secs/batch = 0.1996s, grad.norm=11.28624058
  4991: 3 [ 1010/ 1327], train_loss/perplexity = 4.45268011/85.8567429 secs/batch = 0.1997s, grad.norm=10.47023773
  4996: 3 [ 1015/ 1327], train_loss/perplexity = 5.01944876/151.3278656 secs/batch = 0.1984s, grad.norm=10.74682426
  5001: 3 [ 1020/ 1327], train_loss/perplexity = 5.20858002/182.8342590 secs/batch = 0.1934s, grad.norm=11.09179020
  5006: 3 [ 1025/ 1327], train_loss/perplexity = 5.00923872/149.7906647 secs/batch = 0.1983s, grad.norm=11.05502987
  5011: 3 [ 1030/ 1327], train_loss/perplexity = 4.78686380/119.9246674 secs/batch = 0.1996s, grad.norm=10.48738766
  5016: 3 [ 1035/ 1327], train_loss/perplexity = 4.66192055/105.8391571 secs/batch = 0.1935s, grad.norm=10.33794785
  5021: 3 [ 1040/ 1327], train_loss/perplexity = 4.96451378/143.2388916 secs/batch = 0.2000s, grad.norm=10.96918869
  5026: 3 [ 1045/ 1327], train_loss/perplexity = 4.51030445/90.9495010 secs/batch = 0.1991s, grad.norm=10.80807590
  5031: 3 [ 1050/ 1327], train_loss/perplexity = 4.60518789/100.0017700 secs/batch = 0.2004s, grad.norm=11.59544563
  5036: 3 [ 1055/ 1327], train_loss/perplexity = 4.73935986/114.3609695 secs/batch = 0.1991s, grad.norm=11.97895336
  5041: 3 [ 1060/ 1327], train_loss/perplexity = 4.36229324/78.4368057 secs/batch = 0.2007s, grad.norm=11.48831463
  5046: 3 [ 1065/ 1327], train_loss/perplexity = 4.51561975/91.4342117 secs/batch = 0.1993s, grad.norm=11.04993534
  5051: 3 [ 1070/ 1327], train_loss/perplexity = 4.87884855/131.4791870 secs/batch = 0.1996s, grad.norm=11.12070370
  5056: 3 [ 1075/ 1327], train_loss/perplexity = 4.59384155/98.8735275 secs/batch = 0.1995s, grad.norm=11.10744667
  5061: 3 [ 1080/ 1327], train_loss/perplexity = 4.58071041/97.5836945 secs/batch = 0.1994s, grad.norm=11.51262760
  5066: 3 [ 1085/ 1327], train_loss/perplexity = 4.32720375/75.7322235 secs/batch = 0.1988s, grad.norm=11.17154980
  5071: 3 [ 1090/ 1327], train_loss/perplexity = 4.58945274/98.4405441 secs/batch = 0.1994s, grad.norm=12.19986153
  5076: 3 [ 1095/ 1327], train_loss/perplexity = 4.68951273/108.8001480 secs/batch = 0.1935s, grad.norm=11.49200726
  5081: 3 [ 1100/ 1327], train_loss/perplexity = 4.53527880/93.2495117 secs/batch = 0.1990s, grad.norm=13.27774239
  5086: 3 [ 1105/ 1327], train_loss/perplexity = 4.45054722/85.6738129 secs/batch = 0.1997s, grad.norm=11.47682476
  5091: 3 [ 1110/ 1327], train_loss/perplexity = 4.93429899/138.9756927 secs/batch = 0.1988s, grad.norm=12.72311401
  5096: 3 [ 1115/ 1327], train_loss/perplexity = 4.57596779/97.1219864 secs/batch = 0.1997s, grad.norm=11.04847813
  5101: 3 [ 1120/ 1327], train_loss/perplexity = 4.72601700/112.8452072 secs/batch = 0.1996s, grad.norm=11.08336926
  5106: 3 [ 1125/ 1327], train_loss/perplexity = 5.01757097/151.0439606 secs/batch = 0.1990s, grad.norm=11.85567379
  5111: 3 [ 1130/ 1327], train_loss/perplexity = 4.68256950/108.0473480 secs/batch = 0.1985s, grad.norm=11.35635376
  5116: 3 [ 1135/ 1327], train_loss/perplexity = 4.64960146/104.5433121 secs/batch = 0.1990s, grad.norm=10.93833542
  5121: 3 [ 1140/ 1327], train_loss/perplexity = 4.96782255/143.7136230 secs/batch = 0.1995s, grad.norm=11.42185116
  5126: 3 [ 1145/ 1327], train_loss/perplexity = 4.69448280/109.3422394 secs/batch = 0.2004s, grad.norm=11.47608376
  5131: 3 [ 1150/ 1327], train_loss/perplexity = 4.69059372/108.9178238 secs/batch = 0.1996s, grad.norm=11.01573086
  5136: 3 [ 1155/ 1327], train_loss/perplexity = 4.81537056/123.3925247 secs/batch = 0.2002s, grad.norm=11.22707272
  5141: 3 [ 1160/ 1327], train_loss/perplexity = 4.73270893/113.6028900 secs/batch = 0.1997s, grad.norm=11.32375336
  5146: 3 [ 1165/ 1327], train_loss/perplexity = 4.80603695/122.2461853 secs/batch = 0.2008s, grad.norm=11.69782543
  5151: 3 [ 1170/ 1327], train_loss/perplexity = 4.65619946/105.2353668 secs/batch = 0.1955s, grad.norm=11.90241623
  5156: 3 [ 1175/ 1327], train_loss/perplexity = 4.38890886/80.5524750 secs/batch = 0.2010s, grad.norm=11.69797230
  5161: 3 [ 1180/ 1327], train_loss/perplexity = 4.42378139/83.4111023 secs/batch = 0.1996s, grad.norm=11.34970570
  5166: 3 [ 1185/ 1327], train_loss/perplexity = 4.64851761/104.4300613 secs/batch = 0.2001s, grad.norm=11.29661942
  5171: 3 [ 1190/ 1327], train_loss/perplexity = 4.71674156/111.8033524 secs/batch = 0.2002s, grad.norm=11.43140316
  5176: 3 [ 1195/ 1327], train_loss/perplexity = 4.59395266/98.8845139 secs/batch = 0.2002s, grad.norm=11.39723015
  5181: 3 [ 1200/ 1327], train_loss/perplexity = 4.49684286/89.7333832 secs/batch = 0.2002s, grad.norm=11.41103172
  5186: 3 [ 1205/ 1327], train_loss/perplexity = 4.55573654/95.1768341 secs/batch = 0.2001s, grad.norm=11.69341755
  5191: 3 [ 1210/ 1327], train_loss/perplexity = 4.20439148/66.9798279 secs/batch = 0.1989s, grad.norm=11.90164089
  5196: 3 [ 1215/ 1327], train_loss/perplexity = 4.37807751/79.6846924 secs/batch = 0.1994s, grad.norm=11.28228569
  5201: 3 [ 1220/ 1327], train_loss/perplexity = 4.55918074/95.5052032 secs/batch = 0.1995s, grad.norm=12.16026306
  5206: 3 [ 1225/ 1327], train_loss/perplexity = 4.34092045/76.7781754 secs/batch = 0.1998s, grad.norm=13.43457890
  5211: 3 [ 1230/ 1327], train_loss/perplexity = 4.53256941/92.9972000 secs/batch = 0.1988s, grad.norm=11.59216690
  5216: 3 [ 1235/ 1327], train_loss/perplexity = 4.52863884/92.6323853 secs/batch = 0.1989s, grad.norm=11.64277363
  5221: 3 [ 1240/ 1327], train_loss/perplexity = 4.70128393/110.0884247 secs/batch = 0.2004s, grad.norm=11.08742142
  5226: 3 [ 1245/ 1327], train_loss/perplexity = 4.65012550/104.5981140 secs/batch = 0.1999s, grad.norm=11.72481632
  5231: 3 [ 1250/ 1327], train_loss/perplexity = 4.75257492/115.8822861 secs/batch = 0.1998s, grad.norm=10.83796787
  5236: 3 [ 1255/ 1327], train_loss/perplexity = 4.75181293/115.7940216 secs/batch = 0.1991s, grad.norm=10.62732220
  5241: 3 [ 1260/ 1327], train_loss/perplexity = 4.64263868/103.8179321 secs/batch = 0.1994s, grad.norm=12.04114914
  5246: 3 [ 1265/ 1327], train_loss/perplexity = 4.83092833/125.3272476 secs/batch = 0.1992s, grad.norm=11.31884289
  5251: 3 [ 1270/ 1327], train_loss/perplexity = 4.54448271/94.1117325 secs/batch = 0.1994s, grad.norm=11.52973366
  5256: 3 [ 1275/ 1327], train_loss/perplexity = 4.73705816/114.0980530 secs/batch = 0.1999s, grad.norm=12.23889446
  5261: 3 [ 1280/ 1327], train_loss/perplexity = 4.54047203/93.7350388 secs/batch = 0.1989s, grad.norm=11.66881847
  5266: 3 [ 1285/ 1327], train_loss/perplexity = 4.50035095/90.0487289 secs/batch = 0.2004s, grad.norm=11.58068848
  5271: 3 [ 1290/ 1327], train_loss/perplexity = 4.69831562/109.7621384 secs/batch = 0.1938s, grad.norm=10.68288708
  5276: 3 [ 1295/ 1327], train_loss/perplexity = 4.72789192/113.0569763 secs/batch = 0.1997s, grad.norm=11.61955452
  5281: 3 [ 1300/ 1327], train_loss/perplexity = 4.87115812/130.4719391 secs/batch = 0.1992s, grad.norm=11.47774601
  5286: 3 [ 1305/ 1327], train_loss/perplexity = 4.95752907/142.2418976 secs/batch = 0.1995s, grad.norm=11.63218021
  5291: 3 [ 1310/ 1327], train_loss/perplexity = 5.23589420/187.8970490 secs/batch = 0.1992s, grad.norm=11.68296146
  5296: 3 [ 1315/ 1327], train_loss/perplexity = 5.08394814/161.4100647 secs/batch = 0.1993s, grad.norm=14.52993870
  5301: 3 [ 1320/ 1327], train_loss/perplexity = 5.04671144/155.5102234 secs/batch = 0.1998s, grad.norm=11.56611156
  5306: 3 [ 1325/ 1327], train_loss/perplexity = 4.87651443/131.1726532 secs/batch = 0.2013s, grad.norm=10.99877262
Epoch training time: 264.3551678657532
	> validation loss = 5.06729078, perplexity = 158.74366760
	> validation loss = 4.92808104, perplexity = 138.11422729
	> validation loss = 4.92247295, perplexity = 137.34182739
	> validation loss = 4.94727802, perplexity = 140.79121399
	> validation loss = 5.12219524, perplexity = 167.70310974
	> validation loss = 4.97086000, perplexity = 144.15080261
	> validation loss = 4.92915869, perplexity = 138.26313782
	> validation loss = 4.81232929, perplexity = 123.01782990
	> validation loss = 4.78302383, perplexity = 119.46504974
	> validation loss = 4.76429462, perplexity = 117.24838257
	> validation loss = 4.79512310, perplexity = 120.91926575
	> validation loss = 4.92900372, perplexity = 138.24171448
	> validation loss = 4.82115221, perplexity = 124.10800934
	> validation loss = 4.72876167, perplexity = 113.15534973
	> validation loss = 4.58440018, perplexity = 97.94441986
	> validation loss = 4.54656982, perplexity = 94.30835724
	> validation loss = 5.03647232, perplexity = 153.92605591
	> validation loss = 4.64013004, perplexity = 103.55781555
	> validation loss = 4.99454308, perplexity = 147.60548401
	> validation loss = 4.83946323, perplexity = 126.40148926
	> validation loss = 4.70080137, perplexity = 110.03531647
at the end of epoch: 3
train loss = 4.84296167, perplexity = 126.84446742
validation loss = 4.84588703, perplexity = 127.21607583
Saved model cv/epoch003_4.8459.model
  5313: 4 [    5/ 1327], train_loss/perplexity = 5.03066063/153.0340729 secs/batch = 0.1989s, grad.norm=11.03481102
  5318: 4 [   10/ 1327], train_loss/perplexity = 4.54635334/94.2879410 secs/batch = 0.1991s, grad.norm=10.77119923
  5323: 4 [   15/ 1327], train_loss/perplexity = 4.80404568/122.0030060 secs/batch = 0.2000s, grad.norm=11.15522385
  5328: 4 [   20/ 1327], train_loss/perplexity = 4.91614628/136.4756622 secs/batch = 0.1990s, grad.norm=11.33549595
  5333: 4 [   25/ 1327], train_loss/perplexity = 4.93212795/138.6742859 secs/batch = 0.2002s, grad.norm=11.38447762
  5338: 4 [   30/ 1327], train_loss/perplexity = 4.90729284/135.2727203 secs/batch = 0.2007s, grad.norm=11.30414295
  5343: 4 [   35/ 1327], train_loss/perplexity = 4.74034643/114.4738541 secs/batch = 0.1995s, grad.norm=11.01165581
  5348: 4 [   40/ 1327], train_loss/perplexity = 4.65335989/104.9369736 secs/batch = 0.1995s, grad.norm=11.10894489
  5353: 4 [   45/ 1327], train_loss/perplexity = 4.42908716/83.8548355 secs/batch = 0.1993s, grad.norm=10.85234451
  5358: 4 [   50/ 1327], train_loss/perplexity = 4.68593979/108.4121094 secs/batch = 0.1989s, grad.norm=11.26200199
  5363: 4 [   55/ 1327], train_loss/perplexity = 4.83682823/126.0688553 secs/batch = 0.1993s, grad.norm=21.42076874
  5368: 4 [   60/ 1327], train_loss/perplexity = 5.01046467/149.9744110 secs/batch = 0.1987s, grad.norm=12.44076538
  5373: 4 [   65/ 1327], train_loss/perplexity = 4.54039764/93.7280655 secs/batch = 0.2000s, grad.norm=11.14932442
  5378: 4 [   70/ 1327], train_loss/perplexity = 4.32729387/75.7390518 secs/batch = 0.1992s, grad.norm=11.64062023
  5383: 4 [   75/ 1327], train_loss/perplexity = 4.24225235/69.5643616 secs/batch = 0.1992s, grad.norm=11.61345959
  5388: 4 [   80/ 1327], train_loss/perplexity = 4.67699337/107.4465332 secs/batch = 0.1931s, grad.norm=12.28038502
  5393: 4 [   85/ 1327], train_loss/perplexity = 4.62692642/102.1994629 secs/batch = 0.1992s, grad.norm=11.07995892
  5398: 4 [   90/ 1327], train_loss/perplexity = 4.69068098/108.9273300 secs/batch = 0.1991s, grad.norm=11.93106461
  5403: 4 [   95/ 1327], train_loss/perplexity = 4.51895189/91.7393951 secs/batch = 0.1991s, grad.norm=11.24568462
  5408: 4 [  100/ 1327], train_loss/perplexity = 4.88894081/132.8128204 secs/batch = 0.1989s, grad.norm=11.84643364
  5413: 4 [  105/ 1327], train_loss/perplexity = 4.79762411/121.2220612 secs/batch = 0.2003s, grad.norm=12.76449299
  5418: 4 [  110/ 1327], train_loss/perplexity = 4.57672596/97.1956482 secs/batch = 0.2009s, grad.norm=11.02559471
  5423: 4 [  115/ 1327], train_loss/perplexity = 4.55873203/95.4623566 secs/batch = 0.2006s, grad.norm=12.21882725
  5428: 4 [  120/ 1327], train_loss/perplexity = 4.70178938/110.1440887 secs/batch = 0.1974s, grad.norm=11.69526768
  5433: 4 [  125/ 1327], train_loss/perplexity = 4.78240538/119.3911819 secs/batch = 0.1985s, grad.norm=11.63465118
  5438: 4 [  130/ 1327], train_loss/perplexity = 4.63061523/102.5771561 secs/batch = 0.1995s, grad.norm=12.54260921
  5443: 4 [  135/ 1327], train_loss/perplexity = 4.62119198/101.6150818 secs/batch = 0.1994s, grad.norm=11.90348434
  5448: 4 [  140/ 1327], train_loss/perplexity = 4.93957520/139.7108917 secs/batch = 0.1959s, grad.norm=12.44180965
  5453: 4 [  145/ 1327], train_loss/perplexity = 4.86711359/129.9452972 secs/batch = 0.1980s, grad.norm=12.12905312
  5458: 4 [  150/ 1327], train_loss/perplexity = 4.83755016/126.1599045 secs/batch = 0.1991s, grad.norm=11.66000366
  5463: 4 [  155/ 1327], train_loss/perplexity = 5.12944412/168.9231873 secs/batch = 0.2000s, grad.norm=16.16609001
  5468: 4 [  160/ 1327], train_loss/perplexity = 4.74889994/115.4572067 secs/batch = 0.2003s, grad.norm=10.89968300
  5473: 4 [  165/ 1327], train_loss/perplexity = 4.94265032/140.1411743 secs/batch = 0.1995s, grad.norm=11.08015442
  5478: 4 [  170/ 1327], train_loss/perplexity = 4.71438599/111.5403061 secs/batch = 0.1955s, grad.norm=11.57099915
  5483: 4 [  175/ 1327], train_loss/perplexity = 4.97146368/144.2378540 secs/batch = 0.1998s, grad.norm=11.58066082
  5488: 4 [  180/ 1327], train_loss/perplexity = 4.88081741/131.7383118 secs/batch = 0.1986s, grad.norm=11.73567486
  5493: 4 [  185/ 1327], train_loss/perplexity = 5.10408926/164.6940155 secs/batch = 0.1993s, grad.norm=11.62511063
  5498: 4 [  190/ 1327], train_loss/perplexity = 4.60651445/100.1345139 secs/batch = 0.2007s, grad.norm=10.76052380
  5503: 4 [  195/ 1327], train_loss/perplexity = 4.82646132/124.7686615 secs/batch = 0.1998s, grad.norm=10.26423073
  5508: 4 [  200/ 1327], train_loss/perplexity = 4.78424788/119.6113663 secs/batch = 0.2003s, grad.norm=11.62102985
  5513: 4 [  205/ 1327], train_loss/perplexity = 5.01100159/150.0549469 secs/batch = 0.1997s, grad.norm=10.82802486
  5518: 4 [  210/ 1327], train_loss/perplexity = 4.76877928/117.7753830 secs/batch = 0.1993s, grad.norm=10.70275307
  5523: 4 [  215/ 1327], train_loss/perplexity = 4.91429806/136.2236481 secs/batch = 0.1999s, grad.norm=10.68400478
  5528: 4 [  220/ 1327], train_loss/perplexity = 4.91473293/136.2829132 secs/batch = 0.1987s, grad.norm=10.22962093
  5533: 4 [  225/ 1327], train_loss/perplexity = 5.06659126/158.6326752 secs/batch = 0.1996s, grad.norm=11.84663773
  5538: 4 [  230/ 1327], train_loss/perplexity = 4.88772488/132.6514282 secs/batch = 0.1989s, grad.norm=11.80831623
  5543: 4 [  235/ 1327], train_loss/perplexity = 4.67934704/107.6997223 secs/batch = 0.2001s, grad.norm=11.64697456
  5548: 4 [  240/ 1327], train_loss/perplexity = 4.59324646/98.8147049 secs/batch = 0.1996s, grad.norm=11.62770653
  5553: 4 [  245/ 1327], train_loss/perplexity = 4.86969614/130.2813263 secs/batch = 0.1991s, grad.norm=11.68337727
  5558: 4 [  250/ 1327], train_loss/perplexity = 4.56331921/95.9012680 secs/batch = 0.1984s, grad.norm=10.47681141
  5563: 4 [  255/ 1327], train_loss/perplexity = 4.65343428/104.9447784 secs/batch = 0.1934s, grad.norm=11.24372292
  5568: 4 [  260/ 1327], train_loss/perplexity = 4.94932032/141.0790405 secs/batch = 0.1992s, grad.norm=11.86476898
  5573: 4 [  265/ 1327], train_loss/perplexity = 5.04441357/155.1532898 secs/batch = 0.1990s, grad.norm=11.28742123
  5578: 4 [  270/ 1327], train_loss/perplexity = 5.16188669/174.4933624 secs/batch = 0.1991s, grad.norm=11.18638706
  5583: 4 [  275/ 1327], train_loss/perplexity = 5.16865587/175.6785431 secs/batch = 0.2010s, grad.norm=10.58594799
  5588: 4 [  280/ 1327], train_loss/perplexity = 4.83118820/125.3598251 secs/batch = 0.2011s, grad.norm=10.62149048
  5593: 4 [  285/ 1327], train_loss/perplexity = 5.07629442/160.1793976 secs/batch = 0.1981s, grad.norm=11.08921432
  5598: 4 [  290/ 1327], train_loss/perplexity = 4.90635347/135.1457062 secs/batch = 0.1997s, grad.norm=12.33327579
  5603: 4 [  295/ 1327], train_loss/perplexity = 4.68212700/107.9995422 secs/batch = 0.1999s, grad.norm=11.35008812
  5608: 4 [  300/ 1327], train_loss/perplexity = 4.26882267/71.4374771 secs/batch = 0.1992s, grad.norm=10.65574551
  5613: 4 [  305/ 1327], train_loss/perplexity = 4.71895504/112.0511017 secs/batch = 0.1942s, grad.norm=11.05467224
  5618: 4 [  310/ 1327], train_loss/perplexity = 4.69410419/109.3008499 secs/batch = 0.1994s, grad.norm=12.15400696
  5623: 4 [  315/ 1327], train_loss/perplexity = 4.33129978/76.0430603 secs/batch = 0.1942s, grad.norm=11.04147243
  5628: 4 [  320/ 1327], train_loss/perplexity = 4.35009575/77.4858780 secs/batch = 0.1990s, grad.norm=12.51563644
  5633: 4 [  325/ 1327], train_loss/perplexity = 4.29765606/73.5272446 secs/batch = 0.2001s, grad.norm=11.47011852
  5638: 4 [  330/ 1327], train_loss/perplexity = 4.73788166/114.1920471 secs/batch = 0.1992s, grad.norm=11.62059689
  5643: 4 [  335/ 1327], train_loss/perplexity = 4.20553684/67.0565872 secs/batch = 0.1995s, grad.norm=10.89694309
  5648: 4 [  340/ 1327], train_loss/perplexity = 4.95234632/141.5065918 secs/batch = 0.1999s, grad.norm=11.13785553
  5653: 4 [  345/ 1327], train_loss/perplexity = 4.72339392/112.5495911 secs/batch = 0.1989s, grad.norm=11.37557983
  5658: 4 [  350/ 1327], train_loss/perplexity = 4.81537485/123.3930588 secs/batch = 0.1997s, grad.norm=12.09103298
  5663: 4 [  355/ 1327], train_loss/perplexity = 4.85344028/128.1806030 secs/batch = 0.1951s, grad.norm=11.45390129
  5668: 4 [  360/ 1327], train_loss/perplexity = 4.99869919/148.2202301 secs/batch = 0.2003s, grad.norm=11.68560123
  5673: 4 [  365/ 1327], train_loss/perplexity = 4.92361450/137.4987030 secs/batch = 0.1991s, grad.norm=11.42520237
  5678: 4 [  370/ 1327], train_loss/perplexity = 4.92873049/138.2039490 secs/batch = 0.2001s, grad.norm=11.61720753
  5683: 4 [  375/ 1327], train_loss/perplexity = 4.34130049/76.8073578 secs/batch = 0.2001s, grad.norm=11.22795105
  5688: 4 [  380/ 1327], train_loss/perplexity = 4.48186827/88.3996735 secs/batch = 0.1994s, grad.norm=12.28176498
  5693: 4 [  385/ 1327], train_loss/perplexity = 4.66100502/105.7423019 secs/batch = 0.2006s, grad.norm=12.07359123
  5698: 4 [  390/ 1327], train_loss/perplexity = 4.81501484/123.3486404 secs/batch = 0.1996s, grad.norm=11.31587505
  5703: 4 [  395/ 1327], train_loss/perplexity = 4.86922693/130.2202148 secs/batch = 0.2005s, grad.norm=12.05358601
  5708: 4 [  400/ 1327], train_loss/perplexity = 4.74658298/115.1900024 secs/batch = 0.2000s, grad.norm=11.52203560
  5713: 4 [  405/ 1327], train_loss/perplexity = 5.05140686/156.2421265 secs/batch = 0.1988s, grad.norm=11.65226460
  5718: 4 [  410/ 1327], train_loss/perplexity = 4.65567732/105.1804352 secs/batch = 0.1994s, grad.norm=11.82543564
  5723: 4 [  415/ 1327], train_loss/perplexity = 4.62338734/101.8384094 secs/batch = 0.1994s, grad.norm=11.55988121
  5728: 4 [  420/ 1327], train_loss/perplexity = 4.32066154/75.2383881 secs/batch = 0.2001s, grad.norm=11.16824245
  5733: 4 [  425/ 1327], train_loss/perplexity = 4.66270971/105.9227142 secs/batch = 0.2001s, grad.norm=12.46498299
  5738: 4 [  430/ 1327], train_loss/perplexity = 4.89644241/133.8128815 secs/batch = 0.1999s, grad.norm=12.18239880
  5743: 4 [  435/ 1327], train_loss/perplexity = 4.84084272/126.5759735 secs/batch = 0.1994s, grad.norm=12.32778358
  5748: 4 [  440/ 1327], train_loss/perplexity = 4.57466650/96.9956894 secs/batch = 0.2001s, grad.norm=12.24644470
  5753: 4 [  445/ 1327], train_loss/perplexity = 4.87285519/130.6935425 secs/batch = 0.2001s, grad.norm=12.12832832
  5758: 4 [  450/ 1327], train_loss/perplexity = 4.62317848/101.8171387 secs/batch = 0.1992s, grad.norm=11.76177120
  5763: 4 [  455/ 1327], train_loss/perplexity = 4.56416750/95.9826584 secs/batch = 0.2004s, grad.norm=11.49182606
  5768: 4 [  460/ 1327], train_loss/perplexity = 4.64828634/104.4059143 secs/batch = 0.1993s, grad.norm=11.76807308
  5773: 4 [  465/ 1327], train_loss/perplexity = 4.43694496/84.5163498 secs/batch = 0.1986s, grad.norm=12.32957745
  5778: 4 [  470/ 1327], train_loss/perplexity = 5.07659054/160.2268372 secs/batch = 0.1947s, grad.norm=11.18507290
  5783: 4 [  475/ 1327], train_loss/perplexity = 4.51690006/91.5513535 secs/batch = 0.2007s, grad.norm=11.33443546
  5788: 4 [  480/ 1327], train_loss/perplexity = 4.65696526/105.3159866 secs/batch = 0.2000s, grad.norm=11.64682007
  5793: 4 [  485/ 1327], train_loss/perplexity = 4.59036493/98.5303802 secs/batch = 0.1929s, grad.norm=11.57797050
  5798: 4 [  490/ 1327], train_loss/perplexity = 4.56269646/95.8415680 secs/batch = 0.1923s, grad.norm=12.30140305
  5803: 4 [  495/ 1327], train_loss/perplexity = 4.56624508/96.1822739 secs/batch = 0.1999s, grad.norm=11.87656116
  5808: 4 [  500/ 1327], train_loss/perplexity = 4.81809616/123.7293091 secs/batch = 0.1997s, grad.norm=11.72438335
  5813: 4 [  505/ 1327], train_loss/perplexity = 4.85767269/128.7242737 secs/batch = 0.1999s, grad.norm=10.80054951
  5818: 4 [  510/ 1327], train_loss/perplexity = 5.22862291/186.5357513 secs/batch = 0.2003s, grad.norm=10.94376564
  5823: 4 [  515/ 1327], train_loss/perplexity = 4.84049225/126.5316238 secs/batch = 0.1999s, grad.norm=10.88883495
  5828: 4 [  520/ 1327], train_loss/perplexity = 5.10470200/164.7949524 secs/batch = 0.1991s, grad.norm=13.80951214
  5833: 4 [  525/ 1327], train_loss/perplexity = 4.51674795/91.5374298 secs/batch = 0.2005s, grad.norm=11.67197227
  5838: 4 [  530/ 1327], train_loss/perplexity = 4.64389420/103.9483566 secs/batch = 0.1999s, grad.norm=12.85711002
  5843: 4 [  535/ 1327], train_loss/perplexity = 4.76219845/117.0028687 secs/batch = 0.2006s, grad.norm=11.63731956
  5848: 4 [  540/ 1327], train_loss/perplexity = 4.83960962/126.4199905 secs/batch = 0.1996s, grad.norm=11.30615997
  5853: 4 [  545/ 1327], train_loss/perplexity = 4.91882277/136.8414307 secs/batch = 0.1998s, grad.norm=11.59079456
  5858: 4 [  550/ 1327], train_loss/perplexity = 4.83868933/126.3037033 secs/batch = 0.1955s, grad.norm=11.66491890
  5863: 4 [  555/ 1327], train_loss/perplexity = 4.62402344/101.9032059 secs/batch = 0.1943s, grad.norm=11.34190845
  5868: 4 [  560/ 1327], train_loss/perplexity = 4.77803087/118.8700485 secs/batch = 0.2002s, grad.norm=12.76588631
  5873: 4 [  565/ 1327], train_loss/perplexity = 4.70123005/110.0824966 secs/batch = 0.1992s, grad.norm=12.28086472
  5878: 4 [  570/ 1327], train_loss/perplexity = 4.61667252/101.1568756 secs/batch = 0.1996s, grad.norm=12.36152554
  5883: 4 [  575/ 1327], train_loss/perplexity = 4.42217922/83.2775650 secs/batch = 0.2001s, grad.norm=11.58588505
  5888: 4 [  580/ 1327], train_loss/perplexity = 4.90504789/134.9693756 secs/batch = 0.1991s, grad.norm=12.03012466
  5893: 4 [  585/ 1327], train_loss/perplexity = 4.40578508/81.9234314 secs/batch = 0.2010s, grad.norm=11.44817066
  5898: 4 [  590/ 1327], train_loss/perplexity = 4.72332954/112.5423431 secs/batch = 0.1932s, grad.norm=11.49278831
  5903: 4 [  595/ 1327], train_loss/perplexity = 4.70881176/110.9202805 secs/batch = 0.1995s, grad.norm=12.08840752
  5908: 4 [  600/ 1327], train_loss/perplexity = 4.88412142/132.1742859 secs/batch = 0.1978s, grad.norm=10.80921268
  5913: 4 [  605/ 1327], train_loss/perplexity = 4.89299774/133.3527374 secs/batch = 0.2004s, grad.norm=11.83912086
  5918: 4 [  610/ 1327], train_loss/perplexity = 5.04271412/154.8898315 secs/batch = 0.1947s, grad.norm=12.23926163
  5923: 4 [  615/ 1327], train_loss/perplexity = 4.54385281/94.0524673 secs/batch = 0.2002s, grad.norm=11.44148540
  5928: 4 [  620/ 1327], train_loss/perplexity = 4.88366222/132.1136017 secs/batch = 0.1995s, grad.norm=12.12856102
  5933: 4 [  625/ 1327], train_loss/perplexity = 4.90897083/135.4998932 secs/batch = 0.1987s, grad.norm=11.90307331
  5938: 4 [  630/ 1327], train_loss/perplexity = 4.97145844/144.2370911 secs/batch = 0.1999s, grad.norm=11.27413368
  5943: 4 [  635/ 1327], train_loss/perplexity = 4.72201347/112.3943253 secs/batch = 0.1983s, grad.norm=11.45872688
  5948: 4 [  640/ 1327], train_loss/perplexity = 4.79022169/120.3280411 secs/batch = 0.1990s, grad.norm=12.85459232
  5953: 4 [  645/ 1327], train_loss/perplexity = 5.66886759/289.7062683 secs/batch = 0.1982s, grad.norm=38.80673218
  5958: 4 [  650/ 1327], train_loss/perplexity = 4.51525593/91.4009552 secs/batch = 0.1990s, grad.norm=12.20632458
  5963: 4 [  655/ 1327], train_loss/perplexity = 4.68605900/108.4250336 secs/batch = 0.1993s, grad.norm=12.64090538
  5968: 4 [  660/ 1327], train_loss/perplexity = 4.55157328/94.7814102 secs/batch = 0.1960s, grad.norm=11.31280994
  5973: 4 [  665/ 1327], train_loss/perplexity = 4.75404835/116.0531616 secs/batch = 0.1995s, grad.norm=12.25901413
  5978: 4 [  670/ 1327], train_loss/perplexity = 4.67745972/107.4966507 secs/batch = 0.1958s, grad.norm=11.96801186
  5983: 4 [  675/ 1327], train_loss/perplexity = 4.54187107/93.8662643 secs/batch = 0.1999s, grad.norm=12.23559952
  5988: 4 [  680/ 1327], train_loss/perplexity = 4.69723082/109.6431274 secs/batch = 0.1998s, grad.norm=12.83060741
  5993: 4 [  685/ 1327], train_loss/perplexity = 4.63721943/103.2568359 secs/batch = 0.2000s, grad.norm=11.84946251
  5998: 4 [  690/ 1327], train_loss/perplexity = 4.96353912/143.0993500 secs/batch = 0.1996s, grad.norm=11.12304020
  6003: 4 [  695/ 1327], train_loss/perplexity = 4.72584343/112.8256149 secs/batch = 0.1990s, grad.norm=11.51979828
  6008: 4 [  700/ 1327], train_loss/perplexity = 4.94705772/140.7601929 secs/batch = 0.2005s, grad.norm=12.17707348
  6013: 4 [  705/ 1327], train_loss/perplexity = 4.65104771/104.6946182 secs/batch = 0.1993s, grad.norm=11.18945312
  6018: 4 [  710/ 1327], train_loss/perplexity = 4.62045765/101.5404892 secs/batch = 0.2005s, grad.norm=11.94757175
  6023: 4 [  715/ 1327], train_loss/perplexity = 4.57779217/97.2993393 secs/batch = 0.1994s, grad.norm=13.18473530
  6028: 4 [  720/ 1327], train_loss/perplexity = 4.61165857/100.6509476 secs/batch = 0.2008s, grad.norm=12.03756809
  6033: 4 [  725/ 1327], train_loss/perplexity = 4.51511526/91.3880997 secs/batch = 0.1991s, grad.norm=11.39538002
  6038: 4 [  730/ 1327], train_loss/perplexity = 4.70194817/110.1615753 secs/batch = 0.2002s, grad.norm=11.71158504
  6043: 4 [  735/ 1327], train_loss/perplexity = 4.76965475/117.8785400 secs/batch = 0.1999s, grad.norm=11.66125965
  6048: 4 [  740/ 1327], train_loss/perplexity = 4.21424246/67.6429062 secs/batch = 0.1954s, grad.norm=11.39134598
  6053: 4 [  745/ 1327], train_loss/perplexity = 4.72461224/112.6867905 secs/batch = 0.1991s, grad.norm=11.67549610
  6058: 4 [  750/ 1327], train_loss/perplexity = 4.56512451/96.0745544 secs/batch = 0.2000s, grad.norm=12.32368755
  6063: 4 [  755/ 1327], train_loss/perplexity = 4.58775473/98.2735291 secs/batch = 0.2003s, grad.norm=12.33383656
  6068: 4 [  760/ 1327], train_loss/perplexity = 4.35773993/78.0804672 secs/batch = 0.2002s, grad.norm=12.45698738
  6073: 4 [  765/ 1327], train_loss/perplexity = 4.43736362/84.5517349 secs/batch = 0.1975s, grad.norm=11.96434975
  6078: 4 [  770/ 1327], train_loss/perplexity = 4.43037271/83.9627075 secs/batch = 0.2001s, grad.norm=11.77129555
  6083: 4 [  775/ 1327], train_loss/perplexity = 4.61800766/101.2920227 secs/batch = 0.2007s, grad.norm=11.73032093
  6088: 4 [  780/ 1327], train_loss/perplexity = 4.94881725/141.0080872 secs/batch = 0.2008s, grad.norm=11.22819710
  6093: 4 [  785/ 1327], train_loss/perplexity = 4.68285131/108.0777969 secs/batch = 0.2008s, grad.norm=12.75504589
  6098: 4 [  790/ 1327], train_loss/perplexity = 4.42230797/83.2882919 secs/batch = 0.2009s, grad.norm=11.47413826
  6103: 4 [  795/ 1327], train_loss/perplexity = 4.93430805/138.9769440 secs/batch = 0.1991s, grad.norm=12.23755646
  6108: 4 [  800/ 1327], train_loss/perplexity = 4.75456810/116.1134949 secs/batch = 0.1953s, grad.norm=12.26726532
  6113: 4 [  805/ 1327], train_loss/perplexity = 5.08507872/161.5926514 secs/batch = 0.1992s, grad.norm=11.34333229
  6118: 4 [  810/ 1327], train_loss/perplexity = 4.73669672/114.0568161 secs/batch = 0.1961s, grad.norm=11.74784184
  6123: 4 [  815/ 1327], train_loss/perplexity = 4.59585857/99.0731583 secs/batch = 0.1994s, grad.norm=11.01794243
  6128: 4 [  820/ 1327], train_loss/perplexity = 4.35761070/78.0703812 secs/batch = 0.1996s, grad.norm=10.94588852
  6133: 4 [  825/ 1327], train_loss/perplexity = 4.60012627/99.4968796 secs/batch = 0.1987s, grad.norm=11.50235939
  6138: 4 [  830/ 1327], train_loss/perplexity = 4.41319942/82.5330963 secs/batch = 0.1998s, grad.norm=11.51375961
  6143: 4 [  835/ 1327], train_loss/perplexity = 4.72178793/112.3689804 secs/batch = 0.1986s, grad.norm=12.35209656
  6148: 4 [  840/ 1327], train_loss/perplexity = 4.78567743/119.7824783 secs/batch = 0.1984s, grad.norm=11.44228840
  6153: 4 [  845/ 1327], train_loss/perplexity = 4.56600666/96.1593475 secs/batch = 0.2003s, grad.norm=11.92121220
  6158: 4 [  850/ 1327], train_loss/perplexity = 4.58036137/97.5496368 secs/batch = 0.1997s, grad.norm=11.21163559
  6163: 4 [  855/ 1327], train_loss/perplexity = 4.56100607/95.6796951 secs/batch = 0.1994s, grad.norm=12.08227348
  6168: 4 [  860/ 1327], train_loss/perplexity = 4.36521006/78.6659241 secs/batch = 0.1935s, grad.norm=11.98685646
  6173: 4 [  865/ 1327], train_loss/perplexity = 4.81791067/123.7063599 secs/batch = 0.1991s, grad.norm=11.33064938
  6178: 4 [  870/ 1327], train_loss/perplexity = 4.73507214/113.8716736 secs/batch = 0.2000s, grad.norm=11.72326756
  6183: 4 [  875/ 1327], train_loss/perplexity = 4.29763222/73.5254974 secs/batch = 0.1994s, grad.norm=11.61973953
  6188: 4 [  880/ 1327], train_loss/perplexity = 4.53820896/93.5231476 secs/batch = 0.1995s, grad.norm=11.13256645
  6193: 4 [  885/ 1327], train_loss/perplexity = 4.62163162/101.6597672 secs/batch = 0.1949s, grad.norm=11.44463348
  6198: 4 [  890/ 1327], train_loss/perplexity = 4.95559502/141.9670563 secs/batch = 0.1992s, grad.norm=16.66480637
  6203: 4 [  895/ 1327], train_loss/perplexity = 4.91525364/136.3538971 secs/batch = 0.1997s, grad.norm=11.43036461
  6208: 4 [  900/ 1327], train_loss/perplexity = 4.72060394/112.2360153 secs/batch = 0.1997s, grad.norm=11.80758858
  6213: 4 [  905/ 1327], train_loss/perplexity = 4.46531153/86.9481125 secs/batch = 0.2002s, grad.norm=11.18880463
  6218: 4 [  910/ 1327], train_loss/perplexity = 4.57361794/96.8940353 secs/batch = 0.1994s, grad.norm=12.31215382
  6223: 4 [  915/ 1327], train_loss/perplexity = 4.81188583/122.9632874 secs/batch = 0.2001s, grad.norm=11.83491802
  6228: 4 [  920/ 1327], train_loss/perplexity = 5.01357937/150.4422607 secs/batch = 0.1980s, grad.norm=11.87515068
  6233: 4 [  925/ 1327], train_loss/perplexity = 4.79765654/121.2259979 secs/batch = 0.2010s, grad.norm=11.10741138
  6238: 4 [  930/ 1327], train_loss/perplexity = 4.73587513/113.9631500 secs/batch = 0.1992s, grad.norm=11.12139034
  6243: 4 [  935/ 1327], train_loss/perplexity = 4.86078835/129.1259613 secs/batch = 0.1992s, grad.norm=11.97282505
  6248: 4 [  940/ 1327], train_loss/perplexity = 4.79825974/121.2991409 secs/batch = 0.1971s, grad.norm=11.55949593
  6253: 4 [  945/ 1327], train_loss/perplexity = 4.97181797/144.2889557 secs/batch = 0.1998s, grad.norm=12.27335644
  6258: 4 [  950/ 1327], train_loss/perplexity = 4.74480629/114.9855270 secs/batch = 0.2010s, grad.norm=11.02512169
  6263: 4 [  955/ 1327], train_loss/perplexity = 4.74600124/115.1230164 secs/batch = 0.1994s, grad.norm=11.56179810
  6268: 4 [  960/ 1327], train_loss/perplexity = 4.99514246/147.6939850 secs/batch = 0.1970s, grad.norm=11.06079388
  6273: 4 [  965/ 1327], train_loss/perplexity = 4.78310299/119.4745026 secs/batch = 0.1997s, grad.norm=11.89662743
  6278: 4 [  970/ 1327], train_loss/perplexity = 4.99003506/146.9415741 secs/batch = 0.2003s, grad.norm=11.55466938
  6283: 4 [  975/ 1327], train_loss/perplexity = 4.70842075/110.8769226 secs/batch = 0.1991s, grad.norm=11.85542107
  6288: 4 [  980/ 1327], train_loss/perplexity = 4.50621033/90.5779037 secs/batch = 0.1961s, grad.norm=11.78518486
  6293: 4 [  985/ 1327], train_loss/perplexity = 4.64290810/103.8459015 secs/batch = 0.1997s, grad.norm=12.77032661
  6298: 4 [  990/ 1327], train_loss/perplexity = 4.88235569/131.9411163 secs/batch = 0.1996s, grad.norm=11.50755501
  6303: 4 [  995/ 1327], train_loss/perplexity = 4.88918638/132.8454437 secs/batch = 0.1988s, grad.norm=10.81352425
  6308: 4 [ 1000/ 1327], train_loss/perplexity = 4.27792168/72.0904541 secs/batch = 0.1986s, grad.norm=11.04941750
  6313: 4 [ 1005/ 1327], train_loss/perplexity = 4.85330486/128.1632538 secs/batch = 0.1989s, grad.norm=11.37714100
  6318: 4 [ 1010/ 1327], train_loss/perplexity = 4.36189413/78.4055023 secs/batch = 0.2002s, grad.norm=11.37168217
  6323: 4 [ 1015/ 1327], train_loss/perplexity = 4.94551659/140.5434418 secs/batch = 0.2001s, grad.norm=11.35328865
  6328: 4 [ 1020/ 1327], train_loss/perplexity = 5.05082083/156.1505890 secs/batch = 0.1994s, grad.norm=11.85440540
  6333: 4 [ 1025/ 1327], train_loss/perplexity = 4.90100765/134.4251709 secs/batch = 0.1983s, grad.norm=11.33634567
  6338: 4 [ 1030/ 1327], train_loss/perplexity = 4.70719528/110.7411270 secs/batch = 0.1987s, grad.norm=11.48687744
  6343: 4 [ 1035/ 1327], train_loss/perplexity = 4.56147671/95.7247314 secs/batch = 0.1947s, grad.norm=11.22480869
  6348: 4 [ 1040/ 1327], train_loss/perplexity = 4.87467480/130.9315643 secs/batch = 0.1988s, grad.norm=11.48356438
  6353: 4 [ 1045/ 1327], train_loss/perplexity = 4.42613268/83.6074524 secs/batch = 0.1993s, grad.norm=11.23668098
  6358: 4 [ 1050/ 1327], train_loss/perplexity = 4.49827862/89.8623123 secs/batch = 0.1994s, grad.norm=11.78014755
  6363: 4 [ 1055/ 1327], train_loss/perplexity = 4.63629961/103.1619034 secs/batch = 0.1955s, grad.norm=12.24932480
  6368: 4 [ 1060/ 1327], train_loss/perplexity = 4.25632381/70.5501480 secs/batch = 0.2011s, grad.norm=12.19742203
  6373: 4 [ 1065/ 1327], train_loss/perplexity = 4.40264225/81.6663666 secs/batch = 0.2003s, grad.norm=12.09768486
  6378: 4 [ 1070/ 1327], train_loss/perplexity = 4.75247765/115.8710175 secs/batch = 0.1989s, grad.norm=11.89857101
  6383: 4 [ 1075/ 1327], train_loss/perplexity = 4.50689745/90.6401672 secs/batch = 0.1989s, grad.norm=11.79617500
  6388: 4 [ 1080/ 1327], train_loss/perplexity = 4.40088463/81.5229568 secs/batch = 0.1989s, grad.norm=12.17910576
  6393: 4 [ 1085/ 1327], train_loss/perplexity = 4.29703236/73.4813995 secs/batch = 0.1999s, grad.norm=11.63149071
  6398: 4 [ 1090/ 1327], train_loss/perplexity = 4.51116657/91.0279465 secs/batch = 0.1994s, grad.norm=11.89832878
  6403: 4 [ 1095/ 1327], train_loss/perplexity = 4.64701700/104.2734756 secs/batch = 0.2000s, grad.norm=13.19148254
  6408: 4 [ 1100/ 1327], train_loss/perplexity = 4.43750334/84.5635529 secs/batch = 0.2007s, grad.norm=12.78612137
  6413: 4 [ 1105/ 1327], train_loss/perplexity = 4.36454153/78.6133499 secs/batch = 0.2001s, grad.norm=11.91049480
  6418: 4 [ 1110/ 1327], train_loss/perplexity = 4.78665876/119.9000854 secs/batch = 0.1997s, grad.norm=13.11187744
  6423: 4 [ 1115/ 1327], train_loss/perplexity = 4.41684008/82.8341217 secs/batch = 0.1997s, grad.norm=11.76058197
  6428: 4 [ 1120/ 1327], train_loss/perplexity = 4.65830612/105.4572983 secs/batch = 0.1997s, grad.norm=11.62057972
  6433: 4 [ 1125/ 1327], train_loss/perplexity = 4.90375137/134.7944946 secs/batch = 0.1988s, grad.norm=12.24111366
  6438: 4 [ 1130/ 1327], train_loss/perplexity = 4.55650997/95.2504730 secs/batch = 0.2000s, grad.norm=12.03052044
  6443: 4 [ 1135/ 1327], train_loss/perplexity = 4.72710133/112.9676285 secs/batch = 0.1995s, grad.norm=19.89081001
  6448: 4 [ 1140/ 1327], train_loss/perplexity = 4.88041830/131.6857300 secs/batch = 0.1998s, grad.norm=12.13652992
  6453: 4 [ 1145/ 1327], train_loss/perplexity = 4.64484167/104.0468903 secs/batch = 0.1998s, grad.norm=12.10816574
  6458: 4 [ 1150/ 1327], train_loss/perplexity = 4.65797329/105.4222031 secs/batch = 0.1985s, grad.norm=12.01442528
  6463: 4 [ 1155/ 1327], train_loss/perplexity = 4.70371056/110.3558960 secs/batch = 0.2009s, grad.norm=11.68897438
  6468: 4 [ 1160/ 1327], train_loss/perplexity = 4.69440842/109.3341064 secs/batch = 0.1930s, grad.norm=11.57915592
  6473: 4 [ 1165/ 1327], train_loss/perplexity = 4.71342611/111.4332886 secs/batch = 0.1988s, grad.norm=11.36678791
  6478: 4 [ 1170/ 1327], train_loss/perplexity = 4.54129601/93.8123016 secs/batch = 0.1978s, grad.norm=12.27821541
  6483: 4 [ 1175/ 1327], train_loss/perplexity = 4.33437729/76.2774429 secs/batch = 0.1944s, grad.norm=12.47166443
  6488: 4 [ 1180/ 1327], train_loss/perplexity = 4.36476183/78.6306686 secs/batch = 0.1997s, grad.norm=11.62721539
  6493: 4 [ 1185/ 1327], train_loss/perplexity = 4.58919859/98.4155273 secs/batch = 0.2007s, grad.norm=11.35187149
  6498: 4 [ 1190/ 1327], train_loss/perplexity = 4.64096498/103.6443176 secs/batch = 0.2001s, grad.norm=11.87355232
  6503: 4 [ 1195/ 1327], train_loss/perplexity = 4.47432375/87.7352524 secs/batch = 0.1980s, grad.norm=12.72197914
  6508: 4 [ 1200/ 1327], train_loss/perplexity = 4.45362759/85.9381256 secs/batch = 0.2007s, grad.norm=11.77791214
  6513: 4 [ 1205/ 1327], train_loss/perplexity = 4.43799925/84.6054993 secs/batch = 0.1993s, grad.norm=12.45666599
  6518: 4 [ 1210/ 1327], train_loss/perplexity = 4.08433819/59.4026108 secs/batch = 0.1994s, grad.norm=12.16341591
  6523: 4 [ 1215/ 1327], train_loss/perplexity = 4.32136917/75.2916412 secs/batch = 0.1997s, grad.norm=12.21604633
  6528: 4 [ 1220/ 1327], train_loss/perplexity = 4.51526928/91.4021759 secs/batch = 0.1997s, grad.norm=12.58459759
  6533: 4 [ 1225/ 1327], train_loss/perplexity = 4.30081463/73.7598572 secs/batch = 0.2016s, grad.norm=12.81717968
  6538: 4 [ 1230/ 1327], train_loss/perplexity = 4.46723557/87.1155624 secs/batch = 0.1987s, grad.norm=11.79968643
  6543: 4 [ 1235/ 1327], train_loss/perplexity = 4.46764851/87.1515427 secs/batch = 0.1990s, grad.norm=11.27067661
  6548: 4 [ 1240/ 1327], train_loss/perplexity = 4.72906446/113.1896210 secs/batch = 0.1990s, grad.norm=15.37123203
  6553: 4 [ 1245/ 1327], train_loss/perplexity = 4.54991770/94.6246185 secs/batch = 0.2001s, grad.norm=11.45533276
  6558: 4 [ 1250/ 1327], train_loss/perplexity = 4.60076332/99.5602798 secs/batch = 0.1987s, grad.norm=10.94234085
  6563: 4 [ 1255/ 1327], train_loss/perplexity = 4.71669340/111.7979660 secs/batch = 0.1982s, grad.norm=11.00242043
  6568: 4 [ 1260/ 1327], train_loss/perplexity = 4.50129986/90.1342163 secs/batch = 0.1934s, grad.norm=13.37295818
  6573: 4 [ 1265/ 1327], train_loss/perplexity = 4.76958084/117.8698273 secs/batch = 0.1991s, grad.norm=11.85852146
  6578: 4 [ 1270/ 1327], train_loss/perplexity = 4.40725994/82.0443497 secs/batch = 0.1992s, grad.norm=11.59727383
  6583: 4 [ 1275/ 1327], train_loss/perplexity = 4.67232800/106.9464264 secs/batch = 0.1996s, grad.norm=12.36647224
  6588: 4 [ 1280/ 1327], train_loss/perplexity = 4.39182138/80.7874298 secs/batch = 0.2003s, grad.norm=11.56198025
  6593: 4 [ 1285/ 1327], train_loss/perplexity = 4.43187284/84.0887527 secs/batch = 0.2004s, grad.norm=11.60443020
  6598: 4 [ 1290/ 1327], train_loss/perplexity = 4.61198473/100.6837845 secs/batch = 0.1999s, grad.norm=11.19640541
  6603: 4 [ 1295/ 1327], train_loss/perplexity = 4.62572622/102.0768738 secs/batch = 0.1992s, grad.norm=12.19574070
  6608: 4 [ 1300/ 1327], train_loss/perplexity = 4.78725863/119.9720306 secs/batch = 0.1993s, grad.norm=11.77573586
  6613: 4 [ 1305/ 1327], train_loss/perplexity = 4.89956331/134.2311554 secs/batch = 0.1989s, grad.norm=11.83552265
  6618: 4 [ 1310/ 1327], train_loss/perplexity = 5.08295727/161.2502136 secs/batch = 0.2002s, grad.norm=11.47981262
  6623: 4 [ 1315/ 1327], train_loss/perplexity = 4.97191620/144.3031311 secs/batch = 0.2014s, grad.norm=11.81445599
  6628: 4 [ 1320/ 1327], train_loss/perplexity = 4.97616339/144.9173279 secs/batch = 0.1993s, grad.norm=12.46226215
  6633: 4 [ 1325/ 1327], train_loss/perplexity = 4.84513950/127.1210175 secs/batch = 0.1950s, grad.norm=11.70077515
Epoch training time: 264.2911398410797
	> validation loss = 5.02108145, perplexity = 151.57513428
	> validation loss = 4.91026688, perplexity = 135.67561340
	> validation loss = 4.83083439, perplexity = 125.31547546
	> validation loss = 4.85281515, perplexity = 128.10050964
	> validation loss = 5.07155371, perplexity = 159.42182922
	> validation loss = 4.93317413, perplexity = 138.81944275
	> validation loss = 4.89087868, perplexity = 133.07044983
	> validation loss = 4.76639462, perplexity = 117.49486542
	> validation loss = 4.64900208, perplexity = 104.48066711
	> validation loss = 4.72057009, perplexity = 112.23221588
	> validation loss = 4.73027182, perplexity = 113.32636261
	> validation loss = 4.85203171, perplexity = 128.00018311
	> validation loss = 4.73313427, perplexity = 113.65121460
	> validation loss = 4.65926170, perplexity = 105.55812073
	> validation loss = 4.52015209, perplexity = 91.84956360
	> validation loss = 4.47624779, perplexity = 87.90422058
	> validation loss = 4.99350214, perplexity = 147.45191956
	> validation loss = 4.58552790, perplexity = 98.05493927
	> validation loss = 4.94250202, perplexity = 140.12039185
	> validation loss = 4.80536842, perplexity = 122.16448975
	> validation loss = 4.63178349, perplexity = 102.69705963
at the end of epoch: 4
train loss = 4.76589698, perplexity = 117.43640859
validation loss = 4.78577487, perplexity = 119.79415163
Saved model cv/epoch004_4.7858.model
  6640: 5 [    5/ 1327], train_loss/perplexity = 4.92928505/138.2806091 secs/batch = 0.1991s, grad.norm=11.45662880
  6645: 5 [   10/ 1327], train_loss/perplexity = 4.40011120/81.4599228 secs/batch = 0.1992s, grad.norm=11.43648148
  6650: 5 [   15/ 1327], train_loss/perplexity = 4.70011950/109.9603119 secs/batch = 0.1951s, grad.norm=11.06836987
  6655: 5 [   20/ 1327], train_loss/perplexity = 4.93406963/138.9438171 secs/batch = 0.1939s, grad.norm=12.60012913
  6660: 5 [   25/ 1327], train_loss/perplexity = 4.77485800/118.4934845 secs/batch = 0.1996s, grad.norm=11.66589546
  6665: 5 [   30/ 1327], train_loss/perplexity = 4.79029179/120.3364792 secs/batch = 0.1988s, grad.norm=12.02272797
  6670: 5 [   35/ 1327], train_loss/perplexity = 4.60385990/99.8690567 secs/batch = 0.1992s, grad.norm=10.95118904
  6675: 5 [   40/ 1327], train_loss/perplexity = 4.60739803/100.2230301 secs/batch = 0.2014s, grad.norm=11.67539215
  6680: 5 [   45/ 1327], train_loss/perplexity = 4.33767462/76.5293732 secs/batch = 0.1998s, grad.norm=10.76239395
  6685: 5 [   50/ 1327], train_loss/perplexity = 4.61156654/100.6416855 secs/batch = 0.1991s, grad.norm=12.37091923
  6690: 5 [   55/ 1327], train_loss/perplexity = 4.64999199/104.5841446 secs/batch = 0.1983s, grad.norm=12.70147419
  6695: 5 [   60/ 1327], train_loss/perplexity = 4.90900612/135.5046692 secs/batch = 0.2007s, grad.norm=12.74543667
  6700: 5 [   65/ 1327], train_loss/perplexity = 4.40444040/81.8133469 secs/batch = 0.1966s, grad.norm=11.36567211
  6705: 5 [   70/ 1327], train_loss/perplexity = 4.29989529/73.6920776 secs/batch = 0.2001s, grad.norm=11.79995823
  6710: 5 [   75/ 1327], train_loss/perplexity = 4.12796831/62.0517235 secs/batch = 0.1991s, grad.norm=11.79385376
  6715: 5 [   80/ 1327], train_loss/perplexity = 4.51068783/90.9843750 secs/batch = 0.1998s, grad.norm=12.38975525
  6720: 5 [   85/ 1327], train_loss/perplexity = 4.57704926/97.2270813 secs/batch = 0.1984s, grad.norm=11.83949661
  6725: 5 [   90/ 1327], train_loss/perplexity = 4.58853531/98.3502731 secs/batch = 0.2002s, grad.norm=11.62020111
  6730: 5 [   95/ 1327], train_loss/perplexity = 4.43693066/84.5151367 secs/batch = 0.2000s, grad.norm=11.55818939
  6735: 5 [  100/ 1327], train_loss/perplexity = 4.78580713/119.7980194 secs/batch = 0.1989s, grad.norm=11.47138500
  6740: 5 [  105/ 1327], train_loss/perplexity = 4.69401169/109.2907410 secs/batch = 0.1985s, grad.norm=12.39717007
  6745: 5 [  110/ 1327], train_loss/perplexity = 4.48839188/88.9782410 secs/batch = 0.1995s, grad.norm=11.49806595
  6750: 5 [  115/ 1327], train_loss/perplexity = 4.45299196/85.8835220 secs/batch = 0.2009s, grad.norm=12.42163086
  6755: 5 [  120/ 1327], train_loss/perplexity = 4.58155251/97.6659012 secs/batch = 0.1996s, grad.norm=11.78657627
  6760: 5 [  125/ 1327], train_loss/perplexity = 4.69494009/109.3922577 secs/batch = 0.2001s, grad.norm=12.24211121
  6765: 5 [  130/ 1327], train_loss/perplexity = 4.59468365/98.9568253 secs/batch = 0.1980s, grad.norm=13.23750305
  6770: 5 [  135/ 1327], train_loss/perplexity = 4.60130692/99.6144180 secs/batch = 0.1985s, grad.norm=12.25314713
  6775: 5 [  140/ 1327], train_loss/perplexity = 4.90557814/135.0409546 secs/batch = 0.1998s, grad.norm=12.73002625
  6780: 5 [  145/ 1327], train_loss/perplexity = 4.76824808/117.7128372 secs/batch = 0.1998s, grad.norm=12.74306297
  6785: 5 [  150/ 1327], train_loss/perplexity = 4.73973465/114.4038391 secs/batch = 0.1988s, grad.norm=12.76716518
  6790: 5 [  155/ 1327], train_loss/perplexity = 4.98281050/145.8838196 secs/batch = 0.1994s, grad.norm=11.83264446
  6795: 5 [  160/ 1327], train_loss/perplexity = 4.67808390/107.5637741 secs/batch = 0.2007s, grad.norm=10.97695351
  6800: 5 [  165/ 1327], train_loss/perplexity = 4.86616421/129.8219910 secs/batch = 0.2004s, grad.norm=11.97794437
  6805: 5 [  170/ 1327], train_loss/perplexity = 4.60225630/99.7090378 secs/batch = 0.2000s, grad.norm=11.37903309
  6810: 5 [  175/ 1327], train_loss/perplexity = 4.89303970/133.3583221 secs/batch = 0.1998s, grad.norm=11.54191875
  6815: 5 [  180/ 1327], train_loss/perplexity = 4.70888662/110.9285889 secs/batch = 0.1989s, grad.norm=11.98082733
  6820: 5 [  185/ 1327], train_loss/perplexity = 5.08335257/161.3139648 secs/batch = 0.2003s, grad.norm=11.82474136
  6825: 5 [  190/ 1327], train_loss/perplexity = 4.55398560/95.0103302 secs/batch = 0.1990s, grad.norm=10.91296291
  6830: 5 [  195/ 1327], train_loss/perplexity = 4.81032324/122.7712936 secs/batch = 0.1933s, grad.norm=11.27352715
  6835: 5 [  200/ 1327], train_loss/perplexity = 4.69074440/108.9342422 secs/batch = 0.1993s, grad.norm=12.02995586
  6840: 5 [  205/ 1327], train_loss/perplexity = 4.86188793/129.2680206 secs/batch = 0.1982s, grad.norm=11.67910576
  6845: 5 [  210/ 1327], train_loss/perplexity = 4.75355244/115.9956207 secs/batch = 0.1996s, grad.norm=11.16403675
  6850: 5 [  215/ 1327], train_loss/perplexity = 4.87678289/131.2078705 secs/batch = 0.2000s, grad.norm=11.57898712
  6855: 5 [  220/ 1327], train_loss/perplexity = 4.86054897/129.0950470 secs/batch = 0.1997s, grad.norm=12.01504326
  6860: 5 [  225/ 1327], train_loss/perplexity = 4.99998426/148.4108276 secs/batch = 0.2010s, grad.norm=11.47001076
  6865: 5 [  230/ 1327], train_loss/perplexity = 4.88225698/131.9280853 secs/batch = 0.1997s, grad.norm=12.81736088
  6870: 5 [  235/ 1327], train_loss/perplexity = 4.62445164/101.9468536 secs/batch = 0.2001s, grad.norm=11.24652195
  6875: 5 [  240/ 1327], train_loss/perplexity = 4.47681952/87.9544907 secs/batch = 0.1992s, grad.norm=11.91296577
  6880: 5 [  245/ 1327], train_loss/perplexity = 4.80244017/121.8072815 secs/batch = 0.2006s, grad.norm=11.47128677
  6885: 5 [  250/ 1327], train_loss/perplexity = 4.55478716/95.0865173 secs/batch = 0.2001s, grad.norm=10.92976093
  6890: 5 [  255/ 1327], train_loss/perplexity = 4.54847813/94.4885025 secs/batch = 0.1988s, grad.norm=11.29124069
  6895: 5 [  260/ 1327], train_loss/perplexity = 4.88710499/132.5692291 secs/batch = 0.2001s, grad.norm=12.80337811
  6900: 5 [  265/ 1327], train_loss/perplexity = 4.98015499/145.4969330 secs/batch = 0.1994s, grad.norm=11.35930538
  6905: 5 [  270/ 1327], train_loss/perplexity = 5.05850077/157.3544312 secs/batch = 0.2006s, grad.norm=11.26916599
  6910: 5 [  275/ 1327], train_loss/perplexity = 5.04565239/155.3456116 secs/batch = 0.1996s, grad.norm=11.64639664
  6915: 5 [  280/ 1327], train_loss/perplexity = 4.78884220/120.1621628 secs/batch = 0.2007s, grad.norm=11.77064228
  6920: 5 [  285/ 1327], train_loss/perplexity = 5.05815077/157.2993622 secs/batch = 0.1986s, grad.norm=11.60247803
  6925: 5 [  290/ 1327], train_loss/perplexity = 4.85533285/128.4234314 secs/batch = 0.1988s, grad.norm=11.87950802
  6930: 5 [  295/ 1327], train_loss/perplexity = 4.63131523/102.6489792 secs/batch = 0.1995s, grad.norm=11.82207870
  6935: 5 [  300/ 1327], train_loss/perplexity = 4.20180225/66.8066254 secs/batch = 0.1990s, grad.norm=11.64911747
  6940: 5 [  305/ 1327], train_loss/perplexity = 4.66612387/106.2849655 secs/batch = 0.1993s, grad.norm=11.26696491
  6945: 5 [  310/ 1327], train_loss/perplexity = 4.61461163/100.9486160 secs/batch = 0.1992s, grad.norm=11.84146309
  6950: 5 [  315/ 1327], train_loss/perplexity = 4.26432991/71.1172485 secs/batch = 0.2003s, grad.norm=12.22238445
  6955: 5 [  320/ 1327], train_loss/perplexity = 4.17384195/64.9645615 secs/batch = 0.1963s, grad.norm=12.45075417
  6960: 5 [  325/ 1327], train_loss/perplexity = 4.19667149/66.4647369 secs/batch = 0.1929s, grad.norm=11.21482754
  6965: 5 [  330/ 1327], train_loss/perplexity = 4.65939379/105.5720673 secs/batch = 0.1996s, grad.norm=12.25037098
  6970: 5 [  335/ 1327], train_loss/perplexity = 4.18062305/65.4065933 secs/batch = 0.1983s, grad.norm=11.74035931
  6975: 5 [  340/ 1327], train_loss/perplexity = 4.83606386/125.9725266 secs/batch = 0.2003s, grad.norm=12.87619972
  6980: 5 [  345/ 1327], train_loss/perplexity = 4.70522356/110.5229874 secs/batch = 0.1987s, grad.norm=11.36347771
  6985: 5 [  350/ 1327], train_loss/perplexity = 4.70925522/110.9694824 secs/batch = 0.1990s, grad.norm=12.61661053
  6990: 5 [  355/ 1327], train_loss/perplexity = 4.74378872/114.8685837 secs/batch = 0.1953s, grad.norm=11.45587158
  6995: 5 [  360/ 1327], train_loss/perplexity = 4.89114904/133.1064301 secs/batch = 0.2010s, grad.norm=11.90837002
  7000: 5 [  365/ 1327], train_loss/perplexity = 4.81341934/123.1520004 secs/batch = 0.2002s, grad.norm=11.58290577
  7005: 5 [  370/ 1327], train_loss/perplexity = 4.87973690/131.5960388 secs/batch = 0.1960s, grad.norm=11.62853909
  7010: 5 [  375/ 1327], train_loss/perplexity = 4.20882607/67.2775116 secs/batch = 0.1997s, grad.norm=11.70216179
  7015: 5 [  380/ 1327], train_loss/perplexity = 4.42639112/83.6290665 secs/batch = 0.2004s, grad.norm=12.51126194
  7020: 5 [  385/ 1327], train_loss/perplexity = 4.66342926/105.9989548 secs/batch = 0.1993s, grad.norm=12.47131634
  7025: 5 [  390/ 1327], train_loss/perplexity = 4.74258518/114.7304153 secs/batch = 0.2007s, grad.norm=12.02952480
  7030: 5 [  395/ 1327], train_loss/perplexity = 4.75381184/116.0257111 secs/batch = 0.1988s, grad.norm=13.00948143
  7035: 5 [  400/ 1327], train_loss/perplexity = 4.64507723/104.0714035 secs/batch = 0.2013s, grad.norm=11.75839615
  7040: 5 [  405/ 1327], train_loss/perplexity = 4.94036865/139.8217926 secs/batch = 0.1960s, grad.norm=11.83448792
  7045: 5 [  410/ 1327], train_loss/perplexity = 4.63737726/103.2731323 secs/batch = 0.2001s, grad.norm=11.78845596
  7050: 5 [  415/ 1327], train_loss/perplexity = 4.49959755/89.9809113 secs/batch = 0.1974s, grad.norm=11.83742523
  7055: 5 [  420/ 1327], train_loss/perplexity = 4.26437330/71.1203384 secs/batch = 0.1993s, grad.norm=12.43142796
  7060: 5 [  425/ 1327], train_loss/perplexity = 4.57679987/97.2028351 secs/batch = 0.1998s, grad.norm=13.49476242
  7065: 5 [  430/ 1327], train_loss/perplexity = 4.79677582/121.1192780 secs/batch = 0.1993s, grad.norm=12.51416206
  7070: 5 [  435/ 1327], train_loss/perplexity = 4.79366922/120.7435913 secs/batch = 0.2001s, grad.norm=12.59553051
  7075: 5 [  440/ 1327], train_loss/perplexity = 4.49475956/89.5466385 secs/batch = 0.2004s, grad.norm=12.44835281
  7080: 5 [  445/ 1327], train_loss/perplexity = 4.76085186/116.8454208 secs/batch = 0.1992s, grad.norm=12.57252121
  7085: 5 [  450/ 1327], train_loss/perplexity = 4.54316425/93.9877319 secs/batch = 0.1968s, grad.norm=11.77362919
  7090: 5 [  455/ 1327], train_loss/perplexity = 4.54887199/94.5257263 secs/batch = 0.1996s, grad.norm=11.64226055
  7095: 5 [  460/ 1327], train_loss/perplexity = 4.60505772/99.9887543 secs/batch = 0.2002s, grad.norm=14.70675468
  7100: 5 [  465/ 1327], train_loss/perplexity = 4.33705235/76.4817657 secs/batch = 0.1921s, grad.norm=13.34604740
  7105: 5 [  470/ 1327], train_loss/perplexity = 5.03302097/153.3957214 secs/batch = 0.2000s, grad.norm=11.89978027
  7110: 5 [  475/ 1327], train_loss/perplexity = 4.47095966/87.4405975 secs/batch = 0.1998s, grad.norm=12.49656963
  7115: 5 [  480/ 1327], train_loss/perplexity = 4.63406944/102.9320908 secs/batch = 0.1952s, grad.norm=13.15581226
  7120: 5 [  485/ 1327], train_loss/perplexity = 4.53299665/93.0369415 secs/batch = 0.1999s, grad.norm=11.32667923
  7125: 5 [  490/ 1327], train_loss/perplexity = 4.55082417/94.7104340 secs/batch = 0.2000s, grad.norm=13.91839981
  7130: 5 [  495/ 1327], train_loss/perplexity = 4.54557371/94.2144623 secs/batch = 0.1975s, grad.norm=12.00706959
  7135: 5 [  500/ 1327], train_loss/perplexity = 4.74016142/114.4526749 secs/batch = 0.2011s, grad.norm=11.70944405
  7140: 5 [  505/ 1327], train_loss/perplexity = 4.75401354/116.0491180 secs/batch = 0.2001s, grad.norm=10.77122498
  7145: 5 [  510/ 1327], train_loss/perplexity = 5.15126944/172.6505127 secs/batch = 0.1996s, grad.norm=11.25789070
  7150: 5 [  515/ 1327], train_loss/perplexity = 4.76219368/117.0023117 secs/batch = 0.2000s, grad.norm=11.36235428
  7155: 5 [  520/ 1327], train_loss/perplexity = 5.02603960/152.3285370 secs/batch = 0.1946s, grad.norm=12.09365940
  7160: 5 [  525/ 1327], train_loss/perplexity = 4.50771809/90.7145767 secs/batch = 0.2001s, grad.norm=11.99643803
  7165: 5 [  530/ 1327], train_loss/perplexity = 4.57317543/96.8511658 secs/batch = 0.2014s, grad.norm=13.56047440
  7170: 5 [  535/ 1327], train_loss/perplexity = 4.66379881/106.0381393 secs/batch = 0.2015s, grad.norm=11.77857399
  7175: 5 [  540/ 1327], train_loss/perplexity = 4.73492384/113.8547897 secs/batch = 0.2007s, grad.norm=11.29947186
  7180: 5 [  545/ 1327], train_loss/perplexity = 4.78608418/119.8312073 secs/batch = 0.1996s, grad.norm=12.24096203
  7185: 5 [  550/ 1327], train_loss/perplexity = 4.73217392/113.5421295 secs/batch = 0.1998s, grad.norm=11.62948227
  7190: 5 [  555/ 1327], train_loss/perplexity = 4.56919861/96.4667740 secs/batch = 0.2010s, grad.norm=11.63075924
  7195: 5 [  560/ 1327], train_loss/perplexity = 4.66489124/106.1540375 secs/batch = 0.2007s, grad.norm=13.41893196
  7200: 5 [  565/ 1327], train_loss/perplexity = 4.57155180/96.6940460 secs/batch = 0.1998s, grad.norm=13.58401108
  7205: 5 [  570/ 1327], train_loss/perplexity = 4.56561279/96.1214752 secs/batch = 0.1995s, grad.norm=13.06256294
  7210: 5 [  575/ 1327], train_loss/perplexity = 4.35635138/77.9721222 secs/batch = 0.1988s, grad.norm=11.87335491
  7215: 5 [  580/ 1327], train_loss/perplexity = 4.77452850/118.4544525 secs/batch = 0.2000s, grad.norm=12.22155094
  7220: 5 [  585/ 1327], train_loss/perplexity = 4.27750921/72.0607300 secs/batch = 0.2005s, grad.norm=11.68039703
  7225: 5 [  590/ 1327], train_loss/perplexity = 4.67534876/107.2699738 secs/batch = 0.2003s, grad.norm=12.14195251
  7230: 5 [  595/ 1327], train_loss/perplexity = 4.59339476/98.8293610 secs/batch = 0.1993s, grad.norm=12.17031384
  7235: 5 [  600/ 1327], train_loss/perplexity = 4.83657074/126.0363998 secs/batch = 0.2001s, grad.norm=11.32392502
  7240: 5 [  605/ 1327], train_loss/perplexity = 4.75684977/116.3787308 secs/batch = 0.2006s, grad.norm=11.86930084
  7245: 5 [  610/ 1327], train_loss/perplexity = 4.99463320/147.6187897 secs/batch = 0.1995s, grad.norm=12.81285572
  7250: 5 [  615/ 1327], train_loss/perplexity = 4.49710464/89.7568741 secs/batch = 0.1993s, grad.norm=11.52576733
  7255: 5 [  620/ 1327], train_loss/perplexity = 4.91629457/136.4958954 secs/batch = 0.1944s, grad.norm=11.97745514
  7260: 5 [  625/ 1327], train_loss/perplexity = 4.83523417/125.8680573 secs/batch = 0.1992s, grad.norm=11.40991688
  7265: 5 [  630/ 1327], train_loss/perplexity = 4.89524937/133.6533356 secs/batch = 0.1996s, grad.norm=11.43012714
  7270: 5 [  635/ 1327], train_loss/perplexity = 4.61129379/100.6142426 secs/batch = 0.1989s, grad.norm=11.66531467
  7275: 5 [  640/ 1327], train_loss/perplexity = 4.68798542/108.6341095 secs/batch = 0.1993s, grad.norm=11.73809338
  7280: 5 [  645/ 1327], train_loss/perplexity = 5.01084757/150.0318451 secs/batch = 0.1989s, grad.norm=13.49590111
  7285: 5 [  650/ 1327], train_loss/perplexity = 4.49045372/89.1618881 secs/batch = 0.1944s, grad.norm=12.79320526
  7290: 5 [  655/ 1327], train_loss/perplexity = 4.58088493/97.6007233 secs/batch = 0.2003s, grad.norm=11.91677189
  7295: 5 [  660/ 1327], train_loss/perplexity = 4.45467091/86.0278320 secs/batch = 0.1989s, grad.norm=11.79539680
  7300: 5 [  665/ 1327], train_loss/perplexity = 4.67129183/106.8356628 secs/batch = 0.1948s, grad.norm=11.75584412
  7305: 5 [  670/ 1327], train_loss/perplexity = 4.62198353/101.6955490 secs/batch = 0.1993s, grad.norm=12.04406738
  7310: 5 [  675/ 1327], train_loss/perplexity = 4.42474651/83.4916382 secs/batch = 0.1962s, grad.norm=12.64685917
  7315: 5 [  680/ 1327], train_loss/perplexity = 4.62054014/101.5488663 secs/batch = 0.1998s, grad.norm=13.62410164
  7320: 5 [  685/ 1327], train_loss/perplexity = 4.48887300/89.0210648 secs/batch = 0.1992s, grad.norm=11.76585865
  7325: 5 [  690/ 1327], train_loss/perplexity = 4.88087559/131.7459717 secs/batch = 0.1995s, grad.norm=11.63051987
  7330: 5 [  695/ 1327], train_loss/perplexity = 4.66229820/105.8791351 secs/batch = 0.1993s, grad.norm=11.87797356
  7335: 5 [  700/ 1327], train_loss/perplexity = 4.85479498/128.3543701 secs/batch = 0.1986s, grad.norm=11.86340904
  7340: 5 [  705/ 1327], train_loss/perplexity = 4.63146830/102.6646957 secs/batch = 0.1991s, grad.norm=11.16789436
  7345: 5 [  710/ 1327], train_loss/perplexity = 4.56766939/96.3193665 secs/batch = 0.1995s, grad.norm=12.07690048
  7350: 5 [  715/ 1327], train_loss/perplexity = 4.52441072/92.2415543 secs/batch = 0.1995s, grad.norm=11.82855129
  7355: 5 [  720/ 1327], train_loss/perplexity = 4.52317715/92.1278381 secs/batch = 0.2002s, grad.norm=12.08274841
  7360: 5 [  725/ 1327], train_loss/perplexity = 4.50639629/90.5947495 secs/batch = 0.1993s, grad.norm=11.85102558
  7365: 5 [  730/ 1327], train_loss/perplexity = 4.67314339/107.0336609 secs/batch = 0.1996s, grad.norm=11.89203739
  7370: 5 [  735/ 1327], train_loss/perplexity = 4.80142021/121.6831131 secs/batch = 0.1993s, grad.norm=12.46183586
  7375: 5 [  740/ 1327], train_loss/perplexity = 4.16243553/64.2277603 secs/batch = 0.1958s, grad.norm=11.24821091
  7380: 5 [  745/ 1327], train_loss/perplexity = 4.65027237/104.6134720 secs/batch = 0.1994s, grad.norm=12.82878494
  7385: 5 [  750/ 1327], train_loss/perplexity = 4.51548004/91.4214401 secs/batch = 0.1998s, grad.norm=12.32821178
  7390: 5 [  755/ 1327], train_loss/perplexity = 4.51153469/91.0614624 secs/batch = 0.1996s, grad.norm=16.76755905
  7395: 5 [  760/ 1327], train_loss/perplexity = 4.25594711/70.5235825 secs/batch = 0.1961s, grad.norm=12.28243351
  7400: 5 [  765/ 1327], train_loss/perplexity = 4.42418575/83.4448318 secs/batch = 0.1998s, grad.norm=12.38892651
  7405: 5 [  770/ 1327], train_loss/perplexity = 4.34465981/77.0658188 secs/batch = 0.1998s, grad.norm=12.13981152
  7410: 5 [  775/ 1327], train_loss/perplexity = 4.51197529/91.1015930 secs/batch = 0.1994s, grad.norm=13.78294373
  7415: 5 [  780/ 1327], train_loss/perplexity = 4.88072681/131.7263641 secs/batch = 0.1992s, grad.norm=11.50057697
  7420: 5 [  785/ 1327], train_loss/perplexity = 4.64191294/103.7426147 secs/batch = 0.1991s, grad.norm=12.21846867
  7425: 5 [  790/ 1327], train_loss/perplexity = 4.42146969/83.2184982 secs/batch = 0.1938s, grad.norm=12.49283504
  7430: 5 [  795/ 1327], train_loss/perplexity = 4.83745956/126.1484756 secs/batch = 0.1999s, grad.norm=12.23725414
  7435: 5 [  800/ 1327], train_loss/perplexity = 4.71640015/111.7651901 secs/batch = 0.1993s, grad.norm=12.66271114
  7440: 5 [  805/ 1327], train_loss/perplexity = 5.00200701/148.7113190 secs/batch = 0.2003s, grad.norm=12.32201385
  7445: 5 [  810/ 1327], train_loss/perplexity = 4.70276690/110.2518082 secs/batch = 0.2006s, grad.norm=11.68735409
  7450: 5 [  815/ 1327], train_loss/perplexity = 4.51998615/91.8343277 secs/batch = 0.2002s, grad.norm=11.96189213
  7455: 5 [  820/ 1327], train_loss/perplexity = 4.28280354/72.4432526 secs/batch = 0.2002s, grad.norm=11.59616756
  7460: 5 [  825/ 1327], train_loss/perplexity = 4.55590391/95.1927643 secs/batch = 0.2001s, grad.norm=11.77399540
  7465: 5 [  830/ 1327], train_loss/perplexity = 4.33653164/76.4419479 secs/batch = 0.1999s, grad.norm=11.80838203
  7470: 5 [  835/ 1327], train_loss/perplexity = 4.60621500/100.1045380 secs/batch = 0.2003s, grad.norm=12.60618877
  7475: 5 [  840/ 1327], train_loss/perplexity = 4.66130877/105.7744293 secs/batch = 0.1992s, grad.norm=12.23289013
  7480: 5 [  845/ 1327], train_loss/perplexity = 4.47287416/87.6081619 secs/batch = 0.2004s, grad.norm=12.66827393
  7485: 5 [  850/ 1327], train_loss/perplexity = 4.57295036/96.8293686 secs/batch = 0.2004s, grad.norm=11.73941231
  7490: 5 [  855/ 1327], train_loss/perplexity = 4.54194784/93.8734741 secs/batch = 0.2000s, grad.norm=12.30406284
  7495: 5 [  860/ 1327], train_loss/perplexity = 4.30506754/74.0742188 secs/batch = 0.1990s, grad.norm=11.90234470
  7500: 5 [  865/ 1327], train_loss/perplexity = 4.81721020/123.6197357 secs/batch = 0.1990s, grad.norm=12.89785290
  7505: 5 [  870/ 1327], train_loss/perplexity = 4.72856855/113.1334991 secs/batch = 0.1989s, grad.norm=12.61607647
  7510: 5 [  875/ 1327], train_loss/perplexity = 4.22047806/68.0660172 secs/batch = 0.2002s, grad.norm=12.31314182
  7515: 5 [  880/ 1327], train_loss/perplexity = 4.40086889/81.5216751 secs/batch = 0.1984s, grad.norm=11.62914276
  7520: 5 [  885/ 1327], train_loss/perplexity = 4.64358139/103.9158478 secs/batch = 0.1995s, grad.norm=11.18597603
  7525: 5 [  890/ 1327], train_loss/perplexity = 4.79261589/120.6164780 secs/batch = 0.1999s, grad.norm=11.64828396
  7530: 5 [  895/ 1327], train_loss/perplexity = 4.87074137/130.4175720 secs/batch = 0.1994s, grad.norm=12.05826283
  7535: 5 [  900/ 1327], train_loss/perplexity = 4.59571934/99.0593643 secs/batch = 0.2005s, grad.norm=13.04460526
  7540: 5 [  905/ 1327], train_loss/perplexity = 4.42446089/83.4677963 secs/batch = 0.1913s, grad.norm=12.54785442
  7545: 5 [  910/ 1327], train_loss/perplexity = 4.50412273/90.3890152 secs/batch = 0.2001s, grad.norm=12.32202625
  7550: 5 [  915/ 1327], train_loss/perplexity = 4.68347073/108.1447601 secs/batch = 0.1995s, grad.norm=11.92782307
  7555: 5 [  920/ 1327], train_loss/perplexity = 4.90561676/135.0461731 secs/batch = 0.1993s, grad.norm=11.36612511
  7560: 5 [  925/ 1327], train_loss/perplexity = 4.75298500/115.9298172 secs/batch = 0.1996s, grad.norm=11.61891937
  7565: 5 [  930/ 1327], train_loss/perplexity = 4.63672018/103.2052994 secs/batch = 0.2007s, grad.norm=11.38486958
  7570: 5 [  935/ 1327], train_loss/perplexity = 4.77380371/118.3686295 secs/batch = 0.1991s, grad.norm=12.59152603
  7575: 5 [  940/ 1327], train_loss/perplexity = 4.69770002/109.6945877 secs/batch = 0.1996s, grad.norm=11.33915710
  7580: 5 [  945/ 1327], train_loss/perplexity = 4.93598270/139.2098846 secs/batch = 0.2003s, grad.norm=11.74755669
  7585: 5 [  950/ 1327], train_loss/perplexity = 4.64271498/103.8258514 secs/batch = 0.2002s, grad.norm=11.22417259
  7590: 5 [  955/ 1327], train_loss/perplexity = 4.69784975/109.7110138 secs/batch = 0.2000s, grad.norm=11.90315247
  7595: 5 [  960/ 1327], train_loss/perplexity = 4.96090412/142.7227783 secs/batch = 0.1999s, grad.norm=11.85431576
  7600: 5 [  965/ 1327], train_loss/perplexity = 4.69685841/109.6023026 secs/batch = 0.1987s, grad.norm=11.18878841
  7605: 5 [  970/ 1327], train_loss/perplexity = 4.95802021/142.3117676 secs/batch = 0.1999s, grad.norm=12.07911301
  7610: 5 [  975/ 1327], train_loss/perplexity = 4.65388632/104.9922256 secs/batch = 0.1994s, grad.norm=12.61807823
  7615: 5 [  980/ 1327], train_loss/perplexity = 4.50075769/90.0853653 secs/batch = 0.2004s, grad.norm=11.82659340
  7620: 5 [  985/ 1327], train_loss/perplexity = 4.64249277/103.8027802 secs/batch = 0.1992s, grad.norm=12.49368382
  7625: 5 [  990/ 1327], train_loss/perplexity = 4.76787949/117.6694565 secs/batch = 0.2001s, grad.norm=12.13637161
  7630: 5 [  995/ 1327], train_loss/perplexity = 4.80939770/122.6577148 secs/batch = 0.1991s, grad.norm=12.52884388
  7635: 5 [ 1000/ 1327], train_loss/perplexity = 4.32424688/75.5086212 secs/batch = 0.2001s, grad.norm=11.56085014
  7640: 5 [ 1005/ 1327], train_loss/perplexity = 4.79660845/121.0990067 secs/batch = 0.1993s, grad.norm=12.31788826
  7645: 5 [ 1010/ 1327], train_loss/perplexity = 4.39469862/81.0202103 secs/batch = 0.1996s, grad.norm=10.66815567
  7650: 5 [ 1015/ 1327], train_loss/perplexity = 4.87778759/131.3397675 secs/batch = 0.1995s, grad.norm=11.66294765
  7655: 5 [ 1020/ 1327], train_loss/perplexity = 4.99760056/148.0574799 secs/batch = 0.1992s, grad.norm=12.15391159
  7660: 5 [ 1025/ 1327], train_loss/perplexity = 4.84087515/126.5800781 secs/batch = 0.1995s, grad.norm=11.65310860
  7665: 5 [ 1030/ 1327], train_loss/perplexity = 4.62824678/102.3344955 secs/batch = 0.1974s, grad.norm=11.59887505
  7670: 5 [ 1035/ 1327], train_loss/perplexity = 4.52271461/92.0852356 secs/batch = 0.1995s, grad.norm=11.52120972
  7675: 5 [ 1040/ 1327], train_loss/perplexity = 4.78763390/120.0170593 secs/batch = 0.1933s, grad.norm=12.52046204
  7680: 5 [ 1045/ 1327], train_loss/perplexity = 4.31677437/74.9464874 secs/batch = 0.1991s, grad.norm=11.85657120
  7685: 5 [ 1050/ 1327], train_loss/perplexity = 4.48558712/88.7290268 secs/batch = 0.1994s, grad.norm=12.52346802
  7690: 5 [ 1055/ 1327], train_loss/perplexity = 4.52093077/91.9211121 secs/batch = 0.1971s, grad.norm=13.45836353
  7695: 5 [ 1060/ 1327], train_loss/perplexity = 4.21038723/67.3826294 secs/batch = 0.1996s, grad.norm=12.92432404
  7700: 5 [ 1065/ 1327], train_loss/perplexity = 4.35611248/77.9534988 secs/batch = 0.1940s, grad.norm=12.40489101
  7705: 5 [ 1070/ 1327], train_loss/perplexity = 4.67391682/107.1164780 secs/batch = 0.1993s, grad.norm=12.63372040
  7710: 5 [ 1075/ 1327], train_loss/perplexity = 4.44291401/85.0223389 secs/batch = 0.1998s, grad.norm=11.96538925
  7715: 5 [ 1080/ 1327], train_loss/perplexity = 4.42720222/83.6969223 secs/batch = 0.1937s, grad.norm=12.52258301
  7720: 5 [ 1085/ 1327], train_loss/perplexity = 4.17312956/64.9182968 secs/batch = 0.2002s, grad.norm=11.99860668
  7725: 5 [ 1090/ 1327], train_loss/perplexity = 4.46261835/86.7142639 secs/batch = 0.1991s, grad.norm=13.61208248
  7730: 5 [ 1095/ 1327], train_loss/perplexity = 4.63368034/102.8920441 secs/batch = 0.1994s, grad.norm=12.57398605
  7735: 5 [ 1100/ 1327], train_loss/perplexity = 4.37425232/79.3804626 secs/batch = 0.1999s, grad.norm=13.88898277
  7740: 5 [ 1105/ 1327], train_loss/perplexity = 4.29890919/73.6194458 secs/batch = 0.1992s, grad.norm=12.87390423
  7745: 5 [ 1110/ 1327], train_loss/perplexity = 4.70510387/110.5097656 secs/batch = 0.1996s, grad.norm=14.53057289
  7750: 5 [ 1115/ 1327], train_loss/perplexity = 4.42598629/83.5952148 secs/batch = 0.1995s, grad.norm=12.18797112
  7755: 5 [ 1120/ 1327], train_loss/perplexity = 4.57498693/97.0267715 secs/batch = 0.1993s, grad.norm=11.74893856
  7760: 5 [ 1125/ 1327], train_loss/perplexity = 4.86737347/129.9790802 secs/batch = 0.1990s, grad.norm=14.77990723
  7765: 5 [ 1130/ 1327], train_loss/perplexity = 4.51144314/91.0531235 secs/batch = 0.1985s, grad.norm=11.76894569
  7770: 5 [ 1135/ 1327], train_loss/perplexity = 4.49581528/89.6412201 secs/batch = 0.1995s, grad.norm=11.75663090
  7775: 5 [ 1140/ 1327], train_loss/perplexity = 4.81412077/123.2384109 secs/batch = 0.1998s, grad.norm=12.46151161
  7780: 5 [ 1145/ 1327], train_loss/perplexity = 4.52958632/92.7201996 secs/batch = 0.1927s, grad.norm=11.94556904
  7785: 5 [ 1150/ 1327], train_loss/perplexity = 4.50980234/90.9038467 secs/batch = 0.1994s, grad.norm=11.90140820
  7790: 5 [ 1155/ 1327], train_loss/perplexity = 4.69757032/109.6803589 secs/batch = 0.1929s, grad.norm=12.47998619
  7795: 5 [ 1160/ 1327], train_loss/perplexity = 4.54379225/94.0467758 secs/batch = 0.1996s, grad.norm=12.71840382
  7800: 5 [ 1165/ 1327], train_loss/perplexity = 4.61602211/101.0911026 secs/batch = 0.1989s, grad.norm=11.97180653
  7805: 5 [ 1170/ 1327], train_loss/perplexity = 4.48108196/88.3301926 secs/batch = 0.1999s, grad.norm=12.13879967
  7810: 5 [ 1175/ 1327], train_loss/perplexity = 4.26861620/71.4227295 secs/batch = 0.1990s, grad.norm=12.62050056
  7815: 5 [ 1180/ 1327], train_loss/perplexity = 4.28065968/72.2881088 secs/batch = 0.1935s, grad.norm=11.80232525
  7820: 5 [ 1185/ 1327], train_loss/perplexity = 4.49289083/89.3794556 secs/batch = 0.1997s, grad.norm=11.70076180
  7825: 5 [ 1190/ 1327], train_loss/perplexity = 4.53734589/93.4424667 secs/batch = 0.1952s, grad.norm=11.74311638
  7830: 5 [ 1195/ 1327], train_loss/perplexity = 4.46223736/86.6812286 secs/batch = 0.1944s, grad.norm=12.23001480
  7835: 5 [ 1200/ 1327], train_loss/perplexity = 4.35987186/78.2471085 secs/batch = 0.2011s, grad.norm=12.08520222
  7840: 5 [ 1205/ 1327], train_loss/perplexity = 4.38882399/80.5456390 secs/batch = 0.2004s, grad.norm=12.57461643
  7845: 5 [ 1210/ 1327], train_loss/perplexity = 4.06034231/57.9941597 secs/batch = 0.1993s, grad.norm=12.52064896
  7850: 5 [ 1215/ 1327], train_loss/perplexity = 4.26242495/70.9819031 secs/batch = 0.1996s, grad.norm=11.56879616
  7855: 5 [ 1220/ 1327], train_loss/perplexity = 4.47599792/87.8822556 secs/batch = 0.2003s, grad.norm=12.69135666
  7860: 5 [ 1225/ 1327], train_loss/perplexity = 4.18326044/65.5793228 secs/batch = 0.2004s, grad.norm=13.37611008
  7865: 5 [ 1230/ 1327], train_loss/perplexity = 4.41447067/82.6380844 secs/batch = 0.1953s, grad.norm=11.87577248
  7870: 5 [ 1235/ 1327], train_loss/perplexity = 4.39995432/81.4471512 secs/batch = 0.1988s, grad.norm=12.29987717
  7875: 5 [ 1240/ 1327], train_loss/perplexity = 4.56292248/95.8632278 secs/batch = 0.1998s, grad.norm=12.87298012
  7880: 5 [ 1245/ 1327], train_loss/perplexity = 4.47188330/87.5214005 secs/batch = 0.2008s, grad.norm=11.89289856
  7885: 5 [ 1250/ 1327], train_loss/perplexity = 4.55726576/95.3224869 secs/batch = 0.2003s, grad.norm=11.80953312
  7890: 5 [ 1255/ 1327], train_loss/perplexity = 4.65692329/105.3115692 secs/batch = 0.1992s, grad.norm=11.48604679
  7895: 5 [ 1260/ 1327], train_loss/perplexity = 4.44758129/85.4200897 secs/batch = 0.1994s, grad.norm=13.02080345
  7900: 5 [ 1265/ 1327], train_loss/perplexity = 4.68852377/108.6926041 secs/batch = 0.2004s, grad.norm=11.96849632
  7905: 5 [ 1270/ 1327], train_loss/perplexity = 4.39670706/81.1830978 secs/batch = 0.2009s, grad.norm=12.05040836
  7910: 5 [ 1275/ 1327], train_loss/perplexity = 4.64386177/103.9449844 secs/batch = 0.1996s, grad.norm=13.01270103
  7915: 5 [ 1280/ 1327], train_loss/perplexity = 4.43595791/84.4329681 secs/batch = 0.1994s, grad.norm=12.86664200
  7920: 5 [ 1285/ 1327], train_loss/perplexity = 4.33787823/76.5449524 secs/batch = 0.1998s, grad.norm=12.84651375
  7925: 5 [ 1290/ 1327], train_loss/perplexity = 4.55960655/95.5458832 secs/batch = 0.2004s, grad.norm=12.08614540
  7930: 5 [ 1295/ 1327], train_loss/perplexity = 4.67590141/107.3292694 secs/batch = 0.1998s, grad.norm=14.16535282
  7935: 5 [ 1300/ 1327], train_loss/perplexity = 4.71193171/111.2668839 secs/batch = 0.1933s, grad.norm=11.88202000
  7940: 5 [ 1305/ 1327], train_loss/perplexity = 4.82208300/124.2235794 secs/batch = 0.1995s, grad.norm=12.60981274
  7945: 5 [ 1310/ 1327], train_loss/perplexity = 5.04908371/155.8795624 secs/batch = 0.1989s, grad.norm=12.09237194
  7950: 5 [ 1315/ 1327], train_loss/perplexity = 4.86108208/129.1638947 secs/batch = 0.1993s, grad.norm=11.95901203
  7955: 5 [ 1320/ 1327], train_loss/perplexity = 4.89534187/133.6656952 secs/batch = 0.1979s, grad.norm=11.90639019
  7960: 5 [ 1325/ 1327], train_loss/perplexity = 4.75497055/116.1602325 secs/batch = 0.1995s, grad.norm=11.98442554
Epoch training time: 264.32486510276794
	> validation loss = 4.95040655, perplexity = 141.23237610
	> validation loss = 4.84528160, perplexity = 127.13907623
	> validation loss = 4.83391666, perplexity = 125.70233154
	> validation loss = 4.88244629, perplexity = 131.95306396
	> validation loss = 5.04208183, perplexity = 154.79193115
	> validation loss = 4.90075111, perplexity = 134.39068604
	> validation loss = 4.89580584, perplexity = 133.72772217
	> validation loss = 4.73115730, perplexity = 113.42675781
	> validation loss = 4.55477810, perplexity = 95.08565521
	> validation loss = 4.66342592, perplexity = 105.99860382
	> validation loss = 4.71742725, perplexity = 111.88004303
	> validation loss = 4.88116121, perplexity = 131.78359985
	> validation loss = 4.69311237, perplexity = 109.19249725
	> validation loss = 4.63212013, perplexity = 102.73163605
	> validation loss = 4.51039267, perplexity = 90.95752716
	> validation loss = 4.46904469, perplexity = 87.27330780
	> validation loss = 4.95833349, perplexity = 142.35635376
	> validation loss = 4.51620293, perplexity = 91.48755646
	> validation loss = 4.93100691, perplexity = 138.51892090
	> validation loss = 4.76156902, perplexity = 116.92924500
	> validation loss = 4.61862230, perplexity = 101.35430145
at the end of epoch: 5
train loss = 4.69460098, perplexity = 109.35516542
validation loss = 4.75859084, perplexity = 116.58152848
Saved model cv/epoch005_4.7586.model
  7967: 6 [    5/ 1327], train_loss/perplexity = 4.82080412/124.0648117 secs/batch = 0.2010s, grad.norm=12.23923302
  7972: 6 [   10/ 1327], train_loss/perplexity = 4.37510109/79.4478683 secs/batch = 0.1999s, grad.norm=11.36019611
  7977: 6 [   15/ 1327], train_loss/perplexity = 4.68187523/107.9723587 secs/batch = 0.1953s, grad.norm=11.27716255
  7982: 6 [   20/ 1327], train_loss/perplexity = 4.83579493/125.9386520 secs/batch = 0.2002s, grad.norm=11.82066536
  7987: 6 [   25/ 1327], train_loss/perplexity = 4.70912266/110.9547729 secs/batch = 0.1990s, grad.norm=11.67821598
  7992: 6 [   30/ 1327], train_loss/perplexity = 4.69104910/108.9674377 secs/batch = 0.1995s, grad.norm=11.73743057
  7997: 6 [   35/ 1327], train_loss/perplexity = 4.54669476/94.3201447 secs/batch = 0.1945s, grad.norm=11.81926060
  8002: 6 [   40/ 1327], train_loss/perplexity = 4.51701164/91.5615692 secs/batch = 0.1990s, grad.norm=11.82734871
  8007: 6 [   45/ 1327], train_loss/perplexity = 4.34959412/77.4470215 secs/batch = 0.1950s, grad.norm=11.14812183
  8012: 6 [   50/ 1327], train_loss/perplexity = 4.56342173/95.9111023 secs/batch = 0.2000s, grad.norm=12.02038002
  8017: 6 [   55/ 1327], train_loss/perplexity = 4.52852249/92.6216125 secs/batch = 0.1988s, grad.norm=12.93910599
  8022: 6 [   60/ 1327], train_loss/perplexity = 4.88050747/131.6974792 secs/batch = 0.1969s, grad.norm=12.52188110
  8027: 6 [   65/ 1327], train_loss/perplexity = 4.36413050/78.5810471 secs/batch = 0.1998s, grad.norm=11.92116928
  8032: 6 [   70/ 1327], train_loss/perplexity = 4.28735733/72.7738953 secs/batch = 0.1945s, grad.norm=12.30013084
  8037: 6 [   75/ 1327], train_loss/perplexity = 4.08880329/59.6684418 secs/batch = 0.1992s, grad.norm=11.51145267
  8042: 6 [   80/ 1327], train_loss/perplexity = 4.49180984/89.2828903 secs/batch = 0.2004s, grad.norm=12.45279980
  8047: 6 [   85/ 1327], train_loss/perplexity = 4.56800222/96.3514252 secs/batch = 0.1991s, grad.norm=12.73197460
  8052: 6 [   90/ 1327], train_loss/perplexity = 4.53838778/93.5398712 secs/batch = 0.1946s, grad.norm=12.09304523
  8057: 6 [   95/ 1327], train_loss/perplexity = 4.43228245/84.1232071 secs/batch = 0.1999s, grad.norm=11.43822575
  8062: 6 [  100/ 1327], train_loss/perplexity = 4.70434189/110.4255905 secs/batch = 0.1996s, grad.norm=12.12829876
  8067: 6 [  105/ 1327], train_loss/perplexity = 4.54450989/94.1142883 secs/batch = 0.1989s, grad.norm=13.12436104
  8072: 6 [  110/ 1327], train_loss/perplexity = 4.38137007/79.9474945 secs/batch = 0.1958s, grad.norm=11.81667709
  8077: 6 [  115/ 1327], train_loss/perplexity = 4.36559105/78.6959000 secs/batch = 0.1999s, grad.norm=12.06136131
  8082: 6 [  120/ 1327], train_loss/perplexity = 4.52186203/92.0067596 secs/batch = 0.1996s, grad.norm=12.79167366
  8087: 6 [  125/ 1327], train_loss/perplexity = 4.60669565/100.1526642 secs/batch = 0.1993s, grad.norm=13.51283741
  8092: 6 [  130/ 1327], train_loss/perplexity = 4.49334002/89.4196091 secs/batch = 0.1997s, grad.norm=13.97863388
  8097: 6 [  135/ 1327], train_loss/perplexity = 4.50332117/90.3165894 secs/batch = 0.2001s, grad.norm=13.15488529
  8102: 6 [  140/ 1327], train_loss/perplexity = 4.86861658/130.1407471 secs/batch = 0.2008s, grad.norm=12.45819283
  8107: 6 [  145/ 1327], train_loss/perplexity = 4.65056419/104.6440048 secs/batch = 0.1950s, grad.norm=12.89120388
  8112: 6 [  150/ 1327], train_loss/perplexity = 4.72934246/113.2210922 secs/batch = 0.1990s, grad.norm=12.21540260
  8117: 6 [  155/ 1327], train_loss/perplexity = 4.95820189/142.3376312 secs/batch = 0.1988s, grad.norm=12.16910839
  8122: 6 [  160/ 1327], train_loss/perplexity = 4.59154701/98.6469193 secs/batch = 0.1955s, grad.norm=11.90889072
  8127: 6 [  165/ 1327], train_loss/perplexity = 4.79569197/120.9880753 secs/batch = 0.1965s, grad.norm=12.33387184
  8132: 6 [  170/ 1327], train_loss/perplexity = 4.54670715/94.3213120 secs/batch = 0.2008s, grad.norm=12.31949615
  8137: 6 [  175/ 1327], train_loss/perplexity = 4.84959078/127.6881256 secs/batch = 0.1996s, grad.norm=12.05347347
  8142: 6 [  180/ 1327], train_loss/perplexity = 4.66823435/106.5095139 secs/batch = 0.1993s, grad.norm=12.52714443
  8147: 6 [  185/ 1327], train_loss/perplexity = 4.95564413/141.9740295 secs/batch = 0.1975s, grad.norm=12.11553860
  8152: 6 [  190/ 1327], train_loss/perplexity = 4.42620659/83.6136322 secs/batch = 0.1999s, grad.norm=11.64281559
  8157: 6 [  195/ 1327], train_loss/perplexity = 4.73379135/113.7259216 secs/batch = 0.2007s, grad.norm=11.58900738
  8162: 6 [  200/ 1327], train_loss/perplexity = 4.62153482/101.6499252 secs/batch = 0.1921s, grad.norm=13.03657818
  8167: 6 [  205/ 1327], train_loss/perplexity = 4.79785347/121.2498703 secs/batch = 0.1994s, grad.norm=11.78929424
  8172: 6 [  210/ 1327], train_loss/perplexity = 4.60184145/99.6676788 secs/batch = 0.2000s, grad.norm=11.30309105
  8177: 6 [  215/ 1327], train_loss/perplexity = 4.79634190/121.0667343 secs/batch = 0.1946s, grad.norm=11.66001320
  8182: 6 [  220/ 1327], train_loss/perplexity = 4.80726767/122.3967285 secs/batch = 0.2006s, grad.norm=12.24781036
  8187: 6 [  225/ 1327], train_loss/perplexity = 4.93386793/138.9157867 secs/batch = 0.1980s, grad.norm=12.51298428
  8192: 6 [  230/ 1327], train_loss/perplexity = 4.76853228/117.7462997 secs/batch = 0.1999s, grad.norm=13.00486279
  8197: 6 [  235/ 1327], train_loss/perplexity = 4.58947897/98.4431229 secs/batch = 0.2000s, grad.norm=12.73235607
  8202: 6 [  240/ 1327], train_loss/perplexity = 4.45269012/85.8576050 secs/batch = 0.1985s, grad.norm=13.23140049
  8207: 6 [  245/ 1327], train_loss/perplexity = 4.76311684/117.1103745 secs/batch = 0.2006s, grad.norm=12.22655964
  8212: 6 [  250/ 1327], train_loss/perplexity = 4.48109674/88.3314972 secs/batch = 0.1998s, grad.norm=11.17998219
  8217: 6 [  255/ 1327], train_loss/perplexity = 4.52526617/92.3204956 secs/batch = 0.1998s, grad.norm=12.31572819
  8222: 6 [  260/ 1327], train_loss/perplexity = 4.77463102/118.4665985 secs/batch = 0.2002s, grad.norm=12.67816734
  8227: 6 [  265/ 1327], train_loss/perplexity = 4.85554028/128.4500732 secs/batch = 0.1996s, grad.norm=11.79832554
  8232: 6 [  270/ 1327], train_loss/perplexity = 4.96664190/143.5440369 secs/batch = 0.1993s, grad.norm=12.04760456
  8237: 6 [  275/ 1327], train_loss/perplexity = 5.00748396/149.5280457 secs/batch = 0.1995s, grad.norm=11.25941086
  8242: 6 [  280/ 1327], train_loss/perplexity = 4.74460316/114.9621735 secs/batch = 0.1989s, grad.norm=11.79149342
  8247: 6 [  285/ 1327], train_loss/perplexity = 4.99404478/147.5319519 secs/batch = 0.2012s, grad.norm=11.51695442
  8252: 6 [  290/ 1327], train_loss/perplexity = 4.70412540/110.4016876 secs/batch = 0.1988s, grad.norm=12.28649712
  8257: 6 [  295/ 1327], train_loss/perplexity = 4.50258875/90.2504654 secs/batch = 0.1981s, grad.norm=12.10688019
  8262: 6 [  300/ 1327], train_loss/perplexity = 4.12324715/61.7594604 secs/batch = 0.2004s, grad.norm=12.14205074
  8267: 6 [  305/ 1327], train_loss/perplexity = 4.55587387/95.1899033 secs/batch = 0.1930s, grad.norm=11.94970512
  8272: 6 [  310/ 1327], train_loss/perplexity = 4.62918758/102.4308167 secs/batch = 0.2009s, grad.norm=12.07723141
  8277: 6 [  315/ 1327], train_loss/perplexity = 4.21872234/67.9466171 secs/batch = 0.1997s, grad.norm=12.46096706
  8282: 6 [  320/ 1327], train_loss/perplexity = 4.13281393/62.3531342 secs/batch = 0.2002s, grad.norm=13.00866032
  8287: 6 [  325/ 1327], train_loss/perplexity = 4.12117147/61.6314011 secs/batch = 0.1931s, grad.norm=11.86925888
  8292: 6 [  330/ 1327], train_loss/perplexity = 4.63986301/103.5301666 secs/batch = 0.2001s, grad.norm=12.50832653
  8297: 6 [  335/ 1327], train_loss/perplexity = 4.02646732/56.0625114 secs/batch = 0.1997s, grad.norm=11.72140503
  8302: 6 [  340/ 1327], train_loss/perplexity = 4.80500460/122.1200562 secs/batch = 0.1996s, grad.norm=12.22032356
  8307: 6 [  345/ 1327], train_loss/perplexity = 4.62306547/101.8056335 secs/batch = 0.1983s, grad.norm=11.51867390
  8312: 6 [  350/ 1327], train_loss/perplexity = 4.66483498/106.1480637 secs/batch = 0.1991s, grad.norm=12.46740723
  8317: 6 [  355/ 1327], train_loss/perplexity = 4.72419500/112.6397858 secs/batch = 0.1938s, grad.norm=12.47160149
  8322: 6 [  360/ 1327], train_loss/perplexity = 4.94222021/140.0809174 secs/batch = 0.1942s, grad.norm=13.97219944
  8327: 6 [  365/ 1327], train_loss/perplexity = 4.69716454/109.6358643 secs/batch = 0.1997s, grad.norm=11.66692066
  8332: 6 [  370/ 1327], train_loss/perplexity = 4.79799271/121.2667542 secs/batch = 0.1997s, grad.norm=12.60817528
  8337: 6 [  375/ 1327], train_loss/perplexity = 4.23361063/68.9657898 secs/batch = 0.1941s, grad.norm=11.68160152
  8342: 6 [  380/ 1327], train_loss/perplexity = 4.28936100/72.9198608 secs/batch = 0.1987s, grad.norm=12.26835632
  8347: 6 [  385/ 1327], train_loss/perplexity = 4.58719873/98.2189102 secs/batch = 0.2000s, grad.norm=12.86599827
  8352: 6 [  390/ 1327], train_loss/perplexity = 4.58231401/97.7403030 secs/batch = 0.2006s, grad.norm=11.91191673
  8357: 6 [  395/ 1327], train_loss/perplexity = 4.71303606/111.3898315 secs/batch = 0.1954s, grad.norm=13.77516651
  8362: 6 [  400/ 1327], train_loss/perplexity = 4.61878490/101.3707809 secs/batch = 0.1955s, grad.norm=11.67466545
  8367: 6 [  405/ 1327], train_loss/perplexity = 4.92828083/138.1418152 secs/batch = 0.1941s, grad.norm=12.56464386
  8372: 6 [  410/ 1327], train_loss/perplexity = 4.57157373/96.6961670 secs/batch = 0.2009s, grad.norm=12.35313225
  8377: 6 [  415/ 1327], train_loss/perplexity = 4.47253752/87.5786743 secs/batch = 0.1995s, grad.norm=11.65513229
  8382: 6 [  420/ 1327], train_loss/perplexity = 4.18718195/65.8369980 secs/batch = 0.1995s, grad.norm=11.71985626
  8387: 6 [  425/ 1327], train_loss/perplexity = 4.46851969/87.2275009 secs/batch = 0.1998s, grad.norm=12.85194397
  8392: 6 [  430/ 1327], train_loss/perplexity = 4.72022247/112.1932068 secs/batch = 0.2001s, grad.norm=12.79608345
  8397: 6 [  435/ 1327], train_loss/perplexity = 4.78934240/120.2222824 secs/batch = 0.1990s, grad.norm=12.98641109
  8402: 6 [  440/ 1327], train_loss/perplexity = 4.43673754/84.4988174 secs/batch = 0.1951s, grad.norm=14.78662014
  8407: 6 [  445/ 1327], train_loss/perplexity = 4.67097855/106.8022003 secs/batch = 0.1929s, grad.norm=12.78770733
  8412: 6 [  450/ 1327], train_loss/perplexity = 4.55898952/95.4869461 secs/batch = 0.1997s, grad.norm=14.75338936
  8417: 6 [  455/ 1327], train_loss/perplexity = 4.45293045/85.8782349 secs/batch = 0.2000s, grad.norm=11.63286400
  8422: 6 [  460/ 1327], train_loss/perplexity = 4.52914667/92.6794434 secs/batch = 0.1995s, grad.norm=13.26393032
  8427: 6 [  465/ 1327], train_loss/perplexity = 4.25098610/70.1745758 secs/batch = 0.1989s, grad.norm=14.20125008
  8432: 6 [  470/ 1327], train_loss/perplexity = 4.96312523/143.0401306 secs/batch = 0.1993s, grad.norm=11.34694862
  8437: 6 [  475/ 1327], train_loss/perplexity = 4.38246584/80.0351410 secs/batch = 0.1990s, grad.norm=11.97308254
  8442: 6 [  480/ 1327], train_loss/perplexity = 4.50483799/90.4536896 secs/batch = 0.1990s, grad.norm=12.73399544
  8447: 6 [  485/ 1327], train_loss/perplexity = 4.52279329/92.0924835 secs/batch = 0.1990s, grad.norm=13.90404606
  8452: 6 [  490/ 1327], train_loss/perplexity = 4.47539473/87.8292618 secs/batch = 0.1980s, grad.norm=14.77073956
  8457: 6 [  495/ 1327], train_loss/perplexity = 4.43915510/84.7033463 secs/batch = 0.1936s, grad.norm=12.74560547
  8462: 6 [  500/ 1327], train_loss/perplexity = 4.68246460/108.0360107 secs/batch = 0.1996s, grad.norm=13.69894123
  8467: 6 [  505/ 1327], train_loss/perplexity = 4.80213165/121.7697144 secs/batch = 0.2004s, grad.norm=11.16431427
  8472: 6 [  510/ 1327], train_loss/perplexity = 5.02133751/151.6139526 secs/batch = 0.1997s, grad.norm=11.59032249
  8477: 6 [  515/ 1327], train_loss/perplexity = 4.73207760/113.5311890 secs/batch = 0.2003s, grad.norm=11.44576931
  8482: 6 [  520/ 1327], train_loss/perplexity = 4.95900726/142.4523010 secs/batch = 0.1993s, grad.norm=11.94932365
  8487: 6 [  525/ 1327], train_loss/perplexity = 4.45138979/85.7460327 secs/batch = 0.1965s, grad.norm=11.90746212
  8492: 6 [  530/ 1327], train_loss/perplexity = 4.52415657/92.2181168 secs/batch = 0.1997s, grad.norm=12.69605541
  8497: 6 [  535/ 1327], train_loss/perplexity = 4.66349411/106.0058289 secs/batch = 0.1988s, grad.norm=12.00221920
  8502: 6 [  540/ 1327], train_loss/perplexity = 4.70530605/110.5321121 secs/batch = 0.1991s, grad.norm=12.72147179
  8507: 6 [  545/ 1327], train_loss/perplexity = 4.80675316/122.3337708 secs/batch = 0.2007s, grad.norm=12.43340874
  8512: 6 [  550/ 1327], train_loss/perplexity = 4.61088276/100.5728912 secs/batch = 0.1955s, grad.norm=12.76977444
  8517: 6 [  555/ 1327], train_loss/perplexity = 4.52162313/91.9847794 secs/batch = 0.1997s, grad.norm=12.23704910
  8522: 6 [  560/ 1327], train_loss/perplexity = 4.67291641/107.0093689 secs/batch = 0.1995s, grad.norm=14.01992607
  8527: 6 [  565/ 1327], train_loss/perplexity = 4.55830717/95.4218140 secs/batch = 0.2020s, grad.norm=12.96735477
  8532: 6 [  570/ 1327], train_loss/perplexity = 4.51606846/91.4752502 secs/batch = 0.1998s, grad.norm=13.39753342
  8537: 6 [  575/ 1327], train_loss/perplexity = 4.29466581/73.3077087 secs/batch = 0.2014s, grad.norm=12.43161869
  8542: 6 [  580/ 1327], train_loss/perplexity = 4.76545525/117.3845444 secs/batch = 0.2002s, grad.norm=12.64877415
  8547: 6 [  585/ 1327], train_loss/perplexity = 4.26228142/70.9717178 secs/batch = 0.1951s, grad.norm=12.19268131
  8552: 6 [  590/ 1327], train_loss/perplexity = 4.60970497/100.4545059 secs/batch = 0.1999s, grad.norm=12.75566769
  8557: 6 [  595/ 1327], train_loss/perplexity = 4.59402418/98.8915863 secs/batch = 0.1994s, grad.norm=12.93643093
  8562: 6 [  600/ 1327], train_loss/perplexity = 4.75744581/116.4481125 secs/batch = 0.1987s, grad.norm=11.81418228
  8567: 6 [  605/ 1327], train_loss/perplexity = 4.74573421/115.0922775 secs/batch = 0.1983s, grad.norm=12.39217281
  8572: 6 [  610/ 1327], train_loss/perplexity = 4.97368956/144.5592651 secs/batch = 0.1992s, grad.norm=12.36806011
  8577: 6 [  615/ 1327], train_loss/perplexity = 4.56136894/95.7144165 secs/batch = 0.2000s, grad.norm=18.67922592
  8582: 6 [  620/ 1327], train_loss/perplexity = 4.78837490/120.1060257 secs/batch = 0.1986s, grad.norm=13.04004669
  8587: 6 [  625/ 1327], train_loss/perplexity = 4.80581093/122.2185593 secs/batch = 0.1984s, grad.norm=16.61898613
  8592: 6 [  630/ 1327], train_loss/perplexity = 4.82676411/124.8064499 secs/batch = 0.1980s, grad.norm=11.94804668
  8597: 6 [  635/ 1327], train_loss/perplexity = 4.58929253/98.4247742 secs/batch = 0.1995s, grad.norm=13.44642544
  8602: 6 [  640/ 1327], train_loss/perplexity = 4.66167212/105.8128662 secs/batch = 0.2001s, grad.norm=12.55367184
  8607: 6 [  645/ 1327], train_loss/perplexity = 4.95809317/142.3221588 secs/batch = 0.2003s, grad.norm=12.71711445
  8612: 6 [  650/ 1327], train_loss/perplexity = 4.43424940/84.2888336 secs/batch = 0.1992s, grad.norm=13.28690720
  8617: 6 [  655/ 1327], train_loss/perplexity = 4.52033091/91.8659897 secs/batch = 0.2001s, grad.norm=12.49677467
  8622: 6 [  660/ 1327], train_loss/perplexity = 4.44293213/85.0238800 secs/batch = 0.1991s, grad.norm=12.22917080
  8627: 6 [  665/ 1327], train_loss/perplexity = 4.63985586/103.5294266 secs/batch = 0.1981s, grad.norm=12.62595940
  8632: 6 [  670/ 1327], train_loss/perplexity = 4.56229591/95.8031845 secs/batch = 0.1994s, grad.norm=12.29991817
  8637: 6 [  675/ 1327], train_loss/perplexity = 4.36097622/78.3335648 secs/batch = 0.2006s, grad.norm=12.54440594
  8642: 6 [  680/ 1327], train_loss/perplexity = 4.58447838/97.9520798 secs/batch = 0.1997s, grad.norm=13.78637314
  8647: 6 [  685/ 1327], train_loss/perplexity = 4.45160866/85.7648010 secs/batch = 0.1996s, grad.norm=13.34071732
  8652: 6 [  690/ 1327], train_loss/perplexity = 4.77790117/118.8546295 secs/batch = 0.1964s, grad.norm=12.35980034
  8657: 6 [  695/ 1327], train_loss/perplexity = 4.61969042/101.4626160 secs/batch = 0.1995s, grad.norm=11.92137337
  8662: 6 [  700/ 1327], train_loss/perplexity = 4.80153561/121.6971512 secs/batch = 0.1996s, grad.norm=11.94707489
  8667: 6 [  705/ 1327], train_loss/perplexity = 4.52031946/91.8649368 secs/batch = 0.1995s, grad.norm=11.80469894
  8672: 6 [  710/ 1327], train_loss/perplexity = 4.51663446/91.5270386 secs/batch = 0.1993s, grad.norm=13.00569725
  8677: 6 [  715/ 1327], train_loss/perplexity = 4.44886541/85.5298462 secs/batch = 0.1998s, grad.norm=12.50101662
  8682: 6 [  720/ 1327], train_loss/perplexity = 4.43475103/84.3311234 secs/batch = 0.1967s, grad.norm=11.84618092
  8687: 6 [  725/ 1327], train_loss/perplexity = 4.39703989/81.2101212 secs/batch = 0.1935s, grad.norm=12.29043961
  8692: 6 [  730/ 1327], train_loss/perplexity = 4.61932325/101.4253693 secs/batch = 0.2002s, grad.norm=13.56833458
  8697: 6 [  735/ 1327], train_loss/perplexity = 4.73394680/113.7435989 secs/batch = 0.1997s, grad.norm=12.48959827
  8702: 6 [  740/ 1327], train_loss/perplexity = 4.07814741/59.0359993 secs/batch = 0.2005s, grad.norm=11.36585331
  8707: 6 [  745/ 1327], train_loss/perplexity = 4.63374043/102.8982315 secs/batch = 0.1994s, grad.norm=12.64078331
  8712: 6 [  750/ 1327], train_loss/perplexity = 4.48571062/88.7399902 secs/batch = 0.1999s, grad.norm=12.56163311
  8717: 6 [  755/ 1327], train_loss/perplexity = 4.38411999/80.1676407 secs/batch = 0.1956s, grad.norm=12.19146156
  8722: 6 [  760/ 1327], train_loss/perplexity = 4.20471716/67.0016479 secs/batch = 0.1997s, grad.norm=11.71994972
  8727: 6 [  765/ 1327], train_loss/perplexity = 4.33459806/76.2942886 secs/batch = 0.1985s, grad.norm=12.22613430
  8732: 6 [  770/ 1327], train_loss/perplexity = 4.28310585/72.4651566 secs/batch = 0.2000s, grad.norm=12.64652729
  8737: 6 [  775/ 1327], train_loss/perplexity = 4.45873356/86.3780441 secs/batch = 0.2001s, grad.norm=13.07612801
  8742: 6 [  780/ 1327], train_loss/perplexity = 4.80400896/121.9985275 secs/batch = 0.2000s, grad.norm=12.67069149
  8747: 6 [  785/ 1327], train_loss/perplexity = 4.64621115/104.1894760 secs/batch = 0.1962s, grad.norm=12.93705368
  8752: 6 [  790/ 1327], train_loss/perplexity = 4.33355618/76.2148361 secs/batch = 0.1998s, grad.norm=12.68960762
  8757: 6 [  795/ 1327], train_loss/perplexity = 4.76046658/116.8004074 secs/batch = 0.1992s, grad.norm=12.64481544
  8762: 6 [  800/ 1327], train_loss/perplexity = 4.61674404/101.1641083 secs/batch = 0.1994s, grad.norm=12.19794941
  8767: 6 [  805/ 1327], train_loss/perplexity = 4.97483778/144.7253418 secs/batch = 0.1993s, grad.norm=12.38775253
  8772: 6 [  810/ 1327], train_loss/perplexity = 4.63535357/103.0643539 secs/batch = 0.2000s, grad.norm=11.60103607
  8777: 6 [  815/ 1327], train_loss/perplexity = 4.48028755/88.2600479 secs/batch = 0.1992s, grad.norm=12.30445099
  8782: 6 [  820/ 1327], train_loss/perplexity = 4.26353931/71.0610428 secs/batch = 0.2007s, grad.norm=12.90180874
  8787: 6 [  825/ 1327], train_loss/perplexity = 4.50807190/90.7466812 secs/batch = 0.1999s, grad.norm=11.88577271
  8792: 6 [  830/ 1327], train_loss/perplexity = 4.31498909/74.8128052 secs/batch = 0.1929s, grad.norm=11.51911736
  8797: 6 [  835/ 1327], train_loss/perplexity = 4.55334091/94.9490967 secs/batch = 0.2003s, grad.norm=12.25778961
  8802: 6 [  840/ 1327], train_loss/perplexity = 4.64182377/103.7333603 secs/batch = 0.1985s, grad.norm=11.87331676
  8807: 6 [  845/ 1327], train_loss/perplexity = 4.44845629/85.4948654 secs/batch = 0.1993s, grad.norm=12.78286552
  8812: 6 [  850/ 1327], train_loss/perplexity = 4.53480339/93.2051849 secs/batch = 0.2001s, grad.norm=12.27440071
  8817: 6 [  855/ 1327], train_loss/perplexity = 4.48578167/88.7462921 secs/batch = 0.1998s, grad.norm=12.16808701
  8822: 6 [  860/ 1327], train_loss/perplexity = 4.24406672/69.6906891 secs/batch = 0.2002s, grad.norm=12.30461788
  8827: 6 [  865/ 1327], train_loss/perplexity = 4.72447300/112.6711044 secs/batch = 0.1998s, grad.norm=12.08867645
  8832: 6 [  870/ 1327], train_loss/perplexity = 4.62700462/102.2074585 secs/batch = 0.2005s, grad.norm=12.95134449
  8837: 6 [  875/ 1327], train_loss/perplexity = 4.20762539/67.1967850 secs/batch = 0.1988s, grad.norm=12.68802166
  8842: 6 [  880/ 1327], train_loss/perplexity = 4.46632290/87.0360947 secs/batch = 0.2007s, grad.norm=12.45436573
  8847: 6 [  885/ 1327], train_loss/perplexity = 4.54392004/94.0587921 secs/batch = 0.1994s, grad.norm=11.55893326
  8852: 6 [  890/ 1327], train_loss/perplexity = 4.77073002/118.0053558 secs/batch = 0.1992s, grad.norm=11.43706226
  8857: 6 [  895/ 1327], train_loss/perplexity = 4.78555679/119.7680283 secs/batch = 0.1945s, grad.norm=11.55652332
  8862: 6 [  900/ 1327], train_loss/perplexity = 4.59176111/98.6680450 secs/batch = 0.1992s, grad.norm=12.87157059
  8867: 6 [  905/ 1327], train_loss/perplexity = 4.70386267/110.3726807 secs/batch = 0.1997s, grad.norm=37.61693192
  8872: 6 [  910/ 1327], train_loss/perplexity = 4.42862225/83.8158569 secs/batch = 0.2000s, grad.norm=11.93072987
  8877: 6 [  915/ 1327], train_loss/perplexity = 4.61723852/101.2141418 secs/batch = 0.2000s, grad.norm=11.72926903
  8882: 6 [  920/ 1327], train_loss/perplexity = 4.86102295/129.1562500 secs/batch = 0.2008s, grad.norm=11.69672012
  8887: 6 [  925/ 1327], train_loss/perplexity = 4.64131451/103.6805496 secs/batch = 0.1985s, grad.norm=11.72089195
  8892: 6 [  930/ 1327], train_loss/perplexity = 4.59139967/98.6323853 secs/batch = 0.1999s, grad.norm=11.34967995
  8897: 6 [  935/ 1327], train_loss/perplexity = 4.74305534/114.7843704 secs/batch = 0.1994s, grad.norm=11.75829697
  8902: 6 [  940/ 1327], train_loss/perplexity = 4.69127846/108.9924316 secs/batch = 0.1932s, grad.norm=11.78708458
  8907: 6 [  945/ 1327], train_loss/perplexity = 4.84420681/127.0025024 secs/batch = 0.1947s, grad.norm=12.42287922
  8912: 6 [  950/ 1327], train_loss/perplexity = 4.68060112/107.8348770 secs/batch = 0.1993s, grad.norm=13.73904514
  8917: 6 [  955/ 1327], train_loss/perplexity = 4.63215828/102.7355576 secs/batch = 0.2004s, grad.norm=12.38807201
  8922: 6 [  960/ 1327], train_loss/perplexity = 4.97359896/144.5461731 secs/batch = 0.1952s, grad.norm=14.39500809
  8927: 6 [  965/ 1327], train_loss/perplexity = 4.65820837/105.4469910 secs/batch = 0.2007s, grad.norm=12.53820705
  8932: 6 [  970/ 1327], train_loss/perplexity = 4.86100721/129.1542206 secs/batch = 0.1997s, grad.norm=11.66421604
  8937: 6 [  975/ 1327], train_loss/perplexity = 4.57944632/97.4604187 secs/batch = 0.1946s, grad.norm=13.26011467
  8942: 6 [  980/ 1327], train_loss/perplexity = 4.39918995/81.3849182 secs/batch = 0.2001s, grad.norm=12.19460773
  8947: 6 [  985/ 1327], train_loss/perplexity = 4.56108952/95.6876755 secs/batch = 0.1993s, grad.norm=13.19819546
  8952: 6 [  990/ 1327], train_loss/perplexity = 4.75743866/116.4472809 secs/batch = 0.1991s, grad.norm=13.10479164
  8957: 6 [  995/ 1327], train_loss/perplexity = 4.79482698/120.8834686 secs/batch = 0.1985s, grad.norm=12.02311802
  8962: 6 [ 1000/ 1327], train_loss/perplexity = 4.28899717/72.8933334 secs/batch = 0.1991s, grad.norm=13.60641098
  8967: 6 [ 1005/ 1327], train_loss/perplexity = 4.77192020/118.1458893 secs/batch = 0.1932s, grad.norm=12.30866146
  8972: 6 [ 1010/ 1327], train_loss/perplexity = 4.28454399/72.5694504 secs/batch = 0.1993s, grad.norm=11.77310848
  8977: 6 [ 1015/ 1327], train_loss/perplexity = 4.81287241/123.0846634 secs/batch = 0.1942s, grad.norm=11.70538330
  8982: 6 [ 1020/ 1327], train_loss/perplexity = 4.96582556/143.4269104 secs/batch = 0.1992s, grad.norm=11.66008472
  8987: 6 [ 1025/ 1327], train_loss/perplexity = 4.74155998/114.6128540 secs/batch = 0.1998s, grad.norm=11.58495331
  8992: 6 [ 1030/ 1327], train_loss/perplexity = 4.56739426/96.2928696 secs/batch = 0.1998s, grad.norm=11.89952660
  8997: 6 [ 1035/ 1327], train_loss/perplexity = 4.46349049/86.7899170 secs/batch = 0.2007s, grad.norm=12.16004848
  9002: 6 [ 1040/ 1327], train_loss/perplexity = 4.74719238/115.2602234 secs/batch = 0.2000s, grad.norm=11.99422741
  9007: 6 [ 1045/ 1327], train_loss/perplexity = 4.31481266/74.7996063 secs/batch = 0.2007s, grad.norm=12.06424904
  9012: 6 [ 1050/ 1327], train_loss/perplexity = 4.45592642/86.1359100 secs/batch = 0.1993s, grad.norm=12.82119656
  9017: 6 [ 1055/ 1327], train_loss/perplexity = 4.46613503/87.0197449 secs/batch = 0.1997s, grad.norm=12.96047497
  9022: 6 [ 1060/ 1327], train_loss/perplexity = 4.11689377/61.3683205 secs/batch = 0.1989s, grad.norm=12.60132790
  9027: 6 [ 1065/ 1327], train_loss/perplexity = 4.33638048/76.4303970 secs/batch = 0.1996s, grad.norm=12.92172527
  9032: 6 [ 1070/ 1327], train_loss/perplexity = 4.60771561/100.2548676 secs/batch = 0.1998s, grad.norm=12.83815289
  9037: 6 [ 1075/ 1327], train_loss/perplexity = 4.37779093/79.6618576 secs/batch = 0.1990s, grad.norm=12.72821999
  9042: 6 [ 1080/ 1327], train_loss/perplexity = 4.37785625/79.6670609 secs/batch = 0.1930s, grad.norm=14.34605598
  9047: 6 [ 1085/ 1327], train_loss/perplexity = 4.14522266/63.1316757 secs/batch = 0.2008s, grad.norm=12.32669735
  9052: 6 [ 1090/ 1327], train_loss/perplexity = 4.33054638/75.9857941 secs/batch = 0.2001s, grad.norm=13.15108013
  9057: 6 [ 1095/ 1327], train_loss/perplexity = 4.71232033/111.3101349 secs/batch = 0.1994s, grad.norm=19.32440186
  9062: 6 [ 1100/ 1327], train_loss/perplexity = 4.29505539/73.3362732 secs/batch = 0.1950s, grad.norm=13.77034664
  9067: 6 [ 1105/ 1327], train_loss/perplexity = 4.19543362/66.3825073 secs/batch = 0.1996s, grad.norm=12.69208527
  9072: 6 [ 1110/ 1327], train_loss/perplexity = 4.70558405/110.5628433 secs/batch = 0.1996s, grad.norm=13.06283379
  9077: 6 [ 1115/ 1327], train_loss/perplexity = 4.37480259/79.4241562 secs/batch = 0.1978s, grad.norm=12.29708004
  9082: 6 [ 1120/ 1327], train_loss/perplexity = 4.56776905/96.3289642 secs/batch = 0.1994s, grad.norm=13.24052334
  9087: 6 [ 1125/ 1327], train_loss/perplexity = 4.82385302/124.4436493 secs/batch = 0.2001s, grad.norm=13.55638790
  9092: 6 [ 1130/ 1327], train_loss/perplexity = 4.43310642/84.1925507 secs/batch = 0.2000s, grad.norm=11.92527580
  9097: 6 [ 1135/ 1327], train_loss/perplexity = 4.44673586/85.3479004 secs/batch = 0.2028s, grad.norm=11.89328003
  9102: 6 [ 1140/ 1327], train_loss/perplexity = 4.77799034/118.8652344 secs/batch = 0.1994s, grad.norm=13.24230289
  9107: 6 [ 1145/ 1327], train_loss/perplexity = 4.56432009/95.9972992 secs/batch = 0.1992s, grad.norm=12.82572842
  9112: 6 [ 1150/ 1327], train_loss/perplexity = 4.49147701/89.2531738 secs/batch = 0.2005s, grad.norm=12.92713737
  9117: 6 [ 1155/ 1327], train_loss/perplexity = 4.60785103/100.2684402 secs/batch = 0.1993s, grad.norm=12.64808083
  9122: 6 [ 1160/ 1327], train_loss/perplexity = 4.61060762/100.5452271 secs/batch = 0.2003s, grad.norm=12.82398033
  9127: 6 [ 1165/ 1327], train_loss/perplexity = 4.57582474/97.1080933 secs/batch = 0.1985s, grad.norm=12.39970398
  9132: 6 [ 1170/ 1327], train_loss/perplexity = 4.44702339/85.3724442 secs/batch = 0.2002s, grad.norm=11.90665245
  9137: 6 [ 1175/ 1327], train_loss/perplexity = 4.19162607/66.1302338 secs/batch = 0.1996s, grad.norm=12.30106068
  9142: 6 [ 1180/ 1327], train_loss/perplexity = 4.21894360/67.9616547 secs/batch = 0.1990s, grad.norm=12.65726376
  9147: 6 [ 1185/ 1327], train_loss/perplexity = 4.45831823/86.3421783 secs/batch = 0.1959s, grad.norm=12.50922298
  9152: 6 [ 1190/ 1327], train_loss/perplexity = 4.45715237/86.2415771 secs/batch = 0.1991s, grad.norm=12.71151924
  9157: 6 [ 1195/ 1327], train_loss/perplexity = 4.33813286/76.5644455 secs/batch = 0.2002s, grad.norm=12.28021049
  9162: 6 [ 1200/ 1327], train_loss/perplexity = 4.32861519/75.8391876 secs/batch = 0.1925s, grad.norm=12.00700951
  9167: 6 [ 1205/ 1327], train_loss/perplexity = 4.28129005/72.3336945 secs/batch = 0.2007s, grad.norm=12.21035290
  9172: 6 [ 1210/ 1327], train_loss/perplexity = 3.97694921/53.3540154 secs/batch = 0.1998s, grad.norm=12.81389427
  9177: 6 [ 1215/ 1327], train_loss/perplexity = 4.24147272/69.5101471 secs/batch = 0.2003s, grad.norm=11.89872742
  9182: 6 [ 1220/ 1327], train_loss/perplexity = 4.35058784/77.5240250 secs/batch = 0.1993s, grad.norm=13.43826675
  9187: 6 [ 1225/ 1327], train_loss/perplexity = 4.18007755/65.3709259 secs/batch = 0.1971s, grad.norm=13.20535946
  9192: 6 [ 1230/ 1327], train_loss/perplexity = 4.31135845/74.5416794 secs/batch = 0.2000s, grad.norm=13.65583801
  9197: 6 [ 1235/ 1327], train_loss/perplexity = 4.37835741/79.7070007 secs/batch = 0.1995s, grad.norm=14.68337440
  9202: 6 [ 1240/ 1327], train_loss/perplexity = 4.56746721/96.2998962 secs/batch = 0.1994s, grad.norm=13.75197411
  9207: 6 [ 1245/ 1327], train_loss/perplexity = 4.50569916/90.5316162 secs/batch = 0.2001s, grad.norm=11.79513359
  9212: 6 [ 1250/ 1327], train_loss/perplexity = 4.55312157/94.9282684 secs/batch = 0.1992s, grad.norm=11.48740673
  9217: 6 [ 1255/ 1327], train_loss/perplexity = 4.61728334/101.2186813 secs/batch = 0.2002s, grad.norm=11.89563084
  9222: 6 [ 1260/ 1327], train_loss/perplexity = 4.37540054/79.4716644 secs/batch = 0.1994s, grad.norm=13.16078854
  9227: 6 [ 1265/ 1327], train_loss/perplexity = 4.62204885/101.7021942 secs/batch = 0.1991s, grad.norm=12.99305916
  9232: 6 [ 1270/ 1327], train_loss/perplexity = 4.32455635/75.5319977 secs/batch = 0.1995s, grad.norm=13.27608681
  9237: 6 [ 1275/ 1327], train_loss/perplexity = 4.52918911/92.6833725 secs/batch = 0.1955s, grad.norm=13.23700047
  9242: 6 [ 1280/ 1327], train_loss/perplexity = 4.37339926/79.3127823 secs/batch = 0.1957s, grad.norm=13.30600739
  9247: 6 [ 1285/ 1327], train_loss/perplexity = 4.33636332/76.4290848 secs/batch = 0.1992s, grad.norm=12.71329594
  9252: 6 [ 1290/ 1327], train_loss/perplexity = 4.55085468/94.7133255 secs/batch = 0.2013s, grad.norm=14.22730923
  9257: 6 [ 1295/ 1327], train_loss/perplexity = 4.52988672/92.7480545 secs/batch = 0.1992s, grad.norm=12.55979347
  9262: 6 [ 1300/ 1327], train_loss/perplexity = 4.66670513/106.3467636 secs/batch = 0.1992s, grad.norm=12.90626144
  9267: 6 [ 1305/ 1327], train_loss/perplexity = 4.78876257/120.1525955 secs/batch = 0.1996s, grad.norm=13.56264877
  9272: 6 [ 1310/ 1327], train_loss/perplexity = 5.03443909/153.6134033 secs/batch = 0.1935s, grad.norm=12.58155155
  9277: 6 [ 1315/ 1327], train_loss/perplexity = 4.76084042/116.8440857 secs/batch = 0.1985s, grad.norm=13.15961266
  9282: 6 [ 1320/ 1327], train_loss/perplexity = 4.84348774/126.9112167 secs/batch = 0.1997s, grad.norm=11.95722485
  9287: 6 [ 1325/ 1327], train_loss/perplexity = 4.80150366/121.6932678 secs/batch = 0.1990s, grad.norm=12.33235836
Epoch training time: 264.1958088874817
	> validation loss = 4.96754169, perplexity = 143.67326355
	> validation loss = 4.79955578, perplexity = 121.45645142
	> validation loss = 4.76953888, perplexity = 117.86487579
	> validation loss = 4.84958076, perplexity = 127.68685150
	> validation loss = 5.03566456, perplexity = 153.80177307
	> validation loss = 4.86669683, perplexity = 129.89115906
	> validation loss = 4.83665657, perplexity = 126.04721832
	> validation loss = 4.67334318, perplexity = 107.05504608
	> validation loss = 4.53411627, perplexity = 93.14116669
	> validation loss = 4.66011047, perplexity = 105.64775085
	> validation loss = 4.71066189, perplexity = 111.12568665
	> validation loss = 4.81022739, perplexity = 122.75952911
	> validation loss = 4.72858524, perplexity = 113.13539124
	> validation loss = 4.57532549, perplexity = 97.05962372
	> validation loss = 4.47835350, perplexity = 88.08951569
	> validation loss = 4.43647671, perplexity = 84.47678375
	> validation loss = 4.90315723, perplexity = 134.71443176
	> validation loss = 4.50916481, perplexity = 90.84591675
	> validation loss = 4.91934681, perplexity = 136.91314697
	> validation loss = 4.76771355, perplexity = 117.64993286
	> validation loss = 4.58364010, perplexity = 97.87000275
at the end of epoch: 6
train loss = 4.65266830, perplexity = 104.86442228
validation loss = 4.73585653, perplexity = 113.96102780
Saved model cv/epoch006_4.7359.model
  9294: 7 [    5/ 1327], train_loss/perplexity = 4.77468109/118.4725266 secs/batch = 0.1989s, grad.norm=12.33282566
  9299: 7 [   10/ 1327], train_loss/perplexity = 4.34172344/76.8398514 secs/batch = 0.1947s, grad.norm=11.68238926
  9304: 7 [   15/ 1327], train_loss/perplexity = 4.54811954/94.4546204 secs/batch = 0.2007s, grad.norm=11.35002899
  9309: 7 [   20/ 1327], train_loss/perplexity = 4.83927965/126.3782806 secs/batch = 0.2002s, grad.norm=12.50570202
  9314: 7 [   25/ 1327], train_loss/perplexity = 4.63516426/103.0448456 secs/batch = 0.1993s, grad.norm=12.28544807
  9319: 7 [   30/ 1327], train_loss/perplexity = 4.67475462/107.2062607 secs/batch = 0.2008s, grad.norm=12.64646053
  9324: 7 [   35/ 1327], train_loss/perplexity = 4.46988058/87.3462906 secs/batch = 0.1989s, grad.norm=11.96052647
  9329: 7 [   40/ 1327], train_loss/perplexity = 4.47522068/87.8139801 secs/batch = 0.1988s, grad.norm=13.08602810
  9334: 7 [   45/ 1327], train_loss/perplexity = 4.27243805/71.6962204 secs/batch = 0.1991s, grad.norm=11.67825222
  9339: 7 [   50/ 1327], train_loss/perplexity = 4.52593565/92.3823242 secs/batch = 0.2001s, grad.norm=12.17368221
  9344: 7 [   55/ 1327], train_loss/perplexity = 4.50398636/90.3766861 secs/batch = 0.2004s, grad.norm=14.52985668
  9349: 7 [   60/ 1327], train_loss/perplexity = 4.79108620/120.4321136 secs/batch = 0.1932s, grad.norm=12.85917473
  9354: 7 [   65/ 1327], train_loss/perplexity = 4.34590101/77.1615295 secs/batch = 0.1981s, grad.norm=12.69915867
  9359: 7 [   70/ 1327], train_loss/perplexity = 4.16446447/64.3582077 secs/batch = 0.2003s, grad.norm=12.20311546
  9364: 7 [   75/ 1327], train_loss/perplexity = 3.92115402/50.4586411 secs/batch = 0.1995s, grad.norm=13.13371468
  9369: 7 [   80/ 1327], train_loss/perplexity = 4.44474268/85.1779556 secs/batch = 0.1995s, grad.norm=13.37355232
  9374: 7 [   85/ 1327], train_loss/perplexity = 4.41441393/82.6334000 secs/batch = 0.2003s, grad.norm=12.31515789
  9379: 7 [   90/ 1327], train_loss/perplexity = 4.46878767/87.2508850 secs/batch = 0.1996s, grad.norm=12.63097191
  9384: 7 [   95/ 1327], train_loss/perplexity = 4.35348558/77.7489929 secs/batch = 0.2003s, grad.norm=12.11127472
  9389: 7 [  100/ 1327], train_loss/perplexity = 4.67232704/106.9463196 secs/batch = 0.1948s, grad.norm=12.35629177
  9394: 7 [  105/ 1327], train_loss/perplexity = 4.60059881/99.5439072 secs/batch = 0.1990s, grad.norm=13.81696033
  9399: 7 [  110/ 1327], train_loss/perplexity = 4.41860437/82.9803925 secs/batch = 0.1997s, grad.norm=12.96747017
  9404: 7 [  115/ 1327], train_loss/perplexity = 4.38672686/80.3768997 secs/batch = 0.1987s, grad.norm=12.87771988
  9409: 7 [  120/ 1327], train_loss/perplexity = 4.50403309/90.3809128 secs/batch = 0.1994s, grad.norm=13.64868355
  9414: 7 [  125/ 1327], train_loss/perplexity = 4.56274033/95.8457718 secs/batch = 0.1992s, grad.norm=12.65907574
  9419: 7 [  130/ 1327], train_loss/perplexity = 4.49467373/89.5389481 secs/batch = 0.2001s, grad.norm=13.64080524
  9424: 7 [  135/ 1327], train_loss/perplexity = 4.48133659/88.3526840 secs/batch = 0.1954s, grad.norm=12.42616081
  9429: 7 [  140/ 1327], train_loss/perplexity = 4.80594254/122.2346497 secs/batch = 0.1991s, grad.norm=13.13289261
  9434: 7 [  145/ 1327], train_loss/perplexity = 4.63909149/103.4503174 secs/batch = 0.2006s, grad.norm=13.22331810
  9439: 7 [  150/ 1327], train_loss/perplexity = 4.66410017/106.0700989 secs/batch = 0.1997s, grad.norm=12.34012032
  9444: 7 [  155/ 1327], train_loss/perplexity = 4.94283438/140.1669769 secs/batch = 0.1961s, grad.norm=12.08488464
  9449: 7 [  160/ 1327], train_loss/perplexity = 4.52270651/92.0844879 secs/batch = 0.1993s, grad.norm=11.64244461
  9454: 7 [  165/ 1327], train_loss/perplexity = 4.77375793/118.3632050 secs/batch = 0.1939s, grad.norm=12.78360367
  9459: 7 [  170/ 1327], train_loss/perplexity = 4.51282406/91.1789474 secs/batch = 0.1996s, grad.norm=11.78635311
  9464: 7 [  175/ 1327], train_loss/perplexity = 4.87162733/130.5331573 secs/batch = 0.1995s, grad.norm=14.01891422
  9469: 7 [  180/ 1327], train_loss/perplexity = 4.60935736/100.4195938 secs/batch = 0.1985s, grad.norm=12.71209621
  9474: 7 [  185/ 1327], train_loss/perplexity = 4.89898491/134.1535339 secs/batch = 0.1996s, grad.norm=12.56137657
  9479: 7 [  190/ 1327], train_loss/perplexity = 4.44625092/85.3065262 secs/batch = 0.1999s, grad.norm=11.63008308
  9484: 7 [  195/ 1327], train_loss/perplexity = 4.72098398/112.2786789 secs/batch = 0.1995s, grad.norm=11.57084179
  9489: 7 [  200/ 1327], train_loss/perplexity = 4.60601854/100.0848694 secs/batch = 0.1972s, grad.norm=12.32445621
  9494: 7 [  205/ 1327], train_loss/perplexity = 4.74178934/114.6391449 secs/batch = 0.1931s, grad.norm=11.71226025
  9499: 7 [  210/ 1327], train_loss/perplexity = 4.64362383/103.9202576 secs/batch = 0.2014s, grad.norm=11.34499168
  9504: 7 [  215/ 1327], train_loss/perplexity = 4.80443048/122.0499649 secs/batch = 0.1991s, grad.norm=12.21839237
  9509: 7 [  220/ 1327], train_loss/perplexity = 4.66893673/106.5843506 secs/batch = 0.1998s, grad.norm=12.05441380
  9514: 7 [  225/ 1327], train_loss/perplexity = 4.87618256/131.1291351 secs/batch = 0.1993s, grad.norm=12.33689880
  9519: 7 [  230/ 1327], train_loss/perplexity = 4.75064945/115.6593781 secs/batch = 0.1996s, grad.norm=12.35927677
  9524: 7 [  235/ 1327], train_loss/perplexity = 4.56589651/96.1487503 secs/batch = 0.1999s, grad.norm=12.80216122
  9529: 7 [  240/ 1327], train_loss/perplexity = 4.42864323/83.8176193 secs/batch = 0.1994s, grad.norm=13.03355217
  9534: 7 [  245/ 1327], train_loss/perplexity = 4.67644215/107.3873215 secs/batch = 0.1989s, grad.norm=11.95928288
  9539: 7 [  250/ 1327], train_loss/perplexity = 4.47417259/87.7219849 secs/batch = 0.2001s, grad.norm=12.79203415
  9544: 7 [  255/ 1327], train_loss/perplexity = 4.49729395/89.7738724 secs/batch = 0.1942s, grad.norm=12.54403400
  9549: 7 [  260/ 1327], train_loss/perplexity = 4.72174501/112.3641586 secs/batch = 0.1994s, grad.norm=12.32344341
  9554: 7 [  265/ 1327], train_loss/perplexity = 4.91258717/135.9907837 secs/batch = 0.1992s, grad.norm=12.19503307
  9559: 7 [  270/ 1327], train_loss/perplexity = 4.90280056/134.6663971 secs/batch = 0.2002s, grad.norm=11.96966648
  9564: 7 [  275/ 1327], train_loss/perplexity = 4.86994743/130.3140717 secs/batch = 0.1988s, grad.norm=11.81781483
  9569: 7 [  280/ 1327], train_loss/perplexity = 4.68663883/108.4879227 secs/batch = 0.1952s, grad.norm=12.54722881
  9574: 7 [  285/ 1327], train_loss/perplexity = 4.97454834/144.6834564 secs/batch = 0.1931s, grad.norm=11.97377872
  9579: 7 [  290/ 1327], train_loss/perplexity = 4.67004776/106.7028351 secs/batch = 0.1997s, grad.norm=11.81390572
  9584: 7 [  295/ 1327], train_loss/perplexity = 4.45961809/86.4544830 secs/batch = 0.1995s, grad.norm=12.16652489
  9589: 7 [  300/ 1327], train_loss/perplexity = 4.10258865/60.4966888 secs/batch = 0.2001s, grad.norm=12.40479088
  9594: 7 [  305/ 1327], train_loss/perplexity = 4.56824875/96.3751831 secs/batch = 0.1959s, grad.norm=12.23256016
  9599: 7 [  310/ 1327], train_loss/perplexity = 4.55745029/95.3400803 secs/batch = 0.1953s, grad.norm=11.97244263
  9604: 7 [  315/ 1327], train_loss/perplexity = 4.06691265/58.3764572 secs/batch = 0.1984s, grad.norm=11.82758427
  9609: 7 [  320/ 1327], train_loss/perplexity = 4.11988974/61.5524559 secs/batch = 0.1988s, grad.norm=13.93446064
  9614: 7 [  325/ 1327], train_loss/perplexity = 4.05672073/57.7845078 secs/batch = 0.1937s, grad.norm=11.93943214
  9619: 7 [  330/ 1327], train_loss/perplexity = 4.57651567/97.1752167 secs/batch = 0.1992s, grad.norm=12.81741238
  9624: 7 [  335/ 1327], train_loss/perplexity = 4.01723909/55.5475311 secs/batch = 0.2006s, grad.norm=11.31457329
  9629: 7 [  340/ 1327], train_loss/perplexity = 4.73123598/113.4356766 secs/batch = 0.1993s, grad.norm=12.34634781
  9634: 7 [  345/ 1327], train_loss/perplexity = 4.57890320/97.4075012 secs/batch = 0.1985s, grad.norm=11.81321049
  9639: 7 [  350/ 1327], train_loss/perplexity = 4.68292618/108.0858917 secs/batch = 0.1977s, grad.norm=12.80059433
  9644: 7 [  355/ 1327], train_loss/perplexity = 4.66937351/106.6309204 secs/batch = 0.1991s, grad.norm=12.51473331
  9649: 7 [  360/ 1327], train_loss/perplexity = 4.82429218/124.4983139 secs/batch = 0.1995s, grad.norm=14.78751850
  9654: 7 [  365/ 1327], train_loss/perplexity = 4.73856544/114.2701569 secs/batch = 0.1988s, grad.norm=12.43235207
  9659: 7 [  370/ 1327], train_loss/perplexity = 4.75214767/115.8327866 secs/batch = 0.2003s, grad.norm=12.77629757
  9664: 7 [  375/ 1327], train_loss/perplexity = 4.16694164/64.5178299 secs/batch = 0.1991s, grad.norm=12.06760788
  9669: 7 [  380/ 1327], train_loss/perplexity = 4.26578093/71.2205124 secs/batch = 0.1992s, grad.norm=13.61444378
  9674: 7 [  385/ 1327], train_loss/perplexity = 4.50593472/90.5529480 secs/batch = 0.1991s, grad.norm=13.41879845
  9679: 7 [  390/ 1327], train_loss/perplexity = 4.52403402/92.2068100 secs/batch = 0.1991s, grad.norm=12.00193691
  9684: 7 [  395/ 1327], train_loss/perplexity = 4.65138865/104.7303162 secs/batch = 0.1999s, grad.norm=12.80521297
  9689: 7 [  400/ 1327], train_loss/perplexity = 4.54767799/94.4129257 secs/batch = 0.1993s, grad.norm=12.38237286
  9694: 7 [  405/ 1327], train_loss/perplexity = 4.89879513/134.1280823 secs/batch = 0.1995s, grad.norm=12.32812595
  9699: 7 [  410/ 1327], train_loss/perplexity = 4.47914124/88.1589355 secs/batch = 0.1998s, grad.norm=12.67218781
  9704: 7 [  415/ 1327], train_loss/perplexity = 4.41168880/82.4085159 secs/batch = 0.1985s, grad.norm=13.39272404
  9709: 7 [  420/ 1327], train_loss/perplexity = 4.13280201/62.3523903 secs/batch = 0.1949s, grad.norm=12.22873592
  9714: 7 [  425/ 1327], train_loss/perplexity = 4.41216183/82.4475098 secs/batch = 0.2001s, grad.norm=13.14834023
  9719: 7 [  430/ 1327], train_loss/perplexity = 4.70226955/110.1969833 secs/batch = 0.1989s, grad.norm=12.91381454
  9724: 7 [  435/ 1327], train_loss/perplexity = 4.67205381/106.9171066 secs/batch = 0.1996s, grad.norm=12.79380798
  9729: 7 [  440/ 1327], train_loss/perplexity = 4.37345028/79.3168259 secs/batch = 0.2000s, grad.norm=13.58427811
  9734: 7 [  445/ 1327], train_loss/perplexity = 4.60162115/99.6457291 secs/batch = 0.1998s, grad.norm=13.10579205
  9739: 7 [  450/ 1327], train_loss/perplexity = 4.51649237/91.5140381 secs/batch = 0.1996s, grad.norm=12.98912621
  9744: 7 [  455/ 1327], train_loss/perplexity = 4.41251230/82.4764099 secs/batch = 0.1991s, grad.norm=12.34956264
  9749: 7 [  460/ 1327], train_loss/perplexity = 4.49758244/89.7997742 secs/batch = 0.1999s, grad.norm=12.89623737
  9754: 7 [  465/ 1327], train_loss/perplexity = 4.25636101/70.5527725 secs/batch = 0.2006s, grad.norm=13.69139194
  9759: 7 [  470/ 1327], train_loss/perplexity = 4.89686203/133.8690491 secs/batch = 0.1988s, grad.norm=11.75618076
  9764: 7 [  475/ 1327], train_loss/perplexity = 4.38207054/80.0035095 secs/batch = 0.1995s, grad.norm=12.83931351
  9769: 7 [  480/ 1327], train_loss/perplexity = 4.56015301/95.5981064 secs/batch = 0.1996s, grad.norm=13.17353058
  9774: 7 [  485/ 1327], train_loss/perplexity = 4.50411797/90.3885803 secs/batch = 0.1998s, grad.norm=13.08550835
  9779: 7 [  490/ 1327], train_loss/perplexity = 4.39235306/80.8303909 secs/batch = 0.1950s, grad.norm=13.48017979
  9784: 7 [  495/ 1327], train_loss/perplexity = 4.41409349/82.6069260 secs/batch = 0.1989s, grad.norm=12.19032478
  9789: 7 [  500/ 1327], train_loss/perplexity = 4.61041784/100.5261459 secs/batch = 0.1996s, grad.norm=13.68438816
  9794: 7 [  505/ 1327], train_loss/perplexity = 4.67989731/107.7590027 secs/batch = 0.1938s, grad.norm=11.66089916
  9799: 7 [  510/ 1327], train_loss/perplexity = 5.05491114/156.7906036 secs/batch = 0.1994s, grad.norm=12.68846798
  9804: 7 [  515/ 1327], train_loss/perplexity = 4.66494179/106.1594086 secs/batch = 0.2008s, grad.norm=12.09960461
  9809: 7 [  520/ 1327], train_loss/perplexity = 4.85373640/128.2185669 secs/batch = 0.1993s, grad.norm=12.01291561
  9814: 7 [  525/ 1327], train_loss/perplexity = 4.39681673/81.1920013 secs/batch = 0.1948s, grad.norm=11.83819485
  9819: 7 [  530/ 1327], train_loss/perplexity = 4.53479338/93.2042542 secs/batch = 0.1995s, grad.norm=13.18376064
  9824: 7 [  535/ 1327], train_loss/perplexity = 4.58310604/97.8177490 secs/batch = 0.1992s, grad.norm=12.32921505
  9829: 7 [  540/ 1327], train_loss/perplexity = 4.61495161/100.9829407 secs/batch = 0.1999s, grad.norm=12.42778301
  9834: 7 [  545/ 1327], train_loss/perplexity = 4.75813198/116.5280457 secs/batch = 0.1946s, grad.norm=13.00078106
  9839: 7 [  550/ 1327], train_loss/perplexity = 4.59415770/98.9047928 secs/batch = 0.1998s, grad.norm=11.94161797
  9844: 7 [  555/ 1327], train_loss/perplexity = 6.24501896/515.4390259 secs/batch = 0.1988s, grad.norm=76.28133392
  9849: 7 [  560/ 1327], train_loss/perplexity = 4.58139610/97.6506271 secs/batch = 0.1994s, grad.norm=13.04779625
  9854: 7 [  565/ 1327], train_loss/perplexity = 4.49835682/89.8693390 secs/batch = 0.1990s, grad.norm=13.20609570
  9859: 7 [  570/ 1327], train_loss/perplexity = 4.46744061/87.1334305 secs/batch = 0.1993s, grad.norm=13.73737144
  9864: 7 [  575/ 1327], train_loss/perplexity = 4.32525444/75.5847397 secs/batch = 0.1998s, grad.norm=13.37190247
  9869: 7 [  580/ 1327], train_loss/perplexity = 4.71208096/111.2834930 secs/batch = 0.1992s, grad.norm=12.74723434
  9874: 7 [  585/ 1327], train_loss/perplexity = 4.22790098/68.5731430 secs/batch = 0.1987s, grad.norm=12.68589115
  9879: 7 [  590/ 1327], train_loss/perplexity = 4.57696295/97.2186890 secs/batch = 0.1991s, grad.norm=12.63600826
  9884: 7 [  595/ 1327], train_loss/perplexity = 4.56786346/96.3380585 secs/batch = 0.1998s, grad.norm=13.56285477
  9889: 7 [  600/ 1327], train_loss/perplexity = 4.77102280/118.0399094 secs/batch = 0.1991s, grad.norm=11.70715046
  9894: 7 [  605/ 1327], train_loss/perplexity = 4.64335775/103.8926086 secs/batch = 0.2003s, grad.norm=12.15948200
  9899: 7 [  610/ 1327], train_loss/perplexity = 4.88793659/132.6795197 secs/batch = 0.1953s, grad.norm=12.99687672
  9904: 7 [  615/ 1327], train_loss/perplexity = 4.46716785/87.1096649 secs/batch = 0.2003s, grad.norm=12.20635033
  9909: 7 [  620/ 1327], train_loss/perplexity = 4.74335527/114.8188019 secs/batch = 0.2002s, grad.norm=12.31372929
  9914: 7 [  625/ 1327], train_loss/perplexity = 4.77581024/118.6063766 secs/batch = 0.1995s, grad.norm=11.62866306
  9919: 7 [  630/ 1327], train_loss/perplexity = 4.82333851/124.3796387 secs/batch = 0.2001s, grad.norm=13.29089642
  9924: 7 [  635/ 1327], train_loss/perplexity = 4.60049772/99.5338440 secs/batch = 0.1977s, grad.norm=11.80871391
  9929: 7 [  640/ 1327], train_loss/perplexity = 4.54382086/94.0494614 secs/batch = 0.1986s, grad.norm=12.17462540
  9934: 7 [  645/ 1327], train_loss/perplexity = 4.97682428/145.0131226 secs/batch = 0.1988s, grad.norm=18.47522545
  9939: 7 [  650/ 1327], train_loss/perplexity = 4.32925463/75.8877029 secs/batch = 0.1995s, grad.norm=13.72528172
  9944: 7 [  655/ 1327], train_loss/perplexity = 4.44615412/85.2982635 secs/batch = 0.2002s, grad.norm=12.42914104
  9949: 7 [  660/ 1327], train_loss/perplexity = 4.41645002/82.8018188 secs/batch = 0.1995s, grad.norm=12.67911720
  9954: 7 [  665/ 1327], train_loss/perplexity = 4.58345890/97.8522720 secs/batch = 0.1983s, grad.norm=13.09306526
  9959: 7 [  670/ 1327], train_loss/perplexity = 4.53272390/93.0115738 secs/batch = 0.1935s, grad.norm=13.01740932
  9964: 7 [  675/ 1327], train_loss/perplexity = 4.34963751/77.4503860 secs/batch = 0.1996s, grad.norm=12.82486343
  9969: 7 [  680/ 1327], train_loss/perplexity = 4.57432747/96.9628067 secs/batch = 0.1993s, grad.norm=13.43410492
  9974: 7 [  685/ 1327], train_loss/perplexity = 4.40587044/81.9304276 secs/batch = 0.1987s, grad.norm=12.59735489
  9979: 7 [  690/ 1327], train_loss/perplexity = 4.74101448/114.5503540 secs/batch = 0.1987s, grad.norm=12.60004234
  9984: 7 [  695/ 1327], train_loss/perplexity = 4.50160360/90.1615982 secs/batch = 0.1931s, grad.norm=11.87410927
  9989: 7 [  700/ 1327], train_loss/perplexity = 4.82378721/124.4354630 secs/batch = 0.1986s, grad.norm=12.79150772
  9994: 7 [  705/ 1327], train_loss/perplexity = 4.50396681/90.3749237 secs/batch = 0.2005s, grad.norm=11.60647869
  9999: 7 [  710/ 1327], train_loss/perplexity = 4.44225454/84.9662857 secs/batch = 0.2002s, grad.norm=12.25334740
 10004: 7 [  715/ 1327], train_loss/perplexity = 4.41061401/82.3199921 secs/batch = 0.1996s, grad.norm=11.89970016
 10009: 7 [  720/ 1327], train_loss/perplexity = 4.38742399/80.4329529 secs/batch = 0.1974s, grad.norm=12.32348633
 10014: 7 [  725/ 1327], train_loss/perplexity = 4.32557726/75.6091461 secs/batch = 0.1985s, grad.norm=12.07824898
 10019: 7 [  730/ 1327], train_loss/perplexity = 4.52332258/92.1412354 secs/batch = 0.1986s, grad.norm=12.15610027
 10024: 7 [  735/ 1327], train_loss/perplexity = 4.66048765/105.6876068 secs/batch = 0.2001s, grad.norm=13.44648933
 10029: 7 [  740/ 1327], train_loss/perplexity = 4.06505156/58.2679138 secs/batch = 0.1984s, grad.norm=11.63880539
 10034: 7 [  745/ 1327], train_loss/perplexity = 4.62266588/101.7649612 secs/batch = 0.2005s, grad.norm=13.01029015
 10039: 7 [  750/ 1327], train_loss/perplexity = 4.40312338/81.7056656 secs/batch = 0.1982s, grad.norm=13.03891754
 10044: 7 [  755/ 1327], train_loss/perplexity = 4.34640789/77.2006531 secs/batch = 0.1995s, grad.norm=12.11852074
 10049: 7 [  760/ 1327], train_loss/perplexity = 4.19535398/66.3772202 secs/batch = 0.2001s, grad.norm=12.32959461
 10054: 7 [  765/ 1327], train_loss/perplexity = 4.20356894/66.9247589 secs/batch = 0.1998s, grad.norm=11.98639297
 10059: 7 [  770/ 1327], train_loss/perplexity = 4.27137280/71.6198883 secs/batch = 0.1984s, grad.norm=13.49270630
 10064: 7 [  775/ 1327], train_loss/perplexity = 4.39529228/81.0683212 secs/batch = 0.1995s, grad.norm=12.92560863
 10069: 7 [  780/ 1327], train_loss/perplexity = 4.78205776/119.3496933 secs/batch = 0.1938s, grad.norm=12.18032360
 10074: 7 [  785/ 1327], train_loss/perplexity = 4.61316109/100.8022919 secs/batch = 0.2000s, grad.norm=12.80361176
 10079: 7 [  790/ 1327], train_loss/perplexity = 4.29747486/73.5139236 secs/batch = 0.2000s, grad.norm=12.33446503
 10084: 7 [  795/ 1327], train_loss/perplexity = 4.71201086/111.2756958 secs/batch = 0.1989s, grad.norm=12.79834080
 10089: 7 [  800/ 1327], train_loss/perplexity = 4.58249378/97.7578735 secs/batch = 0.2001s, grad.norm=13.00925255
 10094: 7 [  805/ 1327], train_loss/perplexity = 4.96466160/143.2600555 secs/batch = 0.1993s, grad.norm=12.63602543
 10099: 7 [  810/ 1327], train_loss/perplexity = 4.57688665/97.2112732 secs/batch = 0.1933s, grad.norm=11.68770218
 10104: 7 [  815/ 1327], train_loss/perplexity = 4.40760374/82.0725632 secs/batch = 0.2007s, grad.norm=12.64485931
 10109: 7 [  820/ 1327], train_loss/perplexity = 4.19616747/66.4312439 secs/batch = 0.2002s, grad.norm=12.65820408
 10114: 7 [  825/ 1327], train_loss/perplexity = 4.44516325/85.2137909 secs/batch = 0.1998s, grad.norm=12.14050770
 10119: 7 [  830/ 1327], train_loss/perplexity = 4.21812534/67.9060669 secs/batch = 0.1989s, grad.norm=12.95569897
 10124: 7 [  835/ 1327], train_loss/perplexity = 4.57811689/97.3309402 secs/batch = 0.2001s, grad.norm=13.31751347
 10129: 7 [  840/ 1327], train_loss/perplexity = 4.56712580/96.2670212 secs/batch = 0.2004s, grad.norm=12.79224205
 10134: 7 [  845/ 1327], train_loss/perplexity = 4.37643337/79.5537872 secs/batch = 0.2001s, grad.norm=13.08043480
 10139: 7 [  850/ 1327], train_loss/perplexity = 4.49250221/89.3447266 secs/batch = 0.1996s, grad.norm=11.77536869
 10144: 7 [  855/ 1327], train_loss/perplexity = 4.51429701/91.3133545 secs/batch = 0.1994s, grad.norm=13.68689632
 10149: 7 [  860/ 1327], train_loss/perplexity = 4.19142866/66.1171799 secs/batch = 0.1990s, grad.norm=12.13915730
 10154: 7 [  865/ 1327], train_loss/perplexity = 4.68661547/108.4853821 secs/batch = 0.2002s, grad.norm=12.16847897
 10159: 7 [  870/ 1327], train_loss/perplexity = 4.62011814/101.5060272 secs/batch = 0.1986s, grad.norm=12.56783485
 10164: 7 [  875/ 1327], train_loss/perplexity = 4.17675877/65.1543274 secs/batch = 0.2013s, grad.norm=12.48750877
 10169: 7 [  880/ 1327], train_loss/perplexity = 4.38228273/80.0204926 secs/batch = 0.1994s, grad.norm=11.86083221
 10174: 7 [  885/ 1327], train_loss/perplexity = 4.52253819/92.0689926 secs/batch = 0.1990s, grad.norm=12.05436707
 10179: 7 [  890/ 1327], train_loss/perplexity = 4.69210196/109.0822220 secs/batch = 0.1990s, grad.norm=12.37859344
 10184: 7 [  895/ 1327], train_loss/perplexity = 4.68531227/108.3441010 secs/batch = 0.1994s, grad.norm=12.64718437
 10189: 7 [  900/ 1327], train_loss/perplexity = 4.53594828/93.3119583 secs/batch = 0.1988s, grad.norm=13.70589447
 10194: 7 [  905/ 1327], train_loss/perplexity = 4.43564796/84.4067993 secs/batch = 0.1988s, grad.norm=13.13754272
 10199: 7 [  910/ 1327], train_loss/perplexity = 4.39860630/81.3374329 secs/batch = 0.1989s, grad.norm=11.98793316
 10204: 7 [  915/ 1327], train_loss/perplexity = 4.62099409/101.5949783 secs/batch = 0.1991s, grad.norm=13.02525997
 10209: 7 [  920/ 1327], train_loss/perplexity = 4.80323410/121.9040298 secs/batch = 0.1995s, grad.norm=12.92596245
 10214: 7 [  925/ 1327], train_loss/perplexity = 4.59073544/98.5668945 secs/batch = 0.1991s, grad.norm=11.91277885
 10219: 7 [  930/ 1327], train_loss/perplexity = 4.53923750/93.6193848 secs/batch = 0.1997s, grad.norm=12.08608818
 10224: 7 [  935/ 1327], train_loss/perplexity = 4.69062042/108.9207382 secs/batch = 0.1996s, grad.norm=12.45608234
 10229: 7 [  940/ 1327], train_loss/perplexity = 4.59083605/98.5768127 secs/batch = 0.2002s, grad.norm=11.65178776
 10234: 7 [  945/ 1327], train_loss/perplexity = 4.84586525/127.2133026 secs/batch = 0.1990s, grad.norm=12.22396564
 10239: 7 [  950/ 1327], train_loss/perplexity = 4.57677555/97.2004700 secs/batch = 0.2004s, grad.norm=12.92429256
 10244: 7 [  955/ 1327], train_loss/perplexity = 4.64168930/103.7194138 secs/batch = 0.1981s, grad.norm=12.75097275
 10249: 7 [  960/ 1327], train_loss/perplexity = 4.93851900/139.5634003 secs/batch = 0.1993s, grad.norm=12.71393681
 10254: 7 [  965/ 1327], train_loss/perplexity = 4.60735607/100.2188263 secs/batch = 0.1991s, grad.norm=12.16829586
 10259: 7 [  970/ 1327], train_loss/perplexity = 4.89591742/133.7426453 secs/batch = 0.2002s, grad.norm=13.46218300
 10264: 7 [  975/ 1327], train_loss/perplexity = 4.50901508/90.8323135 secs/batch = 0.1997s, grad.norm=13.76714420
 10269: 7 [  980/ 1327], train_loss/perplexity = 4.37529564/79.4633255 secs/batch = 0.1943s, grad.norm=11.94178486
 10274: 7 [  985/ 1327], train_loss/perplexity = 4.53739023/93.4466095 secs/batch = 0.1995s, grad.norm=12.18764687
 10279: 7 [  990/ 1327], train_loss/perplexity = 4.64891529/104.4716034 secs/batch = 0.2005s, grad.norm=12.79399586
 10284: 7 [  995/ 1327], train_loss/perplexity = 4.66155672/105.8006592 secs/batch = 0.1991s, grad.norm=11.98233032
 10289: 7 [ 1000/ 1327], train_loss/perplexity = 4.19167709/66.1336136 secs/batch = 0.1999s, grad.norm=11.77840805
 10294: 7 [ 1005/ 1327], train_loss/perplexity = 4.68075228/107.8511734 secs/batch = 0.2003s, grad.norm=12.24743652
 10299: 7 [ 1010/ 1327], train_loss/perplexity = 4.28455639/72.5703430 secs/batch = 0.1947s, grad.norm=11.57202435
 10304: 7 [ 1015/ 1327], train_loss/perplexity = 4.80236769/121.7984543 secs/batch = 0.1999s, grad.norm=11.88252449
 10309: 7 [ 1020/ 1327], train_loss/perplexity = 4.92187929/137.2603302 secs/batch = 0.1995s, grad.norm=12.42759895
 10314: 7 [ 1025/ 1327], train_loss/perplexity = 4.69056559/108.9147644 secs/batch = 0.1982s, grad.norm=13.30600357
 10319: 7 [ 1030/ 1327], train_loss/perplexity = 4.53863764/93.5632477 secs/batch = 0.1950s, grad.norm=11.43934345
 10324: 7 [ 1035/ 1327], train_loss/perplexity = 4.44391918/85.1078415 secs/batch = 0.1994s, grad.norm=12.79685974
 10329: 7 [ 1040/ 1327], train_loss/perplexity = 4.71961737/112.1253433 secs/batch = 0.1989s, grad.norm=12.92435646
 10334: 7 [ 1045/ 1327], train_loss/perplexity = 4.23303366/68.9260101 secs/batch = 0.1993s, grad.norm=12.31388092
 10339: 7 [ 1050/ 1327], train_loss/perplexity = 4.38550091/80.2784271 secs/batch = 0.1985s, grad.norm=12.88968086
 10344: 7 [ 1055/ 1327], train_loss/perplexity = 4.45533848/86.0852814 secs/batch = 0.1993s, grad.norm=14.18744659
 10349: 7 [ 1060/ 1327], train_loss/perplexity = 4.01156950/55.2334900 secs/batch = 0.1998s, grad.norm=13.10129547
 10354: 7 [ 1065/ 1327], train_loss/perplexity = 4.23345184/68.9548416 secs/batch = 0.1946s, grad.norm=12.75625992
 10359: 7 [ 1070/ 1327], train_loss/perplexity = 4.58775425/98.2734833 secs/batch = 0.1975s, grad.norm=12.36295414
 10364: 7 [ 1075/ 1327], train_loss/perplexity = 4.28228521/72.4057159 secs/batch = 0.2002s, grad.norm=12.95333862
 10369: 7 [ 1080/ 1327], train_loss/perplexity = 4.36016083/78.2697220 secs/batch = 0.1988s, grad.norm=12.70814037
 10374: 7 [ 1085/ 1327], train_loss/perplexity = 4.13665247/62.5929375 secs/batch = 0.1994s, grad.norm=13.01110077
 10379: 7 [ 1090/ 1327], train_loss/perplexity = 4.34611559/77.1780853 secs/batch = 0.1992s, grad.norm=13.60987759
 10384: 7 [ 1095/ 1327], train_loss/perplexity = 4.50702667/90.6518784 secs/batch = 0.1987s, grad.norm=13.27866268
 10389: 7 [ 1100/ 1327], train_loss/perplexity = 4.24108362/69.4831009 secs/batch = 0.2004s, grad.norm=14.79531956
 10394: 7 [ 1105/ 1327], train_loss/perplexity = 4.21519995/67.7077026 secs/batch = 0.1991s, grad.norm=12.87840176
 10399: 7 [ 1110/ 1327], train_loss/perplexity = 4.63240528/102.7609329 secs/batch = 0.1994s, grad.norm=13.24630356
 10404: 7 [ 1115/ 1327], train_loss/perplexity = 4.30635452/74.1696091 secs/batch = 0.1992s, grad.norm=12.90215302
 10409: 7 [ 1120/ 1327], train_loss/perplexity = 4.50221634/90.2168579 secs/batch = 0.1987s, grad.norm=12.44300747
 10414: 7 [ 1125/ 1327], train_loss/perplexity = 4.76794052/117.6766434 secs/batch = 0.1991s, grad.norm=12.50769138
 10419: 7 [ 1130/ 1327], train_loss/perplexity = 4.38075781/79.8985596 secs/batch = 0.1984s, grad.norm=12.67008209
 10424: 7 [ 1135/ 1327], train_loss/perplexity = 4.38741684/80.4323807 secs/batch = 0.1979s, grad.norm=12.35931301
 10429: 7 [ 1140/ 1327], train_loss/perplexity = 4.71315479/111.4030609 secs/batch = 0.1929s, grad.norm=13.88401794
 10434: 7 [ 1145/ 1327], train_loss/perplexity = 4.51951790/91.7913361 secs/batch = 0.1995s, grad.norm=13.08098888
 10439: 7 [ 1150/ 1327], train_loss/perplexity = 4.46132469/86.6021576 secs/batch = 0.1986s, grad.norm=12.97597599
 10444: 7 [ 1155/ 1327], train_loss/perplexity = 4.55347538/94.9618607 secs/batch = 0.2002s, grad.norm=13.25913239
 10449: 7 [ 1160/ 1327], train_loss/perplexity = 4.58014107/97.5281525 secs/batch = 0.2015s, grad.norm=12.97309303
 10454: 7 [ 1165/ 1327], train_loss/perplexity = 4.53522205/93.2442169 secs/batch = 0.1999s, grad.norm=12.49201107
 10459: 7 [ 1170/ 1327], train_loss/perplexity = 4.43581867/84.4212112 secs/batch = 0.1990s, grad.norm=13.18844509
 10464: 7 [ 1175/ 1327], train_loss/perplexity = 4.13697815/62.6133270 secs/batch = 0.1999s, grad.norm=12.89232731
 10469: 7 [ 1180/ 1327], train_loss/perplexity = 4.19097948/66.0874939 secs/batch = 0.1976s, grad.norm=12.35207844
 10474: 7 [ 1185/ 1327], train_loss/perplexity = 4.41066551/82.3242340 secs/batch = 0.1941s, grad.norm=13.27569485
 10479: 7 [ 1190/ 1327], train_loss/perplexity = 4.52612972/92.4002533 secs/batch = 0.1989s, grad.norm=12.24247265
 10484: 7 [ 1195/ 1327], train_loss/perplexity = 4.27439308/71.8365250 secs/batch = 0.1990s, grad.norm=12.88581657
 10489: 7 [ 1200/ 1327], train_loss/perplexity = 4.24971962/70.0857620 secs/batch = 0.1981s, grad.norm=12.99922848
 10494: 7 [ 1205/ 1327], train_loss/perplexity = 4.28284836/72.4465027 secs/batch = 0.1951s, grad.norm=13.03319359
 10499: 7 [ 1210/ 1327], train_loss/perplexity = 3.93823743/51.3280525 secs/batch = 0.1985s, grad.norm=12.85309792
 10504: 7 [ 1215/ 1327], train_loss/perplexity = 4.10753345/60.7965736 secs/batch = 0.1994s, grad.norm=12.38366318
 10509: 7 [ 1220/ 1327], train_loss/perplexity = 4.31598330/74.8872223 secs/batch = 0.2004s, grad.norm=13.60274982
 10514: 7 [ 1225/ 1327], train_loss/perplexity = 4.11109400/61.0134277 secs/batch = 0.2001s, grad.norm=14.09438705
 10519: 7 [ 1230/ 1327], train_loss/perplexity = 4.42730856/83.7058258 secs/batch = 0.1990s, grad.norm=18.44712830
 10524: 7 [ 1235/ 1327], train_loss/perplexity = 4.32630682/75.6643295 secs/batch = 0.2001s, grad.norm=13.01717091
 10529: 7 [ 1240/ 1327], train_loss/perplexity = 4.48911190/89.0423355 secs/batch = 0.1984s, grad.norm=13.34470940
 10534: 7 [ 1245/ 1327], train_loss/perplexity = 4.36777163/78.8676910 secs/batch = 0.1943s, grad.norm=12.04034519
 10539: 7 [ 1250/ 1327], train_loss/perplexity = 4.44700766/85.3711014 secs/batch = 0.1994s, grad.norm=12.72202110
 10544: 7 [ 1255/ 1327], train_loss/perplexity = 4.55902004/95.4898605 secs/batch = 0.1988s, grad.norm=11.65521049
 10549: 7 [ 1260/ 1327], train_loss/perplexity = 4.33326054/76.1923065 secs/batch = 0.1986s, grad.norm=13.37542439
 10554: 7 [ 1265/ 1327], train_loss/perplexity = 4.61526155/101.0142441 secs/batch = 0.1940s, grad.norm=13.14267921
 10559: 7 [ 1270/ 1327], train_loss/perplexity = 4.29990673/73.6929169 secs/batch = 0.2000s, grad.norm=13.08618546
 10564: 7 [ 1275/ 1327], train_loss/perplexity = 4.48372126/88.5636292 secs/batch = 0.1990s, grad.norm=12.91129875
 10569: 7 [ 1280/ 1327], train_loss/perplexity = 4.29229927/73.1344299 secs/batch = 0.1946s, grad.norm=12.57374477
 10574: 7 [ 1285/ 1327], train_loss/perplexity = 4.26615143/71.2469101 secs/batch = 0.2001s, grad.norm=13.55925751
 10579: 7 [ 1290/ 1327], train_loss/perplexity = 4.49392557/89.4719849 secs/batch = 0.1998s, grad.norm=12.86303425
 10584: 7 [ 1295/ 1327], train_loss/perplexity = 4.49465609/89.5373688 secs/batch = 0.1997s, grad.norm=12.65039253
 10589: 7 [ 1300/ 1327], train_loss/perplexity = 4.61924839/101.4177780 secs/batch = 0.1933s, grad.norm=12.54418755
 10594: 7 [ 1305/ 1327], train_loss/perplexity = 4.77081299/118.0151443 secs/batch = 0.1987s, grad.norm=14.22395611
 10599: 7 [ 1310/ 1327], train_loss/perplexity = 4.96270084/142.9794464 secs/batch = 0.1984s, grad.norm=13.71559715
 10604: 7 [ 1315/ 1327], train_loss/perplexity = 4.73643398/114.0268555 secs/batch = 0.1992s, grad.norm=13.36217880
 10609: 7 [ 1320/ 1327], train_loss/perplexity = 4.82378340/124.4349899 secs/batch = 0.2004s, grad.norm=13.71751785
 10614: 7 [ 1325/ 1327], train_loss/perplexity = 4.64743423/104.3169861 secs/batch = 0.1992s, grad.norm=12.34345531
Epoch training time: 264.11259150505066
	> validation loss = 4.93168926, perplexity = 138.61346436
	> validation loss = 4.80270815, perplexity = 121.83993530
	> validation loss = 4.77030468, perplexity = 117.95517731
	> validation loss = 4.81787348, perplexity = 123.70175934
	> validation loss = 4.99254894, perplexity = 147.31143188
	> validation loss = 4.87303114, perplexity = 130.71653748
	> validation loss = 4.81601858, perplexity = 123.47251129
	> validation loss = 4.69164944, perplexity = 109.03287506
	> validation loss = 4.51190186, perplexity = 91.09490204
	> validation loss = 4.64645004, perplexity = 104.21437073
	> validation loss = 4.67577505, perplexity = 107.31571198
	> validation loss = 4.81195402, perplexity = 122.97167206
	> validation loss = 4.68086672, perplexity = 107.86351776
	> validation loss = 4.55621815, perplexity = 95.22267914
	> validation loss = 4.42337561, perplexity = 83.37725830
	> validation loss = 4.45573092, perplexity = 86.11907196
	> validation loss = 4.89437628, perplexity = 133.53669739
	> validation loss = 4.47878647, perplexity = 88.12766266
	> validation loss = 4.88670731, perplexity = 132.51652527
	> validation loss = 4.75006437, perplexity = 115.59172821
	> validation loss = 4.58671236, perplexity = 98.17115021
at the end of epoch: 7
train loss = 4.61672845, perplexity = 101.16253148
validation loss = 4.72279437, perplexity = 112.48213053
Saved model cv/epoch007_4.7228.model
 10621: 8 [    5/ 1327], train_loss/perplexity = 4.71900511/112.0567093 secs/batch = 0.1994s, grad.norm=12.67882538
 10626: 8 [   10/ 1327], train_loss/perplexity = 4.26963902/71.4958191 secs/batch = 0.1988s, grad.norm=12.79296303
 10631: 8 [   15/ 1327], train_loss/perplexity = 4.57892132/97.4092636 secs/batch = 0.1997s, grad.norm=12.42724323
 10636: 8 [   20/ 1327], train_loss/perplexity = 4.75794935/116.5067673 secs/batch = 0.2001s, grad.norm=12.35670185
 10641: 8 [   25/ 1327], train_loss/perplexity = 4.64174557/103.7252502 secs/batch = 0.2007s, grad.norm=13.44744015
 10646: 8 [   30/ 1327], train_loss/perplexity = 4.61932135/101.4251785 secs/batch = 0.1999s, grad.norm=12.76305676
 10651: 8 [   35/ 1327], train_loss/perplexity = 4.46269798/86.7211685 secs/batch = 0.1996s, grad.norm=12.88877964
 10656: 8 [   40/ 1327], train_loss/perplexity = 4.44244862/84.9827805 secs/batch = 0.1996s, grad.norm=12.44253063
 10661: 8 [   45/ 1327], train_loss/perplexity = 4.19152451/66.1235199 secs/batch = 0.1996s, grad.norm=11.87429237
 10666: 8 [   50/ 1327], train_loss/perplexity = 4.48197174/88.4088211 secs/batch = 0.1956s, grad.norm=12.62258530
 10671: 8 [   55/ 1327], train_loss/perplexity = 4.44023228/84.7946320 secs/batch = 0.1989s, grad.norm=13.67652035
 10676: 8 [   60/ 1327], train_loss/perplexity = 4.74476242/114.9804840 secs/batch = 0.1941s, grad.norm=13.12440777
 10681: 8 [   65/ 1327], train_loss/perplexity = 4.24751616/69.9314957 secs/batch = 0.1993s, grad.norm=12.50848103
 10686: 8 [   70/ 1327], train_loss/perplexity = 4.13418388/62.4386139 secs/batch = 0.1981s, grad.norm=12.83961105
 10691: 8 [   75/ 1327], train_loss/perplexity = 3.95679545/52.2894936 secs/batch = 0.1994s, grad.norm=12.93898869
 10696: 8 [   80/ 1327], train_loss/perplexity = 4.39507961/81.0510864 secs/batch = 0.1991s, grad.norm=14.04019356
 10701: 8 [   85/ 1327], train_loss/perplexity = 4.47541094/87.8306885 secs/batch = 0.2012s, grad.norm=13.21965122
 10706: 8 [   90/ 1327], train_loss/perplexity = 4.49186802/89.2880783 secs/batch = 0.1998s, grad.norm=15.02883244
 10711: 8 [   95/ 1327], train_loss/perplexity = 4.37484550/79.4275665 secs/batch = 0.1987s, grad.norm=12.50018978
 10716: 8 [  100/ 1327], train_loss/perplexity = 4.57997608/97.5120621 secs/batch = 0.1998s, grad.norm=12.79602528
 10721: 8 [  105/ 1327], train_loss/perplexity = 4.47128105/87.4687042 secs/batch = 0.1997s, grad.norm=13.52199745
 10726: 8 [  110/ 1327], train_loss/perplexity = 4.29227543/73.1326904 secs/batch = 0.1983s, grad.norm=12.92897511
 10731: 8 [  115/ 1327], train_loss/perplexity = 4.31192303/74.5837784 secs/batch = 0.1997s, grad.norm=12.89088821
 10736: 8 [  120/ 1327], train_loss/perplexity = 4.39006472/80.6456375 secs/batch = 0.1989s, grad.norm=12.95550156
 10741: 8 [  125/ 1327], train_loss/perplexity = 4.51498413/91.3761139 secs/batch = 0.1990s, grad.norm=12.76678944
 10746: 8 [  130/ 1327], train_loss/perplexity = 4.52381945/92.1870270 secs/batch = 0.1990s, grad.norm=18.50927734
 10751: 8 [  135/ 1327], train_loss/perplexity = 4.40941572/82.2214127 secs/batch = 0.1993s, grad.norm=12.60244846
 10756: 8 [  140/ 1327], train_loss/perplexity = 4.74565363/115.0830002 secs/batch = 0.1998s, grad.norm=13.78004169
 10761: 8 [  145/ 1327], train_loss/perplexity = 4.59558868/99.0464249 secs/batch = 0.1930s, grad.norm=13.80440998
 10766: 8 [  150/ 1327], train_loss/perplexity = 4.56384850/95.9520416 secs/batch = 0.1982s, grad.norm=13.30118275
 10771: 8 [  155/ 1327], train_loss/perplexity = 4.84590769/127.2187042 secs/batch = 0.1985s, grad.norm=13.05009937
 10776: 8 [  160/ 1327], train_loss/perplexity = 4.55532980/95.1381302 secs/batch = 0.2003s, grad.norm=12.66329098
 10781: 8 [  165/ 1327], train_loss/perplexity = 4.60356951/99.8400574 secs/batch = 0.1991s, grad.norm=12.29615688
 10786: 8 [  170/ 1327], train_loss/perplexity = 4.42129755/83.2041779 secs/batch = 0.1990s, grad.norm=12.12889385
 10791: 8 [  175/ 1327], train_loss/perplexity = 4.75199986/115.8156662 secs/batch = 0.2008s, grad.norm=12.37720585
 10796: 8 [  180/ 1327], train_loss/perplexity = 4.56358385/95.9266510 secs/batch = 0.1993s, grad.norm=12.12144852
 10801: 8 [  185/ 1327], train_loss/perplexity = 4.92388725/137.5362091 secs/batch = 0.1993s, grad.norm=13.32647038
 10806: 8 [  190/ 1327], train_loss/perplexity = 4.38120937/79.9346466 secs/batch = 0.1998s, grad.norm=11.87665272
 10811: 8 [  195/ 1327], train_loss/perplexity = 4.71235847/111.3143845 secs/batch = 0.1996s, grad.norm=12.42292690
 10816: 8 [  200/ 1327], train_loss/perplexity = 4.55034208/94.6647873 secs/batch = 0.1928s, grad.norm=13.15797997
 10821: 8 [  205/ 1327], train_loss/perplexity = 4.75843906/116.5638351 secs/batch = 0.1976s, grad.norm=12.34535789
 10826: 8 [  210/ 1327], train_loss/perplexity = 4.55486012/95.0934525 secs/batch = 0.1997s, grad.norm=11.85771275
 10831: 8 [  215/ 1327], train_loss/perplexity = 4.71789408/111.9322815 secs/batch = 0.2000s, grad.norm=11.88054562
 10836: 8 [  220/ 1327], train_loss/perplexity = 4.77121449/118.0625381 secs/batch = 0.1979s, grad.norm=12.18443298
 10841: 8 [  225/ 1327], train_loss/perplexity = 4.85909986/128.9081116 secs/batch = 0.1999s, grad.norm=12.37276649
 10846: 8 [  230/ 1327], train_loss/perplexity = 4.75088215/115.6862946 secs/batch = 0.1979s, grad.norm=14.11255264
 10851: 8 [  235/ 1327], train_loss/perplexity = 4.50136423/90.1400223 secs/batch = 0.1991s, grad.norm=14.58241749
 10856: 8 [  240/ 1327], train_loss/perplexity = 4.29419327/73.2730789 secs/batch = 0.1988s, grad.norm=15.22475433
 10861: 8 [  245/ 1327], train_loss/perplexity = 4.64317417/103.8735352 secs/batch = 0.1995s, grad.norm=12.03234482
 10866: 8 [  250/ 1327], train_loss/perplexity = 4.43493891/84.3469696 secs/batch = 0.1994s, grad.norm=12.03029346
 10871: 8 [  255/ 1327], train_loss/perplexity = 4.40630150/81.9657516 secs/batch = 0.2004s, grad.norm=12.33440018
 10876: 8 [  260/ 1327], train_loss/perplexity = 4.67097807/106.8021545 secs/batch = 0.1994s, grad.norm=13.02342415
 10881: 8 [  265/ 1327], train_loss/perplexity = 4.80358982/121.9474030 secs/batch = 0.1993s, grad.norm=12.19408512
 10886: 8 [  270/ 1327], train_loss/perplexity = 4.92524862/137.7235718 secs/batch = 0.2001s, grad.norm=12.20049191
 10891: 8 [  275/ 1327], train_loss/perplexity = 4.94154787/139.9867706 secs/batch = 0.2013s, grad.norm=12.82633114
 10896: 8 [  280/ 1327], train_loss/perplexity = 4.58784914/98.2828064 secs/batch = 0.2003s, grad.norm=11.75786209
 10901: 8 [  285/ 1327], train_loss/perplexity = 4.93368721/138.8906860 secs/batch = 0.1991s, grad.norm=12.13912582
 10906: 8 [  290/ 1327], train_loss/perplexity = 4.65379143/104.9822617 secs/batch = 0.1987s, grad.norm=12.68556404
 10911: 8 [  295/ 1327], train_loss/perplexity = 4.38500118/80.2383194 secs/batch = 0.2003s, grad.norm=12.18275166
 10916: 8 [  300/ 1327], train_loss/perplexity = 4.02623940/56.0497322 secs/batch = 0.2002s, grad.norm=12.42873955
 10921: 8 [  305/ 1327], train_loss/perplexity = 4.49809170/89.8455124 secs/batch = 0.1957s, grad.norm=12.71606159
 10926: 8 [  310/ 1327], train_loss/perplexity = 4.47621059/87.9009476 secs/batch = 0.1989s, grad.norm=12.61779785
 10931: 8 [  315/ 1327], train_loss/perplexity = 4.02468824/55.9628601 secs/batch = 0.1999s, grad.norm=11.73766994
 10936: 8 [  320/ 1327], train_loss/perplexity = 4.07168722/58.6558456 secs/batch = 0.2002s, grad.norm=13.37530613
 10941: 8 [  325/ 1327], train_loss/perplexity = 4.04373455/57.0389595 secs/batch = 0.1999s, grad.norm=12.03818321
 10946: 8 [  330/ 1327], train_loss/perplexity = 4.55491829/95.0989838 secs/batch = 0.1920s, grad.norm=12.87382412
 10951: 8 [  335/ 1327], train_loss/perplexity = 3.96273422/52.6009521 secs/batch = 0.2000s, grad.norm=11.97753620
 10956: 8 [  340/ 1327], train_loss/perplexity = 4.70663071/110.6786194 secs/batch = 0.1991s, grad.norm=12.17602921
 10961: 8 [  345/ 1327], train_loss/perplexity = 4.52924633/92.6886749 secs/batch = 0.2015s, grad.norm=13.09806252
 10966: 8 [  350/ 1327], train_loss/perplexity = 4.50357008/90.3390732 secs/batch = 0.1926s, grad.norm=12.21531105
 10971: 8 [  355/ 1327], train_loss/perplexity = 4.55880737/95.4695511 secs/batch = 0.1988s, grad.norm=12.67187023
 10976: 8 [  360/ 1327], train_loss/perplexity = 4.75657845/116.3471527 secs/batch = 0.2011s, grad.norm=13.90479374
 10981: 8 [  365/ 1327], train_loss/perplexity = 4.67182589/106.8927383 secs/batch = 0.1992s, grad.norm=12.02101994
 10986: 8 [  370/ 1327], train_loss/perplexity = 4.72660160/112.9111938 secs/batch = 0.1925s, grad.norm=12.49429512
 10991: 8 [  375/ 1327], train_loss/perplexity = 4.34319687/76.9531555 secs/batch = 0.2009s, grad.norm=24.64317894
 10996: 8 [  380/ 1327], train_loss/perplexity = 4.23209572/68.8613968 secs/batch = 0.1992s, grad.norm=13.22101021
 11001: 8 [  385/ 1327], train_loss/perplexity = 4.50469160/90.4404449 secs/batch = 0.1983s, grad.norm=13.05068874
 11006: 8 [  390/ 1327], train_loss/perplexity = 4.55536175/95.1411667 secs/batch = 0.1992s, grad.norm=12.97805405
 11011: 8 [  395/ 1327], train_loss/perplexity = 4.62838268/102.3484039 secs/batch = 0.1991s, grad.norm=17.32943153
 11016: 8 [  400/ 1327], train_loss/perplexity = 4.51954126/91.7934799 secs/batch = 0.1977s, grad.norm=14.03346634
 11021: 8 [  405/ 1327], train_loss/perplexity = 4.84977341/127.7114487 secs/batch = 0.1985s, grad.norm=14.82396030
 11026: 8 [  410/ 1327], train_loss/perplexity = 4.48985004/89.1080856 secs/batch = 0.1992s, grad.norm=13.08436871
 11031: 8 [  415/ 1327], train_loss/perplexity = 4.31062746/74.4872131 secs/batch = 0.1928s, grad.norm=11.92755604
 11036: 8 [  420/ 1327], train_loss/perplexity = 4.13556194/62.5247154 secs/batch = 0.1999s, grad.norm=13.10970688
 11041: 8 [  425/ 1327], train_loss/perplexity = 4.44564342/85.2547150 secs/batch = 0.1991s, grad.norm=14.28176117
 11046: 8 [  430/ 1327], train_loss/perplexity = 4.65925646/105.5575638 secs/batch = 0.1992s, grad.norm=13.58591080
 11051: 8 [  435/ 1327], train_loss/perplexity = 4.62237644/101.7355118 secs/batch = 0.2002s, grad.norm=12.30996037
 11056: 8 [  440/ 1327], train_loss/perplexity = 4.31854582/75.0793686 secs/batch = 0.1986s, grad.norm=13.13879967
 11061: 8 [  445/ 1327], train_loss/perplexity = 4.60299921/99.7831345 secs/batch = 0.1989s, grad.norm=13.44268703
 11066: 8 [  450/ 1327], train_loss/perplexity = 4.46631002/87.0349731 secs/batch = 0.1978s, grad.norm=12.55612659
 11071: 8 [  455/ 1327], train_loss/perplexity = 4.38384628/80.1457062 secs/batch = 0.1992s, grad.norm=12.99111652
 11076: 8 [  460/ 1327], train_loss/perplexity = 4.44577408/85.2658539 secs/batch = 0.1994s, grad.norm=14.04490948
 11081: 8 [  465/ 1327], train_loss/perplexity = 4.20257807/66.8584747 secs/batch = 0.1996s, grad.norm=14.06573391
 11086: 8 [  470/ 1327], train_loss/perplexity = 4.87167978/130.5400085 secs/batch = 0.1988s, grad.norm=11.88170624
 11091: 8 [  475/ 1327], train_loss/perplexity = 4.29450703/73.2960739 secs/batch = 0.1919s, grad.norm=12.36037350
 11096: 8 [  480/ 1327], train_loss/perplexity = 4.47602940/87.8850250 secs/batch = 0.1992s, grad.norm=12.51256371
 11101: 8 [  485/ 1327], train_loss/perplexity = 4.39303970/80.8859177 secs/batch = 0.1998s, grad.norm=12.25823593
 11106: 8 [  490/ 1327], train_loss/perplexity = 4.41416407/82.6127548 secs/batch = 0.2000s, grad.norm=18.09947014
 11111: 8 [  495/ 1327], train_loss/perplexity = 4.42977476/83.9125137 secs/batch = 0.2004s, grad.norm=12.88485336
 11116: 8 [  500/ 1327], train_loss/perplexity = 4.59220505/98.7118530 secs/batch = 0.1956s, grad.norm=14.92669201
 11121: 8 [  505/ 1327], train_loss/perplexity = 4.67125416/106.8316422 secs/batch = 0.2001s, grad.norm=12.34388256
 11126: 8 [  510/ 1327], train_loss/perplexity = 4.96093416/142.7270660 secs/batch = 0.1971s, grad.norm=11.71811962
 11131: 8 [  515/ 1327], train_loss/perplexity = 4.62676954/102.1834335 secs/batch = 0.2003s, grad.norm=12.11102009
 11136: 8 [  520/ 1327], train_loss/perplexity = 4.81132936/122.8948822 secs/batch = 0.2005s, grad.norm=12.61914158
 11141: 8 [  525/ 1327], train_loss/perplexity = 4.36726618/78.8278351 secs/batch = 0.1997s, grad.norm=12.29052162
 11146: 8 [  530/ 1327], train_loss/perplexity = 4.39820957/81.3051682 secs/batch = 0.1995s, grad.norm=12.66611385
 11151: 8 [  535/ 1327], train_loss/perplexity = 4.50655508/90.6091385 secs/batch = 0.1994s, grad.norm=11.80298042
 11156: 8 [  540/ 1327], train_loss/perplexity = 4.58333635/97.8402786 secs/batch = 0.1991s, grad.norm=13.53154469
 11161: 8 [  545/ 1327], train_loss/perplexity = 4.66904640/106.5960464 secs/batch = 0.1982s, grad.norm=13.67953396
 11166: 8 [  550/ 1327], train_loss/perplexity = 4.60004044/99.4883347 secs/batch = 0.1998s, grad.norm=14.02605343
 11171: 8 [  555/ 1327], train_loss/perplexity = 4.51847172/91.6953583 secs/batch = 0.1993s, grad.norm=12.91339588
 11176: 8 [  560/ 1327], train_loss/perplexity = 4.52260876/92.0754852 secs/batch = 0.1991s, grad.norm=13.97291183
 11181: 8 [  565/ 1327], train_loss/perplexity = 4.46032143/86.5153122 secs/batch = 0.1997s, grad.norm=13.56205177
 11186: 8 [  570/ 1327], train_loss/perplexity = 4.44420815/85.1324387 secs/batch = 0.1999s, grad.norm=12.61731529
 11191: 8 [  575/ 1327], train_loss/perplexity = 4.12849569/62.0844574 secs/batch = 0.1987s, grad.norm=12.23178387
 11196: 8 [  580/ 1327], train_loss/perplexity = 4.67527246/107.2617874 secs/batch = 0.1991s, grad.norm=13.37234116
 11201: 8 [  585/ 1327], train_loss/perplexity = 4.20461988/66.9951248 secs/batch = 0.1999s, grad.norm=12.98976040
 11206: 8 [  590/ 1327], train_loss/perplexity = 4.57795143/97.3148346 secs/batch = 0.1995s, grad.norm=12.23638439
 11211: 8 [  595/ 1327], train_loss/perplexity = 4.53843117/93.5439301 secs/batch = 0.1994s, grad.norm=13.09629154
 11216: 8 [  600/ 1327], train_loss/perplexity = 4.67980909/107.7495041 secs/batch = 0.2002s, grad.norm=11.65584755
 11221: 8 [  605/ 1327], train_loss/perplexity = 4.58051777/97.5648956 secs/batch = 0.2001s, grad.norm=12.47602177
 11226: 8 [  610/ 1327], train_loss/perplexity = 4.80131912/121.6708069 secs/batch = 0.2009s, grad.norm=12.95628548
 11231: 8 [  615/ 1327], train_loss/perplexity = 4.40682554/82.0087204 secs/batch = 0.2006s, grad.norm=12.84819984
 11236: 8 [  620/ 1327], train_loss/perplexity = 4.71321440/111.4096985 secs/batch = 0.2001s, grad.norm=12.86118221
 11241: 8 [  625/ 1327], train_loss/perplexity = 4.68248034/108.0377121 secs/batch = 0.2001s, grad.norm=12.90354156
 11246: 8 [  630/ 1327], train_loss/perplexity = 4.85319853/128.1496277 secs/batch = 0.1991s, grad.norm=12.15160179
 11251: 8 [  635/ 1327], train_loss/perplexity = 4.49350548/89.4344101 secs/batch = 0.1950s, grad.norm=12.05327606
 11256: 8 [  640/ 1327], train_loss/perplexity = 4.49157810/89.2621994 secs/batch = 0.1988s, grad.norm=11.87735176
 11261: 8 [  645/ 1327], train_loss/perplexity = 4.82990932/125.1996078 secs/batch = 0.1999s, grad.norm=12.39700699
 11266: 8 [  650/ 1327], train_loss/perplexity = 4.34757042/77.2904510 secs/batch = 0.2002s, grad.norm=12.88759899
 11271: 8 [  655/ 1327], train_loss/perplexity = 4.47458553/87.7582169 secs/batch = 0.1993s, grad.norm=12.92753983
 11276: 8 [  660/ 1327], train_loss/perplexity = 4.35763836/78.0725403 secs/batch = 0.1981s, grad.norm=12.78487587
 11281: 8 [  665/ 1327], train_loss/perplexity = 4.51750898/91.6071167 secs/batch = 0.2000s, grad.norm=12.38935566
 11286: 8 [  670/ 1327], train_loss/perplexity = 4.44692564/85.3640976 secs/batch = 0.2005s, grad.norm=12.46095848
 11291: 8 [  675/ 1327], train_loss/perplexity = 4.31205606/74.5937042 secs/batch = 0.1999s, grad.norm=12.97841167
 11296: 8 [  680/ 1327], train_loss/perplexity = 4.55016613/94.6481323 secs/batch = 0.1989s, grad.norm=13.48568058
 11301: 8 [  685/ 1327], train_loss/perplexity = 4.31997967/75.1871033 secs/batch = 0.1987s, grad.norm=12.84792709
 11306: 8 [  690/ 1327], train_loss/perplexity = 4.70610571/110.6205292 secs/batch = 0.1989s, grad.norm=11.71333122
 11311: 8 [  695/ 1327], train_loss/perplexity = 4.47002840/87.3592072 secs/batch = 0.2001s, grad.norm=12.95158958
 11316: 8 [  700/ 1327], train_loss/perplexity = 4.72711658/112.9693527 secs/batch = 0.1999s, grad.norm=13.23185921
 11321: 8 [  705/ 1327], train_loss/perplexity = 4.47319412/87.6362000 secs/batch = 0.1941s, grad.norm=11.82177734
 11326: 8 [  710/ 1327], train_loss/perplexity = 4.40086365/81.5212402 secs/batch = 0.2000s, grad.norm=13.13841724
 11331: 8 [  715/ 1327], train_loss/perplexity = 4.37848186/79.7169189 secs/batch = 0.1996s, grad.norm=13.09270668
 11336: 8 [  720/ 1327], train_loss/perplexity = 4.26493835/71.1605301 secs/batch = 0.1989s, grad.norm=12.40497589
 11341: 8 [  725/ 1327], train_loss/perplexity = 4.30102205/73.7751541 secs/batch = 0.1989s, grad.norm=12.18360233
 11346: 8 [  730/ 1327], train_loss/perplexity = 4.51786709/91.6399307 secs/batch = 0.1997s, grad.norm=12.54521084
 11351: 8 [  735/ 1327], train_loss/perplexity = 4.58765697/98.2639236 secs/batch = 0.1945s, grad.norm=12.63597012
 11356: 8 [  740/ 1327], train_loss/perplexity = 3.95042753/51.9575768 secs/batch = 0.1992s, grad.norm=12.09252357
 11361: 8 [  745/ 1327], train_loss/perplexity = 4.73516035/113.8817215 secs/batch = 0.2001s, grad.norm=22.06024742
 11366: 8 [  750/ 1327], train_loss/perplexity = 4.38837719/80.5096588 secs/batch = 0.1983s, grad.norm=12.76165104
 11371: 8 [  755/ 1327], train_loss/perplexity = 4.33286667/76.1623077 secs/batch = 0.2001s, grad.norm=12.35279942
 11376: 8 [  760/ 1327], train_loss/perplexity = 4.17419195/64.9873047 secs/batch = 0.1994s, grad.norm=12.83803082
 11381: 8 [  765/ 1327], train_loss/perplexity = 4.25850487/70.7041931 secs/batch = 0.2002s, grad.norm=12.72122860
 11386: 8 [  770/ 1327], train_loss/perplexity = 4.23801136/69.2699585 secs/batch = 0.1996s, grad.norm=13.69661903
 11391: 8 [  775/ 1327], train_loss/perplexity = 4.32980967/75.9298325 secs/batch = 0.2012s, grad.norm=12.26083183
 11396: 8 [  780/ 1327], train_loss/perplexity = 4.70981264/111.0313568 secs/batch = 0.1942s, grad.norm=12.53846741
 11401: 8 [  785/ 1327], train_loss/perplexity = 4.53235626/92.9773788 secs/batch = 0.1996s, grad.norm=13.15709877
 11406: 8 [  790/ 1327], train_loss/perplexity = 4.27638721/71.9799194 secs/batch = 0.2000s, grad.norm=12.16581917
 11411: 8 [  795/ 1327], train_loss/perplexity = 4.64480686/104.0432663 secs/batch = 0.2007s, grad.norm=12.80389309
 11416: 8 [  800/ 1327], train_loss/perplexity = 4.52572393/92.3627625 secs/batch = 0.1977s, grad.norm=12.52683258
 11421: 8 [  805/ 1327], train_loss/perplexity = 4.95809793/142.3228302 secs/batch = 0.1994s, grad.norm=14.05548954
 11426: 8 [  810/ 1327], train_loss/perplexity = 4.58540773/98.0431519 secs/batch = 0.1942s, grad.norm=12.50727367
 11431: 8 [  815/ 1327], train_loss/perplexity = 4.34987450/77.4687424 secs/batch = 0.2002s, grad.norm=12.04898357
 11436: 8 [  820/ 1327], train_loss/perplexity = 4.20831633/67.2432251 secs/batch = 0.1997s, grad.norm=11.83698940
 11441: 8 [  825/ 1327], train_loss/perplexity = 4.42769384/83.7380829 secs/batch = 0.2003s, grad.norm=12.17565632
 11446: 8 [  830/ 1327], train_loss/perplexity = 4.17570496/65.0857086 secs/batch = 0.1995s, grad.norm=12.02978230
 11451: 8 [  835/ 1327], train_loss/perplexity = 4.49105930/89.2159042 secs/batch = 0.1994s, grad.norm=12.69128799
 11456: 8 [  840/ 1327], train_loss/perplexity = 4.50974941/90.8990402 secs/batch = 0.1995s, grad.norm=13.00389481
 11461: 8 [  845/ 1327], train_loss/perplexity = 4.33987331/76.6978226 secs/batch = 0.1991s, grad.norm=12.87964630
 11466: 8 [  850/ 1327], train_loss/perplexity = 4.40521383/81.8766479 secs/batch = 0.2005s, grad.norm=12.11986542
 11471: 8 [  855/ 1327], train_loss/perplexity = 4.45400476/85.9705505 secs/batch = 0.1999s, grad.norm=12.93959236
 11476: 8 [  860/ 1327], train_loss/perplexity = 4.19791269/66.5472794 secs/batch = 0.2000s, grad.norm=12.40540218
 11481: 8 [  865/ 1327], train_loss/perplexity = 4.61450386/100.9377365 secs/batch = 0.1996s, grad.norm=13.02320290
 11486: 8 [  870/ 1327], train_loss/perplexity = 4.54269409/93.9435501 secs/batch = 0.1991s, grad.norm=14.39486599
 11491: 8 [  875/ 1327], train_loss/perplexity = 4.11648417/61.3431892 secs/batch = 0.1978s, grad.norm=12.23399830
 11496: 8 [  880/ 1327], train_loss/perplexity = 4.36132193/78.3606567 secs/batch = 0.1991s, grad.norm=13.96741009
 11501: 8 [  885/ 1327], train_loss/perplexity = 4.51584053/91.4544067 secs/batch = 0.1993s, grad.norm=12.30755234
 11506: 8 [  890/ 1327], train_loss/perplexity = 4.66499710/106.1652756 secs/batch = 0.1994s, grad.norm=12.31995106
 11511: 8 [  895/ 1327], train_loss/perplexity = 4.66650915/106.3259277 secs/batch = 0.1993s, grad.norm=12.38389587
 11516: 8 [  900/ 1327], train_loss/perplexity = 4.45681047/86.2120972 secs/batch = 0.1974s, grad.norm=12.97319698
 11521: 8 [  905/ 1327], train_loss/perplexity = 4.29256010/73.1535110 secs/batch = 0.1986s, grad.norm=11.59028721
 11526: 8 [  910/ 1327], train_loss/perplexity = 4.30305290/73.9251328 secs/batch = 0.1999s, grad.norm=12.07999325
 11531: 8 [  915/ 1327], train_loss/perplexity = 4.55003691/94.6359024 secs/batch = 0.1982s, grad.norm=12.34389877
 11536: 8 [  920/ 1327], train_loss/perplexity = 4.82127523/124.1232758 secs/batch = 0.1992s, grad.norm=12.35130787
 11541: 8 [  925/ 1327], train_loss/perplexity = 4.51866341/91.7129364 secs/batch = 0.2001s, grad.norm=11.71674824
 11546: 8 [  930/ 1327], train_loss/perplexity = 4.51293850/91.1893845 secs/batch = 0.1993s, grad.norm=11.89141846
 11551: 8 [  935/ 1327], train_loss/perplexity = 4.68536472/108.3497849 secs/batch = 0.1993s, grad.norm=12.01110458
 11556: 8 [  940/ 1327], train_loss/perplexity = 4.59524250/99.0121460 secs/batch = 0.2001s, grad.norm=12.35322475
 11561: 8 [  945/ 1327], train_loss/perplexity = 4.77059460/117.9893799 secs/batch = 0.1945s, grad.norm=13.05585957
 11566: 8 [  950/ 1327], train_loss/perplexity = 4.54322195/93.9931564 secs/batch = 0.1993s, grad.norm=13.78153610
 11571: 8 [  955/ 1327], train_loss/perplexity = 4.63591909/103.1226501 secs/batch = 0.1944s, grad.norm=12.62363529
 11576: 8 [  960/ 1327], train_loss/perplexity = 4.80721188/122.3899002 secs/batch = 0.1989s, grad.norm=12.11218929
 11581: 8 [  965/ 1327], train_loss/perplexity = 4.48626947/88.7895966 secs/batch = 0.2011s, grad.norm=12.22728157
 11586: 8 [  970/ 1327], train_loss/perplexity = 4.81930494/123.8789597 secs/batch = 0.1993s, grad.norm=12.27290535
 11591: 8 [  975/ 1327], train_loss/perplexity = 4.52163506/91.9858780 secs/batch = 0.1990s, grad.norm=14.21890354
 11596: 8 [  980/ 1327], train_loss/perplexity = 4.32904100/75.8714905 secs/batch = 0.1980s, grad.norm=11.79008007
 11601: 8 [  985/ 1327], train_loss/perplexity = 4.45109129/85.7204361 secs/batch = 0.1993s, grad.norm=12.58683395
 11606: 8 [  990/ 1327], train_loss/perplexity = 4.62809896/102.3193665 secs/batch = 0.2007s, grad.norm=12.50994492
 11611: 8 [  995/ 1327], train_loss/perplexity = 4.71403599/111.5012741 secs/batch = 0.1990s, grad.norm=11.91999245
 11616: 8 [ 1000/ 1327], train_loss/perplexity = 4.14278412/62.9779167 secs/batch = 0.1989s, grad.norm=12.00465965
 11621: 8 [ 1005/ 1327], train_loss/perplexity = 4.67037678/106.7379532 secs/batch = 0.1932s, grad.norm=12.02569675
 11626: 8 [ 1010/ 1327], train_loss/perplexity = 4.28715229/72.7589798 secs/batch = 0.1988s, grad.norm=12.93720913
 11631: 8 [ 1015/ 1327], train_loss/perplexity = 4.68254089/108.0442505 secs/batch = 0.2003s, grad.norm=16.53432655
 11636: 8 [ 1020/ 1327], train_loss/perplexity = 4.84548855/127.1653900 secs/batch = 0.1996s, grad.norm=11.68457890
 11641: 8 [ 1025/ 1327], train_loss/perplexity = 4.64961672/104.5449066 secs/batch = 0.1989s, grad.norm=11.66362000
 11646: 8 [ 1030/ 1327], train_loss/perplexity = 4.47623920/87.9034653 secs/batch = 0.1985s, grad.norm=11.94950962
 11651: 8 [ 1035/ 1327], train_loss/perplexity = 4.43380642/84.2515030 secs/batch = 0.1994s, grad.norm=11.61639595
 11656: 8 [ 1040/ 1327], train_loss/perplexity = 4.67308998/107.0279465 secs/batch = 0.1947s, grad.norm=12.44030190
 11661: 8 [ 1045/ 1327], train_loss/perplexity = 4.24723911/69.9121246 secs/batch = 0.1916s, grad.norm=11.98535442
 11666: 8 [ 1050/ 1327], train_loss/perplexity = 4.32893705/75.8636017 secs/batch = 0.1993s, grad.norm=12.91790581
 11671: 8 [ 1055/ 1327], train_loss/perplexity = 4.39667368/81.1803894 secs/batch = 0.1999s, grad.norm=13.13775349
 11676: 8 [ 1060/ 1327], train_loss/perplexity = 4.04705524/57.2286835 secs/batch = 0.1932s, grad.norm=13.27661705
 11681: 8 [ 1065/ 1327], train_loss/perplexity = 4.21280670/67.5458527 secs/batch = 0.1987s, grad.norm=13.27062035
 11686: 8 [ 1070/ 1327], train_loss/perplexity = 4.55773783/95.3675003 secs/batch = 0.2006s, grad.norm=13.30513096
 11691: 8 [ 1075/ 1327], train_loss/perplexity = 4.26209497/70.9584808 secs/batch = 0.1985s, grad.norm=12.48609924
 11696: 8 [ 1080/ 1327], train_loss/perplexity = 4.28401709/72.5312195 secs/batch = 0.1980s, grad.norm=13.09537697
 11701: 8 [ 1085/ 1327], train_loss/perplexity = 4.04525232/57.1255989 secs/batch = 0.1996s, grad.norm=13.12506294
 11706: 8 [ 1090/ 1327], train_loss/perplexity = 4.25837135/70.6947556 secs/batch = 0.1987s, grad.norm=14.35368443
 11711: 8 [ 1095/ 1327], train_loss/perplexity = 4.48789358/88.9339142 secs/batch = 0.1989s, grad.norm=14.06950092
 11716: 8 [ 1100/ 1327], train_loss/perplexity = 4.16749001/64.5532227 secs/batch = 0.1984s, grad.norm=13.22091103
 11721: 8 [ 1105/ 1327], train_loss/perplexity = 4.19253206/66.1901779 secs/batch = 0.1991s, grad.norm=14.45044231
 11726: 8 [ 1110/ 1327], train_loss/perplexity = 4.55600166/95.2020645 secs/batch = 0.1999s, grad.norm=14.79086018
 11731: 8 [ 1115/ 1327], train_loss/perplexity = 4.32806349/75.7973633 secs/batch = 0.1983s, grad.norm=13.68579960
 11736: 8 [ 1120/ 1327], train_loss/perplexity = 4.50035000/90.0486450 secs/batch = 0.2005s, grad.norm=12.21339798
 11741: 8 [ 1125/ 1327], train_loss/perplexity = 4.66571712/106.2417450 secs/batch = 0.1990s, grad.norm=13.38078499
 11746: 8 [ 1130/ 1327], train_loss/perplexity = 4.40131426/81.5579834 secs/batch = 0.1997s, grad.norm=12.39987659
 11751: 8 [ 1135/ 1327], train_loss/perplexity = 4.34037399/76.7362366 secs/batch = 0.1970s, grad.norm=12.84137821
 11756: 8 [ 1140/ 1327], train_loss/perplexity = 4.66737127/106.4176331 secs/batch = 0.1989s, grad.norm=12.57137489
 11761: 8 [ 1145/ 1327], train_loss/perplexity = 4.48274946/88.4776077 secs/batch = 0.1978s, grad.norm=12.56888771
 11766: 8 [ 1150/ 1327], train_loss/perplexity = 4.45518112/86.0717392 secs/batch = 0.1978s, grad.norm=12.51341629
 11771: 8 [ 1155/ 1327], train_loss/perplexity = 4.55748749/95.3436279 secs/batch = 0.1986s, grad.norm=12.41909885
 11776: 8 [ 1160/ 1327], train_loss/perplexity = 4.46051741/86.5322723 secs/batch = 0.1988s, grad.norm=14.03735065
 11781: 8 [ 1165/ 1327], train_loss/perplexity = 4.50049925/90.0620804 secs/batch = 0.1988s, grad.norm=14.13035488
 11786: 8 [ 1170/ 1327], train_loss/perplexity = 4.37361717/79.3300629 secs/batch = 0.1992s, grad.norm=13.21036720
 11791: 8 [ 1175/ 1327], train_loss/perplexity = 4.17647123/65.1355972 secs/batch = 0.1947s, grad.norm=13.92347622
 11796: 8 [ 1180/ 1327], train_loss/perplexity = 4.22860050/68.6211319 secs/batch = 0.1972s, grad.norm=14.62760925
 11801: 8 [ 1185/ 1327], train_loss/perplexity = 4.40801716/82.1064987 secs/batch = 0.1980s, grad.norm=12.98560047
 11806: 8 [ 1190/ 1327], train_loss/perplexity = 4.43550777/84.3949661 secs/batch = 0.1991s, grad.norm=12.81730080
 11811: 8 [ 1195/ 1327], train_loss/perplexity = 4.29290485/73.1787338 secs/batch = 0.1976s, grad.norm=13.11745167
 11816: 8 [ 1200/ 1327], train_loss/perplexity = 4.21225452/67.5085678 secs/batch = 0.1964s, grad.norm=12.63177395
 11821: 8 [ 1205/ 1327], train_loss/perplexity = 4.21569681/67.7413559 secs/batch = 0.1933s, grad.norm=12.65630054
 11826: 8 [ 1210/ 1327], train_loss/perplexity = 3.90037847/49.4211502 secs/batch = 0.1983s, grad.norm=12.81597614
 11831: 8 [ 1215/ 1327], train_loss/perplexity = 4.10638237/60.7266350 secs/batch = 0.1988s, grad.norm=12.62856770
 11836: 8 [ 1220/ 1327], train_loss/perplexity = 4.25233459/70.2692719 secs/batch = 0.1992s, grad.norm=13.16005898
 11841: 8 [ 1225/ 1327], train_loss/perplexity = 4.06584167/58.3139687 secs/batch = 0.1933s, grad.norm=15.21614170
 11846: 8 [ 1230/ 1327], train_loss/perplexity = 4.28704929/72.7514801 secs/batch = 0.1966s, grad.norm=12.59066772
 11851: 8 [ 1235/ 1327], train_loss/perplexity = 4.68792343/108.6273727 secs/batch = 0.1983s, grad.norm=38.75033188
 11856: 8 [ 1240/ 1327], train_loss/perplexity = 4.50524044/90.4900970 secs/batch = 0.1983s, grad.norm=14.78211498
 11861: 8 [ 1245/ 1327], train_loss/perplexity = 4.47328854/87.6444702 secs/batch = 0.1987s, grad.norm=17.83599091
 11866: 8 [ 1250/ 1327], train_loss/perplexity = 4.41903687/83.0162888 secs/batch = 0.1922s, grad.norm=12.03888988
 11871: 8 [ 1255/ 1327], train_loss/perplexity = 4.53391552/93.1224747 secs/batch = 0.1994s, grad.norm=12.02287292
 11876: 8 [ 1260/ 1327], train_loss/perplexity = 4.37467337/79.4138947 secs/batch = 0.1993s, grad.norm=14.09239483
 11881: 8 [ 1265/ 1327], train_loss/perplexity = 4.53764725/93.4706268 secs/batch = 0.2000s, grad.norm=12.82837963
 11886: 8 [ 1270/ 1327], train_loss/perplexity = 4.31680298/74.9486313 secs/batch = 0.2005s, grad.norm=13.22626400
 11891: 8 [ 1275/ 1327], train_loss/perplexity = 4.46120262/86.5915833 secs/batch = 0.1978s, grad.norm=14.03122139
 11896: 8 [ 1280/ 1327], train_loss/perplexity = 4.31722975/74.9806213 secs/batch = 0.1985s, grad.norm=12.26431179
 11901: 8 [ 1285/ 1327], train_loss/perplexity = 4.20350504/66.9204788 secs/batch = 0.1927s, grad.norm=12.64072609
 11906: 8 [ 1290/ 1327], train_loss/perplexity = 4.39379215/80.9468002 secs/batch = 0.1989s, grad.norm=12.47349262
 11911: 8 [ 1295/ 1327], train_loss/perplexity = 4.48273325/88.4761734 secs/batch = 0.1988s, grad.norm=13.04403591
 11916: 8 [ 1300/ 1327], train_loss/perplexity = 4.61069345/100.5538559 secs/batch = 0.1945s, grad.norm=12.66909218
 11921: 8 [ 1305/ 1327], train_loss/perplexity = 4.75027514/115.6160889 secs/batch = 0.2003s, grad.norm=13.87567043
 11926: 8 [ 1310/ 1327], train_loss/perplexity = 4.96479750/143.2795258 secs/batch = 0.1993s, grad.norm=12.99673462
 11931: 8 [ 1315/ 1327], train_loss/perplexity = 4.70940399/110.9859924 secs/batch = 0.1979s, grad.norm=12.87751007
 11936: 8 [ 1320/ 1327], train_loss/perplexity = 4.75302076/115.9339676 secs/batch = 0.1996s, grad.norm=12.90157795
 11941: 8 [ 1325/ 1327], train_loss/perplexity = 4.65590334/105.2042160 secs/batch = 0.1991s, grad.norm=13.14909744
Epoch training time: 263.89940905570984
	> validation loss = 4.87551641, perplexity = 131.04180908
	> validation loss = 4.80049276, perplexity = 121.57030487
	> validation loss = 4.74550962, perplexity = 115.06642914
	> validation loss = 4.77148771, perplexity = 118.09480286
	> validation loss = 4.98636103, perplexity = 146.40269470
	> validation loss = 4.80441999, perplexity = 122.04868317
	> validation loss = 4.83116245, perplexity = 125.35659790
	> validation loss = 4.68157339, perplexity = 107.93977356
	> validation loss = 4.48932934, perplexity = 89.06169891
	> validation loss = 4.62143040, perplexity = 101.63931274
	> validation loss = 4.66051149, perplexity = 105.69012451
	> validation loss = 4.77201509, perplexity = 118.15709686
	> validation loss = 4.66984940, perplexity = 106.68167114
	> validation loss = 4.51366234, perplexity = 91.25541687
	> validation loss = 4.45696020, perplexity = 86.22500610
	> validation loss = 4.42859268, perplexity = 83.81338501
	> validation loss = 4.88075161, perplexity = 131.72962952
	> validation loss = 4.45866013, perplexity = 86.37170410
	> validation loss = 4.93221569, perplexity = 138.68646240
	> validation loss = 4.76051092, perplexity = 116.80558777
	> validation loss = 4.55850410, perplexity = 95.44060516
at the end of epoch: 8
train loss = 4.57122531, perplexity = 96.66247885
validation loss = 4.70365610, perplexity = 110.34988568
Saved model cv/epoch008_4.7037.model
 11948: 9 [    5/ 1327], train_loss/perplexity = 4.69752789/109.6757050 secs/batch = 0.1976s, grad.norm=13.39834023
 11953: 9 [   10/ 1327], train_loss/perplexity = 4.26452541/71.1311569 secs/batch = 0.1990s, grad.norm=11.90286732
 11958: 9 [   15/ 1327], train_loss/perplexity = 4.56513023/96.0751038 secs/batch = 0.1991s, grad.norm=11.83262062
 11963: 9 [   20/ 1327], train_loss/perplexity = 4.72072792/112.2499313 secs/batch = 0.1988s, grad.norm=11.80158520
 11968: 9 [   25/ 1327], train_loss/perplexity = 4.59554482/99.0420837 secs/batch = 0.1985s, grad.norm=13.01886177
 11973: 9 [   30/ 1327], train_loss/perplexity = 4.60043335/99.5274353 secs/batch = 0.1989s, grad.norm=12.76979446
 11978: 9 [   35/ 1327], train_loss/perplexity = 4.43655109/84.4830627 secs/batch = 0.1972s, grad.norm=12.97969532
 11983: 9 [   40/ 1327], train_loss/perplexity = 4.40384245/81.7644424 secs/batch = 0.1987s, grad.norm=12.63308907
 11988: 9 [   45/ 1327], train_loss/perplexity = 4.19968033/66.6650162 secs/batch = 0.1976s, grad.norm=12.43697834
 11993: 9 [   50/ 1327], train_loss/perplexity = 4.45543385/86.0934906 secs/batch = 0.1991s, grad.norm=13.37094879
 11998: 9 [   55/ 1327], train_loss/perplexity = 4.39146614/80.7587357 secs/batch = 0.1994s, grad.norm=13.61879921
 12003: 9 [   60/ 1327], train_loss/perplexity = 4.75859261/116.5817337 secs/batch = 0.1989s, grad.norm=12.94893646
 12008: 9 [   65/ 1327], train_loss/perplexity = 4.24756384/69.9348297 secs/batch = 0.1995s, grad.norm=12.36294270
 12013: 9 [   70/ 1327], train_loss/perplexity = 4.09095621/59.7970428 secs/batch = 0.1999s, grad.norm=13.06420040
 12018: 9 [   75/ 1327], train_loss/perplexity = 3.94480562/51.6662941 secs/batch = 0.1934s, grad.norm=11.49279404
 12023: 9 [   80/ 1327], train_loss/perplexity = 4.29751825/73.5171127 secs/batch = 0.1985s, grad.norm=12.16635323
 12028: 9 [   85/ 1327], train_loss/perplexity = 4.43365002/84.2383270 secs/batch = 0.1994s, grad.norm=13.00806713
 12033: 9 [   90/ 1327], train_loss/perplexity = 4.38746595/80.4363327 secs/batch = 0.2001s, grad.norm=12.15549469
 12038: 9 [   95/ 1327], train_loss/perplexity = 4.26108885/70.8871231 secs/batch = 0.1984s, grad.norm=12.85633183
 12043: 9 [  100/ 1327], train_loss/perplexity = 4.64373684/103.9319992 secs/batch = 0.1991s, grad.norm=13.38644600
 12048: 9 [  105/ 1327], train_loss/perplexity = 4.47925186/88.1686859 secs/batch = 0.1988s, grad.norm=13.97789574
 12053: 9 [  110/ 1327], train_loss/perplexity = 4.21219683/67.5046768 secs/batch = 0.1982s, grad.norm=12.59578800
 12058: 9 [  115/ 1327], train_loss/perplexity = 4.25347328/70.3493347 secs/batch = 0.1995s, grad.norm=13.06070137
 12063: 9 [  120/ 1327], train_loss/perplexity = 4.43348980/84.2248306 secs/batch = 0.1954s, grad.norm=13.27904987
 12068: 9 [  125/ 1327], train_loss/perplexity = 4.45920467/86.4187469 secs/batch = 0.1985s, grad.norm=13.24428940
 12073: 9 [  130/ 1327], train_loss/perplexity = 4.39189100/80.7930527 secs/batch = 0.1981s, grad.norm=13.89074326
 12078: 9 [  135/ 1327], train_loss/perplexity = 4.37011814/79.0529709 secs/batch = 0.1913s, grad.norm=12.73001003
 12083: 9 [  140/ 1327], train_loss/perplexity = 4.70864010/110.9012451 secs/batch = 0.1993s, grad.norm=14.17581844
 12088: 9 [  145/ 1327], train_loss/perplexity = 4.56288195/95.8593445 secs/batch = 0.1983s, grad.norm=13.40009022
 12093: 9 [  150/ 1327], train_loss/perplexity = 4.57095003/96.6358719 secs/batch = 0.1988s, grad.norm=12.87957859
 12098: 9 [  155/ 1327], train_loss/perplexity = 4.79623699/121.0540314 secs/batch = 0.1921s, grad.norm=12.44061756
 12103: 9 [  160/ 1327], train_loss/perplexity = 4.51199341/91.1032410 secs/batch = 0.1982s, grad.norm=12.60305405
 12108: 9 [  165/ 1327], train_loss/perplexity = 4.59355450/98.8451538 secs/batch = 0.1988s, grad.norm=13.13300514
 12113: 9 [  170/ 1327], train_loss/perplexity = 4.37011528/79.0527420 secs/batch = 0.1998s, grad.norm=12.87677193
 12118: 9 [  175/ 1327], train_loss/perplexity = 4.97183037/144.2907562 secs/batch = 0.1992s, grad.norm=14.31416607
 12123: 9 [  180/ 1327], train_loss/perplexity = 4.63996983/103.5412216 secs/batch = 0.1980s, grad.norm=13.44610691
 12128: 9 [  185/ 1327], train_loss/perplexity = 4.87390900/130.8313446 secs/batch = 0.1986s, grad.norm=12.09369564
 12133: 9 [  190/ 1327], train_loss/perplexity = 4.34089422/76.7761612 secs/batch = 0.1987s, grad.norm=11.60564613
 12138: 9 [  195/ 1327], train_loss/perplexity = 4.65954399/105.5879211 secs/batch = 0.1981s, grad.norm=12.31040668
 12143: 9 [  200/ 1327], train_loss/perplexity = 4.57178164/96.7162704 secs/batch = 0.1974s, grad.norm=12.76695442
 12148: 9 [  205/ 1327], train_loss/perplexity = 4.65312576/104.9124069 secs/batch = 0.1964s, grad.norm=13.04915142
 12153: 9 [  210/ 1327], train_loss/perplexity = 4.60269594/99.7528839 secs/batch = 0.1986s, grad.norm=11.66136932
 12158: 9 [  215/ 1327], train_loss/perplexity = 4.69072437/108.9320602 secs/batch = 0.1973s, grad.norm=12.21082783
 12163: 9 [  220/ 1327], train_loss/perplexity = 4.66499376/106.1649246 secs/batch = 0.1964s, grad.norm=12.33586407
 12168: 9 [  225/ 1327], train_loss/perplexity = 4.80295038/121.8694458 secs/batch = 0.1979s, grad.norm=12.78519821
 12173: 9 [  230/ 1327], train_loss/perplexity = 4.63421774/102.9473572 secs/batch = 0.1986s, grad.norm=12.58059978
 12178: 9 [  235/ 1327], train_loss/perplexity = 4.53826475/93.5283661 secs/batch = 0.1968s, grad.norm=12.45890236
 12183: 9 [  240/ 1327], train_loss/perplexity = 4.27987432/72.2313614 secs/batch = 0.1983s, grad.norm=13.02908325
 12188: 9 [  245/ 1327], train_loss/perplexity = 4.54755974/94.4017639 secs/batch = 0.1982s, grad.norm=13.86584949
 12193: 9 [  250/ 1327], train_loss/perplexity = 4.40080547/81.5165024 secs/batch = 0.1976s, grad.norm=12.12740421
 12198: 9 [  255/ 1327], train_loss/perplexity = 4.41988468/83.0867004 secs/batch = 0.1981s, grad.norm=12.27853107
 12203: 9 [  260/ 1327], train_loss/perplexity = 4.67490864/107.2227707 secs/batch = 0.1939s, grad.norm=12.76555443
 12208: 9 [  265/ 1327], train_loss/perplexity = 4.76671696/117.5327454 secs/batch = 0.1967s, grad.norm=12.33545113
 12213: 9 [  270/ 1327], train_loss/perplexity = 4.84877586/127.5841141 secs/batch = 0.2012s, grad.norm=12.19736195
 12218: 9 [  275/ 1327], train_loss/perplexity = 4.83034420/125.2540665 secs/batch = 0.1943s, grad.norm=12.11240292
 12223: 9 [  280/ 1327], train_loss/perplexity = 4.59142542/98.6349258 secs/batch = 0.1996s, grad.norm=12.00387955
 12228: 9 [  285/ 1327], train_loss/perplexity = 4.85446262/128.3117218 secs/batch = 0.1993s, grad.norm=11.95849133
 12233: 9 [  290/ 1327], train_loss/perplexity = 4.61333275/100.8195953 secs/batch = 0.1992s, grad.norm=14.17975521
 12238: 9 [  295/ 1327], train_loss/perplexity = 4.38673306/80.3774033 secs/batch = 0.1994s, grad.norm=14.37170315
 12243: 9 [  300/ 1327], train_loss/perplexity = 3.96046519/52.4817352 secs/batch = 0.1941s, grad.norm=11.89816856
 12248: 9 [  305/ 1327], train_loss/perplexity = 4.46886206/87.2573700 secs/batch = 0.1930s, grad.norm=13.85120296
 12253: 9 [  310/ 1327], train_loss/perplexity = 4.49787998/89.8264923 secs/batch = 0.1999s, grad.norm=12.67299271
 12258: 9 [  315/ 1327], train_loss/perplexity = 3.98633146/53.8569489 secs/batch = 0.1997s, grad.norm=12.18465519
 12263: 9 [  320/ 1327], train_loss/perplexity = 4.00935364/55.1112366 secs/batch = 0.2006s, grad.norm=13.27095318
 12268: 9 [  325/ 1327], train_loss/perplexity = 4.05634451/57.7627716 secs/batch = 0.1991s, grad.norm=17.68910980
 12273: 9 [  330/ 1327], train_loss/perplexity = 4.51181889/91.0873489 secs/batch = 0.1993s, grad.norm=12.98276329
 12278: 9 [  335/ 1327], train_loss/perplexity = 3.95053077/51.9629402 secs/batch = 0.1990s, grad.norm=11.69283581
 12283: 9 [  340/ 1327], train_loss/perplexity = 4.72859287/113.1362534 secs/batch = 0.1990s, grad.norm=12.33275795
 12288: 9 [  345/ 1327], train_loss/perplexity = 4.44714165/85.3825378 secs/batch = 0.1985s, grad.norm=12.17492294
 12293: 9 [  350/ 1327], train_loss/perplexity = 4.50632238/90.5880585 secs/batch = 0.2002s, grad.norm=13.57124424
 12298: 9 [  355/ 1327], train_loss/perplexity = 4.57684088/97.2068176 secs/batch = 0.1990s, grad.norm=13.23389626
 12303: 9 [  360/ 1327], train_loss/perplexity = 4.75423527/116.0748520 secs/batch = 0.1997s, grad.norm=13.20617580
 12308: 9 [  365/ 1327], train_loss/perplexity = 4.60068130/99.5521164 secs/batch = 0.1993s, grad.norm=12.85560799
 12313: 9 [  370/ 1327], train_loss/perplexity = 4.66459799/106.1229172 secs/batch = 0.1933s, grad.norm=12.67157459
 12318: 9 [  375/ 1327], train_loss/perplexity = 4.07674170/58.9530678 secs/batch = 0.1997s, grad.norm=12.74428272
 12323: 9 [  380/ 1327], train_loss/perplexity = 4.71703434/111.8360901 secs/batch = 0.1984s, grad.norm=28.65671349
 12328: 9 [  385/ 1327], train_loss/perplexity = 4.52580261/92.3700333 secs/batch = 0.1990s, grad.norm=12.98993015
 12333: 9 [  390/ 1327], train_loss/perplexity = 4.50222540/90.2176819 secs/batch = 0.1987s, grad.norm=12.30020714
 12338: 9 [  395/ 1327], train_loss/perplexity = 4.57131672/96.6713181 secs/batch = 0.1993s, grad.norm=13.23073483
 12343: 9 [  400/ 1327], train_loss/perplexity = 4.47807407/88.0649033 secs/batch = 0.1990s, grad.norm=12.80031776
 12348: 9 [  405/ 1327], train_loss/perplexity = 4.79305172/120.6690521 secs/batch = 0.1991s, grad.norm=12.42706394
 12353: 9 [  410/ 1327], train_loss/perplexity = 4.44868183/85.5141449 secs/batch = 0.1983s, grad.norm=13.37249756
 12358: 9 [  415/ 1327], train_loss/perplexity = 4.32828617/75.8142395 secs/batch = 0.1986s, grad.norm=13.36434460
 12363: 9 [  420/ 1327], train_loss/perplexity = 4.06574106/58.3081017 secs/batch = 0.1996s, grad.norm=13.15095520
 12368: 9 [  425/ 1327], train_loss/perplexity = 4.35234356/77.6602478 secs/batch = 0.1986s, grad.norm=13.29298115
 12373: 9 [  430/ 1327], train_loss/perplexity = 4.62268353/101.7667618 secs/batch = 0.1981s, grad.norm=15.33098888
 12378: 9 [  435/ 1327], train_loss/perplexity = 4.61786652/101.2777252 secs/batch = 0.1982s, grad.norm=13.43649387
 12383: 9 [  440/ 1327], train_loss/perplexity = 4.27939844/72.1969986 secs/batch = 0.1987s, grad.norm=13.52266502
 12388: 9 [  445/ 1327], train_loss/perplexity = 4.58630276/98.1309433 secs/batch = 0.2004s, grad.norm=13.58186340
 12393: 9 [  450/ 1327], train_loss/perplexity = 4.42012930/83.1070328 secs/batch = 0.1995s, grad.norm=12.83030319
 12398: 9 [  455/ 1327], train_loss/perplexity = 4.37362432/79.3306351 secs/batch = 0.1983s, grad.norm=12.65809345
 12403: 9 [  460/ 1327], train_loss/perplexity = 4.45625401/86.1641312 secs/batch = 0.1992s, grad.norm=14.85378838
 12408: 9 [  465/ 1327], train_loss/perplexity = 4.15469408/63.7324638 secs/batch = 0.1993s, grad.norm=15.68113518
 12413: 9 [  470/ 1327], train_loss/perplexity = 4.83780766/126.1923904 secs/batch = 0.1991s, grad.norm=12.50599861
 12418: 9 [  475/ 1327], train_loss/perplexity = 4.30647135/74.1782761 secs/batch = 0.1985s, grad.norm=12.55780315
 12423: 9 [  480/ 1327], train_loss/perplexity = 4.47571564/87.8574524 secs/batch = 0.1989s, grad.norm=12.96760368
 12428: 9 [  485/ 1327], train_loss/perplexity = 4.36480474/78.6340485 secs/batch = 0.1920s, grad.norm=12.85300636
 12433: 9 [  490/ 1327], train_loss/perplexity = 4.32955217/75.9102859 secs/batch = 0.1973s, grad.norm=13.60627079
 12438: 9 [  495/ 1327], train_loss/perplexity = 4.27306080/71.7408829 secs/batch = 0.1949s, grad.norm=12.41555691
 12443: 9 [  500/ 1327], train_loss/perplexity = 4.58387518/97.8930130 secs/batch = 0.1987s, grad.norm=14.00282288
 12448: 9 [  505/ 1327], train_loss/perplexity = 4.66349983/106.0064392 secs/batch = 0.1989s, grad.norm=12.44853878
 12453: 9 [  510/ 1327], train_loss/perplexity = 4.99373293/147.4859467 secs/batch = 0.1988s, grad.norm=12.07594109
 12458: 9 [  515/ 1327], train_loss/perplexity = 4.60451984/99.9349899 secs/batch = 0.1944s, grad.norm=12.08606720
 12463: 9 [  520/ 1327], train_loss/perplexity = 4.76748180/117.6226730 secs/batch = 0.1981s, grad.norm=12.38810730
 12468: 9 [  525/ 1327], train_loss/perplexity = 4.32573223/75.6208649 secs/batch = 0.1930s, grad.norm=12.39331341
 12473: 9 [  530/ 1327], train_loss/perplexity = 4.42949009/83.8886337 secs/batch = 0.1973s, grad.norm=13.26497841
 12478: 9 [  535/ 1327], train_loss/perplexity = 4.49018526/89.1379547 secs/batch = 0.1988s, grad.norm=12.56729889
 12483: 9 [  540/ 1327], train_loss/perplexity = 4.57510567/97.0382919 secs/batch = 0.1988s, grad.norm=12.40821934
 12488: 9 [  545/ 1327], train_loss/perplexity = 4.62743092/102.2510376 secs/batch = 0.1988s, grad.norm=15.90057564
 12493: 9 [  550/ 1327], train_loss/perplexity = 4.58271360/97.7793655 secs/batch = 0.1985s, grad.norm=12.38490963
 12498: 9 [  555/ 1327], train_loss/perplexity = 4.48968935/89.0937653 secs/batch = 0.1919s, grad.norm=13.07136536
 12503: 9 [  560/ 1327], train_loss/perplexity = 4.42943859/83.8843079 secs/batch = 0.1994s, grad.norm=14.53215313
 12508: 9 [  565/ 1327], train_loss/perplexity = 4.42551422/83.5557632 secs/batch = 0.1983s, grad.norm=13.52842236
 12513: 9 [  570/ 1327], train_loss/perplexity = 4.42294025/83.3409729 secs/batch = 0.1981s, grad.norm=14.67249489
 12518: 9 [  575/ 1327], train_loss/perplexity = 4.14453697/63.0884018 secs/batch = 0.1981s, grad.norm=13.14883709
 12523: 9 [  580/ 1327], train_loss/perplexity = 4.64061069/103.6075974 secs/batch = 0.1983s, grad.norm=12.95391083
 12528: 9 [  585/ 1327], train_loss/perplexity = 4.17895317/65.2974625 secs/batch = 0.1993s, grad.norm=12.86334324
 12533: 9 [  590/ 1327], train_loss/perplexity = 4.56733513/96.2871780 secs/batch = 0.1920s, grad.norm=12.83847332
 12538: 9 [  595/ 1327], train_loss/perplexity = 4.50699711/90.6492004 secs/batch = 0.1983s, grad.norm=14.41016579
 12543: 9 [  600/ 1327], train_loss/perplexity = 4.63803911/103.3415070 secs/batch = 0.1977s, grad.norm=12.63489628
 12548: 9 [  605/ 1327], train_loss/perplexity = 4.54181433/93.8609390 secs/batch = 0.1974s, grad.norm=12.89094925
 12553: 9 [  610/ 1327], train_loss/perplexity = 4.80242968/121.8060074 secs/batch = 0.1992s, grad.norm=12.58088112
 12558: 9 [  615/ 1327], train_loss/perplexity = 4.40905094/82.1914215 secs/batch = 0.1987s, grad.norm=12.88854408
 12563: 9 [  620/ 1327], train_loss/perplexity = 4.73971081/114.4011154 secs/batch = 0.1979s, grad.norm=12.60430813
 12568: 9 [  625/ 1327], train_loss/perplexity = 4.68417501/108.2209549 secs/batch = 0.1984s, grad.norm=12.12073803
 12573: 9 [  630/ 1327], train_loss/perplexity = 4.73182678/113.5027161 secs/batch = 0.1989s, grad.norm=12.48223591
 12578: 9 [  635/ 1327], train_loss/perplexity = 4.46940851/87.3050690 secs/batch = 0.1984s, grad.norm=11.98282623
 12583: 9 [  640/ 1327], train_loss/perplexity = 4.48521376/88.6959076 secs/batch = 0.1999s, grad.norm=12.54395390
 12588: 9 [  645/ 1327], train_loss/perplexity = 4.86623383/129.8310242 secs/batch = 0.1989s, grad.norm=13.75971413
 12593: 9 [  650/ 1327], train_loss/perplexity = 4.29637337/73.4329987 secs/batch = 0.1983s, grad.norm=13.20849419
 12598: 9 [  655/ 1327], train_loss/perplexity = 4.32879829/75.8530807 secs/batch = 0.1999s, grad.norm=12.23587322
 12603: 9 [  660/ 1327], train_loss/perplexity = 4.29097176/73.0374069 secs/batch = 0.1984s, grad.norm=12.96183681
 12608: 9 [  665/ 1327], train_loss/perplexity = 4.49924040/89.9487839 secs/batch = 0.1981s, grad.norm=13.63130951
 12613: 9 [  670/ 1327], train_loss/perplexity = 4.47864246/88.1149750 secs/batch = 0.1987s, grad.norm=13.54121494
 12618: 9 [  675/ 1327], train_loss/perplexity = 4.25001621/70.1065521 secs/batch = 0.1956s, grad.norm=13.25770473
 12623: 9 [  680/ 1327], train_loss/perplexity = 4.43959141/84.7403107 secs/batch = 0.1993s, grad.norm=13.69582176
 12628: 9 [  685/ 1327], train_loss/perplexity = 4.26210308/70.9590607 secs/batch = 0.1989s, grad.norm=12.04926395
 12633: 9 [  690/ 1327], train_loss/perplexity = 4.63142109/102.6598511 secs/batch = 0.1989s, grad.norm=12.58193588
 12638: 9 [  695/ 1327], train_loss/perplexity = 4.43940353/84.7243881 secs/batch = 0.1988s, grad.norm=12.85878086
 12643: 9 [  700/ 1327], train_loss/perplexity = 4.80064726/121.5890884 secs/batch = 0.1987s, grad.norm=13.36947536
 12648: 9 [  705/ 1327], train_loss/perplexity = 4.46277428/86.7277832 secs/batch = 0.1980s, grad.norm=12.37952709
 12653: 9 [  710/ 1327], train_loss/perplexity = 4.39385223/80.9516602 secs/batch = 0.1984s, grad.norm=13.72031975
 12658: 9 [  715/ 1327], train_loss/perplexity = 4.32544518/75.5991592 secs/batch = 0.1988s, grad.norm=12.80690956
 12663: 9 [  720/ 1327], train_loss/perplexity = 4.32506609/75.5705109 secs/batch = 0.1991s, grad.norm=13.77374268
 12668: 9 [  725/ 1327], train_loss/perplexity = 4.30131340/73.7966537 secs/batch = 0.1979s, grad.norm=12.47631931
 12673: 9 [  730/ 1327], train_loss/perplexity = 4.45590067/86.1336975 secs/batch = 0.1994s, grad.norm=13.29021549
 12678: 9 [  735/ 1327], train_loss/perplexity = 6.26909542/527.9995728 secs/batch = 0.1986s, grad.norm=68.53716278
 12683: 9 [  740/ 1327], train_loss/perplexity = 3.99660349/54.4130211 secs/batch = 0.1988s, grad.norm=11.99036407
 12688: 9 [  745/ 1327], train_loss/perplexity = 4.60010719/99.4949799 secs/batch = 0.1939s, grad.norm=13.44728470
 12693: 9 [  750/ 1327], train_loss/perplexity = 4.33865786/76.6046524 secs/batch = 0.1977s, grad.norm=12.84446430
 12698: 9 [  755/ 1327], train_loss/perplexity = 4.22820377/68.5939102 secs/batch = 0.1993s, grad.norm=12.49964809
 12703: 9 [  760/ 1327], train_loss/perplexity = 4.09888935/60.2733078 secs/batch = 0.1994s, grad.norm=13.34357548
 12708: 9 [  765/ 1327], train_loss/perplexity = 4.16804409/64.5889969 secs/batch = 0.1982s, grad.norm=12.09838676
 12713: 9 [  770/ 1327], train_loss/perplexity = 4.18822432/65.9056625 secs/batch = 0.1974s, grad.norm=13.29919147
 12718: 9 [  775/ 1327], train_loss/perplexity = 4.33101273/76.0212326 secs/batch = 0.1984s, grad.norm=13.40940189
 12723: 9 [  780/ 1327], train_loss/perplexity = 4.71490145/111.5978088 secs/batch = 0.1983s, grad.norm=14.46114540
 12728: 9 [  785/ 1327], train_loss/perplexity = 4.49587488/89.6465683 secs/batch = 0.1992s, grad.norm=13.82765579
 12733: 9 [  790/ 1327], train_loss/perplexity = 4.24225712/69.5646896 secs/batch = 0.1981s, grad.norm=13.79295444
 12738: 9 [  795/ 1327], train_loss/perplexity = 4.61990833/101.4847260 secs/batch = 0.1988s, grad.norm=12.65747356
 12743: 9 [  800/ 1327], train_loss/perplexity = 4.55379009/94.9917526 secs/batch = 0.1983s, grad.norm=14.80390072
 12748: 9 [  805/ 1327], train_loss/perplexity = 4.91233015/135.9558411 secs/batch = 0.1991s, grad.norm=12.52413464
 12753: 9 [  810/ 1327], train_loss/perplexity = 4.50694370/90.6443558 secs/batch = 0.1987s, grad.norm=12.48425293
 12758: 9 [  815/ 1327], train_loss/perplexity = 4.35063648/77.5277939 secs/batch = 0.1971s, grad.norm=12.11277962
 12763: 9 [  820/ 1327], train_loss/perplexity = 4.18246555/65.5272141 secs/batch = 0.1990s, grad.norm=11.92832375
 12768: 9 [  825/ 1327], train_loss/perplexity = 4.37230396/79.2259521 secs/batch = 0.1994s, grad.norm=12.75523567
 12773: 9 [  830/ 1327], train_loss/perplexity = 4.19435072/66.3106613 secs/batch = 0.1987s, grad.norm=13.16745281
 12778: 9 [  835/ 1327], train_loss/perplexity = 4.49141932/89.2480240 secs/batch = 0.1989s, grad.norm=13.10499859
 12783: 9 [  840/ 1327], train_loss/perplexity = 4.50626659/90.5830002 secs/batch = 0.1984s, grad.norm=13.78799725
 12788: 9 [  845/ 1327], train_loss/perplexity = 4.35524988/77.8862839 secs/batch = 0.1990s, grad.norm=13.73239422
 12793: 9 [  850/ 1327], train_loss/perplexity = 4.38056469/79.8831329 secs/batch = 0.1992s, grad.norm=12.68206501
 12798: 9 [  855/ 1327], train_loss/perplexity = 4.40870190/82.1627350 secs/batch = 0.1989s, grad.norm=14.67873764
 12803: 9 [  860/ 1327], train_loss/perplexity = 4.15528870/63.7703705 secs/batch = 0.1992s, grad.norm=12.87819004
 12808: 9 [  865/ 1327], train_loss/perplexity = 4.61757183/101.2478867 secs/batch = 0.1984s, grad.norm=12.55192852
 12813: 9 [  870/ 1327], train_loss/perplexity = 4.50467634/90.4390717 secs/batch = 0.1985s, grad.norm=13.30325222
 12818: 9 [  875/ 1327], train_loss/perplexity = 4.09015036/59.7488747 secs/batch = 0.1985s, grad.norm=13.15221405
 12823: 9 [  880/ 1327], train_loss/perplexity = 4.31526804/74.8336792 secs/batch = 0.1989s, grad.norm=15.30532360
 12828: 9 [  885/ 1327], train_loss/perplexity = 4.45774078/86.2923355 secs/batch = 0.1952s, grad.norm=13.41019630
 12833: 9 [  890/ 1327], train_loss/perplexity = 4.60860443/100.3440170 secs/batch = 0.1977s, grad.norm=12.28996181
 12838: 9 [  895/ 1327], train_loss/perplexity = 4.62050343/101.5451431 secs/batch = 0.1997s, grad.norm=12.37071419
 12843: 9 [  900/ 1327], train_loss/perplexity = 4.43793392/84.5999680 secs/batch = 0.1981s, grad.norm=13.34942150
 12848: 9 [  905/ 1327], train_loss/perplexity = 4.26279640/71.0082703 secs/batch = 0.1991s, grad.norm=11.58083534
 12853: 9 [  910/ 1327], train_loss/perplexity = 4.35114384/77.5671387 secs/batch = 0.1988s, grad.norm=12.12874603
 12858: 9 [  915/ 1327], train_loss/perplexity = 4.49606752/89.6638336 secs/batch = 0.1992s, grad.norm=12.30097008
 12863: 9 [  920/ 1327], train_loss/perplexity = 4.76470900/117.2969818 secs/batch = 0.1995s, grad.norm=12.67703533
 12868: 9 [  925/ 1327], train_loss/perplexity = 4.48276997/88.4794159 secs/batch = 0.1991s, grad.norm=12.04635143
 12873: 9 [  930/ 1327], train_loss/perplexity = 4.48752022/88.9007187 secs/batch = 0.1986s, grad.norm=11.73395348
 12878: 9 [  935/ 1327], train_loss/perplexity = 4.63250971/102.7716675 secs/batch = 0.1938s, grad.norm=12.44667339
 12883: 9 [  940/ 1327], train_loss/perplexity = 4.58466005/97.9698792 secs/batch = 0.1995s, grad.norm=12.32752514
 12888: 9 [  945/ 1327], train_loss/perplexity = 4.76168394/116.9426880 secs/batch = 0.1997s, grad.norm=12.18780518
 12893: 9 [  950/ 1327], train_loss/perplexity = 4.52906418/92.6717987 secs/batch = 0.1986s, grad.norm=12.62226295
 12898: 9 [  955/ 1327], train_loss/perplexity = 4.51574755/91.4459000 secs/batch = 0.1952s, grad.norm=12.61871529
 12903: 9 [  960/ 1327], train_loss/perplexity = 4.83978415/126.4420547 secs/batch = 0.1990s, grad.norm=13.30152702
 12908: 9 [  965/ 1327], train_loss/perplexity = 4.54564571/94.2212448 secs/batch = 0.1994s, grad.norm=12.26889038
 12913: 9 [  970/ 1327], train_loss/perplexity = 4.75342894/115.9812927 secs/batch = 0.1992s, grad.norm=11.63271523
 12918: 9 [  975/ 1327], train_loss/perplexity = 4.47919607/88.1637650 secs/batch = 0.1993s, grad.norm=14.54362106
 12923: 9 [  980/ 1327], train_loss/perplexity = 4.28104591/72.3160400 secs/batch = 0.1998s, grad.norm=12.04480839
 12928: 9 [  985/ 1327], train_loss/perplexity = 4.48141670/88.3597641 secs/batch = 0.1996s, grad.norm=13.59064960
 12933: 9 [  990/ 1327], train_loss/perplexity = 4.61354160/100.8406525 secs/batch = 0.2012s, grad.norm=13.05447960
 12938: 9 [  995/ 1327], train_loss/perplexity = 4.63911963/103.4532318 secs/batch = 0.1996s, grad.norm=12.63105774
 12943: 9 [ 1000/ 1327], train_loss/perplexity = 4.14542198/63.1442604 secs/batch = 0.1988s, grad.norm=12.34551430
 12948: 9 [ 1005/ 1327], train_loss/perplexity = 4.67480850/107.2120361 secs/batch = 0.1991s, grad.norm=12.50923252
 12953: 9 [ 1010/ 1327], train_loss/perplexity = 4.20646095/67.1185837 secs/batch = 0.1990s, grad.norm=11.94994450
 12958: 9 [ 1015/ 1327], train_loss/perplexity = 4.66268015/105.9195786 secs/batch = 0.1930s, grad.norm=12.38754559
 12963: 9 [ 1020/ 1327], train_loss/perplexity = 4.89759827/133.9676361 secs/batch = 0.1990s, grad.norm=12.30442429
 12968: 9 [ 1025/ 1327], train_loss/perplexity = 4.66556835/106.2259445 secs/batch = 0.1990s, grad.norm=12.25142956
 12973: 9 [ 1030/ 1327], train_loss/perplexity = 4.51204634/91.1080627 secs/batch = 0.1993s, grad.norm=12.04416180
 12978: 9 [ 1035/ 1327], train_loss/perplexity = 4.38692379/80.3927307 secs/batch = 0.1995s, grad.norm=12.15722275
 12983: 9 [ 1040/ 1327], train_loss/perplexity = 5.82556486/338.8524780 secs/batch = 0.1993s, grad.norm=48.52091980
 12988: 9 [ 1045/ 1327], train_loss/perplexity = 4.14578581/63.1672401 secs/batch = 0.1988s, grad.norm=12.40236855
 12993: 9 [ 1050/ 1327], train_loss/perplexity = 4.31477356/74.7966843 secs/batch = 0.1994s, grad.norm=12.26251316
 12998: 9 [ 1055/ 1327], train_loss/perplexity = 4.41160774/82.4018402 secs/batch = 0.1987s, grad.norm=13.70404816
 13003: 9 [ 1060/ 1327], train_loss/perplexity = 4.05861664/57.8941689 secs/batch = 0.1951s, grad.norm=12.82569504
 13008: 9 [ 1065/ 1327], train_loss/perplexity = 4.10074425/60.3852119 secs/batch = 0.1981s, grad.norm=13.10520649
 13013: 9 [ 1070/ 1327], train_loss/perplexity = 4.56636953/96.1942444 secs/batch = 0.1917s, grad.norm=13.72025108
 13018: 9 [ 1075/ 1327], train_loss/perplexity = 4.22634602/68.4665985 secs/batch = 0.1985s, grad.norm=12.23160934
 13023: 9 [ 1080/ 1327], train_loss/perplexity = 4.25823498/70.6851120 secs/batch = 0.1973s, grad.norm=12.48590183
 13028: 9 [ 1085/ 1327], train_loss/perplexity = 4.03858185/56.7458115 secs/batch = 0.1988s, grad.norm=12.46465302
 13033: 9 [ 1090/ 1327], train_loss/perplexity = 4.32724380/75.7352600 secs/batch = 0.1992s, grad.norm=13.41622543
 13038: 9 [ 1095/ 1327], train_loss/perplexity = 4.49672651/89.7229462 secs/batch = 0.1992s, grad.norm=14.12170410
 13043: 9 [ 1100/ 1327], train_loss/perplexity = 4.11408091/61.1959419 secs/batch = 0.1983s, grad.norm=14.41434479
 13048: 9 [ 1105/ 1327], train_loss/perplexity = 4.17560959/65.0794983 secs/batch = 0.1930s, grad.norm=13.29134083
 13053: 9 [ 1110/ 1327], train_loss/perplexity = 4.54979801/94.6132965 secs/batch = 0.1983s, grad.norm=14.21031189
 13058: 9 [ 1115/ 1327], train_loss/perplexity = 4.27136230/71.6191330 secs/batch = 0.1976s, grad.norm=12.88863182
 13063: 9 [ 1120/ 1327], train_loss/perplexity = 4.46842194/87.2189789 secs/batch = 0.1979s, grad.norm=12.74412155
 13068: 9 [ 1125/ 1327], train_loss/perplexity = 4.69455862/109.3505325 secs/batch = 0.1990s, grad.norm=14.58403778
 13073: 9 [ 1130/ 1327], train_loss/perplexity = 4.32111835/75.2727661 secs/batch = 0.1987s, grad.norm=13.91459751
 13078: 9 [ 1135/ 1327], train_loss/perplexity = 4.33717251/76.4909592 secs/batch = 0.1988s, grad.norm=12.41597652
 13083: 9 [ 1140/ 1327], train_loss/perplexity = 4.64420223/103.9803772 secs/batch = 0.1994s, grad.norm=14.67030621
 13088: 9 [ 1145/ 1327], train_loss/perplexity = 4.41223335/82.4534073 secs/batch = 0.1996s, grad.norm=12.36125278
 13093: 9 [ 1150/ 1327], train_loss/perplexity = 4.39044666/80.6764450 secs/batch = 0.1987s, grad.norm=12.83234692
 13098: 9 [ 1155/ 1327], train_loss/perplexity = 4.44989395/85.6178665 secs/batch = 0.1993s, grad.norm=12.89190769
 13103: 9 [ 1160/ 1327], train_loss/perplexity = 4.50321293/90.3068161 secs/batch = 0.1994s, grad.norm=12.92819023
 13108: 9 [ 1165/ 1327], train_loss/perplexity = 4.43817854/84.6206665 secs/batch = 0.1990s, grad.norm=13.20069408
 13113: 9 [ 1170/ 1327], train_loss/perplexity = 4.29229927/73.1344299 secs/batch = 0.1981s, grad.norm=13.74459267
 13118: 9 [ 1175/ 1327], train_loss/perplexity = 4.07075739/58.6013298 secs/batch = 0.2003s, grad.norm=12.64079094
 13123: 9 [ 1180/ 1327], train_loss/perplexity = 4.24510527/69.7630997 secs/batch = 0.1994s, grad.norm=17.30341339
 13128: 9 [ 1185/ 1327], train_loss/perplexity = 4.32086754/75.2538834 secs/batch = 0.1937s, grad.norm=13.18546677
 13133: 9 [ 1190/ 1327], train_loss/perplexity = 4.44095421/84.8558731 secs/batch = 0.1990s, grad.norm=13.35638237
 13138: 9 [ 1195/ 1327], train_loss/perplexity = 4.20192289/66.8146820 secs/batch = 0.1990s, grad.norm=13.39723587
 13143: 9 [ 1200/ 1327], train_loss/perplexity = 4.21130466/67.4444733 secs/batch = 0.1990s, grad.norm=13.05623245
 13148: 9 [ 1205/ 1327], train_loss/perplexity = 4.22959042/68.6890945 secs/batch = 0.1926s, grad.norm=12.86861706
 13153: 9 [ 1210/ 1327], train_loss/perplexity = 3.92430091/50.6176796 secs/batch = 0.1973s, grad.norm=13.91499710
 13158: 9 [ 1215/ 1327], train_loss/perplexity = 4.05908060/57.9210358 secs/batch = 0.1988s, grad.norm=12.55262184
 13163: 9 [ 1220/ 1327], train_loss/perplexity = 4.25913239/70.7485733 secs/batch = 0.1994s, grad.norm=13.60233402
 13168: 9 [ 1225/ 1327], train_loss/perplexity = 4.05973339/57.9588585 secs/batch = 0.1981s, grad.norm=17.83917046
 13173: 9 [ 1230/ 1327], train_loss/perplexity = 4.41803503/82.9331665 secs/batch = 0.1995s, grad.norm=17.14833260
 13178: 9 [ 1235/ 1327], train_loss/perplexity = 4.30565596/74.1178207 secs/batch = 0.1986s, grad.norm=12.71721172
 13183: 9 [ 1240/ 1327], train_loss/perplexity = 4.43966722/84.7467346 secs/batch = 0.1955s, grad.norm=13.00746250
 13188: 9 [ 1245/ 1327], train_loss/perplexity = 4.37474012/79.4191971 secs/batch = 0.1986s, grad.norm=12.25928307
 13193: 9 [ 1250/ 1327], train_loss/perplexity = 4.47228098/87.5562057 secs/batch = 0.1989s, grad.norm=12.78232956
 13198: 9 [ 1255/ 1327], train_loss/perplexity = 4.52216625/92.0347519 secs/batch = 0.1992s, grad.norm=12.33437252
 13203: 9 [ 1260/ 1327], train_loss/perplexity = 4.23729897/69.2206345 secs/batch = 0.1989s, grad.norm=14.67145729
 13208: 9 [ 1265/ 1327], train_loss/perplexity = 4.54704618/94.3532944 secs/batch = 0.1992s, grad.norm=14.00785065
 13213: 9 [ 1270/ 1327], train_loss/perplexity = 4.23944759/69.3695221 secs/batch = 0.1923s, grad.norm=13.51370430
 13218: 9 [ 1275/ 1327], train_loss/perplexity = 4.42676735/83.6605377 secs/batch = 0.1996s, grad.norm=13.60152912
 13223: 9 [ 1280/ 1327], train_loss/perplexity = 4.29024076/72.9840393 secs/batch = 0.1990s, grad.norm=12.38531017
 13228: 9 [ 1285/ 1327], train_loss/perplexity = 4.18185711/65.4873581 secs/batch = 0.1995s, grad.norm=12.74893761
 13233: 9 [ 1290/ 1327], train_loss/perplexity = 4.43592548/84.4302292 secs/batch = 0.1983s, grad.norm=12.53594780
 13238: 9 [ 1295/ 1327], train_loss/perplexity = 4.38199568/79.9975204 secs/batch = 0.1986s, grad.norm=12.67033863
 13243: 9 [ 1300/ 1327], train_loss/perplexity = 4.54095316/93.7801437 secs/batch = 0.1985s, grad.norm=12.03204441
 13248: 9 [ 1305/ 1327], train_loss/perplexity = 4.70831299/110.8649750 secs/batch = 0.1991s, grad.norm=13.63698006
 13253: 9 [ 1310/ 1327], train_loss/perplexity = 4.87412739/130.8599091 secs/batch = 0.1990s, grad.norm=13.48841667
 13258: 9 [ 1315/ 1327], train_loss/perplexity = 4.73296642/113.6321411 secs/batch = 0.1988s, grad.norm=13.65369034
 13263: 9 [ 1320/ 1327], train_loss/perplexity = 4.68099880/107.8777695 secs/batch = 0.1990s, grad.norm=12.88478184
 13268: 9 [ 1325/ 1327], train_loss/perplexity = 4.60276699/99.7599716 secs/batch = 0.1941s, grad.norm=12.99947929
Epoch training time: 263.14713525772095
	> validation loss = 4.88508272, perplexity = 132.30140686
	> validation loss = 4.80312920, perplexity = 121.89124298
	> validation loss = 4.77602530, perplexity = 118.63188171
	> validation loss = 4.79697800, perplexity = 121.14376831
	> validation loss = 4.97475004, perplexity = 144.71264648
	> validation loss = 4.81576729, perplexity = 123.44149017
	> validation loss = 4.81563473, perplexity = 123.42512512
	> validation loss = 4.68464661, perplexity = 108.27200317
	> validation loss = 4.42732382, perplexity = 83.70709991
	> validation loss = 4.58263922, perplexity = 97.77209473
	> validation loss = 4.69013214, perplexity = 108.86756134
	> validation loss = 4.81905460, perplexity = 123.84795380
	> validation loss = 4.65492821, perplexity = 105.10167694
	> validation loss = 4.56075430, perplexity = 95.65560913
	> validation loss = 4.44165039, perplexity = 84.91497040
	> validation loss = 4.40930176, perplexity = 82.21203613
	> validation loss = 4.87612009, perplexity = 131.12094116
	> validation loss = 4.47510338, perplexity = 87.80368042
	> validation loss = 4.87498856, perplexity = 130.97265625
	> validation loss = 4.74197578, perplexity = 114.66052246
	> validation loss = 4.56313705, perplexity = 95.88380432
at the end of epoch: 9
train loss = 4.52710853, perplexity = 92.49073988
validation loss = 4.70050776, perplexity = 110.00301328
Saved model cv/epoch009_4.7005.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 1.0
new learning rate is: 0.5
 13275: 10 [    5/ 1327], train_loss/perplexity = 4.67608309/107.3487701 secs/batch = 0.1982s, grad.norm=12.54453945
 13280: 10 [   10/ 1327], train_loss/perplexity = 4.27218866/71.6783447 secs/batch = 0.1993s, grad.norm=13.09561157
 13285: 10 [   15/ 1327], train_loss/perplexity = 4.50619268/90.5763092 secs/batch = 0.1990s, grad.norm=11.73335934
 13290: 10 [   20/ 1327], train_loss/perplexity = 4.66599178/106.2709274 secs/batch = 0.1996s, grad.norm=12.37263680
 13295: 10 [   25/ 1327], train_loss/perplexity = 4.57778120/97.2982712 secs/batch = 0.1995s, grad.norm=13.02798271
 13300: 10 [   30/ 1327], train_loss/perplexity = 4.55720043/95.3162613 secs/batch = 0.1985s, grad.norm=12.90818787
 13305: 10 [   35/ 1327], train_loss/perplexity = 4.38381672/80.1433334 secs/batch = 0.1998s, grad.norm=12.58704185
 13310: 10 [   40/ 1327], train_loss/perplexity = 4.34665680/77.2198715 secs/batch = 0.1994s, grad.norm=12.59768867
 13315: 10 [   45/ 1327], train_loss/perplexity = 4.07607460/58.9137535 secs/batch = 0.1979s, grad.norm=12.49003410
 13320: 10 [   50/ 1327], train_loss/perplexity = 4.40053463/81.4944229 secs/batch = 0.1981s, grad.norm=12.24971199
 13325: 10 [   55/ 1327], train_loss/perplexity = 4.32710123/75.7244568 secs/batch = 0.1983s, grad.norm=12.63026810
 13330: 10 [   60/ 1327], train_loss/perplexity = 4.64859581/104.4382324 secs/batch = 0.1987s, grad.norm=12.86343765
 13335: 10 [   65/ 1327], train_loss/perplexity = 4.18633509/65.7812653 secs/batch = 0.1987s, grad.norm=11.50597286
 13340: 10 [   70/ 1327], train_loss/perplexity = 4.03736210/56.6766396 secs/batch = 0.1985s, grad.norm=11.77964401
 13345: 10 [   75/ 1327], train_loss/perplexity = 3.88721681/48.7749481 secs/batch = 0.1988s, grad.norm=11.68959332
 13350: 10 [   80/ 1327], train_loss/perplexity = 4.33475733/76.3064423 secs/batch = 0.1990s, grad.norm=13.33118916
 13355: 10 [   85/ 1327], train_loss/perplexity = 4.32219505/75.3538513 secs/batch = 0.1989s, grad.norm=13.20706081
 13360: 10 [   90/ 1327], train_loss/perplexity = 4.35737705/78.0521393 secs/batch = 0.2001s, grad.norm=12.35861301
 13365: 10 [   95/ 1327], train_loss/perplexity = 4.20196104/66.8172302 secs/batch = 0.1988s, grad.norm=12.83463383
 13370: 10 [  100/ 1327], train_loss/perplexity = 4.56774235/96.3263931 secs/batch = 0.1990s, grad.norm=12.23635387
 13375: 10 [  105/ 1327], train_loss/perplexity = 4.36413097/78.5810776 secs/batch = 0.1986s, grad.norm=12.84453487
 13380: 10 [  110/ 1327], train_loss/perplexity = 4.23534060/69.0852051 secs/batch = 0.1985s, grad.norm=12.38058281
 13385: 10 [  115/ 1327], train_loss/perplexity = 4.19671822/66.4678421 secs/batch = 0.1991s, grad.norm=12.41825581
 13390: 10 [  120/ 1327], train_loss/perplexity = 4.32115555/75.2755661 secs/batch = 0.1932s, grad.norm=12.67349529
 13395: 10 [  125/ 1327], train_loss/perplexity = 4.39385414/80.9518204 secs/batch = 0.1991s, grad.norm=13.01577854
 13400: 10 [  130/ 1327], train_loss/perplexity = 4.36170006/78.3902893 secs/batch = 0.1983s, grad.norm=12.80517387
 13405: 10 [  135/ 1327], train_loss/perplexity = 4.27071047/71.5724640 secs/batch = 0.1990s, grad.norm=12.38155651
 13410: 10 [  140/ 1327], train_loss/perplexity = 4.58656549/98.1567307 secs/batch = 0.1986s, grad.norm=12.46746731
 13415: 10 [  145/ 1327], train_loss/perplexity = 4.47863245/88.1140900 secs/batch = 0.1984s, grad.norm=13.26934814
 13420: 10 [  150/ 1327], train_loss/perplexity = 4.48899269/89.0317154 secs/batch = 0.1988s, grad.norm=12.10537338
 13425: 10 [  155/ 1327], train_loss/perplexity = 4.73017502/113.3153915 secs/batch = 0.1985s, grad.norm=11.98937988
 13430: 10 [  160/ 1327], train_loss/perplexity = 4.42513227/83.5238571 secs/batch = 0.1990s, grad.norm=11.97878456
 13435: 10 [  165/ 1327], train_loss/perplexity = 4.59460831/98.9493713 secs/batch = 0.1991s, grad.norm=11.62127876
 13440: 10 [  170/ 1327], train_loss/perplexity = 4.30893230/74.3610535 secs/batch = 0.1961s, grad.norm=11.59716034
 13445: 10 [  175/ 1327], train_loss/perplexity = 4.63633442/103.1654892 secs/batch = 0.1978s, grad.norm=13.60029125
 13450: 10 [  180/ 1327], train_loss/perplexity = 4.47192335/87.5249023 secs/batch = 0.1946s, grad.norm=12.90802765
 13455: 10 [  185/ 1327], train_loss/perplexity = 4.78715134/119.9591599 secs/batch = 0.1990s, grad.norm=11.57413864
 13460: 10 [  190/ 1327], train_loss/perplexity = 4.29204750/73.1160202 secs/batch = 0.1996s, grad.norm=10.91744804
 13465: 10 [  195/ 1327], train_loss/perplexity = 4.55853415/95.4434738 secs/batch = 0.1992s, grad.norm=11.15448952
 13470: 10 [  200/ 1327], train_loss/perplexity = 4.51075888/90.9908447 secs/batch = 0.1991s, grad.norm=12.26029682
 13475: 10 [  205/ 1327], train_loss/perplexity = 4.60812044/100.2954636 secs/batch = 0.1992s, grad.norm=11.87104702
 13480: 10 [  210/ 1327], train_loss/perplexity = 4.43123198/84.0348816 secs/batch = 0.1988s, grad.norm=11.01263618
 13485: 10 [  215/ 1327], train_loss/perplexity = 4.61581612/101.0702820 secs/batch = 0.1993s, grad.norm=11.14257145
 13490: 10 [  220/ 1327], train_loss/perplexity = 4.56095409/95.6747208 secs/batch = 0.1982s, grad.norm=11.82602787
 13495: 10 [  225/ 1327], train_loss/perplexity = 4.74941540/115.5167313 secs/batch = 0.1981s, grad.norm=12.01503849
 13500: 10 [  230/ 1327], train_loss/perplexity = 4.58785963/98.2838440 secs/batch = 0.1936s, grad.norm=12.23542786
 13505: 10 [  235/ 1327], train_loss/perplexity = 4.41197920/82.4324493 secs/batch = 0.1976s, grad.norm=11.69787693
 13510: 10 [  240/ 1327], train_loss/perplexity = 4.14871073/63.3522682 secs/batch = 0.1988s, grad.norm=11.81113815
 13515: 10 [  245/ 1327], train_loss/perplexity = 4.43497324/84.3498688 secs/batch = 0.1994s, grad.norm=11.53491783
 13520: 10 [  250/ 1327], train_loss/perplexity = 4.29409885/73.2661591 secs/batch = 0.1999s, grad.norm=10.86818600
 13525: 10 [  255/ 1327], train_loss/perplexity = 4.25249815/70.2807617 secs/batch = 0.1987s, grad.norm=11.91139984
 13530: 10 [  260/ 1327], train_loss/perplexity = 4.51111746/91.0234756 secs/batch = 0.1997s, grad.norm=12.31756973
 13535: 10 [  265/ 1327], train_loss/perplexity = 4.69082642/108.9431763 secs/batch = 0.1985s, grad.norm=12.14688873
 13540: 10 [  270/ 1327], train_loss/perplexity = 4.75946331/116.6832886 secs/batch = 0.1993s, grad.norm=12.52433872
 13545: 10 [  275/ 1327], train_loss/perplexity = 4.73948240/114.3749847 secs/batch = 0.1982s, grad.norm=11.95061016
 13550: 10 [  280/ 1327], train_loss/perplexity = 4.53841591/93.5425034 secs/batch = 0.1946s, grad.norm=11.32517910
 13555: 10 [  285/ 1327], train_loss/perplexity = 4.72738934/113.0001678 secs/batch = 0.1986s, grad.norm=11.79343224
 13560: 10 [  290/ 1327], train_loss/perplexity = 4.55059814/94.6890259 secs/batch = 0.1981s, grad.norm=11.94513702
 13565: 10 [  295/ 1327], train_loss/perplexity = 4.24376726/69.6698227 secs/batch = 0.1990s, grad.norm=11.41485214
 13570: 10 [  300/ 1327], train_loss/perplexity = 3.81807089/45.5163193 secs/batch = 0.1976s, grad.norm=11.36562252
 13575: 10 [  305/ 1327], train_loss/perplexity = 4.37547731/79.4777679 secs/batch = 0.1985s, grad.norm=12.94762230
 13580: 10 [  310/ 1327], train_loss/perplexity = 4.35447407/77.8258820 secs/batch = 0.1990s, grad.norm=12.17077637
 13585: 10 [  315/ 1327], train_loss/perplexity = 3.90638757/49.7190208 secs/batch = 0.1997s, grad.norm=11.08242130
 13590: 10 [  320/ 1327], train_loss/perplexity = 3.88692212/48.7605743 secs/batch = 0.1984s, grad.norm=11.85899067
 13595: 10 [  325/ 1327], train_loss/perplexity = 3.91701794/50.2503700 secs/batch = 0.1986s, grad.norm=11.83854866
 13600: 10 [  330/ 1327], train_loss/perplexity = 4.44127178/84.8828278 secs/batch = 0.1988s, grad.norm=12.13600731
 13605: 10 [  335/ 1327], train_loss/perplexity = 3.81157756/45.2217216 secs/batch = 0.1983s, grad.norm=10.87018490
 13610: 10 [  340/ 1327], train_loss/perplexity = 4.59121466/98.6141434 secs/batch = 0.1992s, grad.norm=11.89016533
 13615: 10 [  345/ 1327], train_loss/perplexity = 4.39304495/80.8863373 secs/batch = 0.1993s, grad.norm=11.93576050
 13620: 10 [  350/ 1327], train_loss/perplexity = 4.44387150/85.1037827 secs/batch = 0.1982s, grad.norm=12.94752693
 13625: 10 [  355/ 1327], train_loss/perplexity = 4.44336796/85.0609436 secs/batch = 0.1966s, grad.norm=12.62177181
 13630: 10 [  360/ 1327], train_loss/perplexity = 4.58539248/98.0416565 secs/batch = 0.1987s, grad.norm=13.21910572
 13635: 10 [  365/ 1327], train_loss/perplexity = 4.52127409/91.9526825 secs/batch = 0.1984s, grad.norm=11.35563564
 13640: 10 [  370/ 1327], train_loss/perplexity = 4.53396034/93.1266479 secs/batch = 0.1984s, grad.norm=11.82622623
 13645: 10 [  375/ 1327], train_loss/perplexity = 3.95639563/52.2685890 secs/batch = 0.1992s, grad.norm=12.16593933
 13650: 10 [  380/ 1327], train_loss/perplexity = 4.12297058/61.7423820 secs/batch = 0.1908s, grad.norm=11.95828915
 13655: 10 [  385/ 1327], train_loss/perplexity = 4.36705685/78.8113327 secs/batch = 0.1972s, grad.norm=12.91522789
 13660: 10 [  390/ 1327], train_loss/perplexity = 4.35653782/77.9866638 secs/batch = 0.1988s, grad.norm=11.63793182
 13665: 10 [  395/ 1327], train_loss/perplexity = 4.45508862/86.0637741 secs/batch = 0.1990s, grad.norm=13.83088589
 13670: 10 [  400/ 1327], train_loss/perplexity = 4.41587496/82.7542191 secs/batch = 0.1985s, grad.norm=11.57485867
 13675: 10 [  405/ 1327], train_loss/perplexity = 4.63010311/102.5246353 secs/batch = 0.1988s, grad.norm=12.13375854
 13680: 10 [  410/ 1327], train_loss/perplexity = 4.34857082/77.3678131 secs/batch = 0.1968s, grad.norm=12.32459450
 13685: 10 [  415/ 1327], train_loss/perplexity = 4.27154779/71.6324234 secs/batch = 0.1991s, grad.norm=11.91400337
 13690: 10 [  420/ 1327], train_loss/perplexity = 3.96007276/52.4611435 secs/batch = 0.1990s, grad.norm=12.11997890
 13695: 10 [  425/ 1327], train_loss/perplexity = 4.26775837/71.3614883 secs/batch = 0.1992s, grad.norm=12.87266254
 13700: 10 [  430/ 1327], train_loss/perplexity = 4.48735714/88.8862228 secs/batch = 0.2005s, grad.norm=13.00649261
 13705: 10 [  435/ 1327], train_loss/perplexity = 4.54127884/93.8106918 secs/batch = 0.1995s, grad.norm=11.85999775
 13710: 10 [  440/ 1327], train_loss/perplexity = 4.05453825/57.6585312 secs/batch = 0.1993s, grad.norm=13.12872410
 13715: 10 [  445/ 1327], train_loss/perplexity = 4.45892668/86.3947296 secs/batch = 0.1999s, grad.norm=12.76356888
 13720: 10 [  450/ 1327], train_loss/perplexity = 4.29684353/73.4675293 secs/batch = 0.1956s, grad.norm=12.27104855
 13725: 10 [  455/ 1327], train_loss/perplexity = 4.25092888/70.1705627 secs/batch = 0.1987s, grad.norm=11.69293213
 13730: 10 [  460/ 1327], train_loss/perplexity = 4.33717585/76.4912109 secs/batch = 0.1990s, grad.norm=12.44769478
 13735: 10 [  465/ 1327], train_loss/perplexity = 3.97681880/53.3470573 secs/batch = 0.1989s, grad.norm=12.78142834
 13740: 10 [  470/ 1327], train_loss/perplexity = 4.70130253/110.0904770 secs/batch = 0.1990s, grad.norm=11.37693119
 13745: 10 [  475/ 1327], train_loss/perplexity = 4.13898373/62.7390289 secs/batch = 0.1989s, grad.norm=12.19807816
 13750: 10 [  480/ 1327], train_loss/perplexity = 4.32477856/75.5487823 secs/batch = 0.1988s, grad.norm=12.40019321
 13755: 10 [  485/ 1327], train_loss/perplexity = 4.30002880/73.7019196 secs/batch = 0.1983s, grad.norm=11.68733978
 13760: 10 [  490/ 1327], train_loss/perplexity = 4.22502232/68.3760300 secs/batch = 0.1994s, grad.norm=13.53554344
 13765: 10 [  495/ 1327], train_loss/perplexity = 4.16832829/64.6073532 secs/batch = 0.1985s, grad.norm=11.94602299
 13770: 10 [  500/ 1327], train_loss/perplexity = 4.41661215/82.8152466 secs/batch = 0.1992s, grad.norm=12.32043266
 13775: 10 [  505/ 1327], train_loss/perplexity = 4.49804449/89.8412704 secs/batch = 0.1992s, grad.norm=11.04456520
 13780: 10 [  510/ 1327], train_loss/perplexity = 4.84961462/127.6911697 secs/batch = 0.1989s, grad.norm=11.17595291
 13785: 10 [  515/ 1327], train_loss/perplexity = 4.47137976/87.4773407 secs/batch = 0.1995s, grad.norm=11.14708424
 13790: 10 [  520/ 1327], train_loss/perplexity = 4.62421846/101.9230881 secs/batch = 0.1995s, grad.norm=11.84836102
 13795: 10 [  525/ 1327], train_loss/perplexity = 4.19604731/66.4232635 secs/batch = 0.1990s, grad.norm=11.57411575
 13800: 10 [  530/ 1327], train_loss/perplexity = 4.31525898/74.8330002 secs/batch = 0.1990s, grad.norm=12.42532158
 13805: 10 [  535/ 1327], train_loss/perplexity = 4.37023735/79.0623932 secs/batch = 0.2009s, grad.norm=11.67711735
 13810: 10 [  540/ 1327], train_loss/perplexity = 4.45634031/86.1715698 secs/batch = 0.1988s, grad.norm=11.48900032
 13815: 10 [  545/ 1327], train_loss/perplexity = 4.50410891/90.3877640 secs/batch = 0.1990s, grad.norm=12.02632904
 13820: 10 [  550/ 1327], train_loss/perplexity = 4.53760147/93.4663467 secs/batch = 0.1945s, grad.norm=12.18780899
 13825: 10 [  555/ 1327], train_loss/perplexity = 4.32606506/75.6460342 secs/batch = 0.1972s, grad.norm=11.39610767
 13830: 10 [  560/ 1327], train_loss/perplexity = 4.36880827/78.9494858 secs/batch = 0.1951s, grad.norm=13.04740334
 13835: 10 [  565/ 1327], train_loss/perplexity = 4.29816008/73.5643158 secs/batch = 0.1986s, grad.norm=12.67948246
 13840: 10 [  570/ 1327], train_loss/perplexity = 4.30997753/74.4388199 secs/batch = 0.1988s, grad.norm=12.49361038
 13845: 10 [  575/ 1327], train_loss/perplexity = 3.98828030/53.9620094 secs/batch = 0.1940s, grad.norm=12.07913017
 13850: 10 [  580/ 1327], train_loss/perplexity = 4.48318863/88.5164719 secs/batch = 0.1987s, grad.norm=12.41488934
 13855: 10 [  585/ 1327], train_loss/perplexity = 4.04802608/57.2842712 secs/batch = 0.1994s, grad.norm=11.61749840
 13860: 10 [  590/ 1327], train_loss/perplexity = 4.44026804/84.7976685 secs/batch = 0.1949s, grad.norm=11.32473373
 13865: 10 [  595/ 1327], train_loss/perplexity = 4.36560488/78.6969910 secs/batch = 0.1981s, grad.norm=12.04415607
 13870: 10 [  600/ 1327], train_loss/perplexity = 4.51750565/91.6068115 secs/batch = 0.1991s, grad.norm=11.08346272
 13875: 10 [  605/ 1327], train_loss/perplexity = 4.42113590/83.1907272 secs/batch = 0.1990s, grad.norm=11.70936298
 13880: 10 [  610/ 1327], train_loss/perplexity = 4.61060238/100.5447006 secs/batch = 0.1996s, grad.norm=11.17929268
 13885: 10 [  615/ 1327], train_loss/perplexity = 4.24036264/69.4330292 secs/batch = 0.2001s, grad.norm=11.62196064
 13890: 10 [  620/ 1327], train_loss/perplexity = 4.61445808/100.9331131 secs/batch = 0.2001s, grad.norm=11.38244343
 13895: 10 [  625/ 1327], train_loss/perplexity = 4.57059860/96.6019211 secs/batch = 0.1995s, grad.norm=11.56143188
 13900: 10 [  630/ 1327], train_loss/perplexity = 4.63563871/103.0937424 secs/batch = 0.1977s, grad.norm=11.65695000
 13905: 10 [  635/ 1327], train_loss/perplexity = 4.38692665/80.3929596 secs/batch = 0.1983s, grad.norm=11.38850403
 13910: 10 [  640/ 1327], train_loss/perplexity = 4.30143499/73.8056259 secs/batch = 0.1995s, grad.norm=11.63929844
 13915: 10 [  645/ 1327], train_loss/perplexity = 4.64935827/104.5178909 secs/batch = 0.1996s, grad.norm=12.81117821
 13920: 10 [  650/ 1327], train_loss/perplexity = 4.15356302/63.6604195 secs/batch = 0.1995s, grad.norm=13.13424587
 13925: 10 [  655/ 1327], train_loss/perplexity = 4.20413446/66.9626160 secs/batch = 0.1989s, grad.norm=11.77079105
 13930: 10 [  660/ 1327], train_loss/perplexity = 4.16557026/64.4294128 secs/batch = 0.1983s, grad.norm=11.70021343
 13935: 10 [  665/ 1327], train_loss/perplexity = 4.33631992/76.4257660 secs/batch = 0.1994s, grad.norm=12.14171505
 13940: 10 [  670/ 1327], train_loss/perplexity = 4.32742977/75.7493439 secs/batch = 0.1984s, grad.norm=11.74325275
 13945: 10 [  675/ 1327], train_loss/perplexity = 4.08254576/59.2962303 secs/batch = 0.1950s, grad.norm=12.18547630
 13950: 10 [  680/ 1327], train_loss/perplexity = 4.26143551/70.9117050 secs/batch = 0.1990s, grad.norm=12.50429916
 13955: 10 [  685/ 1327], train_loss/perplexity = 4.10339689/60.5456047 secs/batch = 0.1983s, grad.norm=11.58053493
 13960: 10 [  690/ 1327], train_loss/perplexity = 4.50464916/90.4366074 secs/batch = 0.1977s, grad.norm=11.47172546
 13965: 10 [  695/ 1327], train_loss/perplexity = 4.30940151/74.3959503 secs/batch = 0.1979s, grad.norm=11.90811539
 13970: 10 [  700/ 1327], train_loss/perplexity = 4.59390068/98.8793793 secs/batch = 0.1999s, grad.norm=12.32967472
 13975: 10 [  705/ 1327], train_loss/perplexity = 4.32792425/75.7868118 secs/batch = 0.1999s, grad.norm=11.61314583
 13980: 10 [  710/ 1327], train_loss/perplexity = 4.23627949/69.1501007 secs/batch = 0.1995s, grad.norm=11.70048618
 13985: 10 [  715/ 1327], train_loss/perplexity = 4.20378208/66.9390182 secs/batch = 0.1991s, grad.norm=12.64437962
 13990: 10 [  720/ 1327], train_loss/perplexity = 4.19510555/66.3607330 secs/batch = 0.1991s, grad.norm=12.36209106
 13995: 10 [  725/ 1327], train_loss/perplexity = 4.14592743/63.1761856 secs/batch = 0.1986s, grad.norm=12.39686584
 14000: 10 [  730/ 1327], train_loss/perplexity = 4.33795357/76.5507202 secs/batch = 0.2004s, grad.norm=12.44573593
 14005: 10 [  735/ 1327], train_loss/perplexity = 4.52707195/92.4873581 secs/batch = 0.2000s, grad.norm=12.43918705
 14010: 10 [  740/ 1327], train_loss/perplexity = 3.90001678/49.4032784 secs/batch = 0.1997s, grad.norm=11.52160931
 14015: 10 [  745/ 1327], train_loss/perplexity = 4.42976761/83.9119110 secs/batch = 0.2002s, grad.norm=11.91713619
 14020: 10 [  750/ 1327], train_loss/perplexity = 4.15248966/63.5921249 secs/batch = 0.1988s, grad.norm=11.56745529
 14025: 10 [  755/ 1327], train_loss/perplexity = 4.11987686/61.5516624 secs/batch = 0.1990s, grad.norm=11.77266407
 14030: 10 [  760/ 1327], train_loss/perplexity = 3.97038627/53.0050011 secs/batch = 0.1991s, grad.norm=11.20467758
 14035: 10 [  765/ 1327], train_loss/perplexity = 4.07856750/59.0608063 secs/batch = 0.1991s, grad.norm=11.63972855
 14040: 10 [  770/ 1327], train_loss/perplexity = 4.00907707/55.0959969 secs/batch = 0.1947s, grad.norm=11.54610825
 14045: 10 [  775/ 1327], train_loss/perplexity = 4.17254305/64.8802338 secs/batch = 0.1981s, grad.norm=12.95195484
 14050: 10 [  780/ 1327], train_loss/perplexity = 4.54389763/94.0566864 secs/batch = 0.1990s, grad.norm=11.90084267
 14055: 10 [  785/ 1327], train_loss/perplexity = 4.32475185/75.5467606 secs/batch = 0.1992s, grad.norm=12.60826588
 14060: 10 [  790/ 1327], train_loss/perplexity = 4.06777763/58.4269714 secs/batch = 0.1993s, grad.norm=12.13754559
 14065: 10 [  795/ 1327], train_loss/perplexity = 4.45700741/86.2290726 secs/batch = 0.1995s, grad.norm=13.69683170
 14070: 10 [  800/ 1327], train_loss/perplexity = 4.38215399/80.0101852 secs/batch = 0.1990s, grad.norm=12.38875389
 14075: 10 [  805/ 1327], train_loss/perplexity = 4.77254152/118.2193146 secs/batch = 0.1986s, grad.norm=12.60701275
 14080: 10 [  810/ 1327], train_loss/perplexity = 4.30099297/73.7730103 secs/batch = 0.1990s, grad.norm=11.63026714
 14085: 10 [  815/ 1327], train_loss/perplexity = 4.16099644/64.1353989 secs/batch = 0.1998s, grad.norm=11.24207783
 14090: 10 [  820/ 1327], train_loss/perplexity = 4.03629112/56.6159706 secs/batch = 0.1983s, grad.norm=11.42069435
 14095: 10 [  825/ 1327], train_loss/perplexity = 4.22572613/68.4241714 secs/batch = 0.1991s, grad.norm=12.20280647
 14100: 10 [  830/ 1327], train_loss/perplexity = 3.98946738/54.0261078 secs/batch = 0.1979s, grad.norm=12.02590656
 14105: 10 [  835/ 1327], train_loss/perplexity = 4.28795099/72.8171158 secs/batch = 0.1970s, grad.norm=11.96907520
 14110: 10 [  840/ 1327], train_loss/perplexity = 4.37187958/79.1923370 secs/batch = 0.1990s, grad.norm=11.78538895
 14115: 10 [  845/ 1327], train_loss/perplexity = 4.22339058/68.2645493 secs/batch = 0.1986s, grad.norm=12.48215675
 14120: 10 [  850/ 1327], train_loss/perplexity = 4.18253851/65.5319977 secs/batch = 0.1975s, grad.norm=11.59655666
 14125: 10 [  855/ 1327], train_loss/perplexity = 4.25834084/70.6925964 secs/batch = 0.1989s, grad.norm=12.12964058
 14130: 10 [  860/ 1327], train_loss/perplexity = 3.98608232/53.8435326 secs/batch = 0.1993s, grad.norm=11.24076462
 14135: 10 [  865/ 1327], train_loss/perplexity = 4.42172718/83.2399292 secs/batch = 0.1995s, grad.norm=11.86286736
 14140: 10 [  870/ 1327], train_loss/perplexity = 4.26967287/71.4982452 secs/batch = 0.1984s, grad.norm=12.15104771
 14145: 10 [  875/ 1327], train_loss/perplexity = 3.94041419/51.4399033 secs/batch = 0.1989s, grad.norm=11.81698036
 14150: 10 [  880/ 1327], train_loss/perplexity = 4.18499947/65.6934662 secs/batch = 0.2005s, grad.norm=11.27427387
 14155: 10 [  885/ 1327], train_loss/perplexity = 4.30465460/74.0436325 secs/batch = 0.1988s, grad.norm=11.31000805
 14160: 10 [  890/ 1327], train_loss/perplexity = 4.39225626/80.8225708 secs/batch = 0.1937s, grad.norm=11.33023834
 14165: 10 [  895/ 1327], train_loss/perplexity = 4.38344622/80.1136475 secs/batch = 0.1991s, grad.norm=11.38907528
 14170: 10 [  900/ 1327], train_loss/perplexity = 4.25355244/70.3548965 secs/batch = 0.1984s, grad.norm=11.48728943
 14175: 10 [  905/ 1327], train_loss/perplexity = 4.17458963/65.0131531 secs/batch = 0.1997s, grad.norm=11.35049820
 14180: 10 [  910/ 1327], train_loss/perplexity = 4.24840355/69.9935837 secs/batch = 0.1994s, grad.norm=11.34729004
 14185: 10 [  915/ 1327], train_loss/perplexity = 4.37548447/79.4783325 secs/batch = 0.1991s, grad.norm=10.98390293
 14190: 10 [  920/ 1327], train_loss/perplexity = 4.61380577/100.8672943 secs/batch = 0.1987s, grad.norm=11.43563461
 14195: 10 [  925/ 1327], train_loss/perplexity = 4.43805599/84.6102982 secs/batch = 0.1992s, grad.norm=12.45199299
 14200: 10 [  930/ 1327], train_loss/perplexity = 4.35209179/77.6407013 secs/batch = 0.1943s, grad.norm=11.03702068
 14205: 10 [  935/ 1327], train_loss/perplexity = 4.42871237/83.8234100 secs/batch = 0.2003s, grad.norm=11.66723347
 14210: 10 [  940/ 1327], train_loss/perplexity = 4.41033602/82.2971115 secs/batch = 0.1998s, grad.norm=11.23581600
 14215: 10 [  945/ 1327], train_loss/perplexity = 4.57337475/96.8704758 secs/batch = 0.2000s, grad.norm=11.52227974
 14220: 10 [  950/ 1327], train_loss/perplexity = 4.36379957/78.5550461 secs/batch = 0.1988s, grad.norm=11.41204357
 14225: 10 [  955/ 1327], train_loss/perplexity = 4.34010363/76.7154922 secs/batch = 0.2007s, grad.norm=11.51111984
 14230: 10 [  960/ 1327], train_loss/perplexity = 4.66895008/106.5857773 secs/batch = 0.1987s, grad.norm=11.78472710
 14235: 10 [  965/ 1327], train_loss/perplexity = 4.34753418/77.2876511 secs/batch = 0.1931s, grad.norm=11.36673355
 14240: 10 [  970/ 1327], train_loss/perplexity = 4.58437729/97.9421768 secs/batch = 0.2001s, grad.norm=11.56993961
 14245: 10 [  975/ 1327], train_loss/perplexity = 4.28398371/72.5288010 secs/batch = 0.1977s, grad.norm=12.61380768
 14250: 10 [  980/ 1327], train_loss/perplexity = 4.16268921/64.2440567 secs/batch = 0.1999s, grad.norm=11.89841461
 14255: 10 [  985/ 1327], train_loss/perplexity = 4.26657248/71.2769165 secs/batch = 0.1986s, grad.norm=12.09034443
 14260: 10 [  990/ 1327], train_loss/perplexity = 4.43848801/84.6468582 secs/batch = 0.1997s, grad.norm=11.79971886
 14265: 10 [  995/ 1327], train_loss/perplexity = 4.50743771/90.6891479 secs/batch = 0.1938s, grad.norm=11.49464321
 14270: 10 [ 1000/ 1327], train_loss/perplexity = 3.94437504/51.6440506 secs/batch = 0.1992s, grad.norm=11.61289597
 14275: 10 [ 1005/ 1327], train_loss/perplexity = 4.45241976/85.8343887 secs/batch = 0.2000s, grad.norm=12.18971062
 14280: 10 [ 1010/ 1327], train_loss/perplexity = 4.10074329/60.3851547 secs/batch = 0.1996s, grad.norm=10.91142941
 14285: 10 [ 1015/ 1327], train_loss/perplexity = 4.58169746/97.6800613 secs/batch = 0.2006s, grad.norm=11.55803967
 14290: 10 [ 1020/ 1327], train_loss/perplexity = 4.60844421/100.3279419 secs/batch = 0.1997s, grad.norm=11.88537121
 14295: 10 [ 1025/ 1327], train_loss/perplexity = 4.46659660/87.0599213 secs/batch = 0.2001s, grad.norm=11.67825413
 14300: 10 [ 1030/ 1327], train_loss/perplexity = 4.29935789/73.6524887 secs/batch = 0.1997s, grad.norm=11.23099327
 14305: 10 [ 1035/ 1327], train_loss/perplexity = 4.19662476/66.4616241 secs/batch = 0.2002s, grad.norm=11.22465801
 14310: 10 [ 1040/ 1327], train_loss/perplexity = 4.55768728/95.3626785 secs/batch = 0.2007s, grad.norm=11.91938114
 14315: 10 [ 1045/ 1327], train_loss/perplexity = 4.01327229/55.3276215 secs/batch = 0.1995s, grad.norm=11.37505531
 14320: 10 [ 1050/ 1327], train_loss/perplexity = 4.07478046/58.8375626 secs/batch = 0.1998s, grad.norm=12.22082996
 14325: 10 [ 1055/ 1327], train_loss/perplexity = 4.22329092/68.2577438 secs/batch = 0.1996s, grad.norm=11.85416985
 14330: 10 [ 1060/ 1327], train_loss/perplexity = 3.83630776/46.3540077 secs/batch = 0.2002s, grad.norm=11.74930954
 14335: 10 [ 1065/ 1327], train_loss/perplexity = 4.01589584/55.4729691 secs/batch = 0.2001s, grad.norm=11.98320198
 14340: 10 [ 1070/ 1327], train_loss/perplexity = 4.29473686/73.3129196 secs/batch = 0.1989s, grad.norm=11.86383152
 14345: 10 [ 1075/ 1327], train_loss/perplexity = 4.05714178/57.8088455 secs/batch = 0.2005s, grad.norm=11.73102379
 14350: 10 [ 1080/ 1327], train_loss/perplexity = 4.04363108/57.0330582 secs/batch = 0.1989s, grad.norm=11.66101742
 14355: 10 [ 1085/ 1327], train_loss/perplexity = 3.89113951/48.9666519 secs/batch = 0.1986s, grad.norm=12.36256313
 14360: 10 [ 1090/ 1327], train_loss/perplexity = 4.09771252/60.2024193 secs/batch = 0.2002s, grad.norm=12.30254650
 14365: 10 [ 1095/ 1327], train_loss/perplexity = 4.32607365/75.6466904 secs/batch = 0.1940s, grad.norm=12.60085964
 14370: 10 [ 1100/ 1327], train_loss/perplexity = 4.00316525/54.7712402 secs/batch = 0.2016s, grad.norm=12.59555626
 14375: 10 [ 1105/ 1327], train_loss/perplexity = 3.93542838/51.1840706 secs/batch = 0.2000s, grad.norm=12.06176281
 14380: 10 [ 1110/ 1327], train_loss/perplexity = 4.33493710/76.3201599 secs/batch = 0.1973s, grad.norm=13.97491264
 14385: 10 [ 1115/ 1327], train_loss/perplexity = 4.03115177/56.3257484 secs/batch = 0.1986s, grad.norm=11.48118401
 14390: 10 [ 1120/ 1327], train_loss/perplexity = 4.25656128/70.5669022 secs/batch = 0.1996s, grad.norm=11.53494167
 14395: 10 [ 1125/ 1327], train_loss/perplexity = 4.47377491/87.6871109 secs/batch = 0.2005s, grad.norm=12.72924328
 14400: 10 [ 1130/ 1327], train_loss/perplexity = 4.14777470/63.2929955 secs/batch = 0.2006s, grad.norm=11.47180271
 14405: 10 [ 1135/ 1327], train_loss/perplexity = 4.14243126/62.9556961 secs/batch = 0.1948s, grad.norm=11.46805668
 14410: 10 [ 1140/ 1327], train_loss/perplexity = 4.48753166/88.9017334 secs/batch = 0.1943s, grad.norm=12.53034306
 14415: 10 [ 1145/ 1327], train_loss/perplexity = 4.26519108/71.1785202 secs/batch = 0.2000s, grad.norm=11.28688431
 14420: 10 [ 1150/ 1327], train_loss/perplexity = 4.21409225/67.6327438 secs/batch = 0.1997s, grad.norm=12.45935059
 14425: 10 [ 1155/ 1327], train_loss/perplexity = 4.30769730/74.2692719 secs/batch = 0.2002s, grad.norm=12.00464821
 14430: 10 [ 1160/ 1327], train_loss/perplexity = 4.23491907/69.0560913 secs/batch = 0.2003s, grad.norm=11.75818920
 14435: 10 [ 1165/ 1327], train_loss/perplexity = 4.30426550/74.0148315 secs/batch = 0.1995s, grad.norm=12.01365948
 14440: 10 [ 1170/ 1327], train_loss/perplexity = 4.16286325/64.2552414 secs/batch = 0.2019s, grad.norm=11.58542061
 14445: 10 [ 1175/ 1327], train_loss/perplexity = 3.95083427/51.9787140 secs/batch = 0.2004s, grad.norm=11.55313206
 14450: 10 [ 1180/ 1327], train_loss/perplexity = 4.05144691/57.4805679 secs/batch = 0.2001s, grad.norm=12.08056068
 14455: 10 [ 1185/ 1327], train_loss/perplexity = 4.13609600/62.5581169 secs/batch = 0.1994s, grad.norm=11.62626266
 14460: 10 [ 1190/ 1327], train_loss/perplexity = 4.20773745/67.2043152 secs/batch = 0.1993s, grad.norm=12.39428043
 14465: 10 [ 1195/ 1327], train_loss/perplexity = 4.01762009/55.5686989 secs/batch = 0.1995s, grad.norm=11.50030804
 14470: 10 [ 1200/ 1327], train_loss/perplexity = 3.96181393/52.5525665 secs/batch = 0.1992s, grad.norm=11.75618744
 14475: 10 [ 1205/ 1327], train_loss/perplexity = 4.04658556/57.2018127 secs/batch = 0.1989s, grad.norm=12.45402813
 14480: 10 [ 1210/ 1327], train_loss/perplexity = 3.71411800/41.0223885 secs/batch = 0.2000s, grad.norm=13.96086311
 14485: 10 [ 1215/ 1327], train_loss/perplexity = 3.89896870/49.3515282 secs/batch = 0.1926s, grad.norm=11.76293278
 14490: 10 [ 1220/ 1327], train_loss/perplexity = 4.10007381/60.3447418 secs/batch = 0.2007s, grad.norm=12.40656567
 14495: 10 [ 1225/ 1327], train_loss/perplexity = 3.88335156/48.5867844 secs/batch = 0.1994s, grad.norm=12.94248104
 14500: 10 [ 1230/ 1327], train_loss/perplexity = 4.12781429/62.0421677 secs/batch = 0.1994s, grad.norm=11.83420944
 14505: 10 [ 1235/ 1327], train_loss/perplexity = 4.13019276/62.1899109 secs/batch = 0.2001s, grad.norm=12.14352894
 14510: 10 [ 1240/ 1327], train_loss/perplexity = 4.22966003/68.6938782 secs/batch = 0.1988s, grad.norm=12.96702576
 14515: 10 [ 1245/ 1327], train_loss/perplexity = 4.17018652/64.7275238 secs/batch = 0.1997s, grad.norm=11.52851295
 14520: 10 [ 1250/ 1327], train_loss/perplexity = 4.23199463/68.8544312 secs/batch = 0.2006s, grad.norm=11.60715771
 14525: 10 [ 1255/ 1327], train_loss/perplexity = 4.29292011/73.1798477 secs/batch = 0.1983s, grad.norm=11.43385220
 14530: 10 [ 1260/ 1327], train_loss/perplexity = 4.10247707/60.4899406 secs/batch = 0.2003s, grad.norm=12.82588005
 14535: 10 [ 1265/ 1327], train_loss/perplexity = 4.31188869/74.5812149 secs/batch = 0.2004s, grad.norm=12.57363892
 14540: 10 [ 1270/ 1327], train_loss/perplexity = 4.07321453/58.7454987 secs/batch = 0.1992s, grad.norm=12.49421501
 14545: 10 [ 1275/ 1327], train_loss/perplexity = 4.19858217/66.5918503 secs/batch = 0.1997s, grad.norm=12.42669678
 14550: 10 [ 1280/ 1327], train_loss/perplexity = 4.05966377/57.9548225 secs/batch = 0.2002s, grad.norm=12.11892605
 14555: 10 [ 1285/ 1327], train_loss/perplexity = 3.98841381/53.9692154 secs/batch = 0.1954s, grad.norm=12.36437798
 14560: 10 [ 1290/ 1327], train_loss/perplexity = 4.23259735/68.8959503 secs/batch = 0.2001s, grad.norm=11.72200775
 14565: 10 [ 1295/ 1327], train_loss/perplexity = 4.25148487/70.2095871 secs/batch = 0.1990s, grad.norm=11.76535702
 14570: 10 [ 1300/ 1327], train_loss/perplexity = 4.40308762/81.7027435 secs/batch = 0.1997s, grad.norm=12.53573608
 14575: 10 [ 1305/ 1327], train_loss/perplexity = 4.50506639/90.4743500 secs/batch = 0.1996s, grad.norm=12.78357792
 14580: 10 [ 1310/ 1327], train_loss/perplexity = 4.67972660/107.7406158 secs/batch = 0.1992s, grad.norm=12.43337727
 14585: 10 [ 1315/ 1327], train_loss/perplexity = 4.53080940/92.8336716 secs/batch = 0.1988s, grad.norm=12.49376869
 14590: 10 [ 1320/ 1327], train_loss/perplexity = 4.53651953/93.3652802 secs/batch = 0.1989s, grad.norm=12.05467796
 14595: 10 [ 1325/ 1327], train_loss/perplexity = 4.46575308/86.9865112 secs/batch = 0.2004s, grad.norm=12.01130295
Epoch training time: 263.69786190986633
	> validation loss = 4.83689165, perplexity = 126.07685089
	> validation loss = 4.73057079, perplexity = 113.36025238
	> validation loss = 4.69462776, perplexity = 109.35809326
	> validation loss = 4.70816660, perplexity = 110.84873962
	> validation loss = 4.87899637, perplexity = 131.49862671
	> validation loss = 4.75001240, perplexity = 115.58571625
	> validation loss = 4.76206589, perplexity = 116.98735809
	> validation loss = 4.55816364, perplexity = 95.40811920
	> validation loss = 4.39914322, perplexity = 81.38111115
	> validation loss = 4.49117708, perplexity = 89.22640991
	> validation loss = 4.61824274, perplexity = 101.31583405
	> validation loss = 4.71564388, perplexity = 111.68069458
	> validation loss = 4.60227680, perplexity = 99.71108246
	> validation loss = 4.46127319, perplexity = 86.59769440
	> validation loss = 4.33360529, perplexity = 76.21858215
	> validation loss = 4.34207726, perplexity = 76.86704254
	> validation loss = 4.77388287, perplexity = 118.37799835
	> validation loss = 4.41309690, perplexity = 82.52463531
	> validation loss = 4.84570694, perplexity = 127.19316864
	> validation loss = 4.67619658, perplexity = 107.36095428
	> validation loss = 4.49154758, perplexity = 89.25947571
at the end of epoch: 10
train loss = 4.35618161, perplexity = 77.95888755
validation loss = 4.62595034, perplexity = 102.09975611
Saved model cv/epoch010_4.6260.model
 14602: 11 [    5/ 1327], train_loss/perplexity = 4.50372648/90.3532028 secs/batch = 0.1995s, grad.norm=12.55820084
 14607: 11 [   10/ 1327], train_loss/perplexity = 4.06860924/58.4755821 secs/batch = 0.2001s, grad.norm=11.31226444
 14612: 11 [   15/ 1327], train_loss/perplexity = 4.41450930/82.6412811 secs/batch = 0.1993s, grad.norm=10.98189163
 14617: 11 [   20/ 1327], train_loss/perplexity = 4.56976795/96.5217133 secs/batch = 0.1985s, grad.norm=12.02790546
 14622: 11 [   25/ 1327], train_loss/perplexity = 4.44357014/85.0781403 secs/batch = 0.1988s, grad.norm=12.11420250
 14627: 11 [   30/ 1327], train_loss/perplexity = 4.46361780/86.8009720 secs/batch = 0.2000s, grad.norm=11.97706032
 14632: 11 [   35/ 1327], train_loss/perplexity = 4.29360628/73.2300797 secs/batch = 0.1990s, grad.norm=12.44114494
 14637: 11 [   40/ 1327], train_loss/perplexity = 4.25330210/70.3372879 secs/batch = 0.1917s, grad.norm=11.98206902
 14642: 11 [   45/ 1327], train_loss/perplexity = 4.05119276/57.4659576 secs/batch = 0.1987s, grad.norm=11.18354893
 14647: 11 [   50/ 1327], train_loss/perplexity = 4.29716825/73.4913864 secs/batch = 0.1990s, grad.norm=11.62717915
 14652: 11 [   55/ 1327], train_loss/perplexity = 4.26004601/70.8132401 secs/batch = 0.1992s, grad.norm=11.91607571
 14657: 11 [   60/ 1327], train_loss/perplexity = 4.50253582/90.2456894 secs/batch = 0.1978s, grad.norm=12.56883621
 14662: 11 [   65/ 1327], train_loss/perplexity = 4.14557457/63.1538963 secs/batch = 0.1965s, grad.norm=12.30211449
 14667: 11 [   70/ 1327], train_loss/perplexity = 3.93797803/51.3147392 secs/batch = 0.1996s, grad.norm=12.37324715
 14672: 11 [   75/ 1327], train_loss/perplexity = 3.78147578/43.8807526 secs/batch = 0.1988s, grad.norm=11.06746006
 14677: 11 [   80/ 1327], train_loss/perplexity = 4.13868332/62.7201843 secs/batch = 0.2002s, grad.norm=12.17770386
 14682: 11 [   85/ 1327], train_loss/perplexity = 4.30881023/74.3519745 secs/batch = 0.1995s, grad.norm=12.25734711
 14687: 11 [   90/ 1327], train_loss/perplexity = 4.27200747/71.6653595 secs/batch = 0.1998s, grad.norm=12.07680321
 14692: 11 [   95/ 1327], train_loss/perplexity = 4.15068150/63.4772453 secs/batch = 0.1992s, grad.norm=11.64538002
 14697: 11 [  100/ 1327], train_loss/perplexity = 4.46348286/86.7892609 secs/batch = 0.1997s, grad.norm=12.29350662
 14702: 11 [  105/ 1327], train_loss/perplexity = 4.33836889/76.5825195 secs/batch = 0.1998s, grad.norm=12.92841339
 14707: 11 [  110/ 1327], train_loss/perplexity = 4.14238644/62.9528770 secs/batch = 0.1993s, grad.norm=11.70782471
 14712: 11 [  115/ 1327], train_loss/perplexity = 4.12928963/62.1337700 secs/batch = 0.1997s, grad.norm=11.96399689
 14717: 11 [  120/ 1327], train_loss/perplexity = 4.25343990/70.3469849 secs/batch = 0.2009s, grad.norm=12.44805908
 14722: 11 [  125/ 1327], train_loss/perplexity = 4.31178474/74.5734634 secs/batch = 0.1997s, grad.norm=12.15028000
 14727: 11 [  130/ 1327], train_loss/perplexity = 4.21977282/68.0180283 secs/batch = 0.1998s, grad.norm=12.53001785
 14732: 11 [  135/ 1327], train_loss/perplexity = 4.18576336/65.7436676 secs/batch = 0.2009s, grad.norm=11.99826717
 14737: 11 [  140/ 1327], train_loss/perplexity = 4.49542904/89.6066055 secs/batch = 0.2006s, grad.norm=12.19617939
 14742: 11 [  145/ 1327], train_loss/perplexity = 4.36375809/78.5517883 secs/batch = 0.2005s, grad.norm=13.62001133
 14747: 11 [  150/ 1327], train_loss/perplexity = 4.38332462/80.1039047 secs/batch = 0.2000s, grad.norm=12.52517891
 14752: 11 [  155/ 1327], train_loss/perplexity = 4.59851742/99.3369293 secs/batch = 0.2004s, grad.norm=11.90682793
 14757: 11 [  160/ 1327], train_loss/perplexity = 4.27758121/72.0659180 secs/batch = 0.2000s, grad.norm=11.44550705
 14762: 11 [  165/ 1327], train_loss/perplexity = 4.47758198/88.0215759 secs/batch = 0.1993s, grad.norm=11.69630814
 14767: 11 [  170/ 1327], train_loss/perplexity = 4.19742823/66.5150528 secs/batch = 0.1996s, grad.norm=11.36284542
 14772: 11 [  175/ 1327], train_loss/perplexity = 4.59571505/99.0589447 secs/batch = 0.1944s, grad.norm=11.74445152
 14777: 11 [  180/ 1327], train_loss/perplexity = 4.37632561/79.5452118 secs/batch = 0.1985s, grad.norm=11.74325848
 14782: 11 [  185/ 1327], train_loss/perplexity = 4.64918613/104.4999008 secs/batch = 0.2001s, grad.norm=11.82037735
 14787: 11 [  190/ 1327], train_loss/perplexity = 4.15669060/63.8598328 secs/batch = 0.1989s, grad.norm=11.17695141
 14792: 11 [  195/ 1327], train_loss/perplexity = 4.48152447/88.3692856 secs/batch = 0.1996s, grad.norm=12.05759621
 14797: 11 [  200/ 1327], train_loss/perplexity = 4.34232426/76.8860321 secs/batch = 0.1982s, grad.norm=12.17864990
 14802: 11 [  205/ 1327], train_loss/perplexity = 4.58796597/98.2942963 secs/batch = 0.2000s, grad.norm=11.67520714
 14807: 11 [  210/ 1327], train_loss/perplexity = 4.37950134/79.7982330 secs/batch = 0.1997s, grad.norm=11.27816105
 14812: 11 [  215/ 1327], train_loss/perplexity = 4.58096981/97.6090088 secs/batch = 0.1994s, grad.norm=12.00380230
 14817: 11 [  220/ 1327], train_loss/perplexity = 4.41410303/82.6077118 secs/batch = 0.1998s, grad.norm=11.83259487
 14822: 11 [  225/ 1327], train_loss/perplexity = 4.65726089/105.3471298 secs/batch = 0.1994s, grad.norm=11.88642979
 14827: 11 [  230/ 1327], train_loss/perplexity = 4.53565598/93.2846909 secs/batch = 0.2001s, grad.norm=11.97829342
 14832: 11 [  235/ 1327], train_loss/perplexity = 4.34601068/77.1699905 secs/batch = 0.1996s, grad.norm=11.96425247
 14837: 11 [  240/ 1327], train_loss/perplexity = 4.15290117/63.6183014 secs/batch = 0.1997s, grad.norm=12.23158741
 14842: 11 [  245/ 1327], train_loss/perplexity = 4.43490505/84.3441162 secs/batch = 0.1996s, grad.norm=11.34153748
 14847: 11 [  250/ 1327], train_loss/perplexity = 4.15587902/63.8080292 secs/batch = 0.1996s, grad.norm=11.02410316
 14852: 11 [  255/ 1327], train_loss/perplexity = 4.23351765/68.9593811 secs/batch = 0.1991s, grad.norm=11.34617329
 14857: 11 [  260/ 1327], train_loss/perplexity = 4.43347645/84.2237091 secs/batch = 0.1999s, grad.norm=12.35634327
 14862: 11 [  265/ 1327], train_loss/perplexity = 4.57390404/96.9217606 secs/batch = 0.1992s, grad.norm=11.26792145
 14867: 11 [  270/ 1327], train_loss/perplexity = 4.67461967/107.1917953 secs/batch = 0.1954s, grad.norm=11.70858002
 14872: 11 [  275/ 1327], train_loss/perplexity = 4.63182354/102.7011719 secs/batch = 0.2009s, grad.norm=11.66992474
 14877: 11 [  280/ 1327], train_loss/perplexity = 4.43192434/84.0930862 secs/batch = 0.1992s, grad.norm=11.46475792
 14882: 11 [  285/ 1327], train_loss/perplexity = 4.73731565/114.1274338 secs/batch = 0.1989s, grad.norm=11.38721561
 14887: 11 [  290/ 1327], train_loss/perplexity = 4.44880199/85.5244217 secs/batch = 0.1994s, grad.norm=12.05112553
 14892: 11 [  295/ 1327], train_loss/perplexity = 4.23084497/68.7753220 secs/batch = 0.1955s, grad.norm=11.38306141
 14897: 11 [  300/ 1327], train_loss/perplexity = 3.79915118/44.6632576 secs/batch = 0.2000s, grad.norm=10.87716961
 14902: 11 [  305/ 1327], train_loss/perplexity = 4.29976749/73.6826630 secs/batch = 0.1999s, grad.norm=11.33079529
 14907: 11 [  310/ 1327], train_loss/perplexity = 4.22242737/68.1988297 secs/batch = 0.1999s, grad.norm=11.81867313
 14912: 11 [  315/ 1327], train_loss/perplexity = 3.81117439/45.2034950 secs/batch = 0.1939s, grad.norm=11.35233402
 14917: 11 [  320/ 1327], train_loss/perplexity = 3.82378316/45.7770615 secs/batch = 0.1991s, grad.norm=11.79243183
 14922: 11 [  325/ 1327], train_loss/perplexity = 3.81855941/45.5385590 secs/batch = 0.1997s, grad.norm=10.95362663
 14927: 11 [  330/ 1327], train_loss/perplexity = 4.39039278/80.6720963 secs/batch = 0.1995s, grad.norm=11.60107327
 14932: 11 [  335/ 1327], train_loss/perplexity = 3.75619984/42.7855263 secs/batch = 0.1993s, grad.norm=11.08559608
 14937: 11 [  340/ 1327], train_loss/perplexity = 4.56490278/96.0532532 secs/batch = 0.1941s, grad.norm=11.52442455
 14942: 11 [  345/ 1327], train_loss/perplexity = 4.36016321/78.2699051 secs/batch = 0.1998s, grad.norm=11.33704758
 14947: 11 [  350/ 1327], train_loss/perplexity = 4.36118221/78.3497086 secs/batch = 0.1996s, grad.norm=12.20548153
 14952: 11 [  355/ 1327], train_loss/perplexity = 4.36306190/78.4971161 secs/batch = 0.2002s, grad.norm=12.11373615
 14957: 11 [  360/ 1327], train_loss/perplexity = 4.47802067/88.0601959 secs/batch = 0.1995s, grad.norm=12.52605629
 14962: 11 [  365/ 1327], train_loss/perplexity = 4.39227343/80.8239594 secs/batch = 0.2012s, grad.norm=11.61513138
 14967: 11 [  370/ 1327], train_loss/perplexity = 4.50377464/90.3575592 secs/batch = 0.1989s, grad.norm=12.42672729
 14972: 11 [  375/ 1327], train_loss/perplexity = 3.98044324/53.5407600 secs/batch = 0.2004s, grad.norm=11.65493965
 14977: 11 [  380/ 1327], train_loss/perplexity = 4.03635120/56.6193733 secs/batch = 0.1990s, grad.norm=11.83749294
 14982: 11 [  385/ 1327], train_loss/perplexity = 4.20099449/66.7526855 secs/batch = 0.1999s, grad.norm=12.60417938
 14987: 11 [  390/ 1327], train_loss/perplexity = 4.37715483/79.6112061 secs/batch = 0.2003s, grad.norm=11.82452869
 14992: 11 [  395/ 1327], train_loss/perplexity = 4.43826771/84.6282120 secs/batch = 0.1996s, grad.norm=12.69600773
 14997: 11 [  400/ 1327], train_loss/perplexity = 4.33892918/76.6254425 secs/batch = 0.1999s, grad.norm=12.28593731
 15002: 11 [  405/ 1327], train_loss/perplexity = 4.62693739/102.2005844 secs/batch = 0.1983s, grad.norm=12.26597309
 15007: 11 [  410/ 1327], train_loss/perplexity = 4.31506252/74.8182983 secs/batch = 0.1999s, grad.norm=11.56558895
 15012: 11 [  415/ 1327], train_loss/perplexity = 4.19939995/66.6463242 secs/batch = 0.1994s, grad.norm=12.26204872
 15017: 11 [  420/ 1327], train_loss/perplexity = 3.86699867/47.7987099 secs/batch = 0.2000s, grad.norm=11.85714340
 15022: 11 [  425/ 1327], train_loss/perplexity = 4.12045813/61.5874519 secs/batch = 0.2004s, grad.norm=12.92518425
 15027: 11 [  430/ 1327], train_loss/perplexity = 4.40886974/82.1765289 secs/batch = 0.2001s, grad.norm=12.17811871
 15032: 11 [  435/ 1327], train_loss/perplexity = 4.43490839/84.3443985 secs/batch = 0.1996s, grad.norm=13.11560059
 15037: 11 [  440/ 1327], train_loss/perplexity = 3.99580812/54.3697586 secs/batch = 0.2002s, grad.norm=12.37452126
 15042: 11 [  445/ 1327], train_loss/perplexity = 4.34807253/77.3292694 secs/batch = 0.1993s, grad.norm=11.99568653
 15047: 11 [  450/ 1327], train_loss/perplexity = 4.26321316/71.0378723 secs/batch = 0.2006s, grad.norm=11.90636063
 15052: 11 [  455/ 1327], train_loss/perplexity = 4.25168753/70.2238159 secs/batch = 0.1955s, grad.norm=11.70815182
 15057: 11 [  460/ 1327], train_loss/perplexity = 4.19745588/66.5168915 secs/batch = 0.1999s, grad.norm=12.39476776
 15062: 11 [  465/ 1327], train_loss/perplexity = 3.96038532/52.4775429 secs/batch = 0.1944s, grad.norm=13.28206444
 15067: 11 [  470/ 1327], train_loss/perplexity = 4.64507771/104.0714493 secs/batch = 0.1934s, grad.norm=11.78337383
 15072: 11 [  475/ 1327], train_loss/perplexity = 4.04964352/57.3769989 secs/batch = 0.1994s, grad.norm=11.63787746
 15077: 11 [  480/ 1327], train_loss/perplexity = 4.22026825/68.0517349 secs/batch = 0.1995s, grad.norm=12.38213730
 15082: 11 [  485/ 1327], train_loss/perplexity = 4.20726967/67.1728821 secs/batch = 0.2001s, grad.norm=11.98033524
 15087: 11 [  490/ 1327], train_loss/perplexity = 4.17039967/64.7413254 secs/batch = 0.1941s, grad.norm=13.24434948
 15092: 11 [  495/ 1327], train_loss/perplexity = 4.15255308/63.5961609 secs/batch = 0.1989s, grad.norm=11.69712830
 15097: 11 [  500/ 1327], train_loss/perplexity = 4.39361238/80.9322510 secs/batch = 0.1995s, grad.norm=13.39010906
 15102: 11 [  505/ 1327], train_loss/perplexity = 4.44555426/85.2471161 secs/batch = 0.2008s, grad.norm=11.26190472
 15107: 11 [  510/ 1327], train_loss/perplexity = 4.77614260/118.6458054 secs/batch = 0.1999s, grad.norm=11.16858006
 15112: 11 [  515/ 1327], train_loss/perplexity = 4.45108509/85.7199097 secs/batch = 0.1997s, grad.norm=11.44228458
 15117: 11 [  520/ 1327], train_loss/perplexity = 4.56338835/95.9078979 secs/batch = 0.1921s, grad.norm=11.41230488
 15122: 11 [  525/ 1327], train_loss/perplexity = 4.15612984/63.8240356 secs/batch = 0.1994s, grad.norm=11.54074574
 15127: 11 [  530/ 1327], train_loss/perplexity = 4.24599743/69.8253708 secs/batch = 0.1987s, grad.norm=11.99996662
 15132: 11 [  535/ 1327], train_loss/perplexity = 4.32912970/75.8782196 secs/batch = 0.2001s, grad.norm=12.32323456
 15137: 11 [  540/ 1327], train_loss/perplexity = 4.38746834/80.4365234 secs/batch = 0.1997s, grad.norm=11.99937916
 15142: 11 [  545/ 1327], train_loss/perplexity = 4.36611366/78.7370377 secs/batch = 0.1992s, grad.norm=12.49345970
 15147: 11 [  550/ 1327], train_loss/perplexity = 4.35799837/78.1006470 secs/batch = 0.1996s, grad.norm=12.07190228
 15152: 11 [  555/ 1327], train_loss/perplexity = 4.31580591/74.8739395 secs/batch = 0.2006s, grad.norm=12.14931965
 15157: 11 [  560/ 1327], train_loss/perplexity = 4.32581282/75.6269608 secs/batch = 0.1995s, grad.norm=12.52257633
 15162: 11 [  565/ 1327], train_loss/perplexity = 4.21057892/67.3955460 secs/batch = 0.1998s, grad.norm=12.59438610
 15167: 11 [  570/ 1327], train_loss/perplexity = 4.21784830/67.8872528 secs/batch = 0.1997s, grad.norm=12.60301685
 15172: 11 [  575/ 1327], train_loss/perplexity = 3.91257477/50.0275955 secs/batch = 0.1985s, grad.norm=12.16459751
 15177: 11 [  580/ 1327], train_loss/perplexity = 4.42733479/83.7080231 secs/batch = 0.1992s, grad.norm=12.15084553
 15182: 11 [  585/ 1327], train_loss/perplexity = 3.97136641/53.0569801 secs/batch = 0.1931s, grad.norm=12.09522247
 15187: 11 [  590/ 1327], train_loss/perplexity = 4.32907772/75.8742752 secs/batch = 0.1995s, grad.norm=12.17411041
 15192: 11 [  595/ 1327], train_loss/perplexity = 4.24481106/69.7425842 secs/batch = 0.1994s, grad.norm=12.46399212
 15197: 11 [  600/ 1327], train_loss/perplexity = 4.43477535/84.3331757 secs/batch = 0.2000s, grad.norm=11.15695095
 15202: 11 [  605/ 1327], train_loss/perplexity = 4.41633892/82.7926178 secs/batch = 0.1992s, grad.norm=11.72994423
 15207: 11 [  610/ 1327], train_loss/perplexity = 4.57343960/96.8767548 secs/batch = 0.1998s, grad.norm=11.73161697
 15212: 11 [  615/ 1327], train_loss/perplexity = 4.15651321/63.8485069 secs/batch = 0.1996s, grad.norm=11.74857521
 15217: 11 [  620/ 1327], train_loss/perplexity = 4.51645851/91.5109406 secs/batch = 0.1991s, grad.norm=11.59234047
 15222: 11 [  625/ 1327], train_loss/perplexity = 4.51837063/91.6860886 secs/batch = 0.1992s, grad.norm=11.17282867
 15227: 11 [  630/ 1327], train_loss/perplexity = 4.60526085/100.0090637 secs/batch = 0.2004s, grad.norm=11.51566982
 15232: 11 [  635/ 1327], train_loss/perplexity = 4.24962807/70.0793457 secs/batch = 0.1992s, grad.norm=12.04046059
 15237: 11 [  640/ 1327], train_loss/perplexity = 4.28022623/72.2567825 secs/batch = 0.1955s, grad.norm=11.18797016
 15242: 11 [  645/ 1327], train_loss/perplexity = 4.63576984/103.1072617 secs/batch = 0.1988s, grad.norm=13.08277225
 15247: 11 [  650/ 1327], train_loss/perplexity = 4.05478287/57.6726379 secs/batch = 0.1980s, grad.norm=12.13129044
 15252: 11 [  655/ 1327], train_loss/perplexity = 4.15461540/63.7274513 secs/batch = 0.2003s, grad.norm=11.45058346
 15257: 11 [  660/ 1327], train_loss/perplexity = 4.07650375/58.9390450 secs/batch = 0.1977s, grad.norm=11.63001251
 15262: 11 [  665/ 1327], train_loss/perplexity = 4.25565910/70.5032730 secs/batch = 0.2007s, grad.norm=11.78625393
 15267: 11 [  670/ 1327], train_loss/perplexity = 4.21052027/67.3915939 secs/batch = 0.1987s, grad.norm=11.88646793
 15272: 11 [  675/ 1327], train_loss/perplexity = 4.04192352/56.9357529 secs/batch = 0.1994s, grad.norm=12.13890171
 15277: 11 [  680/ 1327], train_loss/perplexity = 4.26407146/71.0988693 secs/batch = 0.2001s, grad.norm=12.67416191
 15282: 11 [  685/ 1327], train_loss/perplexity = 4.09350109/59.9494133 secs/batch = 0.1997s, grad.norm=11.60804462
 15287: 11 [  690/ 1327], train_loss/perplexity = 4.45295858/85.8806534 secs/batch = 0.1950s, grad.norm=11.24908352
 15292: 11 [  695/ 1327], train_loss/perplexity = 4.22923994/68.6650238 secs/batch = 0.1985s, grad.norm=11.52539158
 15297: 11 [  700/ 1327], train_loss/perplexity = 4.56989861/96.5343246 secs/batch = 0.1928s, grad.norm=12.24259377
 15302: 11 [  705/ 1327], train_loss/perplexity = 4.23666000/69.1764145 secs/batch = 0.1995s, grad.norm=11.67579556
 15307: 11 [  710/ 1327], train_loss/perplexity = 4.19789648/66.5462036 secs/batch = 0.1990s, grad.norm=11.66132069
 15312: 11 [  715/ 1327], train_loss/perplexity = 4.13499880/62.4895172 secs/batch = 0.2000s, grad.norm=12.14869881
 15317: 11 [  720/ 1327], train_loss/perplexity = 4.08398247/59.3814850 secs/batch = 0.1996s, grad.norm=12.21921635
 15322: 11 [  725/ 1327], train_loss/perplexity = 4.13147211/62.2695236 secs/batch = 0.1994s, grad.norm=11.73787785
 15327: 11 [  730/ 1327], train_loss/perplexity = 4.32567406/75.6164627 secs/batch = 0.2009s, grad.norm=12.37448788
 15332: 11 [  735/ 1327], train_loss/perplexity = 4.43942404/84.7261276 secs/batch = 0.1994s, grad.norm=12.60588074
 15337: 11 [  740/ 1327], train_loss/perplexity = 3.78197432/43.9026337 secs/batch = 0.1998s, grad.norm=11.81553173
 15342: 11 [  745/ 1327], train_loss/perplexity = 4.37737226/79.6285172 secs/batch = 0.2001s, grad.norm=11.88045979
 15347: 11 [  750/ 1327], train_loss/perplexity = 4.09369802/59.9612198 secs/batch = 0.2005s, grad.norm=11.95358181
 15352: 11 [  755/ 1327], train_loss/perplexity = 4.06601810/58.3242569 secs/batch = 0.1943s, grad.norm=12.15454292
 15357: 11 [  760/ 1327], train_loss/perplexity = 3.88988495/48.9052582 secs/batch = 0.1997s, grad.norm=11.29860020
 15362: 11 [  765/ 1327], train_loss/perplexity = 4.04155684/56.9148827 secs/batch = 0.1985s, grad.norm=11.39962006
 15367: 11 [  770/ 1327], train_loss/perplexity = 3.97445393/53.2210464 secs/batch = 0.1932s, grad.norm=11.86482906
 15372: 11 [  775/ 1327], train_loss/perplexity = 4.05540323/57.7084274 secs/batch = 0.1993s, grad.norm=12.86464214
 15377: 11 [  780/ 1327], train_loss/perplexity = 4.50912380/90.8421860 secs/batch = 0.1988s, grad.norm=12.02530003
 15382: 11 [  785/ 1327], train_loss/perplexity = 4.33820391/76.5698929 secs/batch = 0.1997s, grad.norm=12.95249271
 15387: 11 [  790/ 1327], train_loss/perplexity = 4.01894331/55.6422768 secs/batch = 0.1978s, grad.norm=11.83818340
 15392: 11 [  795/ 1327], train_loss/perplexity = 4.42990780/83.9236755 secs/batch = 0.1992s, grad.norm=12.33691406
 15397: 11 [  800/ 1327], train_loss/perplexity = 4.31612349/74.8977203 secs/batch = 0.2008s, grad.norm=11.85904217
 15402: 11 [  805/ 1327], train_loss/perplexity = 4.65915442/105.5467987 secs/batch = 0.1997s, grad.norm=11.82080936
 15407: 11 [  810/ 1327], train_loss/perplexity = 4.27853394/72.1346054 secs/batch = 0.1993s, grad.norm=11.25724030
 15412: 11 [  815/ 1327], train_loss/perplexity = 4.17552090/65.0737305 secs/batch = 0.1976s, grad.norm=11.68871307
 15417: 11 [  820/ 1327], train_loss/perplexity = 3.98716211/53.9017067 secs/batch = 0.1995s, grad.norm=11.48300266
 15422: 11 [  825/ 1327], train_loss/perplexity = 4.19622421/66.4350128 secs/batch = 0.1998s, grad.norm=12.20409298
 15427: 11 [  830/ 1327], train_loss/perplexity = 3.94827604/51.8459091 secs/batch = 0.1996s, grad.norm=12.05991077
 15432: 11 [  835/ 1327], train_loss/perplexity = 4.23966408/69.3845367 secs/batch = 0.2000s, grad.norm=12.15678883
 15437: 11 [  840/ 1327], train_loss/perplexity = 4.31703186/74.9657898 secs/batch = 0.1992s, grad.norm=12.09338760
 15442: 11 [  845/ 1327], train_loss/perplexity = 4.18662930/65.8006210 secs/batch = 0.1979s, grad.norm=12.32316589
 15447: 11 [  850/ 1327], train_loss/perplexity = 4.17440414/65.0010986 secs/batch = 0.1938s, grad.norm=11.04655170
 15452: 11 [  855/ 1327], train_loss/perplexity = 4.26362991/71.0674820 secs/batch = 0.1996s, grad.norm=12.66245651
 15457: 11 [  860/ 1327], train_loss/perplexity = 3.93872333/51.3529968 secs/batch = 0.2002s, grad.norm=11.62421417
 15462: 11 [  865/ 1327], train_loss/perplexity = 4.36654091/78.7706833 secs/batch = 0.2000s, grad.norm=11.74419022
 15467: 11 [  870/ 1327], train_loss/perplexity = 4.25951004/70.7752991 secs/batch = 0.1993s, grad.norm=11.90411949
 15472: 11 [  875/ 1327], train_loss/perplexity = 3.94662571/51.7604179 secs/batch = 0.1954s, grad.norm=11.87342930
 15477: 11 [  880/ 1327], train_loss/perplexity = 4.13298416/62.3637505 secs/batch = 0.1993s, grad.norm=11.84021854
 15482: 11 [  885/ 1327], train_loss/perplexity = 4.27814245/72.1063766 secs/batch = 0.2000s, grad.norm=11.81829548
 15487: 11 [  890/ 1327], train_loss/perplexity = 4.46932983/87.2982025 secs/batch = 0.1996s, grad.norm=11.83024597
 15492: 11 [  895/ 1327], train_loss/perplexity = 4.43796825/84.6028748 secs/batch = 0.2001s, grad.norm=12.07710171
 15497: 11 [  900/ 1327], train_loss/perplexity = 4.22849560/68.6139297 secs/batch = 0.2005s, grad.norm=11.67199230
 15502: 11 [  905/ 1327], train_loss/perplexity = 4.11325264/61.1452789 secs/batch = 0.1998s, grad.norm=11.24775124
 15507: 11 [  910/ 1327], train_loss/perplexity = 4.13652706/62.5850906 secs/batch = 0.1976s, grad.norm=10.82130146
 15512: 11 [  915/ 1327], train_loss/perplexity = 4.35372829/77.7678680 secs/batch = 0.1986s, grad.norm=11.19304848
 15517: 11 [  920/ 1327], train_loss/perplexity = 4.51495552/91.3735046 secs/batch = 0.1946s, grad.norm=11.81869030
 15522: 11 [  925/ 1327], train_loss/perplexity = 4.33523798/76.3431244 secs/batch = 0.1991s, grad.norm=11.27053261
 15527: 11 [  930/ 1327], train_loss/perplexity = 4.20956230/67.3270645 secs/batch = 0.1995s, grad.norm=11.04309273
 15532: 11 [  935/ 1327], train_loss/perplexity = 4.38922024/80.5775604 secs/batch = 0.1953s, grad.norm=11.15833569
 15537: 11 [  940/ 1327], train_loss/perplexity = 4.35688925/78.0140762 secs/batch = 0.1991s, grad.norm=11.14930153
 15542: 11 [  945/ 1327], train_loss/perplexity = 4.51550579/91.4237976 secs/batch = 0.1928s, grad.norm=11.34601974
 15547: 11 [  950/ 1327], train_loss/perplexity = 4.27288532/71.7282944 secs/batch = 0.1995s, grad.norm=11.39097118
 15552: 11 [  955/ 1327], train_loss/perplexity = 4.34000397/76.7078400 secs/batch = 0.1993s, grad.norm=13.35870075
 15557: 11 [  960/ 1327], train_loss/perplexity = 4.60043526/99.5276260 secs/batch = 0.1945s, grad.norm=11.60298061
 15562: 11 [  965/ 1327], train_loss/perplexity = 4.29234791/73.1379852 secs/batch = 0.1996s, grad.norm=11.55937195
 15567: 11 [  970/ 1327], train_loss/perplexity = 4.50053692/90.0654755 secs/batch = 0.2004s, grad.norm=11.62049961
 15572: 11 [  975/ 1327], train_loss/perplexity = 4.32657003/75.6842499 secs/batch = 0.2001s, grad.norm=12.88003063
 15577: 11 [  980/ 1327], train_loss/perplexity = 4.06330538/58.1662560 secs/batch = 0.1993s, grad.norm=11.94411278
 15582: 11 [  985/ 1327], train_loss/perplexity = 4.23922396/69.3540115 secs/batch = 0.1997s, grad.norm=11.92004395
 15587: 11 [  990/ 1327], train_loss/perplexity = 4.44563484/85.2539825 secs/batch = 0.1998s, grad.norm=11.62407684
 15592: 11 [  995/ 1327], train_loss/perplexity = 4.51490736/91.3691025 secs/batch = 0.2002s, grad.norm=11.20165825
 15597: 11 [ 1000/ 1327], train_loss/perplexity = 3.96799922/52.8786278 secs/batch = 0.1925s, grad.norm=11.61961937
 15602: 11 [ 1005/ 1327], train_loss/perplexity = 4.41306210/82.5217667 secs/batch = 0.1993s, grad.norm=11.61118984
 15607: 11 [ 1010/ 1327], train_loss/perplexity = 4.01206112/55.2606506 secs/batch = 0.2000s, grad.norm=11.01305294
 15612: 11 [ 1015/ 1327], train_loss/perplexity = 4.53106642/92.8575363 secs/batch = 0.1991s, grad.norm=11.42617035
 15617: 11 [ 1020/ 1327], train_loss/perplexity = 4.65103960/104.6937714 secs/batch = 0.1969s, grad.norm=11.68337917
 15622: 11 [ 1025/ 1327], train_loss/perplexity = 4.44413042/85.1258240 secs/batch = 0.1944s, grad.norm=11.26987934
 15627: 11 [ 1030/ 1327], train_loss/perplexity = 4.30179691/73.8323441 secs/batch = 0.1941s, grad.norm=11.30761528
 15632: 11 [ 1035/ 1327], train_loss/perplexity = 4.18137503/65.4557953 secs/batch = 0.1995s, grad.norm=10.95035172
 15637: 11 [ 1040/ 1327], train_loss/perplexity = 4.50755548/90.6998291 secs/batch = 0.1990s, grad.norm=11.76073265
 15642: 11 [ 1045/ 1327], train_loss/perplexity = 4.04128122/56.8991966 secs/batch = 0.1992s, grad.norm=11.01722336
 15647: 11 [ 1050/ 1327], train_loss/perplexity = 4.04245138/56.9658165 secs/batch = 0.1998s, grad.norm=11.14637280
 15652: 11 [ 1055/ 1327], train_loss/perplexity = 4.17562771/65.0806808 secs/batch = 0.1998s, grad.norm=11.87681293
 15657: 11 [ 1060/ 1327], train_loss/perplexity = 3.83858061/46.4594841 secs/batch = 0.1988s, grad.norm=12.61445236
 15662: 11 [ 1065/ 1327], train_loss/perplexity = 3.91138172/49.9679451 secs/batch = 0.1998s, grad.norm=11.54328632
 15667: 11 [ 1070/ 1327], train_loss/perplexity = 4.28927565/72.9136353 secs/batch = 0.1934s, grad.norm=12.21108341
 15672: 11 [ 1075/ 1327], train_loss/perplexity = 4.05131435/57.4729462 secs/batch = 0.1997s, grad.norm=12.06425381
 15677: 11 [ 1080/ 1327], train_loss/perplexity = 3.97423506/53.2094002 secs/batch = 0.1989s, grad.norm=12.26747417
 15682: 11 [ 1085/ 1327], train_loss/perplexity = 3.82794547/45.9679985 secs/batch = 0.1992s, grad.norm=11.64575863
 15687: 11 [ 1090/ 1327], train_loss/perplexity = 4.16928053/64.6689072 secs/batch = 0.1988s, grad.norm=12.82058525
 15692: 11 [ 1095/ 1327], train_loss/perplexity = 4.29251623/73.1502991 secs/batch = 0.1924s, grad.norm=12.77911472
 15697: 11 [ 1100/ 1327], train_loss/perplexity = 3.92107773/50.4547920 secs/batch = 0.1995s, grad.norm=13.05726433
 15702: 11 [ 1105/ 1327], train_loss/perplexity = 3.90942407/49.8702202 secs/batch = 0.1993s, grad.norm=12.26300335
 15707: 11 [ 1110/ 1327], train_loss/perplexity = 4.27957392/72.2096634 secs/batch = 0.1937s, grad.norm=12.85083961
 15712: 11 [ 1115/ 1327], train_loss/perplexity = 4.06403875/58.2089272 secs/batch = 0.1990s, grad.norm=11.49014950
 15717: 11 [ 1120/ 1327], train_loss/perplexity = 4.21315289/67.5692444 secs/batch = 0.2001s, grad.norm=11.83648109
 15722: 11 [ 1125/ 1327], train_loss/perplexity = 4.44940996/85.5764389 secs/batch = 0.1927s, grad.norm=12.41497612
 15727: 11 [ 1130/ 1327], train_loss/perplexity = 4.12940979/62.1412354 secs/batch = 0.2005s, grad.norm=12.74887276
 15732: 11 [ 1135/ 1327], train_loss/perplexity = 4.12381172/61.7943382 secs/batch = 0.1993s, grad.norm=11.71689415
 15737: 11 [ 1140/ 1327], train_loss/perplexity = 4.44393826/85.1094666 secs/batch = 0.1984s, grad.norm=13.12504864
 15742: 11 [ 1145/ 1327], train_loss/perplexity = 4.17329741/64.9291992 secs/batch = 0.1983s, grad.norm=11.52868652
 15747: 11 [ 1150/ 1327], train_loss/perplexity = 4.18740749/65.8518448 secs/batch = 0.2004s, grad.norm=11.97323704
 15752: 11 [ 1155/ 1327], train_loss/perplexity = 4.28103638/72.3153458 secs/batch = 0.2008s, grad.norm=12.07567310
 15757: 11 [ 1160/ 1327], train_loss/perplexity = 4.23010731/68.7246094 secs/batch = 0.1979s, grad.norm=11.95607567
 15762: 11 [ 1165/ 1327], train_loss/perplexity = 4.25876045/70.7222672 secs/batch = 0.1964s, grad.norm=11.75265408
 15767: 11 [ 1170/ 1327], train_loss/perplexity = 4.15201950/63.5622330 secs/batch = 0.1923s, grad.norm=11.48237133
 15772: 11 [ 1175/ 1327], train_loss/perplexity = 3.91395473/50.0966797 secs/batch = 0.1995s, grad.norm=13.36125851
 15777: 11 [ 1180/ 1327], train_loss/perplexity = 3.93185139/51.0013123 secs/batch = 0.1995s, grad.norm=11.91720963
 15782: 11 [ 1185/ 1327], train_loss/perplexity = 4.10246086/60.4889603 secs/batch = 0.1986s, grad.norm=12.01060581
 15787: 11 [ 1190/ 1327], train_loss/perplexity = 4.20701551/67.1558151 secs/batch = 0.2003s, grad.norm=11.98973560
 15792: 11 [ 1195/ 1327], train_loss/perplexity = 3.95326090/52.1049995 secs/batch = 0.1994s, grad.norm=11.51029873
 15797: 11 [ 1200/ 1327], train_loss/perplexity = 3.98545003/53.8095016 secs/batch = 0.1955s, grad.norm=12.18716908
 15802: 11 [ 1205/ 1327], train_loss/perplexity = 3.96769786/52.8626938 secs/batch = 0.1997s, grad.norm=12.29683685
 15807: 11 [ 1210/ 1327], train_loss/perplexity = 3.71788692/41.1772919 secs/batch = 0.1995s, grad.norm=12.22150803
 15812: 11 [ 1215/ 1327], train_loss/perplexity = 3.83634090/46.3555450 secs/batch = 0.1977s, grad.norm=11.47076035
 15817: 11 [ 1220/ 1327], train_loss/perplexity = 4.04567623/57.1498184 secs/batch = 0.2001s, grad.norm=12.55582428
 15822: 11 [ 1225/ 1327], train_loss/perplexity = 3.83481050/46.2846565 secs/batch = 0.2004s, grad.norm=13.06057739
 15827: 11 [ 1230/ 1327], train_loss/perplexity = 4.06544352/58.2907562 secs/batch = 0.1950s, grad.norm=11.46330547
 15832: 11 [ 1235/ 1327], train_loss/perplexity = 4.07911253/59.0930023 secs/batch = 0.2000s, grad.norm=11.93520260
 15837: 11 [ 1240/ 1327], train_loss/perplexity = 4.25135040/70.2001495 secs/batch = 0.2003s, grad.norm=12.13972759
 15842: 11 [ 1245/ 1327], train_loss/perplexity = 4.14471531/63.0996552 secs/batch = 0.1996s, grad.norm=11.66125965
 15847: 11 [ 1250/ 1327], train_loss/perplexity = 4.21420431/67.6403275 secs/batch = 0.1989s, grad.norm=11.62367249
 15852: 11 [ 1255/ 1327], train_loss/perplexity = 4.29086018/73.0292587 secs/batch = 0.1996s, grad.norm=11.61556816
 15857: 11 [ 1260/ 1327], train_loss/perplexity = 4.09296751/59.9174347 secs/batch = 0.1985s, grad.norm=12.95463371
 15862: 11 [ 1265/ 1327], train_loss/perplexity = 4.31207657/74.5952301 secs/batch = 0.1935s, grad.norm=13.45534229
 15867: 11 [ 1270/ 1327], train_loss/perplexity = 4.05110550/57.4609451 secs/batch = 0.1975s, grad.norm=12.48404121
 15872: 11 [ 1275/ 1327], train_loss/perplexity = 4.18700790/65.8255386 secs/batch = 0.1996s, grad.norm=12.31450176
 15877: 11 [ 1280/ 1327], train_loss/perplexity = 4.06729603/58.3988419 secs/batch = 0.1999s, grad.norm=11.92967701
 15882: 11 [ 1285/ 1327], train_loss/perplexity = 3.96121192/52.5209389 secs/batch = 0.1999s, grad.norm=12.47804642
 15887: 11 [ 1290/ 1327], train_loss/perplexity = 4.22960329/68.6899796 secs/batch = 0.1996s, grad.norm=11.67225456
 15892: 11 [ 1295/ 1327], train_loss/perplexity = 4.16380692/64.3159027 secs/batch = 0.1999s, grad.norm=11.56927872
 15897: 11 [ 1300/ 1327], train_loss/perplexity = 4.34398174/77.0135803 secs/batch = 0.1995s, grad.norm=11.43496132
 15902: 11 [ 1305/ 1327], train_loss/perplexity = 4.48927069/89.0564728 secs/batch = 0.1992s, grad.norm=12.76938057
 15907: 11 [ 1310/ 1327], train_loss/perplexity = 4.62408829/101.9098206 secs/batch = 0.1997s, grad.norm=12.69823933
 15912: 11 [ 1315/ 1327], train_loss/perplexity = 4.52545929/92.3383255 secs/batch = 0.1933s, grad.norm=12.52522755
 15917: 11 [ 1320/ 1327], train_loss/perplexity = 4.49089479/89.2012253 secs/batch = 0.2000s, grad.norm=12.20525169
 15922: 11 [ 1325/ 1327], train_loss/perplexity = 4.40601540/81.9423065 secs/batch = 0.1997s, grad.norm=11.75337601
Epoch training time: 264.18789935112
	> validation loss = 4.81066084, perplexity = 122.81275177
	> validation loss = 4.74725533, perplexity = 115.26747894
	> validation loss = 4.68681908, perplexity = 108.50747681
	> validation loss = 4.69052362, perplexity = 108.91019440
	> validation loss = 4.88968563, perplexity = 132.91178894
	> validation loss = 4.73768330, perplexity = 114.16939545
	> validation loss = 4.74806881, perplexity = 115.36128235
	> validation loss = 4.57744026, perplexity = 97.26509857
	> validation loss = 4.36507654, perplexity = 78.65541840
	> validation loss = 4.49247789, perplexity = 89.34255219
	> validation loss = 4.61188412, perplexity = 100.67365265
	> validation loss = 4.72469997, perplexity = 112.69667816
	> validation loss = 4.59255457, perplexity = 98.74636078
	> validation loss = 4.42224503, perplexity = 83.28305054
	> validation loss = 4.35169172, perplexity = 77.60964966
	> validation loss = 4.33791876, perplexity = 76.54805756
	> validation loss = 4.76116133, perplexity = 116.88158417
	> validation loss = 4.33557081, perplexity = 76.36853790
	> validation loss = 4.84304905, perplexity = 126.85555267
	> validation loss = 4.66385889, perplexity = 106.04450989
	> validation loss = 4.51561069, perplexity = 91.43338776
at the end of epoch: 11
train loss = 4.32233777, perplexity = 75.36460732
validation loss = 4.61585732, perplexity = 101.07444450
Saved model cv/epoch011_4.6159.model
 15929: 12 [    5/ 1327], train_loss/perplexity = 4.45302010/85.8859329 secs/batch = 0.1988s, grad.norm=12.29568005
 15934: 12 [   10/ 1327], train_loss/perplexity = 3.99505854/54.3290215 secs/batch = 0.2001s, grad.norm=11.38772678
 15939: 12 [   15/ 1327], train_loss/perplexity = 4.32975769/75.9258881 secs/batch = 0.2001s, grad.norm=11.52084351
 15944: 12 [   20/ 1327], train_loss/perplexity = 4.54217243/93.8945618 secs/batch = 0.1960s, grad.norm=11.65060425
 15949: 12 [   25/ 1327], train_loss/perplexity = 4.39517689/81.0589676 secs/batch = 0.1959s, grad.norm=12.24617195
 15954: 12 [   30/ 1327], train_loss/perplexity = 4.36835861/78.9139938 secs/batch = 0.1999s, grad.norm=11.57618427
 15959: 12 [   35/ 1327], train_loss/perplexity = 4.21161032/67.4650955 secs/batch = 0.1986s, grad.norm=12.03097630
 15964: 12 [   40/ 1327], train_loss/perplexity = 4.19700050/66.4866028 secs/batch = 0.1996s, grad.norm=11.83490944
 15969: 12 [   45/ 1327], train_loss/perplexity = 3.98373604/53.7173500 secs/batch = 0.1994s, grad.norm=11.29422855
 15974: 12 [   50/ 1327], train_loss/perplexity = 4.22897768/68.6470184 secs/batch = 0.1997s, grad.norm=12.15793896
 15979: 12 [   55/ 1327], train_loss/perplexity = 4.17477417/65.0251541 secs/batch = 0.1999s, grad.norm=11.83119202
 15984: 12 [   60/ 1327], train_loss/perplexity = 4.56594419/96.1533356 secs/batch = 0.1990s, grad.norm=12.86731148
 15989: 12 [   65/ 1327], train_loss/perplexity = 4.09593058/60.0952377 secs/batch = 0.1991s, grad.norm=11.36044216
 15994: 12 [   70/ 1327], train_loss/perplexity = 3.89615917/49.2130661 secs/batch = 0.1997s, grad.norm=11.83690453
 15999: 12 [   75/ 1327], train_loss/perplexity = 3.72227645/41.3584366 secs/batch = 0.1994s, grad.norm=11.29370403
 16004: 12 [   80/ 1327], train_loss/perplexity = 4.17570257/65.0855484 secs/batch = 0.1988s, grad.norm=12.04832172
 16009: 12 [   85/ 1327], train_loss/perplexity = 4.24317694/69.6287079 secs/batch = 0.1993s, grad.norm=12.58900356
 16014: 12 [   90/ 1327], train_loss/perplexity = 4.23006487/68.7216873 secs/batch = 0.2002s, grad.norm=12.51282883
 16019: 12 [   95/ 1327], train_loss/perplexity = 4.03264761/56.4100647 secs/batch = 0.2008s, grad.norm=12.42523670
 16024: 12 [  100/ 1327], train_loss/perplexity = 4.38749647/80.4387894 secs/batch = 0.1927s, grad.norm=12.27225971
 16029: 12 [  105/ 1327], train_loss/perplexity = 4.26768255/71.3560791 secs/batch = 0.1996s, grad.norm=12.50723743
 16034: 12 [  110/ 1327], train_loss/perplexity = 4.08354568/59.3555527 secs/batch = 0.2000s, grad.norm=11.95130157
 16039: 12 [  115/ 1327], train_loss/perplexity = 4.05278444/57.5574989 secs/batch = 0.1948s, grad.norm=12.11562252
 16044: 12 [  120/ 1327], train_loss/perplexity = 4.19014597/66.0324326 secs/batch = 0.2007s, grad.norm=12.11858654
 16049: 12 [  125/ 1327], train_loss/perplexity = 4.24553442/69.7930527 secs/batch = 0.2001s, grad.norm=12.60595131
 16054: 12 [  130/ 1327], train_loss/perplexity = 4.15402794/63.6900253 secs/batch = 0.2020s, grad.norm=12.86007881
 16059: 12 [  135/ 1327], train_loss/perplexity = 4.08579636/59.4892921 secs/batch = 0.2005s, grad.norm=12.55012894
 16064: 12 [  140/ 1327], train_loss/perplexity = 4.46638966/87.0419006 secs/batch = 0.2012s, grad.norm=11.96895981
 16069: 12 [  145/ 1327], train_loss/perplexity = 4.35116148/77.5685043 secs/batch = 0.1996s, grad.norm=13.38535786
 16074: 12 [  150/ 1327], train_loss/perplexity = 4.34524441/77.1108780 secs/batch = 0.1989s, grad.norm=12.35474110
 16079: 12 [  155/ 1327], train_loss/perplexity = 4.59439135/98.9279022 secs/batch = 0.2010s, grad.norm=12.30305767
 16084: 12 [  160/ 1327], train_loss/perplexity = 4.22039223/68.0601730 secs/batch = 0.2011s, grad.norm=11.78683853
 16089: 12 [  165/ 1327], train_loss/perplexity = 4.37641239/79.5521164 secs/batch = 0.2000s, grad.norm=12.02727222
 16094: 12 [  170/ 1327], train_loss/perplexity = 4.22661161/68.4847870 secs/batch = 0.1990s, grad.norm=11.78495121
 16099: 12 [  175/ 1327], train_loss/perplexity = 4.51568890/91.4405365 secs/batch = 0.1992s, grad.norm=12.60893059
 16104: 12 [  180/ 1327], train_loss/perplexity = 4.29065704/73.0144272 secs/batch = 0.1980s, grad.norm=12.28254223
 16109: 12 [  185/ 1327], train_loss/perplexity = 4.65223789/104.8192978 secs/batch = 0.1995s, grad.norm=11.98149109
 16114: 12 [  190/ 1327], train_loss/perplexity = 4.16805267/64.5895538 secs/batch = 0.1998s, grad.norm=11.62847328
 16119: 12 [  195/ 1327], train_loss/perplexity = 4.38417053/80.1716995 secs/batch = 0.1963s, grad.norm=11.65031242
 16124: 12 [  200/ 1327], train_loss/perplexity = 4.34445715/77.0502014 secs/batch = 0.2004s, grad.norm=12.25184250
 16129: 12 [  205/ 1327], train_loss/perplexity = 4.52812433/92.5847397 secs/batch = 0.1961s, grad.norm=11.94325733
 16134: 12 [  210/ 1327], train_loss/perplexity = 4.36935806/78.9929047 secs/batch = 0.2009s, grad.norm=11.56730270
 16139: 12 [  215/ 1327], train_loss/perplexity = 4.48791313/88.9356537 secs/batch = 0.1998s, grad.norm=11.50367165
 16144: 12 [  220/ 1327], train_loss/perplexity = 4.40206623/81.6193390 secs/batch = 0.1987s, grad.norm=11.79086304
 16149: 12 [  225/ 1327], train_loss/perplexity = 4.59006596/98.5009232 secs/batch = 0.1994s, grad.norm=12.00571918
 16154: 12 [  230/ 1327], train_loss/perplexity = 4.44587803/85.2747192 secs/batch = 0.1994s, grad.norm=12.29844761
 16159: 12 [  235/ 1327], train_loss/perplexity = 4.33504295/76.3282394 secs/batch = 0.1994s, grad.norm=12.88149452
 16164: 12 [  240/ 1327], train_loss/perplexity = 4.09388542/59.9724579 secs/batch = 0.2000s, grad.norm=12.10213661
 16169: 12 [  245/ 1327], train_loss/perplexity = 4.37260532/79.2498322 secs/batch = 0.1986s, grad.norm=11.75437164
 16174: 12 [  250/ 1327], train_loss/perplexity = 4.13819313/62.6894493 secs/batch = 0.2000s, grad.norm=11.28664780
 16179: 12 [  255/ 1327], train_loss/perplexity = 4.18250322/65.5296860 secs/batch = 0.1991s, grad.norm=11.85753155
 16184: 12 [  260/ 1327], train_loss/perplexity = 4.38062239/79.8877411 secs/batch = 0.1999s, grad.norm=12.38734436
 16189: 12 [  265/ 1327], train_loss/perplexity = 4.60053015/99.5370712 secs/batch = 0.1994s, grad.norm=11.79475594
 16194: 12 [  270/ 1327], train_loss/perplexity = 4.64138412/103.6877670 secs/batch = 0.1954s, grad.norm=12.14918137
 16199: 12 [  275/ 1327], train_loss/perplexity = 4.59387255/98.8765945 secs/batch = 0.1947s, grad.norm=11.73485565
 16204: 12 [  280/ 1327], train_loss/perplexity = 4.40132999/81.5592728 secs/batch = 0.1997s, grad.norm=11.96995831
 16209: 12 [  285/ 1327], train_loss/perplexity = 4.64945698/104.5282059 secs/batch = 0.1995s, grad.norm=11.84770012
 16214: 12 [  290/ 1327], train_loss/perplexity = 4.43483877/84.3385239 secs/batch = 0.1931s, grad.norm=12.15840816
 16219: 12 [  295/ 1327], train_loss/perplexity = 4.15742636/63.9068375 secs/batch = 0.2001s, grad.norm=11.94679451
 16224: 12 [  300/ 1327], train_loss/perplexity = 3.76975989/43.3696518 secs/batch = 0.1995s, grad.norm=11.11026955
 16229: 12 [  305/ 1327], train_loss/perplexity = 4.23299599/68.9234161 secs/batch = 0.1991s, grad.norm=11.36738396
 16234: 12 [  310/ 1327], train_loss/perplexity = 4.23854113/69.3066711 secs/batch = 0.1999s, grad.norm=12.05804539
 16239: 12 [  315/ 1327], train_loss/perplexity = 3.73876166/42.0458908 secs/batch = 0.1991s, grad.norm=11.12060356
 16244: 12 [  320/ 1327], train_loss/perplexity = 3.79106855/44.3037148 secs/batch = 0.1999s, grad.norm=12.31344509
 16249: 12 [  325/ 1327], train_loss/perplexity = 3.78784227/44.1610107 secs/batch = 0.1996s, grad.norm=11.54665279
 16254: 12 [  330/ 1327], train_loss/perplexity = 4.27317715/71.7492294 secs/batch = 0.2000s, grad.norm=12.27475548
 16259: 12 [  335/ 1327], train_loss/perplexity = 3.72184706/41.3406830 secs/batch = 0.1993s, grad.norm=11.10737610
 16264: 12 [  340/ 1327], train_loss/perplexity = 4.45737171/86.2604904 secs/batch = 0.1997s, grad.norm=11.77831173
 16269: 12 [  345/ 1327], train_loss/perplexity = 4.25421619/70.4016113 secs/batch = 0.2007s, grad.norm=12.21701241
 16274: 12 [  350/ 1327], train_loss/perplexity = 4.32226467/75.3591003 secs/batch = 0.1993s, grad.norm=12.37790871
 16279: 12 [  355/ 1327], train_loss/perplexity = 4.36502218/78.6511459 secs/batch = 0.1941s, grad.norm=12.87188339
 16284: 12 [  360/ 1327], train_loss/perplexity = 4.52219534/92.0374298 secs/batch = 0.1994s, grad.norm=13.47643757
 16289: 12 [  365/ 1327], train_loss/perplexity = 4.39126158/80.7422180 secs/batch = 0.1994s, grad.norm=11.69050407
 16294: 12 [  370/ 1327], train_loss/perplexity = 4.42424440/83.4497299 secs/batch = 0.1993s, grad.norm=12.71937370
 16299: 12 [  375/ 1327], train_loss/perplexity = 3.92060924/50.4311600 secs/batch = 0.1996s, grad.norm=11.60802746
 16304: 12 [  380/ 1327], train_loss/perplexity = 3.94872212/51.8690414 secs/batch = 0.1990s, grad.norm=12.22671986
 16309: 12 [  385/ 1327], train_loss/perplexity = 4.18129539/65.4505844 secs/batch = 0.1980s, grad.norm=11.96973133
 16314: 12 [  390/ 1327], train_loss/perplexity = 4.29902840/73.6282196 secs/batch = 0.1935s, grad.norm=11.59442806
 16319: 12 [  395/ 1327], train_loss/perplexity = 4.35429764/77.8121567 secs/batch = 0.1981s, grad.norm=12.54255104
 16324: 12 [  400/ 1327], train_loss/perplexity = 4.28442001/72.5604477 secs/batch = 0.2000s, grad.norm=11.60805321
 16329: 12 [  405/ 1327], train_loss/perplexity = 4.57220650/96.7573700 secs/batch = 0.1945s, grad.norm=12.07787132
 16334: 12 [  410/ 1327], train_loss/perplexity = 4.19548798/66.3861160 secs/batch = 0.2000s, grad.norm=11.88866806
 16339: 12 [  415/ 1327], train_loss/perplexity = 4.09153938/59.8319244 secs/batch = 0.2001s, grad.norm=11.48439980
 16344: 12 [  420/ 1327], train_loss/perplexity = 3.81575084/45.4108391 secs/batch = 0.2019s, grad.norm=12.27722454
 16349: 12 [  425/ 1327], train_loss/perplexity = 4.17396259/64.9724045 secs/batch = 0.2000s, grad.norm=12.49992180
 16354: 12 [  430/ 1327], train_loss/perplexity = 4.37334919/79.3088074 secs/batch = 0.2006s, grad.norm=13.28332615
 16359: 12 [  435/ 1327], train_loss/perplexity = 4.44536638/85.2311020 secs/batch = 0.1997s, grad.norm=12.16659260
 16364: 12 [  440/ 1327], train_loss/perplexity = 3.97819161/53.4203415 secs/batch = 0.2013s, grad.norm=13.35547256
 16369: 12 [  445/ 1327], train_loss/perplexity = 4.31025696/74.4596176 secs/batch = 0.1990s, grad.norm=12.70024967
 16374: 12 [  450/ 1327], train_loss/perplexity = 4.23480129/69.0479584 secs/batch = 0.1981s, grad.norm=11.77284241
 16379: 12 [  455/ 1327], train_loss/perplexity = 4.14365292/63.0326538 secs/batch = 0.1959s, grad.norm=11.57815456
 16384: 12 [  460/ 1327], train_loss/perplexity = 4.15572071/63.7979279 secs/batch = 0.1992s, grad.norm=12.30517578
 16389: 12 [  465/ 1327], train_loss/perplexity = 3.89361882/49.0882072 secs/batch = 0.1959s, grad.norm=12.94967270
 16394: 12 [  470/ 1327], train_loss/perplexity = 4.59781122/99.2668076 secs/batch = 0.1992s, grad.norm=11.33395290
 16399: 12 [  475/ 1327], train_loss/perplexity = 4.06292915/58.1443748 secs/batch = 0.1994s, grad.norm=11.98904419
 16404: 12 [  480/ 1327], train_loss/perplexity = 4.21019650/67.3697739 secs/batch = 0.1993s, grad.norm=13.20507526
 16409: 12 [  485/ 1327], train_loss/perplexity = 4.18202019/65.4980392 secs/batch = 0.1989s, grad.norm=12.03647137
 16414: 12 [  490/ 1327], train_loss/perplexity = 4.06352997/58.1793213 secs/batch = 0.2002s, grad.norm=12.83612156
 16419: 12 [  495/ 1327], train_loss/perplexity = 4.06517887/58.2753296 secs/batch = 0.1938s, grad.norm=12.83129597
 16424: 12 [  500/ 1327], train_loss/perplexity = 4.34368038/76.9903717 secs/batch = 0.2020s, grad.norm=14.67694759
 16429: 12 [  505/ 1327], train_loss/perplexity = 4.41088104/82.3419800 secs/batch = 0.1957s, grad.norm=11.58807659
 16434: 12 [  510/ 1327], train_loss/perplexity = 4.69517660/109.4181290 secs/batch = 0.1995s, grad.norm=11.55580044
 16439: 12 [  515/ 1327], train_loss/perplexity = 4.39129639/80.7450256 secs/batch = 0.1992s, grad.norm=11.05461121
 16444: 12 [  520/ 1327], train_loss/perplexity = 4.51920652/91.7627563 secs/batch = 0.1951s, grad.norm=11.63916302
 16449: 12 [  525/ 1327], train_loss/perplexity = 4.11208916/61.0741768 secs/batch = 0.2001s, grad.norm=11.71419716
 16454: 12 [  530/ 1327], train_loss/perplexity = 4.18008661/65.3715134 secs/batch = 0.2001s, grad.norm=12.53321648
 16459: 12 [  535/ 1327], train_loss/perplexity = 4.27879620/72.1535263 secs/batch = 0.1990s, grad.norm=11.79642868
 16464: 12 [  540/ 1327], train_loss/perplexity = 4.34082174/76.7705994 secs/batch = 0.2005s, grad.norm=12.25944901
 16469: 12 [  545/ 1327], train_loss/perplexity = 4.39357948/80.9295883 secs/batch = 0.1947s, grad.norm=11.94388390
 16474: 12 [  550/ 1327], train_loss/perplexity = 4.29319620/73.2000580 secs/batch = 0.1998s, grad.norm=11.63263321
 16479: 12 [  555/ 1327], train_loss/perplexity = 4.23411655/69.0006943 secs/batch = 0.1986s, grad.norm=12.26084423
 16484: 12 [  560/ 1327], train_loss/perplexity = 4.24374866/69.6685257 secs/batch = 0.1998s, grad.norm=12.84498215
 16489: 12 [  565/ 1327], train_loss/perplexity = 4.23262501/68.8978500 secs/batch = 0.1997s, grad.norm=12.48988152
 16494: 12 [  570/ 1327], train_loss/perplexity = 4.15490580/63.7459602 secs/batch = 0.2002s, grad.norm=12.58463955
 16499: 12 [  575/ 1327], train_loss/perplexity = 3.99452066/54.2998047 secs/batch = 0.1995s, grad.norm=12.45204067
 16504: 12 [  580/ 1327], train_loss/perplexity = 4.42287779/83.3357620 secs/batch = 0.1995s, grad.norm=12.35461998
 16509: 12 [  585/ 1327], train_loss/perplexity = 3.97582102/53.2938538 secs/batch = 0.1982s, grad.norm=11.85176849
 16514: 12 [  590/ 1327], train_loss/perplexity = 4.33331776/76.1966705 secs/batch = 0.1999s, grad.norm=12.14927387
 16519: 12 [  595/ 1327], train_loss/perplexity = 4.29702139/73.4805984 secs/batch = 0.2004s, grad.norm=12.65899372
 16524: 12 [  600/ 1327], train_loss/perplexity = 4.38344908/80.1138763 secs/batch = 0.2004s, grad.norm=11.39493465
 16529: 12 [  605/ 1327], train_loss/perplexity = 4.37994480/79.8336258 secs/batch = 0.2001s, grad.norm=11.72841454
 16534: 12 [  610/ 1327], train_loss/perplexity = 4.52810955/92.5833740 secs/batch = 0.1996s, grad.norm=11.84584236
 16539: 12 [  615/ 1327], train_loss/perplexity = 4.11533499/61.2727356 secs/batch = 0.1984s, grad.norm=11.97302055
 16544: 12 [  620/ 1327], train_loss/perplexity = 4.49007893/89.1284790 secs/batch = 0.1985s, grad.norm=12.15756035
 16549: 12 [  625/ 1327], train_loss/perplexity = 4.45193768/85.7930222 secs/batch = 0.1936s, grad.norm=11.40497684
 16554: 12 [  630/ 1327], train_loss/perplexity = 4.54778528/94.4230576 secs/batch = 0.1998s, grad.norm=12.12986755
 16559: 12 [  635/ 1327], train_loss/perplexity = 4.22817564/68.5919800 secs/batch = 0.2007s, grad.norm=11.78248310
 16564: 12 [  640/ 1327], train_loss/perplexity = 4.20431232/66.9745255 secs/batch = 0.1999s, grad.norm=11.55354595
 16569: 12 [  645/ 1327], train_loss/perplexity = 4.59587908/99.0751953 secs/batch = 0.1997s, grad.norm=12.97252464
 16574: 12 [  650/ 1327], train_loss/perplexity = 4.04192972/56.9361076 secs/batch = 0.1975s, grad.norm=12.59817028
 16579: 12 [  655/ 1327], train_loss/perplexity = 4.14531088/63.1372452 secs/batch = 0.1991s, grad.norm=12.14902020
 16584: 12 [  660/ 1327], train_loss/perplexity = 4.11980486/61.5472298 secs/batch = 0.1999s, grad.norm=11.90209961
 16589: 12 [  665/ 1327], train_loss/perplexity = 4.30909681/74.3732834 secs/batch = 0.1998s, grad.norm=12.31892109
 16594: 12 [  670/ 1327], train_loss/perplexity = 4.23748255/69.2333374 secs/batch = 0.1937s, grad.norm=12.08437634
 16599: 12 [  675/ 1327], train_loss/perplexity = 4.10652733/60.7354355 secs/batch = 0.1991s, grad.norm=12.49751472
 16604: 12 [  680/ 1327], train_loss/perplexity = 4.19300652/66.2215881 secs/batch = 0.1990s, grad.norm=12.36462307
 16609: 12 [  685/ 1327], train_loss/perplexity = 4.01736069/55.5542870 secs/batch = 0.2003s, grad.norm=11.69452953
 16614: 12 [  690/ 1327], train_loss/perplexity = 4.42843771/83.8003922 secs/batch = 0.1952s, grad.norm=11.34607506
 16619: 12 [  695/ 1327], train_loss/perplexity = 4.21639347/67.7885590 secs/batch = 0.1997s, grad.norm=11.63721657
 16624: 12 [  700/ 1327], train_loss/perplexity = 4.48114777/88.3360062 secs/batch = 0.1994s, grad.norm=11.87624550
 16629: 12 [  705/ 1327], train_loss/perplexity = 4.15880346/63.9949036 secs/batch = 0.2008s, grad.norm=11.28880882
 16634: 12 [  710/ 1327], train_loss/perplexity = 4.09234905/59.8803902 secs/batch = 0.1946s, grad.norm=11.85116291
 16639: 12 [  715/ 1327], train_loss/perplexity = 4.05118942/57.4657669 secs/batch = 0.1996s, grad.norm=11.54158401
 16644: 12 [  720/ 1327], train_loss/perplexity = 3.99190855/54.1581535 secs/batch = 0.1995s, grad.norm=11.86007118
 16649: 12 [  725/ 1327], train_loss/perplexity = 4.03419495/56.4974174 secs/batch = 0.1994s, grad.norm=11.75469017
 16654: 12 [  730/ 1327], train_loss/perplexity = 4.30474567/74.0503769 secs/batch = 0.1999s, grad.norm=12.21770573
 16659: 12 [  735/ 1327], train_loss/perplexity = 4.39769363/81.2632294 secs/batch = 0.2006s, grad.norm=12.39553070
 16664: 12 [  740/ 1327], train_loss/perplexity = 3.78201723/43.9045181 secs/batch = 0.2000s, grad.norm=11.09262943
 16669: 12 [  745/ 1327], train_loss/perplexity = 4.33091497/76.0138092 secs/batch = 0.1996s, grad.norm=12.11555767
 16674: 12 [  750/ 1327], train_loss/perplexity = 4.05908394/57.9212265 secs/batch = 0.1995s, grad.norm=12.17692947
 16679: 12 [  755/ 1327], train_loss/perplexity = 4.02742863/56.1164284 secs/batch = 0.1989s, grad.norm=11.57328033
 16684: 12 [  760/ 1327], train_loss/perplexity = 3.85577917/47.2654305 secs/batch = 0.1995s, grad.norm=11.36634636
 16689: 12 [  765/ 1327], train_loss/perplexity = 3.96716690/52.8346329 secs/batch = 0.1995s, grad.norm=11.52093410
 16694: 12 [  770/ 1327], train_loss/perplexity = 3.92047644/50.4244614 secs/batch = 0.2016s, grad.norm=13.37848759
 16699: 12 [  775/ 1327], train_loss/perplexity = 4.06073570/58.0169792 secs/batch = 0.1997s, grad.norm=12.60330391
 16704: 12 [  780/ 1327], train_loss/perplexity = 4.39412642/80.9738617 secs/batch = 0.1993s, grad.norm=12.05235386
 16709: 12 [  785/ 1327], train_loss/perplexity = 4.31635475/74.9150467 secs/batch = 0.1988s, grad.norm=12.63063717
 16714: 12 [  790/ 1327], train_loss/perplexity = 3.98616147/53.8477974 secs/batch = 0.2006s, grad.norm=11.87431622
 16719: 12 [  795/ 1327], train_loss/perplexity = 4.39489794/81.0363617 secs/batch = 0.1998s, grad.norm=13.90288544
 16724: 12 [  800/ 1327], train_loss/perplexity = 4.32568407/75.6172256 secs/batch = 0.2010s, grad.norm=12.76345253
 16729: 12 [  805/ 1327], train_loss/perplexity = 4.64364767/103.9227295 secs/batch = 0.1997s, grad.norm=12.24047375
 16734: 12 [  810/ 1327], train_loss/perplexity = 4.23459101/69.0334396 secs/batch = 0.2000s, grad.norm=12.53663921
 16739: 12 [  815/ 1327], train_loss/perplexity = 4.10309315/60.5272179 secs/batch = 0.1998s, grad.norm=11.79033184
 16744: 12 [  820/ 1327], train_loss/perplexity = 3.89846039/49.3264465 secs/batch = 0.1982s, grad.norm=11.04130363
 16749: 12 [  825/ 1327], train_loss/perplexity = 4.16452074/64.3618317 secs/batch = 0.1992s, grad.norm=12.14548779
 16754: 12 [  830/ 1327], train_loss/perplexity = 3.90640473/49.7198753 secs/batch = 0.2007s, grad.norm=12.06594276
 16759: 12 [  835/ 1327], train_loss/perplexity = 4.20199966/66.8198166 secs/batch = 0.1996s, grad.norm=12.89400864
 16764: 12 [  840/ 1327], train_loss/perplexity = 4.24368048/69.6637802 secs/batch = 0.2004s, grad.norm=12.21235943
 16769: 12 [  845/ 1327], train_loss/perplexity = 4.07493782/58.8468208 secs/batch = 0.1962s, grad.norm=12.05101109
 16774: 12 [  850/ 1327], train_loss/perplexity = 4.14105320/62.8689995 secs/batch = 0.1981s, grad.norm=11.66807556
 16779: 12 [  855/ 1327], train_loss/perplexity = 4.23218870/68.8677979 secs/batch = 0.2003s, grad.norm=12.90408897
 16784: 12 [  860/ 1327], train_loss/perplexity = 3.86652112/47.7758904 secs/batch = 0.2002s, grad.norm=11.53823376
 16789: 12 [  865/ 1327], train_loss/perplexity = 4.40465879/81.8312149 secs/batch = 0.1999s, grad.norm=11.67177582
 16794: 12 [  870/ 1327], train_loss/perplexity = 4.21986437/68.0242538 secs/batch = 0.1996s, grad.norm=11.69867325
 16799: 12 [  875/ 1327], train_loss/perplexity = 3.82184148/45.6882668 secs/batch = 0.1955s, grad.norm=11.67874718
 16804: 12 [  880/ 1327], train_loss/perplexity = 4.11528635/61.2697563 secs/batch = 0.1997s, grad.norm=11.98525333
 16809: 12 [  885/ 1327], train_loss/perplexity = 4.22567844/68.4209061 secs/batch = 0.1997s, grad.norm=11.76506233
 16814: 12 [  890/ 1327], train_loss/perplexity = 4.45818043/86.3302841 secs/batch = 0.1993s, grad.norm=11.64315033
 16819: 12 [  895/ 1327], train_loss/perplexity = 4.30344534/73.9541550 secs/batch = 0.1933s, grad.norm=11.48497963
 16824: 12 [  900/ 1327], train_loss/perplexity = 4.19391727/66.2819290 secs/batch = 0.1989s, grad.norm=12.06585789
 16829: 12 [  905/ 1327], train_loss/perplexity = 4.01413727/55.3754997 secs/batch = 0.1991s, grad.norm=11.17768192
 16834: 12 [  910/ 1327], train_loss/perplexity = 4.09199190/59.8590050 secs/batch = 0.1998s, grad.norm=11.40477657
 16839: 12 [  915/ 1327], train_loss/perplexity = 4.32038546/75.2176132 secs/batch = 0.2008s, grad.norm=11.39783669
 16844: 12 [  920/ 1327], train_loss/perplexity = 4.47092676/87.4377213 secs/batch = 0.1998s, grad.norm=12.03818417
 16849: 12 [  925/ 1327], train_loss/perplexity = 4.29388046/73.2501602 secs/batch = 0.1923s, grad.norm=12.17672443
 16854: 12 [  930/ 1327], train_loss/perplexity = 4.24896383/70.0328064 secs/batch = 0.1997s, grad.norm=11.69438839
 16859: 12 [  935/ 1327], train_loss/perplexity = 4.38488197/80.2287521 secs/batch = 0.1997s, grad.norm=12.11469078
 16864: 12 [  940/ 1327], train_loss/perplexity = 4.31564617/74.8619843 secs/batch = 0.1984s, grad.norm=11.18139362
 16869: 12 [  945/ 1327], train_loss/perplexity = 4.53152657/92.9002686 secs/batch = 0.1947s, grad.norm=11.67195702
 16874: 12 [  950/ 1327], train_loss/perplexity = 4.31794262/75.0340958 secs/batch = 0.1989s, grad.norm=11.43875790
 16879: 12 [  955/ 1327], train_loss/perplexity = 4.22907066/68.6533966 secs/batch = 0.1994s, grad.norm=12.09265327
 16884: 12 [  960/ 1327], train_loss/perplexity = 4.58003044/97.5173645 secs/batch = 0.2000s, grad.norm=11.71389484
 16889: 12 [  965/ 1327], train_loss/perplexity = 4.29103947/73.0423508 secs/batch = 0.1997s, grad.norm=11.45168114
 16894: 12 [  970/ 1327], train_loss/perplexity = 4.51188183/91.0930786 secs/batch = 0.1998s, grad.norm=11.78697968
 16899: 12 [  975/ 1327], train_loss/perplexity = 4.27004910/71.5251465 secs/batch = 0.2009s, grad.norm=12.48940468
 16904: 12 [  980/ 1327], train_loss/perplexity = 4.02587128/56.0291061 secs/batch = 0.1977s, grad.norm=11.61783028
 16909: 12 [  985/ 1327], train_loss/perplexity = 4.17891550/65.2950058 secs/batch = 0.2006s, grad.norm=11.75615120
 16914: 12 [  990/ 1327], train_loss/perplexity = 4.38732290/80.4248276 secs/batch = 0.1991s, grad.norm=12.22865391
 16919: 12 [  995/ 1327], train_loss/perplexity = 4.41146469/82.3900528 secs/batch = 0.2006s, grad.norm=11.44593811
 16924: 12 [ 1000/ 1327], train_loss/perplexity = 3.90826225/49.8123169 secs/batch = 0.1967s, grad.norm=11.18758869
 16929: 12 [ 1005/ 1327], train_loss/perplexity = 4.42196989/83.2601395 secs/batch = 0.2002s, grad.norm=11.78699112
 16934: 12 [ 1010/ 1327], train_loss/perplexity = 3.98753047/53.9215622 secs/batch = 0.1983s, grad.norm=11.30543137
 16939: 12 [ 1015/ 1327], train_loss/perplexity = 4.45324039/85.9048615 secs/batch = 0.2003s, grad.norm=11.48062325
 16944: 12 [ 1020/ 1327], train_loss/perplexity = 4.55771923/95.3657227 secs/batch = 0.1989s, grad.norm=11.51983929
 16949: 12 [ 1025/ 1327], train_loss/perplexity = 4.42612219/83.6065750 secs/batch = 0.2003s, grad.norm=11.61927891
 16954: 12 [ 1030/ 1327], train_loss/perplexity = 4.22402000/68.3075333 secs/batch = 0.1993s, grad.norm=11.90851879
 16959: 12 [ 1035/ 1327], train_loss/perplexity = 4.14019632/62.8151512 secs/batch = 0.1986s, grad.norm=11.25789642
 16964: 12 [ 1040/ 1327], train_loss/perplexity = 4.43822479/84.6245804 secs/batch = 0.2005s, grad.norm=12.70755577
 16969: 12 [ 1045/ 1327], train_loss/perplexity = 3.95845413/52.3762970 secs/batch = 0.2005s, grad.norm=11.49447346
 16974: 12 [ 1050/ 1327], train_loss/perplexity = 4.07265091/58.7123985 secs/batch = 0.1987s, grad.norm=11.87566090
 16979: 12 [ 1055/ 1327], train_loss/perplexity = 4.12420559/61.8186798 secs/batch = 0.2011s, grad.norm=12.22626400
 16984: 12 [ 1060/ 1327], train_loss/perplexity = 3.76611900/43.2120323 secs/batch = 0.1991s, grad.norm=12.44335938
 16989: 12 [ 1065/ 1327], train_loss/perplexity = 3.85846949/47.3927612 secs/batch = 0.1993s, grad.norm=11.57961178
 16994: 12 [ 1070/ 1327], train_loss/perplexity = 4.31304216/74.6672974 secs/batch = 0.1996s, grad.norm=12.39957619
 16999: 12 [ 1075/ 1327], train_loss/perplexity = 4.00195169/54.7048111 secs/batch = 0.1996s, grad.norm=12.28890324
 17004: 12 [ 1080/ 1327], train_loss/perplexity = 3.95890808/52.4000778 secs/batch = 0.1988s, grad.norm=11.93737125
 17009: 12 [ 1085/ 1327], train_loss/perplexity = 3.82160640/45.6775246 secs/batch = 0.1994s, grad.norm=12.14099979
 17014: 12 [ 1090/ 1327], train_loss/perplexity = 4.08221197/59.2764435 secs/batch = 0.1985s, grad.norm=12.27080250
 17019: 12 [ 1095/ 1327], train_loss/perplexity = 4.21294594/67.5552597 secs/batch = 0.1975s, grad.norm=12.66751480
 17024: 12 [ 1100/ 1327], train_loss/perplexity = 3.89981461/49.3932915 secs/batch = 0.1992s, grad.norm=13.20022011
 17029: 12 [ 1105/ 1327], train_loss/perplexity = 3.87572575/48.2176781 secs/batch = 0.1999s, grad.norm=12.37359333
 17034: 12 [ 1110/ 1327], train_loss/perplexity = 4.27062368/71.5662537 secs/batch = 0.1988s, grad.norm=12.90311432
 17039: 12 [ 1115/ 1327], train_loss/perplexity = 3.96556807/52.7502251 secs/batch = 0.1998s, grad.norm=11.78999043
 17044: 12 [ 1120/ 1327], train_loss/perplexity = 4.25106621/70.1801987 secs/batch = 0.2000s, grad.norm=11.76805973
 17049: 12 [ 1125/ 1327], train_loss/perplexity = 4.42655230/83.6425476 secs/batch = 0.1998s, grad.norm=12.50807381
 17054: 12 [ 1130/ 1327], train_loss/perplexity = 4.11080313/60.9956856 secs/batch = 0.2011s, grad.norm=12.39265347
 17059: 12 [ 1135/ 1327], train_loss/perplexity = 4.02292633/55.8643456 secs/batch = 0.1988s, grad.norm=11.62657928
 17064: 12 [ 1140/ 1327], train_loss/perplexity = 4.38123465/79.9366684 secs/batch = 0.2006s, grad.norm=12.68589878
 17069: 12 [ 1145/ 1327], train_loss/perplexity = 4.14022779/62.8171310 secs/batch = 0.2000s, grad.norm=11.51181221
 17074: 12 [ 1150/ 1327], train_loss/perplexity = 4.16886282/64.6418991 secs/batch = 0.1990s, grad.norm=12.08030891
 17079: 12 [ 1155/ 1327], train_loss/perplexity = 4.19364023/66.2635651 secs/batch = 0.1991s, grad.norm=12.85871983
 17084: 12 [ 1160/ 1327], train_loss/perplexity = 4.11219549/61.0806732 secs/batch = 0.1990s, grad.norm=11.82690239
 17089: 12 [ 1165/ 1327], train_loss/perplexity = 4.19576168/66.4042892 secs/batch = 0.2000s, grad.norm=12.08148384
 17094: 12 [ 1170/ 1327], train_loss/perplexity = 4.09152126/59.8308411 secs/batch = 0.2014s, grad.norm=12.07890606
 17099: 12 [ 1175/ 1327], train_loss/perplexity = 3.97188163/53.0843201 secs/batch = 0.1988s, grad.norm=11.89192390
 17104: 12 [ 1180/ 1327], train_loss/perplexity = 3.91053820/49.9258156 secs/batch = 0.1955s, grad.norm=12.43391132
 17109: 12 [ 1185/ 1327], train_loss/perplexity = 4.06018972/57.9853096 secs/batch = 0.1998s, grad.norm=12.64318466
 17114: 12 [ 1190/ 1327], train_loss/perplexity = 4.17465496/65.0174026 secs/batch = 0.1986s, grad.norm=12.52610397
 17119: 12 [ 1195/ 1327], train_loss/perplexity = 3.89142418/48.9805946 secs/batch = 0.2000s, grad.norm=11.48885536
 17124: 12 [ 1200/ 1327], train_loss/perplexity = 3.97307086/53.1474876 secs/batch = 0.1985s, grad.norm=11.84697151
 17129: 12 [ 1205/ 1327], train_loss/perplexity = 3.96818256/52.8883209 secs/batch = 0.2001s, grad.norm=12.42233467
 17134: 12 [ 1210/ 1327], train_loss/perplexity = 3.59936500/36.5750008 secs/batch = 0.1994s, grad.norm=12.06034851
 17139: 12 [ 1215/ 1327], train_loss/perplexity = 3.79120255/44.3096542 secs/batch = 0.1994s, grad.norm=11.43646240
 17144: 12 [ 1220/ 1327], train_loss/perplexity = 3.96759939/52.8574867 secs/batch = 0.1944s, grad.norm=12.79008865
 17149: 12 [ 1225/ 1327], train_loss/perplexity = 3.84508610/46.7627106 secs/batch = 0.1946s, grad.norm=13.43314934
 17154: 12 [ 1230/ 1327], train_loss/perplexity = 4.04832172/57.3012085 secs/batch = 0.1990s, grad.norm=11.91735458
 17159: 12 [ 1235/ 1327], train_loss/perplexity = 3.99716163/54.4434013 secs/batch = 0.2000s, grad.norm=12.25469971
 17164: 12 [ 1240/ 1327], train_loss/perplexity = 4.20339632/66.9132080 secs/batch = 0.1982s, grad.norm=12.95759583
 17169: 12 [ 1245/ 1327], train_loss/perplexity = 4.10102987/60.4024620 secs/batch = 0.1996s, grad.norm=11.74875736
 17174: 12 [ 1250/ 1327], train_loss/perplexity = 4.15520239/63.7648697 secs/batch = 0.1987s, grad.norm=11.68847084
 17179: 12 [ 1255/ 1327], train_loss/perplexity = 4.28036499/72.2668152 secs/batch = 0.1990s, grad.norm=11.75682449
 17184: 12 [ 1260/ 1327], train_loss/perplexity = 4.02760887/56.1265450 secs/batch = 0.1991s, grad.norm=12.61114502
 17189: 12 [ 1265/ 1327], train_loss/perplexity = 4.21226025/67.5089569 secs/batch = 0.1923s, grad.norm=12.44130611
 17194: 12 [ 1270/ 1327], train_loss/perplexity = 3.99279213/54.2060280 secs/batch = 0.1997s, grad.norm=12.61442471
 17199: 12 [ 1275/ 1327], train_loss/perplexity = 4.19133568/66.1110382 secs/batch = 0.1997s, grad.norm=13.22892189
 17204: 12 [ 1280/ 1327], train_loss/perplexity = 3.97026539/52.9985924 secs/batch = 0.1953s, grad.norm=12.07035637
 17209: 12 [ 1285/ 1327], train_loss/perplexity = 3.92526078/50.6662903 secs/batch = 0.1993s, grad.norm=11.87543488
 17214: 12 [ 1290/ 1327], train_loss/perplexity = 4.18047619/65.3969879 secs/batch = 0.1987s, grad.norm=11.88254833
 17219: 12 [ 1295/ 1327], train_loss/perplexity = 4.18933010/65.9785767 secs/batch = 0.2007s, grad.norm=11.67004108
 17224: 12 [ 1300/ 1327], train_loss/perplexity = 4.28064871/72.2873154 secs/batch = 0.1996s, grad.norm=11.27825260
 17229: 12 [ 1305/ 1327], train_loss/perplexity = 4.44129038/84.8844070 secs/batch = 0.2004s, grad.norm=12.80360413
 17234: 12 [ 1310/ 1327], train_loss/perplexity = 4.56384277/95.9514923 secs/batch = 0.1981s, grad.norm=12.26995754
 17239: 12 [ 1315/ 1327], train_loss/perplexity = 4.44827938/85.4797363 secs/batch = 0.2000s, grad.norm=13.04295826
 17244: 12 [ 1320/ 1327], train_loss/perplexity = 4.41210651/82.4429474 secs/batch = 0.2009s, grad.norm=12.41708946
 17249: 12 [ 1325/ 1327], train_loss/perplexity = 4.37126923/79.1440201 secs/batch = 0.2001s, grad.norm=12.32620239
Epoch training time: 264.35089564323425
	> validation loss = 4.81982994, perplexity = 123.94400787
	> validation loss = 4.73072290, perplexity = 113.37749481
	> validation loss = 4.65869236, perplexity = 105.49803925
	> validation loss = 4.70176888, perplexity = 110.14183044
	> validation loss = 4.88379812, perplexity = 132.13156128
	> validation loss = 4.72827339, perplexity = 113.10011292
	> validation loss = 4.76374435, perplexity = 117.18388367
	> validation loss = 4.55748177, perplexity = 95.34307861
	> validation loss = 4.33888006, perplexity = 76.62168121
	> validation loss = 4.45903969, perplexity = 86.40449524
	> validation loss = 4.62336159, perplexity = 101.83578491
	> validation loss = 4.67954254, perplexity = 107.72078705
	> validation loss = 4.57940197, perplexity = 97.45609283
	> validation loss = 4.42035484, perplexity = 83.12577820
	> validation loss = 4.32878542, perplexity = 75.85210419
	> validation loss = 4.33318758, perplexity = 76.18675232
	> validation loss = 4.76981163, perplexity = 117.89703369
	> validation loss = 4.32616997, perplexity = 75.65397644
	> validation loss = 4.82713938, perplexity = 124.85329437
	> validation loss = 4.66067600, perplexity = 105.70751953
	> validation loss = 4.49929571, perplexity = 89.95375824
at the end of epoch: 12
train loss = 4.28603705, perplexity = 72.67787859
validation loss = 4.60865784, perplexity = 100.34937444
Saved model cv/epoch012_4.6087.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.5
new learning rate is: 0.25
 17256: 13 [    5/ 1327], train_loss/perplexity = 4.41488123/82.6720200 secs/batch = 0.1946s, grad.norm=12.35138130
 17261: 13 [   10/ 1327], train_loss/perplexity = 3.94722033/51.7912025 secs/batch = 0.1994s, grad.norm=11.49307728
 17266: 13 [   15/ 1327], train_loss/perplexity = 4.38115025/79.9299164 secs/batch = 0.1943s, grad.norm=11.63492298
 17271: 13 [   20/ 1327], train_loss/perplexity = 4.51946545/91.7865219 secs/batch = 0.1990s, grad.norm=11.59097385
 17276: 13 [   25/ 1327], train_loss/perplexity = 4.30925369/74.3849564 secs/batch = 0.2010s, grad.norm=11.96538544
 17281: 13 [   30/ 1327], train_loss/perplexity = 4.37048483/79.0819626 secs/batch = 0.2000s, grad.norm=11.98410797
 17286: 13 [   35/ 1327], train_loss/perplexity = 4.21031857/67.3779984 secs/batch = 0.1995s, grad.norm=11.60601997
 17291: 13 [   40/ 1327], train_loss/perplexity = 4.18137598/65.4558563 secs/batch = 0.1989s, grad.norm=11.72154140
 17296: 13 [   45/ 1327], train_loss/perplexity = 3.97979116/53.5058594 secs/batch = 0.1931s, grad.norm=11.21361637
 17301: 13 [   50/ 1327], train_loss/perplexity = 4.18630695/65.7794189 secs/batch = 0.1992s, grad.norm=11.87136841
 17306: 13 [   55/ 1327], train_loss/perplexity = 4.16355181/64.2994995 secs/batch = 0.1990s, grad.norm=12.48400211
 17311: 13 [   60/ 1327], train_loss/perplexity = 4.48225451/88.4338226 secs/batch = 0.1992s, grad.norm=12.68814564
 17316: 13 [   65/ 1327], train_loss/perplexity = 3.98595190/53.8365135 secs/batch = 0.1996s, grad.norm=12.26237679
 17321: 13 [   70/ 1327], train_loss/perplexity = 3.85851622/47.3949738 secs/batch = 0.2001s, grad.norm=11.90136719
 17326: 13 [   75/ 1327], train_loss/perplexity = 3.66666937/39.1213913 secs/batch = 0.1928s, grad.norm=11.30468941
 17331: 13 [   80/ 1327], train_loss/perplexity = 4.10655785/60.7372894 secs/batch = 0.1944s, grad.norm=12.20620441
 17336: 13 [   85/ 1327], train_loss/perplexity = 4.10522413/60.6563377 secs/batch = 0.1993s, grad.norm=12.17331314
 17341: 13 [   90/ 1327], train_loss/perplexity = 4.18607426/65.7641068 secs/batch = 0.2018s, grad.norm=12.04206753
 17346: 13 [   95/ 1327], train_loss/perplexity = 3.99425316/54.2852821 secs/batch = 0.1995s, grad.norm=12.08378029
 17351: 13 [  100/ 1327], train_loss/perplexity = 4.34957123/77.4452515 secs/batch = 0.1993s, grad.norm=13.34340000
 17356: 13 [  105/ 1327], train_loss/perplexity = 4.24531555/69.7777786 secs/batch = 0.1983s, grad.norm=13.10195827
 17361: 13 [  110/ 1327], train_loss/perplexity = 4.07507706/58.8550148 secs/batch = 0.1999s, grad.norm=11.92968178
 17366: 13 [  115/ 1327], train_loss/perplexity = 4.00194073/54.7042122 secs/batch = 0.1998s, grad.norm=12.27048969
 17371: 13 [  120/ 1327], train_loss/perplexity = 4.06586218/58.3151665 secs/batch = 0.1936s, grad.norm=11.96989632
 17376: 13 [  125/ 1327], train_loss/perplexity = 4.15992308/64.0665970 secs/batch = 0.1993s, grad.norm=12.15017223
 17381: 13 [  130/ 1327], train_loss/perplexity = 4.07714796/58.9770241 secs/batch = 0.1937s, grad.norm=12.65409088
 17386: 13 [  135/ 1327], train_loss/perplexity = 4.08150291/59.2344284 secs/batch = 0.1994s, grad.norm=11.91797256
 17391: 13 [  140/ 1327], train_loss/perplexity = 4.33410978/76.2570419 secs/batch = 0.1991s, grad.norm=12.28219223
 17396: 13 [  145/ 1327], train_loss/perplexity = 4.30406427/73.9999390 secs/batch = 0.1988s, grad.norm=12.95413303
 17401: 13 [  150/ 1327], train_loss/perplexity = 4.35475302/77.8475952 secs/batch = 0.1996s, grad.norm=12.55461597
 17406: 13 [  155/ 1327], train_loss/perplexity = 4.48250484/88.4559631 secs/batch = 0.2005s, grad.norm=12.06165409
 17411: 13 [  160/ 1327], train_loss/perplexity = 4.18161011/65.4711838 secs/batch = 0.1926s, grad.norm=12.25751495
 17416: 13 [  165/ 1327], train_loss/perplexity = 4.38916206/80.5728760 secs/batch = 0.2003s, grad.norm=12.33517742
 17421: 13 [  170/ 1327], train_loss/perplexity = 4.16178131/64.1857529 secs/batch = 0.1993s, grad.norm=11.37371254
 17426: 13 [  175/ 1327], train_loss/perplexity = 4.49519730/89.5858459 secs/batch = 0.1998s, grad.norm=12.83208370
 17431: 13 [  180/ 1327], train_loss/perplexity = 4.34712267/77.2558517 secs/batch = 0.1995s, grad.norm=11.91490173
 17436: 13 [  185/ 1327], train_loss/perplexity = 4.56007004/95.5901718 secs/batch = 0.1997s, grad.norm=12.52187061
 17441: 13 [  190/ 1327], train_loss/perplexity = 4.10418701/60.5934639 secs/batch = 0.1992s, grad.norm=11.76563835
 17446: 13 [  195/ 1327], train_loss/perplexity = 4.39044094/80.6759872 secs/batch = 0.1995s, grad.norm=11.40880775
 17451: 13 [  200/ 1327], train_loss/perplexity = 4.35364914/77.7617111 secs/batch = 0.2007s, grad.norm=12.05733395
 17456: 13 [  205/ 1327], train_loss/perplexity = 4.42730904/83.7058640 secs/batch = 0.1949s, grad.norm=12.03112030
 17461: 13 [  210/ 1327], train_loss/perplexity = 4.31643391/74.9209747 secs/batch = 0.2004s, grad.norm=11.38962460
 17466: 13 [  215/ 1327], train_loss/perplexity = 4.43070507/83.9906158 secs/batch = 0.1978s, grad.norm=11.56038761
 17471: 13 [  220/ 1327], train_loss/perplexity = 4.39129019/80.7445297 secs/batch = 0.1993s, grad.norm=11.29660988
 17476: 13 [  225/ 1327], train_loss/perplexity = 4.58250093/97.7585754 secs/batch = 0.1997s, grad.norm=11.88836193
 17481: 13 [  230/ 1327], train_loss/perplexity = 4.40852737/82.1483994 secs/batch = 0.1998s, grad.norm=11.95766354
 17486: 13 [  235/ 1327], train_loss/perplexity = 4.24524117/69.7725830 secs/batch = 0.1997s, grad.norm=12.16518879
 17491: 13 [  240/ 1327], train_loss/perplexity = 4.04931307/57.3580437 secs/batch = 0.1995s, grad.norm=11.94250011
 17496: 13 [  245/ 1327], train_loss/perplexity = 4.32010841/75.1967773 secs/batch = 0.1948s, grad.norm=12.26730824
 17501: 13 [  250/ 1327], train_loss/perplexity = 4.12167263/61.6622925 secs/batch = 0.1986s, grad.norm=11.47701359
 17506: 13 [  255/ 1327], train_loss/perplexity = 4.13560772/62.5275803 secs/batch = 0.1990s, grad.norm=11.12285709
 17511: 13 [  260/ 1327], train_loss/perplexity = 4.29056168/73.0074615 secs/batch = 0.1986s, grad.norm=12.04276752
 17516: 13 [  265/ 1327], train_loss/perplexity = 4.50812817/90.7517853 secs/batch = 0.1991s, grad.norm=11.53106022
 17521: 13 [  270/ 1327], train_loss/perplexity = 4.61118937/100.6037369 secs/batch = 0.1990s, grad.norm=11.90190697
 17526: 13 [  275/ 1327], train_loss/perplexity = 4.51221514/91.1234436 secs/batch = 0.2001s, grad.norm=11.40674496
 17531: 13 [  280/ 1327], train_loss/perplexity = 4.32626152/75.6609039 secs/batch = 0.1985s, grad.norm=11.64179039
 17536: 13 [  285/ 1327], train_loss/perplexity = 4.60329533/99.8126907 secs/batch = 0.1991s, grad.norm=11.79915237
 17541: 13 [  290/ 1327], train_loss/perplexity = 4.29511547/73.3406830 secs/batch = 0.2000s, grad.norm=11.74659824
 17546: 13 [  295/ 1327], train_loss/perplexity = 4.09362555/59.9568748 secs/batch = 0.1995s, grad.norm=11.52185059
 17551: 13 [  300/ 1327], train_loss/perplexity = 3.69741774/40.3429947 secs/batch = 0.1994s, grad.norm=10.70665836
 17556: 13 [  305/ 1327], train_loss/perplexity = 4.11714220/61.3835678 secs/batch = 0.1979s, grad.norm=11.99719620
 17561: 13 [  310/ 1327], train_loss/perplexity = 4.13028049/62.1953659 secs/batch = 0.1952s, grad.norm=11.91180515
 17566: 13 [  315/ 1327], train_loss/perplexity = 3.67776918/39.5580482 secs/batch = 0.1999s, grad.norm=11.11573410
 17571: 13 [  320/ 1327], train_loss/perplexity = 3.68889284/40.0005341 secs/batch = 0.1983s, grad.norm=12.70748425
 17576: 13 [  325/ 1327], train_loss/perplexity = 3.68151236/39.7063980 secs/batch = 0.1997s, grad.norm=11.02133179
 17581: 13 [  330/ 1327], train_loss/perplexity = 4.23877811/69.3230972 secs/batch = 0.2000s, grad.norm=12.02051735
 17586: 13 [  335/ 1327], train_loss/perplexity = 3.66485929/39.0506401 secs/batch = 0.2000s, grad.norm=10.99286461
 17591: 13 [  340/ 1327], train_loss/perplexity = 4.44024038/84.7953262 secs/batch = 0.2000s, grad.norm=11.83928871
 17596: 13 [  345/ 1327], train_loss/perplexity = 4.23275328/68.9066925 secs/batch = 0.2003s, grad.norm=11.87861824
 17601: 13 [  350/ 1327], train_loss/perplexity = 4.21306372/67.5632172 secs/batch = 0.1997s, grad.norm=12.03321266
 17606: 13 [  355/ 1327], train_loss/perplexity = 4.24057341/69.4476624 secs/batch = 0.1992s, grad.norm=11.53322506
 17611: 13 [  360/ 1327], train_loss/perplexity = 4.34821129/77.3400040 secs/batch = 0.1995s, grad.norm=12.80183029
 17616: 13 [  365/ 1327], train_loss/perplexity = 4.33393955/76.2440643 secs/batch = 0.1993s, grad.norm=12.26345634
 17621: 13 [  370/ 1327], train_loss/perplexity = 4.41746521/82.8859177 secs/batch = 0.1998s, grad.norm=12.41913891
 17626: 13 [  375/ 1327], train_loss/perplexity = 3.78735375/44.1394424 secs/batch = 0.1991s, grad.norm=11.80723000
 17631: 13 [  380/ 1327], train_loss/perplexity = 3.92516971/50.6616745 secs/batch = 0.2012s, grad.norm=11.72207642
 17636: 13 [  385/ 1327], train_loss/perplexity = 4.10312462/60.5291214 secs/batch = 0.1992s, grad.norm=12.49699211
 17641: 13 [  390/ 1327], train_loss/perplexity = 4.23871613/69.3187943 secs/batch = 0.2007s, grad.norm=12.10906982
 17646: 13 [  395/ 1327], train_loss/perplexity = 4.27035856/71.5472870 secs/batch = 0.1949s, grad.norm=12.49869919
 17651: 13 [  400/ 1327], train_loss/perplexity = 4.19932652/66.6414337 secs/batch = 0.1989s, grad.norm=12.03471088
 17656: 13 [  405/ 1327], train_loss/perplexity = 4.45831633/86.3420181 secs/batch = 0.1997s, grad.norm=11.80791187
 17661: 13 [  410/ 1327], train_loss/perplexity = 4.13993263/62.7985916 secs/batch = 0.1990s, grad.norm=11.85850906
 17666: 13 [  415/ 1327], train_loss/perplexity = 4.08324385/59.3376389 secs/batch = 0.1992s, grad.norm=11.87731361
 17671: 13 [  420/ 1327], train_loss/perplexity = 3.78716469/44.1310959 secs/batch = 0.1994s, grad.norm=11.65066719
 17676: 13 [  425/ 1327], train_loss/perplexity = 4.03690386/56.6506729 secs/batch = 0.1991s, grad.norm=12.28159142
 17681: 13 [  430/ 1327], train_loss/perplexity = 4.27923346/72.1850891 secs/batch = 0.1939s, grad.norm=12.58004665
 17686: 13 [  435/ 1327], train_loss/perplexity = 4.29693604/73.4743271 secs/batch = 0.2006s, grad.norm=12.09003639
 17691: 13 [  440/ 1327], train_loss/perplexity = 3.91633105/50.2158661 secs/batch = 0.1998s, grad.norm=12.32557964
 17696: 13 [  445/ 1327], train_loss/perplexity = 4.20937824/67.3146744 secs/batch = 0.1951s, grad.norm=12.10869598
 17701: 13 [  450/ 1327], train_loss/perplexity = 4.15249252/63.5923080 secs/batch = 0.1987s, grad.norm=11.76290512
 17706: 13 [  455/ 1327], train_loss/perplexity = 4.05466652/57.6659279 secs/batch = 0.1994s, grad.norm=11.91225433
 17711: 13 [  460/ 1327], train_loss/perplexity = 4.11298323/61.1288071 secs/batch = 0.2000s, grad.norm=12.78886032
 17716: 13 [  465/ 1327], train_loss/perplexity = 3.79754853/44.5917358 secs/batch = 0.2000s, grad.norm=13.14756870
 17721: 13 [  470/ 1327], train_loss/perplexity = 4.49975681/89.9952393 secs/batch = 0.1992s, grad.norm=11.64213276
 17726: 13 [  475/ 1327], train_loss/perplexity = 4.00467920/54.8542252 secs/batch = 0.1957s, grad.norm=12.06260967
 17731: 13 [  480/ 1327], train_loss/perplexity = 4.07157421/58.6492157 secs/batch = 0.2010s, grad.norm=11.90347958
 17736: 13 [  485/ 1327], train_loss/perplexity = 4.07993031/59.1413460 secs/batch = 0.1992s, grad.norm=12.58432770
 17741: 13 [  490/ 1327], train_loss/perplexity = 3.98130417/53.5868759 secs/batch = 0.2011s, grad.norm=12.50069904
 17746: 13 [  495/ 1327], train_loss/perplexity = 4.06576490/58.3094940 secs/batch = 0.2002s, grad.norm=11.90324593
 17751: 13 [  500/ 1327], train_loss/perplexity = 4.25715113/70.6085434 secs/batch = 0.1990s, grad.norm=13.73287868
 17756: 13 [  505/ 1327], train_loss/perplexity = 4.34311056/76.9465179 secs/batch = 0.1993s, grad.norm=11.33382893
 17761: 13 [  510/ 1327], train_loss/perplexity = 4.63449764/102.9761734 secs/batch = 0.2002s, grad.norm=11.38613605
 17766: 13 [  515/ 1327], train_loss/perplexity = 4.32112646/75.2733765 secs/batch = 0.1995s, grad.norm=11.18135452
 17771: 13 [  520/ 1327], train_loss/perplexity = 4.45386410/85.9584579 secs/batch = 0.1996s, grad.norm=11.81934834
 17776: 13 [  525/ 1327], train_loss/perplexity = 3.99651456/54.4081841 secs/batch = 0.2006s, grad.norm=11.63610744
 17781: 13 [  530/ 1327], train_loss/perplexity = 4.11417627/61.2017784 secs/batch = 0.2005s, grad.norm=12.01761055
 17786: 13 [  535/ 1327], train_loss/perplexity = 4.21691799/67.8241272 secs/batch = 0.1994s, grad.norm=11.43828869
 17791: 13 [  540/ 1327], train_loss/perplexity = 4.29875183/73.6078644 secs/batch = 0.1996s, grad.norm=11.84452534
 17796: 13 [  545/ 1327], train_loss/perplexity = 4.26478720/71.1497803 secs/batch = 0.1952s, grad.norm=12.38700581
 17801: 13 [  550/ 1327], train_loss/perplexity = 4.22610569/68.4501495 secs/batch = 0.1996s, grad.norm=11.50239372
 17806: 13 [  555/ 1327], train_loss/perplexity = 4.12914085/62.1245270 secs/batch = 0.1993s, grad.norm=13.47135162
 17811: 13 [  560/ 1327], train_loss/perplexity = 4.16824436/64.6019363 secs/batch = 0.1997s, grad.norm=12.31876373
 17816: 13 [  565/ 1327], train_loss/perplexity = 4.02028656/55.7170715 secs/batch = 0.1993s, grad.norm=12.27801323
 17821: 13 [  570/ 1327], train_loss/perplexity = 4.06338120/58.1706657 secs/batch = 0.2007s, grad.norm=12.32090855
 17826: 13 [  575/ 1327], train_loss/perplexity = 3.79379797/44.4248047 secs/batch = 0.1944s, grad.norm=11.97449589
 17831: 13 [  580/ 1327], train_loss/perplexity = 4.30299473/73.9208374 secs/batch = 0.1995s, grad.norm=11.76993084
 17836: 13 [  585/ 1327], train_loss/perplexity = 3.90542531/49.6711998 secs/batch = 0.1986s, grad.norm=11.76652145
 17841: 13 [  590/ 1327], train_loss/perplexity = 4.28833103/72.8447876 secs/batch = 0.1993s, grad.norm=11.41344070
 17846: 13 [  595/ 1327], train_loss/perplexity = 4.22970104/68.6966934 secs/batch = 0.1995s, grad.norm=12.73766327
 17851: 13 [  600/ 1327], train_loss/perplexity = 4.34630632/77.1928101 secs/batch = 0.1997s, grad.norm=11.69709110
 17856: 13 [  605/ 1327], train_loss/perplexity = 4.28653431/72.7140274 secs/batch = 0.1993s, grad.norm=11.78686619
 17861: 13 [  610/ 1327], train_loss/perplexity = 4.49411631/89.4890518 secs/batch = 0.1957s, grad.norm=11.81051922
 17866: 13 [  615/ 1327], train_loss/perplexity = 4.08214569/59.2725143 secs/batch = 0.1987s, grad.norm=11.96893120
 17871: 13 [  620/ 1327], train_loss/perplexity = 4.38152313/79.9597321 secs/batch = 0.1946s, grad.norm=11.75082111
 17876: 13 [  625/ 1327], train_loss/perplexity = 4.38993883/80.6354828 secs/batch = 0.1991s, grad.norm=11.53905678
 17881: 13 [  630/ 1327], train_loss/perplexity = 4.41546249/82.7200928 secs/batch = 0.1994s, grad.norm=11.76945496
 17886: 13 [  635/ 1327], train_loss/perplexity = 4.19558764/66.3927383 secs/batch = 0.1999s, grad.norm=11.84174442
 17891: 13 [  640/ 1327], train_loss/perplexity = 4.14984322/63.4240570 secs/batch = 0.2006s, grad.norm=11.53291416
 17896: 13 [  645/ 1327], train_loss/perplexity = 4.50709629/90.6581879 secs/batch = 0.1994s, grad.norm=13.17783070
 17901: 13 [  650/ 1327], train_loss/perplexity = 3.90808725/49.8036003 secs/batch = 0.2000s, grad.norm=11.88974094
 17906: 13 [  655/ 1327], train_loss/perplexity = 4.09090900/59.7942200 secs/batch = 0.1999s, grad.norm=12.47889042
 17911: 13 [  660/ 1327], train_loss/perplexity = 4.05812263/57.8655739 secs/batch = 0.2001s, grad.norm=12.23733521
 17916: 13 [  665/ 1327], train_loss/perplexity = 4.13489866/62.4832573 secs/batch = 0.1993s, grad.norm=12.15472889
 17921: 13 [  670/ 1327], train_loss/perplexity = 4.06361580/58.1843147 secs/batch = 0.1956s, grad.norm=12.02634907
 17926: 13 [  675/ 1327], train_loss/perplexity = 3.89415550/49.1145592 secs/batch = 0.1950s, grad.norm=12.45425797
 17931: 13 [  680/ 1327], train_loss/perplexity = 4.12446404/61.8346596 secs/batch = 0.1985s, grad.norm=12.69101620
 17936: 13 [  685/ 1327], train_loss/perplexity = 3.92717242/50.7632370 secs/batch = 0.1996s, grad.norm=11.21974087
 17941: 13 [  690/ 1327], train_loss/perplexity = 4.33021307/75.9604721 secs/batch = 0.1994s, grad.norm=11.39277649
 17946: 13 [  695/ 1327], train_loss/perplexity = 4.18243313/65.5250931 secs/batch = 0.1992s, grad.norm=11.92440128
 17951: 13 [  700/ 1327], train_loss/perplexity = 4.41244125/82.4705505 secs/batch = 0.2005s, grad.norm=12.40224266
 17956: 13 [  705/ 1327], train_loss/perplexity = 4.07705259/58.9714012 secs/batch = 0.1982s, grad.norm=11.31384373
 17961: 13 [  710/ 1327], train_loss/perplexity = 4.09279490/59.9070930 secs/batch = 0.1991s, grad.norm=11.56797886
 17966: 13 [  715/ 1327], train_loss/perplexity = 3.94770765/51.8164482 secs/batch = 0.1996s, grad.norm=11.82067013
 17971: 13 [  720/ 1327], train_loss/perplexity = 3.94739056/51.8000221 secs/batch = 0.1936s, grad.norm=12.11760712
 17976: 13 [  725/ 1327], train_loss/perplexity = 3.96762180/52.8586731 secs/batch = 0.1990s, grad.norm=11.53835678
 17981: 13 [  730/ 1327], train_loss/perplexity = 4.21098328/67.4228058 secs/batch = 0.2011s, grad.norm=11.98192883
 17986: 13 [  735/ 1327], train_loss/perplexity = 4.29253149/73.1514130 secs/batch = 0.2003s, grad.norm=12.17959595
 17991: 13 [  740/ 1327], train_loss/perplexity = 3.73220181/41.7709808 secs/batch = 0.1987s, grad.norm=11.47568321
 17996: 13 [  745/ 1327], train_loss/perplexity = 4.20988798/67.3489914 secs/batch = 0.2001s, grad.norm=11.99115944
 18001: 13 [  750/ 1327], train_loss/perplexity = 4.04079485/56.8715286 secs/batch = 0.1986s, grad.norm=11.70927715
 18006: 13 [  755/ 1327], train_loss/perplexity = 3.85873985/47.4055748 secs/batch = 0.2005s, grad.norm=11.54697514
 18011: 13 [  760/ 1327], train_loss/perplexity = 3.74477911/42.2996635 secs/batch = 0.1987s, grad.norm=11.71006489
 18016: 13 [  765/ 1327], train_loss/perplexity = 3.90474534/49.6374359 secs/batch = 0.1996s, grad.norm=11.44072914
 18021: 13 [  770/ 1327], train_loss/perplexity = 3.85064220/47.0232506 secs/batch = 0.1996s, grad.norm=12.45565128
 18026: 13 [  775/ 1327], train_loss/perplexity = 3.97638035/53.3236732 secs/batch = 0.1983s, grad.norm=12.29155636
 18031: 13 [  780/ 1327], train_loss/perplexity = 4.30318499/73.9348984 secs/batch = 0.1981s, grad.norm=11.88584042
 18036: 13 [  785/ 1327], train_loss/perplexity = 4.17070198/64.7608948 secs/batch = 0.1987s, grad.norm=12.15187836
 18041: 13 [  790/ 1327], train_loss/perplexity = 3.95017672/51.9445457 secs/batch = 0.2006s, grad.norm=12.10255241
 18046: 13 [  795/ 1327], train_loss/perplexity = 4.27487659/71.8712692 secs/batch = 0.1998s, grad.norm=12.11684513
 18051: 13 [  800/ 1327], train_loss/perplexity = 4.19346237/66.2517853 secs/batch = 0.1997s, grad.norm=11.92035007
 18056: 13 [  805/ 1327], train_loss/perplexity = 4.53610992/93.3270416 secs/batch = 0.2015s, grad.norm=12.00375652
 18061: 13 [  810/ 1327], train_loss/perplexity = 4.14887857/63.3629036 secs/batch = 0.2000s, grad.norm=11.83956623
 18066: 13 [  815/ 1327], train_loss/perplexity = 3.98030615/53.5334206 secs/batch = 0.1999s, grad.norm=11.59109020
 18071: 13 [  820/ 1327], train_loss/perplexity = 3.83494616/46.2909355 secs/batch = 0.1918s, grad.norm=11.23151875
 18076: 13 [  825/ 1327], train_loss/perplexity = 4.06786966/58.4323502 secs/batch = 0.1986s, grad.norm=11.89383411
 18081: 13 [  830/ 1327], train_loss/perplexity = 3.76873422/43.3251915 secs/batch = 0.1995s, grad.norm=11.52093220
 18086: 13 [  835/ 1327], train_loss/perplexity = 4.11229753/61.0869064 secs/batch = 0.1985s, grad.norm=12.26829433
 18091: 13 [  840/ 1327], train_loss/perplexity = 4.12574720/61.9140549 secs/batch = 0.1996s, grad.norm=11.93744659
 18096: 13 [  845/ 1327], train_loss/perplexity = 3.98780394/53.9363098 secs/batch = 0.1991s, grad.norm=12.13738537
 18101: 13 [  850/ 1327], train_loss/perplexity = 4.00038338/54.6190872 secs/batch = 0.2007s, grad.norm=11.65697670
 18106: 13 [  855/ 1327], train_loss/perplexity = 4.08783627/59.6107712 secs/batch = 0.2005s, grad.norm=12.63406658
 18111: 13 [  860/ 1327], train_loss/perplexity = 3.82205558/45.6980476 secs/batch = 0.1995s, grad.norm=11.68664074
 18116: 13 [  865/ 1327], train_loss/perplexity = 4.22231388/68.1910858 secs/batch = 0.1981s, grad.norm=11.76618004
 18121: 13 [  870/ 1327], train_loss/perplexity = 4.07495737/58.8479729 secs/batch = 0.1999s, grad.norm=12.09681225
 18126: 13 [  875/ 1327], train_loss/perplexity = 3.82599401/45.8783798 secs/batch = 0.1984s, grad.norm=11.98140144
 18131: 13 [  880/ 1327], train_loss/perplexity = 3.98323083/53.6902199 secs/batch = 0.2001s, grad.norm=11.21464348
 18136: 13 [  885/ 1327], train_loss/perplexity = 4.20775509/67.2054977 secs/batch = 0.1994s, grad.norm=11.75651169
 18141: 13 [  890/ 1327], train_loss/perplexity = 4.31403208/74.7412415 secs/batch = 0.1995s, grad.norm=11.65533924
 18146: 13 [  895/ 1327], train_loss/perplexity = 4.27479410/71.8653412 secs/batch = 0.1975s, grad.norm=11.65709305
 18151: 13 [  900/ 1327], train_loss/perplexity = 4.08690548/59.5553093 secs/batch = 0.1998s, grad.norm=11.54963684
 18156: 13 [  905/ 1327], train_loss/perplexity = 3.97770858/53.3945427 secs/batch = 0.2006s, grad.norm=11.34812641
 18161: 13 [  910/ 1327], train_loss/perplexity = 4.05001259/57.3981781 secs/batch = 0.1988s, grad.norm=10.55022240
 18166: 13 [  915/ 1327], train_loss/perplexity = 4.21160221/67.4645462 secs/batch = 0.1991s, grad.norm=11.56442165
 18171: 13 [  920/ 1327], train_loss/perplexity = 4.39850712/81.3293610 secs/batch = 0.2000s, grad.norm=11.70393944
 18176: 13 [  925/ 1327], train_loss/perplexity = 4.17614698/65.1144791 secs/batch = 0.1994s, grad.norm=11.80634689
 18181: 13 [  930/ 1327], train_loss/perplexity = 4.19711637/66.4943085 secs/batch = 0.1993s, grad.norm=11.44708061
 18186: 13 [  935/ 1327], train_loss/perplexity = 4.22667217/68.4889297 secs/batch = 0.1997s, grad.norm=11.81968498
 18191: 13 [  940/ 1327], train_loss/perplexity = 4.14788008/63.2996674 secs/batch = 0.1996s, grad.norm=11.50159645
 18196: 13 [  945/ 1327], train_loss/perplexity = 4.41548014/82.7215500 secs/batch = 0.1990s, grad.norm=11.53672600
 18201: 13 [  950/ 1327], train_loss/perplexity = 4.21345139/67.5894165 secs/batch = 0.1940s, grad.norm=11.58846092
 18206: 13 [  955/ 1327], train_loss/perplexity = 4.16109753/64.1418839 secs/batch = 0.1993s, grad.norm=11.80625916
 18211: 13 [  960/ 1327], train_loss/perplexity = 4.43796778/84.6028366 secs/batch = 0.1981s, grad.norm=11.96267414
 18216: 13 [  965/ 1327], train_loss/perplexity = 4.11781883/61.4251175 secs/batch = 0.1992s, grad.norm=11.89345551
 18221: 13 [  970/ 1327], train_loss/perplexity = 4.40499020/81.8583374 secs/batch = 0.1998s, grad.norm=11.74078369
 18226: 13 [  975/ 1327], train_loss/perplexity = 4.10093451/60.3967018 secs/batch = 0.1995s, grad.norm=12.06639194
 18231: 13 [  980/ 1327], train_loss/perplexity = 3.99369907/54.2552109 secs/batch = 0.1997s, grad.norm=11.72441769
 18236: 13 [  985/ 1327], train_loss/perplexity = 4.07066250/58.5957680 secs/batch = 0.1999s, grad.norm=11.91507435
 18241: 13 [  990/ 1327], train_loss/perplexity = 4.27435637/71.8338928 secs/batch = 0.2000s, grad.norm=11.93717194
 18246: 13 [  995/ 1327], train_loss/perplexity = 4.29653502/73.4448700 secs/batch = 0.1986s, grad.norm=11.67249298
 18251: 13 [ 1000/ 1327], train_loss/perplexity = 3.82563114/45.8617363 secs/batch = 0.1989s, grad.norm=11.10404396
 18256: 13 [ 1005/ 1327], train_loss/perplexity = 4.33110476/76.0282364 secs/batch = 0.1995s, grad.norm=11.72583675
 18261: 13 [ 1010/ 1327], train_loss/perplexity = 3.87737703/48.2973671 secs/batch = 0.1992s, grad.norm=10.77618313
 18266: 13 [ 1015/ 1327], train_loss/perplexity = 4.42206573/83.2681198 secs/batch = 0.1994s, grad.norm=11.54530907
 18271: 13 [ 1020/ 1327], train_loss/perplexity = 4.43689060/84.5117493 secs/batch = 0.1992s, grad.norm=11.45702362
 18276: 13 [ 1025/ 1327], train_loss/perplexity = 4.34087420/76.7746277 secs/batch = 0.1984s, grad.norm=11.70337868
 18281: 13 [ 1030/ 1327], train_loss/perplexity = 4.08862972/59.6580887 secs/batch = 0.1992s, grad.norm=11.34032917
 18286: 13 [ 1035/ 1327], train_loss/perplexity = 4.04368639/57.0362129 secs/batch = 0.1998s, grad.norm=11.88544559
 18291: 13 [ 1040/ 1327], train_loss/perplexity = 4.34373140/76.9943008 secs/batch = 0.1997s, grad.norm=12.05670357
 18296: 13 [ 1045/ 1327], train_loss/perplexity = 3.82944179/46.0368309 secs/batch = 0.1999s, grad.norm=11.16385269
 18301: 13 [ 1050/ 1327], train_loss/perplexity = 3.90268397/49.5352211 secs/batch = 0.1964s, grad.norm=10.98616314
 18306: 13 [ 1055/ 1327], train_loss/perplexity = 4.06190109/58.0846291 secs/batch = 0.1939s, grad.norm=11.73171139
 18311: 13 [ 1060/ 1327], train_loss/perplexity = 3.67687368/39.5226402 secs/batch = 0.1954s, grad.norm=12.08613300
 18316: 13 [ 1065/ 1327], train_loss/perplexity = 3.75760746/42.8457947 secs/batch = 0.1979s, grad.norm=11.66303730
 18321: 13 [ 1070/ 1327], train_loss/perplexity = 4.11109638/61.0135765 secs/batch = 0.1983s, grad.norm=11.91871738
 18326: 13 [ 1075/ 1327], train_loss/perplexity = 3.86359119/47.6361160 secs/batch = 0.1996s, grad.norm=11.87954330
 18331: 13 [ 1080/ 1327], train_loss/perplexity = 3.80014110/44.7074928 secs/batch = 0.2016s, grad.norm=11.84753513
 18336: 13 [ 1085/ 1327], train_loss/perplexity = 3.70587063/40.6854553 secs/batch = 0.1998s, grad.norm=11.69574547
 18341: 13 [ 1090/ 1327], train_loss/perplexity = 3.98058176/53.5481758 secs/batch = 0.1991s, grad.norm=12.53825092
 18346: 13 [ 1095/ 1327], train_loss/perplexity = 4.08235693/59.2850342 secs/batch = 0.1947s, grad.norm=12.41969490
 18351: 13 [ 1100/ 1327], train_loss/perplexity = 3.79938459/44.6736832 secs/batch = 0.1990s, grad.norm=12.85204220
 18356: 13 [ 1105/ 1327], train_loss/perplexity = 3.79925680/44.6679764 secs/batch = 0.2004s, grad.norm=12.27093697
 18361: 13 [ 1110/ 1327], train_loss/perplexity = 4.11410046/61.1971397 secs/batch = 0.1930s, grad.norm=12.47625637
 18366: 13 [ 1115/ 1327], train_loss/perplexity = 3.87024736/47.9542465 secs/batch = 0.2002s, grad.norm=11.54858971
 18371: 13 [ 1120/ 1327], train_loss/perplexity = 4.09242392/59.8848724 secs/batch = 0.1988s, grad.norm=11.91989994
 18376: 13 [ 1125/ 1327], train_loss/perplexity = 4.30929899/74.3883209 secs/batch = 0.1983s, grad.norm=22.06596947
 18381: 13 [ 1130/ 1327], train_loss/perplexity = 3.95367956/52.1268196 secs/batch = 0.2001s, grad.norm=11.80896854
 18386: 13 [ 1135/ 1327], train_loss/perplexity = 3.94567156/51.7110519 secs/batch = 0.1991s, grad.norm=11.56182194
 18391: 13 [ 1140/ 1327], train_loss/perplexity = 4.27566242/71.9277725 secs/batch = 0.1991s, grad.norm=12.39321709
 18396: 13 [ 1145/ 1327], train_loss/perplexity = 4.08224821/59.2785912 secs/batch = 0.1998s, grad.norm=11.32873631
 18401: 13 [ 1150/ 1327], train_loss/perplexity = 3.99118018/54.1187210 secs/batch = 0.2003s, grad.norm=11.87596321
 18406: 13 [ 1155/ 1327], train_loss/perplexity = 4.13006401/62.1819038 secs/batch = 0.1990s, grad.norm=12.25749683
 18411: 13 [ 1160/ 1327], train_loss/perplexity = 4.05632257/57.7615051 secs/batch = 0.2004s, grad.norm=12.12188053
 18416: 13 [ 1165/ 1327], train_loss/perplexity = 4.05780649/57.8472824 secs/batch = 0.1929s, grad.norm=11.65650558
 18421: 13 [ 1170/ 1327], train_loss/perplexity = 3.94465518/51.6585236 secs/batch = 0.1995s, grad.norm=11.72137833
 18426: 13 [ 1175/ 1327], train_loss/perplexity = 3.78226542/43.9154167 secs/batch = 0.1942s, grad.norm=11.60397816
 18431: 13 [ 1180/ 1327], train_loss/perplexity = 3.80445051/44.9005699 secs/batch = 0.1979s, grad.norm=11.96325111
 18436: 13 [ 1185/ 1327], train_loss/perplexity = 3.98663163/53.8731194 secs/batch = 0.1999s, grad.norm=12.08575344
 18441: 13 [ 1190/ 1327], train_loss/perplexity = 4.08206701/59.2678490 secs/batch = 0.1939s, grad.norm=11.99012184
 18446: 13 [ 1195/ 1327], train_loss/perplexity = 3.81920052/45.5677643 secs/batch = 0.1998s, grad.norm=11.73686314
 18451: 13 [ 1200/ 1327], train_loss/perplexity = 3.81261516/45.2686691 secs/batch = 0.1997s, grad.norm=11.94769478
 18456: 13 [ 1205/ 1327], train_loss/perplexity = 3.83695912/46.3842125 secs/batch = 0.2003s, grad.norm=12.02778435
 18461: 13 [ 1210/ 1327], train_loss/perplexity = 3.53518391/34.3013229 secs/batch = 0.1994s, grad.norm=11.80799198
 18466: 13 [ 1215/ 1327], train_loss/perplexity = 3.70929766/40.8251228 secs/batch = 0.1999s, grad.norm=11.32637691
 18471: 13 [ 1220/ 1327], train_loss/perplexity = 3.88090420/48.4680214 secs/batch = 0.1991s, grad.norm=12.99503326
 18476: 13 [ 1225/ 1327], train_loss/perplexity = 3.64923286/38.4451637 secs/batch = 0.1995s, grad.norm=12.29871368
 18481: 13 [ 1230/ 1327], train_loss/perplexity = 3.95058918/51.9659767 secs/batch = 0.1991s, grad.norm=11.92807770
 18486: 13 [ 1235/ 1327], train_loss/perplexity = 3.95289183/52.0857735 secs/batch = 0.2000s, grad.norm=12.07648563
 18491: 13 [ 1240/ 1327], train_loss/perplexity = 4.05965996/57.9546013 secs/batch = 0.1999s, grad.norm=12.37990665
 18496: 13 [ 1245/ 1327], train_loss/perplexity = 3.96464729/52.7016792 secs/batch = 0.1954s, grad.norm=12.83830452
 18501: 13 [ 1250/ 1327], train_loss/perplexity = 4.16141653/64.1623459 secs/batch = 0.1992s, grad.norm=11.73017120
 18506: 13 [ 1255/ 1327], train_loss/perplexity = 4.10204697/60.4639282 secs/batch = 0.2000s, grad.norm=11.61645985
 18511: 13 [ 1260/ 1327], train_loss/perplexity = 3.89077330/48.9487228 secs/batch = 0.2000s, grad.norm=12.62994576
 18516: 13 [ 1265/ 1327], train_loss/perplexity = 4.17975855/65.3500748 secs/batch = 0.1975s, grad.norm=12.81650162
 18521: 13 [ 1270/ 1327], train_loss/perplexity = 3.87858081/48.3555412 secs/batch = 0.1990s, grad.norm=12.13476276
 18526: 13 [ 1275/ 1327], train_loss/perplexity = 4.02091217/55.7519379 secs/batch = 0.1994s, grad.norm=12.27200794
 18531: 13 [ 1280/ 1327], train_loss/perplexity = 3.87180042/48.0287819 secs/batch = 0.1991s, grad.norm=12.22973728
 18536: 13 [ 1285/ 1327], train_loss/perplexity = 3.80137396/44.7626457 secs/batch = 0.2005s, grad.norm=12.18944836
 18541: 13 [ 1290/ 1327], train_loss/perplexity = 4.10093594/60.3967896 secs/batch = 0.1932s, grad.norm=12.63632488
 18546: 13 [ 1295/ 1327], train_loss/perplexity = 3.96982980/52.9755135 secs/batch = 0.1955s, grad.norm=12.00998116
 18551: 13 [ 1300/ 1327], train_loss/perplexity = 4.16201401/64.2006912 secs/batch = 0.1975s, grad.norm=11.53981400
 18556: 13 [ 1305/ 1327], train_loss/perplexity = 4.28195095/72.3815155 secs/batch = 0.2006s, grad.norm=12.83730412
 18561: 13 [ 1310/ 1327], train_loss/perplexity = 4.54794216/94.4378738 secs/batch = 0.1992s, grad.norm=11.85346889
 18566: 13 [ 1315/ 1327], train_loss/perplexity = 4.30794334/74.2875443 secs/batch = 0.2006s, grad.norm=12.39741898
 18571: 13 [ 1320/ 1327], train_loss/perplexity = 4.30250406/73.8845749 secs/batch = 0.2003s, grad.norm=11.82499313
 18576: 13 [ 1325/ 1327], train_loss/perplexity = 4.27504826/71.8836060 secs/batch = 0.1992s, grad.norm=12.09761429
Epoch training time: 264.1407480239868
	> validation loss = 4.76485920, perplexity = 117.31459808
	> validation loss = 4.67407942, perplexity = 107.13389587
	> validation loss = 4.60768604, perplexity = 100.25189972
	> validation loss = 4.62905931, perplexity = 102.41767883
	> validation loss = 4.84849119, perplexity = 127.54779816
	> validation loss = 4.67672539, perplexity = 107.41774750
	> validation loss = 4.70558405, perplexity = 110.56284332
	> validation loss = 4.50747538, perplexity = 90.69256592
	> validation loss = 4.31533575, perplexity = 74.83874512
	> validation loss = 4.42607498, perplexity = 83.60263062
	> validation loss = 4.57303619, perplexity = 96.83768463
	> validation loss = 4.64089108, perplexity = 103.63665771
	> validation loss = 4.54207706, perplexity = 93.88560486
	> validation loss = 4.37791157, perplexity = 79.67147064
	> validation loss = 4.30714226, perplexity = 74.22805786
	> validation loss = 4.28418541, perplexity = 72.54342651
	> validation loss = 4.75343895, perplexity = 115.98246002
	> validation loss = 4.28261518, perplexity = 72.42961121
	> validation loss = 4.80089045, perplexity = 121.61866760
	> validation loss = 4.62452269, perplexity = 101.95409393
	> validation loss = 4.46475554, perplexity = 86.89978027
at the end of epoch: 13
train loss = 4.16748467, perplexity = 64.55287585
validation loss = 4.57165890, perplexity = 96.70439990
Saved model cv/epoch013_4.5717.model
 18583: 14 [    5/ 1327], train_loss/perplexity = 4.34780598/77.3086624 secs/batch = 0.1999s, grad.norm=11.85192776
 18588: 14 [   10/ 1327], train_loss/perplexity = 3.89756918/49.2825050 secs/batch = 0.1994s, grad.norm=12.37688637
 18593: 14 [   15/ 1327], train_loss/perplexity = 4.27654171/71.9910431 secs/batch = 0.1986s, grad.norm=11.23931217
 18598: 14 [   20/ 1327], train_loss/perplexity = 4.45492887/86.0500336 secs/batch = 0.1989s, grad.norm=11.42045116
 18603: 14 [   25/ 1327], train_loss/perplexity = 4.31477642/74.7968979 secs/batch = 0.1995s, grad.norm=12.09165192
 18608: 14 [   30/ 1327], train_loss/perplexity = 4.30590487/74.1362686 secs/batch = 0.2001s, grad.norm=11.60175419
 18613: 14 [   35/ 1327], train_loss/perplexity = 4.11131287/61.0267868 secs/batch = 0.2000s, grad.norm=12.24282455
 18618: 14 [   40/ 1327], train_loss/perplexity = 4.13678885/62.6014748 secs/batch = 0.1992s, grad.norm=11.95906353
 18623: 14 [   45/ 1327], train_loss/perplexity = 3.89849091/49.3279533 secs/batch = 0.1986s, grad.norm=11.28372955
 18628: 14 [   50/ 1327], train_loss/perplexity = 4.16353083/64.2981491 secs/batch = 0.1990s, grad.norm=11.86244202
 18633: 14 [   55/ 1327], train_loss/perplexity = 4.11152744/61.0398827 secs/batch = 0.1999s, grad.norm=11.74904633
 18638: 14 [   60/ 1327], train_loss/perplexity = 4.42063427/83.1490097 secs/batch = 0.1991s, grad.norm=12.61469936
 18643: 14 [   65/ 1327], train_loss/perplexity = 3.91862488/50.3311844 secs/batch = 0.1969s, grad.norm=11.69521618
 18648: 14 [   70/ 1327], train_loss/perplexity = 3.79085302/44.2941666 secs/batch = 0.1998s, grad.norm=11.76800823
 18653: 14 [   75/ 1327], train_loss/perplexity = 3.62173128/37.4022675 secs/batch = 0.1990s, grad.norm=11.09665203
 18658: 14 [   80/ 1327], train_loss/perplexity = 4.06586885/58.3155556 secs/batch = 0.1990s, grad.norm=12.26646423
 18663: 14 [   85/ 1327], train_loss/perplexity = 4.07984543/59.1363297 secs/batch = 0.2008s, grad.norm=12.60036755
 18668: 14 [   90/ 1327], train_loss/perplexity = 4.13400650/62.4275398 secs/batch = 0.1995s, grad.norm=12.53334808
 18673: 14 [   95/ 1327], train_loss/perplexity = 3.91279387/50.0385590 secs/batch = 0.2000s, grad.norm=12.08248901
 18678: 14 [  100/ 1327], train_loss/perplexity = 4.25074005/70.1573105 secs/batch = 0.1990s, grad.norm=12.10707951
 18683: 14 [  105/ 1327], train_loss/perplexity = 4.06310844/58.1548004 secs/batch = 0.1994s, grad.norm=12.74840355
 18688: 14 [  110/ 1327], train_loss/perplexity = 3.96935582/52.9504089 secs/batch = 0.1974s, grad.norm=12.32613850
 18693: 14 [  115/ 1327], train_loss/perplexity = 4.01355886/55.3434792 secs/batch = 0.1988s, grad.norm=12.14828587
 18698: 14 [  120/ 1327], train_loss/perplexity = 4.05414820/57.6360474 secs/batch = 0.1992s, grad.norm=12.25002098
 18703: 14 [  125/ 1327], train_loss/perplexity = 4.15018082/63.4454727 secs/batch = 0.1987s, grad.norm=12.49713135
 18708: 14 [  130/ 1327], train_loss/perplexity = 4.10637093/60.7259369 secs/batch = 0.2001s, grad.norm=12.56665516
 18713: 14 [  135/ 1327], train_loss/perplexity = 4.03525734/56.5574722 secs/batch = 0.1988s, grad.norm=12.52508450
 18718: 14 [  140/ 1327], train_loss/perplexity = 4.33490515/76.3177185 secs/batch = 0.1995s, grad.norm=12.39048100
 18723: 14 [  145/ 1327], train_loss/perplexity = 4.24506187/69.7600784 secs/batch = 0.2000s, grad.norm=13.61248112
 18728: 14 [  150/ 1327], train_loss/perplexity = 4.33289099/76.1641617 secs/batch = 0.1988s, grad.norm=11.94580936
 18733: 14 [  155/ 1327], train_loss/perplexity = 4.48548317/88.7198105 secs/batch = 0.2000s, grad.norm=12.34000683
 18738: 14 [  160/ 1327], train_loss/perplexity = 4.16149616/64.1674576 secs/batch = 0.2004s, grad.norm=11.94581795
 18743: 14 [  165/ 1327], train_loss/perplexity = 4.28055620/72.2806320 secs/batch = 0.2014s, grad.norm=11.94922924
 18748: 14 [  170/ 1327], train_loss/perplexity = 4.09610653/60.1058121 secs/batch = 0.1994s, grad.norm=11.91976547
 18753: 14 [  175/ 1327], train_loss/perplexity = 4.39682627/81.1927719 secs/batch = 0.2003s, grad.norm=12.24222374
 18758: 14 [  180/ 1327], train_loss/perplexity = 4.20961666/67.3307266 secs/batch = 0.2010s, grad.norm=11.72286606
 18763: 14 [  185/ 1327], train_loss/perplexity = 4.45410299/85.9789963 secs/batch = 0.2012s, grad.norm=11.84806824
 18768: 14 [  190/ 1327], train_loss/perplexity = 4.09917021/60.2902374 secs/batch = 0.1996s, grad.norm=11.68860435
 18773: 14 [  195/ 1327], train_loss/perplexity = 4.40293884/81.6905899 secs/batch = 0.1989s, grad.norm=11.58393574
 18778: 14 [  200/ 1327], train_loss/perplexity = 4.17178345/64.8309708 secs/batch = 0.1999s, grad.norm=11.71487141
 18783: 14 [  205/ 1327], train_loss/perplexity = 4.38683605/80.3856812 secs/batch = 0.1951s, grad.norm=11.83496666
 18788: 14 [  210/ 1327], train_loss/perplexity = 4.25320339/70.3303452 secs/batch = 0.1986s, grad.norm=11.54647732
 18793: 14 [  215/ 1327], train_loss/perplexity = 4.31894159/75.1090927 secs/batch = 0.2003s, grad.norm=11.37570763
 18798: 14 [  220/ 1327], train_loss/perplexity = 4.27888060/72.1596222 secs/batch = 0.1997s, grad.norm=11.63639641
 18803: 14 [  225/ 1327], train_loss/perplexity = 4.48682690/88.8391037 secs/batch = 0.2009s, grad.norm=12.37164497
 18808: 14 [  230/ 1327], train_loss/perplexity = 4.29081154/73.0257111 secs/batch = 0.1983s, grad.norm=12.45214558
 18813: 14 [  235/ 1327], train_loss/perplexity = 4.14673948/63.2275085 secs/batch = 0.2002s, grad.norm=12.58365059
 18818: 14 [  240/ 1327], train_loss/perplexity = 3.99098563/54.1081924 secs/batch = 0.2004s, grad.norm=12.01550484
 18823: 14 [  245/ 1327], train_loss/perplexity = 4.27739239/72.0523071 secs/batch = 0.1997s, grad.norm=11.71338081
 18828: 14 [  250/ 1327], train_loss/perplexity = 4.06601238/58.3239250 secs/batch = 0.1996s, grad.norm=11.04715729
 18833: 14 [  255/ 1327], train_loss/perplexity = 4.07691622/58.9633598 secs/batch = 0.1972s, grad.norm=11.31552792
 18838: 14 [  260/ 1327], train_loss/perplexity = 4.28362370/72.5026932 secs/batch = 0.1994s, grad.norm=12.32197189
 18843: 14 [  265/ 1327], train_loss/perplexity = 4.42305899/83.3508682 secs/batch = 0.1991s, grad.norm=11.55254745
 18848: 14 [  270/ 1327], train_loss/perplexity = 4.52922678/92.6868668 secs/batch = 0.1996s, grad.norm=12.08390808
 18853: 14 [  275/ 1327], train_loss/perplexity = 4.45090342/85.7043381 secs/batch = 0.1998s, grad.norm=12.24723434
 18858: 14 [  280/ 1327], train_loss/perplexity = 4.25715780/70.6090164 secs/batch = 0.1996s, grad.norm=11.85565948
 18863: 14 [  285/ 1327], train_loss/perplexity = 4.52215767/92.0339661 secs/batch = 0.1994s, grad.norm=11.28338146
 18868: 14 [  290/ 1327], train_loss/perplexity = 4.24200964/69.5474777 secs/batch = 0.1997s, grad.norm=11.60844994
 18873: 14 [  295/ 1327], train_loss/perplexity = 4.01357174/55.3441925 secs/batch = 0.1993s, grad.norm=11.48212433
 18878: 14 [  300/ 1327], train_loss/perplexity = 3.63393211/37.8614006 secs/batch = 0.1987s, grad.norm=11.25319195
 18883: 14 [  305/ 1327], train_loss/perplexity = 4.06072235/58.0162048 secs/batch = 0.1990s, grad.norm=11.32042408
 18888: 14 [  310/ 1327], train_loss/perplexity = 4.15799046/63.9428978 secs/batch = 0.1982s, grad.norm=11.96826363
 18893: 14 [  315/ 1327], train_loss/perplexity = 3.62973690/37.7028961 secs/batch = 0.1949s, grad.norm=11.24336243
 18898: 14 [  320/ 1327], train_loss/perplexity = 3.60402489/36.7458344 secs/batch = 0.1998s, grad.norm=12.71022034
 18903: 14 [  325/ 1327], train_loss/perplexity = 3.67458153/39.4321518 secs/batch = 0.1933s, grad.norm=11.47810173
 18908: 14 [  330/ 1327], train_loss/perplexity = 4.20818567/67.2344437 secs/batch = 0.1993s, grad.norm=11.85454464
 18913: 14 [  335/ 1327], train_loss/perplexity = 3.61896825/37.2990646 secs/batch = 0.1986s, grad.norm=10.81312943
 18918: 14 [  340/ 1327], train_loss/perplexity = 4.31708908/74.9700775 secs/batch = 0.1990s, grad.norm=11.49533558
 18923: 14 [  345/ 1327], train_loss/perplexity = 4.20527363/67.0389404 secs/batch = 0.1998s, grad.norm=11.51889324
 18928: 14 [  350/ 1327], train_loss/perplexity = 4.14889240/63.3637810 secs/batch = 0.1996s, grad.norm=11.87419128
 18933: 14 [  355/ 1327], train_loss/perplexity = 4.24950123/70.0704575 secs/batch = 0.1991s, grad.norm=12.20355988
 18938: 14 [  360/ 1327], train_loss/perplexity = 4.34826994/77.3445358 secs/batch = 0.2008s, grad.norm=12.93036747
 18943: 14 [  365/ 1327], train_loss/perplexity = 4.30253410/73.8867950 secs/batch = 0.1975s, grad.norm=11.97635078
 18948: 14 [  370/ 1327], train_loss/perplexity = 4.30765915/74.2664413 secs/batch = 0.1967s, grad.norm=11.83376789
 18953: 14 [  375/ 1327], train_loss/perplexity = 3.77685213/43.6783333 secs/batch = 0.1974s, grad.norm=11.53087139
 18958: 14 [  380/ 1327], train_loss/perplexity = 3.81566429/45.4069099 secs/batch = 0.1928s, grad.norm=11.46956921
 18963: 14 [  385/ 1327], train_loss/perplexity = 4.04249334/56.9682083 secs/batch = 0.1981s, grad.norm=12.69986057
 18968: 14 [  390/ 1327], train_loss/perplexity = 4.17741680/65.1972198 secs/batch = 0.1991s, grad.norm=11.90891647
 18973: 14 [  395/ 1327], train_loss/perplexity = 4.20649290/67.1207275 secs/batch = 0.1992s, grad.norm=12.50151539
 18978: 14 [  400/ 1327], train_loss/perplexity = 4.19718695/66.4990005 secs/batch = 0.1988s, grad.norm=12.01055050
 18983: 14 [  405/ 1327], train_loss/perplexity = 4.47402000/87.7086029 secs/batch = 0.1996s, grad.norm=12.09576893
 18988: 14 [  410/ 1327], train_loss/perplexity = 4.10867691/60.8661346 secs/batch = 0.2000s, grad.norm=11.82198334
 18993: 14 [  415/ 1327], train_loss/perplexity = 3.99295330/54.2147675 secs/batch = 0.1996s, grad.norm=11.92882061
 18998: 14 [  420/ 1327], train_loss/perplexity = 3.69937849/40.4221725 secs/batch = 0.1998s, grad.norm=11.55116272
 19003: 14 [  425/ 1327], train_loss/perplexity = 4.06345510/58.1749649 secs/batch = 0.1978s, grad.norm=12.44241333
 19008: 14 [  430/ 1327], train_loss/perplexity = 4.26300144/71.0228348 secs/batch = 0.1995s, grad.norm=12.76913357
 19013: 14 [  435/ 1327], train_loss/perplexity = 4.23209906/68.8616257 secs/batch = 0.1999s, grad.norm=13.19082737
 19018: 14 [  440/ 1327], train_loss/perplexity = 3.83095312/46.1064606 secs/batch = 0.1999s, grad.norm=12.13939095
 19023: 14 [  445/ 1327], train_loss/perplexity = 4.15045357/63.4627800 secs/batch = 0.1993s, grad.norm=12.42140293
 19028: 14 [  450/ 1327], train_loss/perplexity = 4.02360439/55.9022369 secs/batch = 0.1993s, grad.norm=11.90802383
 19033: 14 [  455/ 1327], train_loss/perplexity = 4.07427120/58.8076057 secs/batch = 0.1918s, grad.norm=11.49956131
 19038: 14 [  460/ 1327], train_loss/perplexity = 4.03382349/56.4764366 secs/batch = 0.1969s, grad.norm=11.94413757
 19043: 14 [  465/ 1327], train_loss/perplexity = 3.79238439/44.3620491 secs/batch = 0.1996s, grad.norm=12.91549110
 19048: 14 [  470/ 1327], train_loss/perplexity = 4.54806805/94.4497604 secs/batch = 0.1994s, grad.norm=11.32165909
 19053: 14 [  475/ 1327], train_loss/perplexity = 3.91781187/50.2902832 secs/batch = 0.2001s, grad.norm=12.26927853
 19058: 14 [  480/ 1327], train_loss/perplexity = 4.08378744/59.3699036 secs/batch = 0.1994s, grad.norm=12.14833069
 19063: 14 [  485/ 1327], train_loss/perplexity = 4.08073759/59.1891098 secs/batch = 0.2004s, grad.norm=12.31780815
 19068: 14 [  490/ 1327], train_loss/perplexity = 3.95226645/52.0532112 secs/batch = 0.1996s, grad.norm=12.80089664
 19073: 14 [  495/ 1327], train_loss/perplexity = 3.97008133/52.9888420 secs/batch = 0.1996s, grad.norm=11.66174698
 19078: 14 [  500/ 1327], train_loss/perplexity = 4.21259689/67.5316849 secs/batch = 0.1988s, grad.norm=12.54871464
 19083: 14 [  505/ 1327], train_loss/perplexity = 4.31353426/74.7040482 secs/batch = 0.1937s, grad.norm=10.98264122
 19088: 14 [  510/ 1327], train_loss/perplexity = 4.55370665/94.9838257 secs/batch = 0.2009s, grad.norm=11.65860653
 19093: 14 [  515/ 1327], train_loss/perplexity = 4.30200434/73.8476639 secs/batch = 0.2012s, grad.norm=12.02038860
 19098: 14 [  520/ 1327], train_loss/perplexity = 4.39248800/80.8413010 secs/batch = 0.1996s, grad.norm=12.35941887
 19103: 14 [  525/ 1327], train_loss/perplexity = 3.98107028/53.5743446 secs/batch = 0.1988s, grad.norm=11.46636868
 19108: 14 [  530/ 1327], train_loss/perplexity = 4.09325504/59.9346657 secs/batch = 0.1978s, grad.norm=12.53291321
 19113: 14 [  535/ 1327], train_loss/perplexity = 4.18817043/65.9021072 secs/batch = 0.1994s, grad.norm=11.89743614
 19118: 14 [  540/ 1327], train_loss/perplexity = 4.27833366/72.1201630 secs/batch = 0.1988s, grad.norm=11.97714520
 19123: 14 [  545/ 1327], train_loss/perplexity = 4.24273682/69.5980682 secs/batch = 0.1983s, grad.norm=12.08300781
 19128: 14 [  550/ 1327], train_loss/perplexity = 4.20936346/67.3136749 secs/batch = 0.1944s, grad.norm=11.88707638
 19133: 14 [  555/ 1327], train_loss/perplexity = 4.09791231/60.2144470 secs/batch = 0.1998s, grad.norm=11.96630001
 19138: 14 [  560/ 1327], train_loss/perplexity = 4.12824821/62.0690956 secs/batch = 0.1999s, grad.norm=12.71682072
 19143: 14 [  565/ 1327], train_loss/perplexity = 4.02402115/55.9255409 secs/batch = 0.2000s, grad.norm=12.71642685
 19148: 14 [  570/ 1327], train_loss/perplexity = 4.04022598/56.8391876 secs/batch = 0.1994s, grad.norm=12.15616989
 19153: 14 [  575/ 1327], train_loss/perplexity = 3.76930785/43.3500481 secs/batch = 0.1996s, grad.norm=11.98224068
 19158: 14 [  580/ 1327], train_loss/perplexity = 4.30221415/73.8631592 secs/batch = 0.1996s, grad.norm=12.64667225
 19163: 14 [  585/ 1327], train_loss/perplexity = 3.83782983/46.4246140 secs/batch = 0.1997s, grad.norm=11.55996704
 19168: 14 [  590/ 1327], train_loss/perplexity = 4.24603462/69.8279648 secs/batch = 0.2001s, grad.norm=12.37263489
 19173: 14 [  595/ 1327], train_loss/perplexity = 4.16679859/64.5086060 secs/batch = 0.2011s, grad.norm=12.49077511
 19178: 14 [  600/ 1327], train_loss/perplexity = 4.27030945/71.5437698 secs/batch = 0.1991s, grad.norm=11.39999199
 19183: 14 [  605/ 1327], train_loss/perplexity = 4.23950815/69.3737259 secs/batch = 0.1962s, grad.norm=11.70756054
 19188: 14 [  610/ 1327], train_loss/perplexity = 4.44158459/84.9093781 secs/batch = 0.2006s, grad.norm=11.86535835
 19193: 14 [  615/ 1327], train_loss/perplexity = 4.09264278/59.8979797 secs/batch = 0.1990s, grad.norm=11.84200001
 19198: 14 [  620/ 1327], train_loss/perplexity = 4.40010691/81.4595795 secs/batch = 0.2002s, grad.norm=11.81423569
 19203: 14 [  625/ 1327], train_loss/perplexity = 4.39873409/81.3478241 secs/batch = 0.1994s, grad.norm=11.78073883
 19208: 14 [  630/ 1327], train_loss/perplexity = 4.44352388/85.0742035 secs/batch = 0.2003s, grad.norm=12.02546310
 19213: 14 [  635/ 1327], train_loss/perplexity = 4.13696337/62.6124001 secs/batch = 0.2003s, grad.norm=11.82188702
 19218: 14 [  640/ 1327], train_loss/perplexity = 4.16799355/64.5857315 secs/batch = 0.1988s, grad.norm=11.50619411
 19223: 14 [  645/ 1327], train_loss/perplexity = 4.45202541/85.8005524 secs/batch = 0.1996s, grad.norm=12.58694363
 19228: 14 [  650/ 1327], train_loss/perplexity = 3.92612267/50.7099762 secs/batch = 0.1990s, grad.norm=12.58449554
 19233: 14 [  655/ 1327], train_loss/perplexity = 4.05144978/57.4807320 secs/batch = 0.1996s, grad.norm=12.47699261
 19238: 14 [  660/ 1327], train_loss/perplexity = 3.99941349/54.5661354 secs/batch = 0.1988s, grad.norm=12.11957836
 19243: 14 [  665/ 1327], train_loss/perplexity = 4.11047697/60.9757957 secs/batch = 0.1938s, grad.norm=11.96505833
 19248: 14 [  670/ 1327], train_loss/perplexity = 4.10051107/60.3711357 secs/batch = 0.1972s, grad.norm=11.65773296
 19253: 14 [  675/ 1327], train_loss/perplexity = 3.86428022/47.6689491 secs/batch = 0.1979s, grad.norm=12.06339359
 19258: 14 [  680/ 1327], train_loss/perplexity = 4.08162355/59.2415733 secs/batch = 0.1995s, grad.norm=12.17838669
 19263: 14 [  685/ 1327], train_loss/perplexity = 3.91237020/50.0173645 secs/batch = 0.2008s, grad.norm=11.68815327
 19268: 14 [  690/ 1327], train_loss/perplexity = 4.28500605/72.6029892 secs/batch = 0.2002s, grad.norm=11.51088333
 19273: 14 [  695/ 1327], train_loss/perplexity = 4.08166695/59.2441444 secs/batch = 0.1995s, grad.norm=11.79802227
 19278: 14 [  700/ 1327], train_loss/perplexity = 4.37827110/79.7001190 secs/batch = 0.1984s, grad.norm=12.44588470
 19283: 14 [  705/ 1327], train_loss/perplexity = 4.08870649/59.6626663 secs/batch = 0.1994s, grad.norm=11.50322151
 19288: 14 [  710/ 1327], train_loss/perplexity = 3.99423981/54.2845573 secs/batch = 0.1992s, grad.norm=11.89465904
 19293: 14 [  715/ 1327], train_loss/perplexity = 3.95212460/52.0458260 secs/batch = 0.2001s, grad.norm=11.99450397
 19298: 14 [  720/ 1327], train_loss/perplexity = 3.84365344/46.6957626 secs/batch = 0.1943s, grad.norm=12.40118694
 19303: 14 [  725/ 1327], train_loss/perplexity = 3.91839981/50.3198586 secs/batch = 0.1994s, grad.norm=11.93736076
 19308: 14 [  730/ 1327], train_loss/perplexity = 4.16503334/64.3948288 secs/batch = 0.1991s, grad.norm=12.17822552
 19313: 14 [  735/ 1327], train_loss/perplexity = 4.23313618/68.9330826 secs/batch = 0.1955s, grad.norm=12.29337883
 19318: 14 [  740/ 1327], train_loss/perplexity = 3.70758057/40.7550812 secs/batch = 0.1994s, grad.norm=11.70479107
 19323: 14 [  745/ 1327], train_loss/perplexity = 4.17948294/65.3320618 secs/batch = 0.1988s, grad.norm=12.54147911
 19328: 14 [  750/ 1327], train_loss/perplexity = 3.95908880/52.4095497 secs/batch = 0.1990s, grad.norm=11.89250565
 19333: 14 [  755/ 1327], train_loss/perplexity = 3.87033629/47.9585114 secs/batch = 0.1946s, grad.norm=12.05950451
 19338: 14 [  760/ 1327], train_loss/perplexity = 3.72009325/41.2682419 secs/batch = 0.1927s, grad.norm=11.45629597
 19343: 14 [  765/ 1327], train_loss/perplexity = 3.86434579/47.6720734 secs/batch = 0.1991s, grad.norm=11.29750729
 19348: 14 [  770/ 1327], train_loss/perplexity = 3.81357074/45.3119469 secs/batch = 0.2005s, grad.norm=12.01100159
 19353: 14 [  775/ 1327], train_loss/perplexity = 3.96960402/52.9635544 secs/batch = 0.1997s, grad.norm=12.28673458
 19358: 14 [  780/ 1327], train_loss/perplexity = 4.29649639/73.4420319 secs/batch = 0.1999s, grad.norm=11.92949867
 19363: 14 [  785/ 1327], train_loss/perplexity = 4.16736031/64.5448456 secs/batch = 0.1934s, grad.norm=12.44238091
 19368: 14 [  790/ 1327], train_loss/perplexity = 3.89787149/49.2974091 secs/batch = 0.1991s, grad.norm=11.81805420
 19373: 14 [  795/ 1327], train_loss/perplexity = 4.28844070/72.8527832 secs/batch = 0.1986s, grad.norm=12.51419163
 19378: 14 [  800/ 1327], train_loss/perplexity = 4.15977573/64.0571518 secs/batch = 0.1989s, grad.norm=12.40838528
 19383: 14 [  805/ 1327], train_loss/perplexity = 4.49804115/89.8409729 secs/batch = 0.2005s, grad.norm=12.19805336
 19388: 14 [  810/ 1327], train_loss/perplexity = 4.08318281/59.3340187 secs/batch = 0.1999s, grad.norm=11.29868793
 19393: 14 [  815/ 1327], train_loss/perplexity = 3.99789166/54.4831619 secs/batch = 0.2006s, grad.norm=11.82921410
 19398: 14 [  820/ 1327], train_loss/perplexity = 3.84013271/46.5316505 secs/batch = 0.1997s, grad.norm=11.26870441
 19403: 14 [  825/ 1327], train_loss/perplexity = 3.99194050/54.1598854 secs/batch = 0.1981s, grad.norm=11.92641926
 19408: 14 [  830/ 1327], train_loss/perplexity = 3.74351883/42.2463875 secs/batch = 0.1995s, grad.norm=11.88848972
 19413: 14 [  835/ 1327], train_loss/perplexity = 4.08227110/59.2799492 secs/batch = 0.1979s, grad.norm=12.12418747
 19418: 14 [  840/ 1327], train_loss/perplexity = 4.14669180/63.2244949 secs/batch = 0.1952s, grad.norm=12.19123554
 19423: 14 [  845/ 1327], train_loss/perplexity = 4.00653458/54.9560928 secs/batch = 0.1945s, grad.norm=12.36634064
 19428: 14 [  850/ 1327], train_loss/perplexity = 4.04472780/57.0956421 secs/batch = 0.2003s, grad.norm=12.51833820
 19433: 14 [  855/ 1327], train_loss/perplexity = 4.06427383/58.2226143 secs/batch = 0.1997s, grad.norm=11.99767208
 19438: 14 [  860/ 1327], train_loss/perplexity = 3.73760462/41.9972687 secs/batch = 0.1992s, grad.norm=11.22034836
 19443: 14 [  865/ 1327], train_loss/perplexity = 4.20177841/66.8050308 secs/batch = 0.1979s, grad.norm=12.15879917
 19448: 14 [  870/ 1327], train_loss/perplexity = 4.06765366/58.4197273 secs/batch = 0.1988s, grad.norm=12.21017551
 19453: 14 [  875/ 1327], train_loss/perplexity = 3.73149490/41.7414627 secs/batch = 0.1996s, grad.norm=11.79812145
 19458: 14 [  880/ 1327], train_loss/perplexity = 3.96779895/52.8680382 secs/batch = 0.1988s, grad.norm=12.04510212
 19463: 14 [  885/ 1327], train_loss/perplexity = 4.16649485/64.4890137 secs/batch = 0.1995s, grad.norm=11.77160549
 19468: 14 [  890/ 1327], train_loss/perplexity = 4.27816010/72.1076431 secs/batch = 0.1990s, grad.norm=11.73120975
 19473: 14 [  895/ 1327], train_loss/perplexity = 4.16846180/64.6159821 secs/batch = 0.1967s, grad.norm=11.79013634
 19478: 14 [  900/ 1327], train_loss/perplexity = 4.06739616/58.4046898 secs/batch = 0.2006s, grad.norm=11.69563770
 19483: 14 [  905/ 1327], train_loss/perplexity = 3.92196894/50.4997787 secs/batch = 0.2004s, grad.norm=11.11425400
 19488: 14 [  910/ 1327], train_loss/perplexity = 4.01696301/55.5321999 secs/batch = 0.1982s, grad.norm=10.92946625
 19493: 14 [  915/ 1327], train_loss/perplexity = 4.23539686/69.0890884 secs/batch = 0.1996s, grad.norm=12.00529575
 19498: 14 [  920/ 1327], train_loss/perplexity = 4.31412601/74.7482681 secs/batch = 0.2004s, grad.norm=12.10928726
 19503: 14 [  925/ 1327], train_loss/perplexity = 4.16371727/64.3101349 secs/batch = 0.1973s, grad.norm=11.72710609
 19508: 14 [  930/ 1327], train_loss/perplexity = 4.11894035/61.4940453 secs/batch = 0.1998s, grad.norm=11.58653069
 19513: 14 [  935/ 1327], train_loss/perplexity = 4.26831055/71.4009018 secs/batch = 0.2001s, grad.norm=11.60755157
 19518: 14 [  940/ 1327], train_loss/perplexity = 4.23637199/69.1564941 secs/batch = 0.1994s, grad.norm=11.61813736
 19523: 14 [  945/ 1327], train_loss/perplexity = 4.45710230/86.2372589 secs/batch = 0.1990s, grad.norm=11.88358498
 19528: 14 [  950/ 1327], train_loss/perplexity = 4.13677502/62.6006088 secs/batch = 0.1993s, grad.norm=11.72105789
 19533: 14 [  955/ 1327], train_loss/perplexity = 4.13778687/62.6639824 secs/batch = 0.1992s, grad.norm=11.83280373
 19538: 14 [  960/ 1327], train_loss/perplexity = 4.42405891/83.4342499 secs/batch = 0.1963s, grad.norm=11.64478874
 19543: 14 [  965/ 1327], train_loss/perplexity = 4.09142351/59.8249931 secs/batch = 0.1994s, grad.norm=11.94425011
 19548: 14 [  970/ 1327], train_loss/perplexity = 4.39294100/80.8779297 secs/batch = 0.1991s, grad.norm=11.71483803
 19553: 14 [  975/ 1327], train_loss/perplexity = 4.06081390/58.0215149 secs/batch = 0.1989s, grad.norm=12.55537701
 19558: 14 [  980/ 1327], train_loss/perplexity = 3.90595245/49.6973915 secs/batch = 0.1921s, grad.norm=11.65301132
 19563: 14 [  985/ 1327], train_loss/perplexity = 4.04842138/57.3069191 secs/batch = 0.1996s, grad.norm=12.31746197
 19568: 14 [  990/ 1327], train_loss/perplexity = 4.16467047/64.3714676 secs/batch = 0.1942s, grad.norm=11.94111919
 19573: 14 [  995/ 1327], train_loss/perplexity = 4.31586599/74.8784409 secs/batch = 0.2003s, grad.norm=11.63881111
 19578: 14 [ 1000/ 1327], train_loss/perplexity = 3.80370855/44.8672676 secs/batch = 0.2001s, grad.norm=11.74028206
 19583: 14 [ 1005/ 1327], train_loss/perplexity = 4.26917362/71.4625549 secs/batch = 0.1992s, grad.norm=11.57892704
 19588: 14 [ 1010/ 1327], train_loss/perplexity = 3.87689948/48.2743073 secs/batch = 0.1995s, grad.norm=11.20473862
 19593: 14 [ 1015/ 1327], train_loss/perplexity = 4.39568043/81.0997925 secs/batch = 0.1996s, grad.norm=11.63662338
 19598: 14 [ 1020/ 1327], train_loss/perplexity = 4.39990044/81.4427567 secs/batch = 0.1999s, grad.norm=11.64734745
 19603: 14 [ 1025/ 1327], train_loss/perplexity = 4.27373075/71.7889633 secs/batch = 0.2007s, grad.norm=11.48658180
 19608: 14 [ 1030/ 1327], train_loss/perplexity = 4.05420446/57.6392899 secs/batch = 0.2007s, grad.norm=11.36213493
 19613: 14 [ 1035/ 1327], train_loss/perplexity = 3.98374867/53.7180290 secs/batch = 0.1985s, grad.norm=12.22391891
 19618: 14 [ 1040/ 1327], train_loss/perplexity = 4.34000492/76.7079163 secs/batch = 0.1991s, grad.norm=12.36253357
 19623: 14 [ 1045/ 1327], train_loss/perplexity = 3.80294085/44.8328362 secs/batch = 0.1993s, grad.norm=11.65802479
 19628: 14 [ 1050/ 1327], train_loss/perplexity = 3.91245961/50.0218353 secs/batch = 0.1990s, grad.norm=11.34952831
 19633: 14 [ 1055/ 1327], train_loss/perplexity = 4.07332516/58.7519989 secs/batch = 0.1989s, grad.norm=12.15202999
 19638: 14 [ 1060/ 1327], train_loss/perplexity = 3.66944742/39.2302208 secs/batch = 0.1955s, grad.norm=12.13250065
 19643: 14 [ 1065/ 1327], train_loss/perplexity = 3.75650167/42.7984390 secs/batch = 0.1981s, grad.norm=11.92237568
 19648: 14 [ 1070/ 1327], train_loss/perplexity = 4.10450459/60.6127090 secs/batch = 0.1991s, grad.norm=12.26085281
 19653: 14 [ 1075/ 1327], train_loss/perplexity = 3.87967849/48.4086494 secs/batch = 0.2000s, grad.norm=12.67926788
 19658: 14 [ 1080/ 1327], train_loss/perplexity = 3.87352300/48.1115837 secs/batch = 0.1993s, grad.norm=12.42068386
 19663: 14 [ 1085/ 1327], train_loss/perplexity = 3.66703272/39.1356087 secs/batch = 0.1997s, grad.norm=11.76933956
 19668: 14 [ 1090/ 1327], train_loss/perplexity = 3.94408369/51.6290092 secs/batch = 0.1992s, grad.norm=12.40363884
 19673: 14 [ 1095/ 1327], train_loss/perplexity = 4.08073902/59.1891975 secs/batch = 0.1984s, grad.norm=12.90024376
 19678: 14 [ 1100/ 1327], train_loss/perplexity = 3.76470327/43.1509018 secs/batch = 0.1994s, grad.norm=12.65468121
 19683: 14 [ 1105/ 1327], train_loss/perplexity = 3.78690100/44.1194611 secs/batch = 0.2003s, grad.norm=12.09839058
 19688: 14 [ 1110/ 1327], train_loss/perplexity = 4.10854244/60.8579483 secs/batch = 0.1995s, grad.norm=12.48444462
 19693: 14 [ 1115/ 1327], train_loss/perplexity = 3.82215500/45.7025909 secs/batch = 0.1986s, grad.norm=11.40529728
 19698: 14 [ 1120/ 1327], train_loss/perplexity = 4.10370159/60.5640564 secs/batch = 0.1998s, grad.norm=12.28856564
 19703: 14 [ 1125/ 1327], train_loss/perplexity = 4.28505468/72.6065140 secs/batch = 0.1987s, grad.norm=12.64169598
 19708: 14 [ 1130/ 1327], train_loss/perplexity = 3.96474051/52.7065926 secs/batch = 0.1956s, grad.norm=11.66554070
 19713: 14 [ 1135/ 1327], train_loss/perplexity = 3.92388630/50.5966988 secs/batch = 0.1996s, grad.norm=11.69976044
 19718: 14 [ 1140/ 1327], train_loss/perplexity = 4.23984623/69.3971786 secs/batch = 0.2005s, grad.norm=12.58256149
 19723: 14 [ 1145/ 1327], train_loss/perplexity = 4.03423262/56.4995461 secs/batch = 0.2006s, grad.norm=11.75034142
 19728: 14 [ 1150/ 1327], train_loss/perplexity = 3.97923493/53.4761047 secs/batch = 0.1979s, grad.norm=12.03730965
 19733: 14 [ 1155/ 1327], train_loss/perplexity = 4.04189634/56.9342079 secs/batch = 0.2000s, grad.norm=12.16097832
 19738: 14 [ 1160/ 1327], train_loss/perplexity = 4.01106882/55.2058449 secs/batch = 0.1936s, grad.norm=12.12329197
 19743: 14 [ 1165/ 1327], train_loss/perplexity = 4.02830744/56.1657677 secs/batch = 0.1996s, grad.norm=11.45501804
 19748: 14 [ 1170/ 1327], train_loss/perplexity = 3.97047138/53.0095139 secs/batch = 0.1995s, grad.norm=12.21049976
 19753: 14 [ 1175/ 1327], train_loss/perplexity = 3.76341081/43.0951653 secs/batch = 0.2006s, grad.norm=11.60559654
 19758: 14 [ 1180/ 1327], train_loss/perplexity = 3.80046320/44.7218933 secs/batch = 0.1996s, grad.norm=12.25810051
 19763: 14 [ 1185/ 1327], train_loss/perplexity = 3.94567108/51.7110291 secs/batch = 0.1999s, grad.norm=12.04196358
 19768: 14 [ 1190/ 1327], train_loss/perplexity = 4.00145388/54.6775856 secs/batch = 0.1991s, grad.norm=12.45547962
 19773: 14 [ 1195/ 1327], train_loss/perplexity = 3.76377344/43.1107941 secs/batch = 0.1994s, grad.norm=11.65286541
 19778: 14 [ 1200/ 1327], train_loss/perplexity = 3.77837157/43.7447472 secs/batch = 0.1989s, grad.norm=11.93503380
 19783: 14 [ 1205/ 1327], train_loss/perplexity = 3.81540155/45.3949814 secs/batch = 0.1994s, grad.norm=12.14101696
 19788: 14 [ 1210/ 1327], train_loss/perplexity = 3.50346708/33.2304649 secs/batch = 0.1993s, grad.norm=12.73980427
 19793: 14 [ 1215/ 1327], train_loss/perplexity = 3.70573449/40.6799164 secs/batch = 0.2000s, grad.norm=11.76011562
 19798: 14 [ 1220/ 1327], train_loss/perplexity = 3.89900255/49.3531990 secs/batch = 0.1994s, grad.norm=12.29784393
 19803: 14 [ 1225/ 1327], train_loss/perplexity = 3.62821937/37.6457253 secs/batch = 0.1986s, grad.norm=13.23502445
 19808: 14 [ 1230/ 1327], train_loss/perplexity = 3.92496824/50.6514702 secs/batch = 0.2004s, grad.norm=11.82814980
 19813: 14 [ 1235/ 1327], train_loss/perplexity = 3.81853676/45.5375290 secs/batch = 0.1943s, grad.norm=11.96146011
 19818: 14 [ 1240/ 1327], train_loss/perplexity = 4.04416418/57.0634727 secs/batch = 0.2008s, grad.norm=12.38850307
 19823: 14 [ 1245/ 1327], train_loss/perplexity = 3.96405530/52.6704865 secs/batch = 0.1995s, grad.norm=12.08321953
 19828: 14 [ 1250/ 1327], train_loss/perplexity = 4.07985115/59.1366653 secs/batch = 0.1991s, grad.norm=11.94763374
 19833: 14 [ 1255/ 1327], train_loss/perplexity = 4.10005188/60.3434181 secs/batch = 0.1998s, grad.norm=12.05399227
 19838: 14 [ 1260/ 1327], train_loss/perplexity = 3.89305353/49.0604668 secs/batch = 0.1989s, grad.norm=12.94587517
 19843: 14 [ 1265/ 1327], train_loss/perplexity = 4.10678434/60.7510490 secs/batch = 0.2001s, grad.norm=12.44079399
 19848: 14 [ 1270/ 1327], train_loss/perplexity = 3.83544898/46.3142166 secs/batch = 0.1993s, grad.norm=12.15734291
 19853: 14 [ 1275/ 1327], train_loss/perplexity = 4.03065252/56.2976341 secs/batch = 0.2009s, grad.norm=12.11714363
 19858: 14 [ 1280/ 1327], train_loss/perplexity = 3.90729284/49.7640495 secs/batch = 0.1990s, grad.norm=12.25014973
 19863: 14 [ 1285/ 1327], train_loss/perplexity = 3.72502279/41.4721794 secs/batch = 0.1995s, grad.norm=11.97633076
 19868: 14 [ 1290/ 1327], train_loss/perplexity = 4.05630684/57.7605972 secs/batch = 0.1953s, grad.norm=11.83881187
 19873: 14 [ 1295/ 1327], train_loss/perplexity = 3.97977543/53.5050163 secs/batch = 0.2010s, grad.norm=12.17617035
 19878: 14 [ 1300/ 1327], train_loss/perplexity = 4.15374947/63.6722908 secs/batch = 0.1986s, grad.norm=11.92425156
 19883: 14 [ 1305/ 1327], train_loss/perplexity = 4.25791788/70.6627045 secs/batch = 0.1993s, grad.norm=12.98141384
 19888: 14 [ 1310/ 1327], train_loss/perplexity = 4.51765013/91.6200485 secs/batch = 0.1992s, grad.norm=12.22473907
 19893: 14 [ 1315/ 1327], train_loss/perplexity = 4.33454704/76.2903976 secs/batch = 0.2002s, grad.norm=12.29262352
 19898: 14 [ 1320/ 1327], train_loss/perplexity = 4.23439503/69.0199127 secs/batch = 0.1992s, grad.norm=11.90555191
 19903: 14 [ 1325/ 1327], train_loss/perplexity = 4.22878933/68.6340866 secs/batch = 0.1999s, grad.norm=11.96132088
Epoch training time: 264.19478464126587
	> validation loss = 4.75431728, perplexity = 116.08437347
	> validation loss = 4.66746187, perplexity = 106.42727661
	> validation loss = 4.59996557, perplexity = 99.48088837
	> validation loss = 4.65699291, perplexity = 105.31890106
	> validation loss = 4.81879902, perplexity = 123.81629944
	> validation loss = 4.67327309, perplexity = 107.04754639
	> validation loss = 4.70834970, perplexity = 110.86904144
	> validation loss = 4.51141071, perplexity = 91.05017090
	> validation loss = 4.32552147, perplexity = 75.60492706
	> validation loss = 4.41263914, perplexity = 82.48686981
	> validation loss = 4.56574631, perplexity = 96.13431549
	> validation loss = 4.64598322, perplexity = 104.16573334
	> validation loss = 4.54173279, perplexity = 93.85328674
	> validation loss = 4.35759878, perplexity = 78.06945038
	> validation loss = 4.29065800, perplexity = 73.01449585
	> validation loss = 4.28272057, perplexity = 72.43724060
	> validation loss = 4.73551512, perplexity = 113.92212677
	> validation loss = 4.28944302, perplexity = 72.92584229
	> validation loss = 4.78126287, perplexity = 119.25485992
	> validation loss = 4.63768291, perplexity = 103.30470276
	> validation loss = 4.45992279, perplexity = 86.48083496
at the end of epoch: 14
train loss = 4.14827348, perplexity = 63.32457483
validation loss = 4.56558623, perplexity = 96.11892495
Saved model cv/epoch014_4.5656.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.25
new learning rate is: 0.125
 19910: 15 [    5/ 1327], train_loss/perplexity = 4.31478834/74.7977905 secs/batch = 0.1994s, grad.norm=12.08470058
 19915: 15 [   10/ 1327], train_loss/perplexity = 3.93178010/50.9976768 secs/batch = 0.1996s, grad.norm=11.74529552
 19920: 15 [   15/ 1327], train_loss/perplexity = 4.24848270/69.9991226 secs/batch = 0.2000s, grad.norm=11.70105648
 19925: 15 [   20/ 1327], train_loss/perplexity = 4.37298393/79.2798462 secs/batch = 0.1995s, grad.norm=11.51521301
 19930: 15 [   25/ 1327], train_loss/perplexity = 4.31653309/74.9284058 secs/batch = 0.1997s, grad.norm=12.01303196
 19935: 15 [   30/ 1327], train_loss/perplexity = 4.26072454/70.8613052 secs/batch = 0.2000s, grad.norm=12.67861748
 19940: 15 [   35/ 1327], train_loss/perplexity = 4.09417057/59.9895630 secs/batch = 0.2001s, grad.norm=11.70374298
 19945: 15 [   40/ 1327], train_loss/perplexity = 4.03893900/56.7660828 secs/batch = 0.1989s, grad.norm=12.01927567
 19950: 15 [   45/ 1327], train_loss/perplexity = 3.84951591/46.9703217 secs/batch = 0.1949s, grad.norm=11.05230713
 19955: 15 [   50/ 1327], train_loss/perplexity = 4.09451389/60.0101624 secs/batch = 0.1992s, grad.norm=11.82844448
 19960: 15 [   55/ 1327], train_loss/perplexity = 4.05003119/57.3992462 secs/batch = 0.1993s, grad.norm=12.17260551
 19965: 15 [   60/ 1327], train_loss/perplexity = 4.41883707/82.9997101 secs/batch = 0.1991s, grad.norm=12.20071411
 19970: 15 [   65/ 1327], train_loss/perplexity = 3.94808173/51.8358345 secs/batch = 0.1989s, grad.norm=11.67712498
 19975: 15 [   70/ 1327], train_loss/perplexity = 3.74522734/42.3186264 secs/batch = 0.1976s, grad.norm=11.89026833
 19980: 15 [   75/ 1327], train_loss/perplexity = 3.57835245/35.8144875 secs/batch = 0.1994s, grad.norm=11.09736443
 19985: 15 [   80/ 1327], train_loss/perplexity = 4.06380272/58.1951904 secs/batch = 0.1996s, grad.norm=12.47508335
 19990: 15 [   85/ 1327], train_loss/perplexity = 4.03691149/56.6511040 secs/batch = 0.1984s, grad.norm=12.55968571
 19995: 15 [   90/ 1327], train_loss/perplexity = 4.11902952/61.4995308 secs/batch = 0.1961s, grad.norm=12.08377171
 20000: 15 [   95/ 1327], train_loss/perplexity = 3.93281579/51.0505219 secs/batch = 0.1995s, grad.norm=11.83005810
 20005: 15 [  100/ 1327], train_loss/perplexity = 4.21372509/67.6079178 secs/batch = 0.1994s, grad.norm=11.84541225
 20010: 15 [  105/ 1327], train_loss/perplexity = 4.07032490/58.5759926 secs/batch = 0.1994s, grad.norm=13.52443409
 20015: 15 [  110/ 1327], train_loss/perplexity = 3.85311604/47.1397247 secs/batch = 0.1984s, grad.norm=11.92525005
 20020: 15 [  115/ 1327], train_loss/perplexity = 3.96675277/52.8127556 secs/batch = 0.1943s, grad.norm=12.51735973
 20025: 15 [  120/ 1327], train_loss/perplexity = 4.03979445/56.8146629 secs/batch = 0.2002s, grad.norm=12.26965523
 20030: 15 [  125/ 1327], train_loss/perplexity = 4.08547926/59.4704323 secs/batch = 0.1988s, grad.norm=12.57960415
 20035: 15 [  130/ 1327], train_loss/perplexity = 4.08673429/59.5451164 secs/batch = 0.1994s, grad.norm=12.88724232
 20040: 15 [  135/ 1327], train_loss/perplexity = 3.92029214/50.4151726 secs/batch = 0.1981s, grad.norm=12.16767883
 20045: 15 [  140/ 1327], train_loss/perplexity = 4.28114319/72.3230743 secs/batch = 0.1989s, grad.norm=12.16910362
 20050: 15 [  145/ 1327], train_loss/perplexity = 4.19266701/66.1991119 secs/batch = 0.1987s, grad.norm=12.95240116
 20055: 15 [  150/ 1327], train_loss/perplexity = 4.28689528/72.7402802 secs/batch = 0.1989s, grad.norm=12.74058437
 20060: 15 [  155/ 1327], train_loss/perplexity = 4.45214510/85.8108215 secs/batch = 0.1977s, grad.norm=12.25407314
 20065: 15 [  160/ 1327], train_loss/perplexity = 4.10559225/60.6786728 secs/batch = 0.1993s, grad.norm=11.86511993
 20070: 15 [  165/ 1327], train_loss/perplexity = 4.29141760/73.0699768 secs/batch = 0.1989s, grad.norm=12.25344372
 20075: 15 [  170/ 1327], train_loss/perplexity = 4.07082081/58.6050453 secs/batch = 0.1996s, grad.norm=11.46455193
 20080: 15 [  175/ 1327], train_loss/perplexity = 4.43126440/84.0376053 secs/batch = 0.1931s, grad.norm=12.25379086
 20085: 15 [  180/ 1327], train_loss/perplexity = 4.10506487/60.6466789 secs/batch = 0.1994s, grad.norm=11.55383396
 20090: 15 [  185/ 1327], train_loss/perplexity = 4.50554991/90.5181046 secs/batch = 0.2010s, grad.norm=12.36568451
 20095: 15 [  190/ 1327], train_loss/perplexity = 4.04052258/56.8560448 secs/batch = 0.1996s, grad.norm=11.89099216
 20100: 15 [  195/ 1327], train_loss/perplexity = 4.28819561/72.8349304 secs/batch = 0.1988s, grad.norm=11.74757481
 20105: 15 [  200/ 1327], train_loss/perplexity = 4.16965485/64.6931229 secs/batch = 0.1995s, grad.norm=11.95289612
 20110: 15 [  205/ 1327], train_loss/perplexity = 4.36013317/78.2675552 secs/batch = 0.1989s, grad.norm=12.04615784
 20115: 15 [  210/ 1327], train_loss/perplexity = 4.17167473/64.8239212 secs/batch = 0.1992s, grad.norm=11.49580765
 20120: 15 [  215/ 1327], train_loss/perplexity = 4.33260632/76.1424789 secs/batch = 0.1994s, grad.norm=11.72999287
 20125: 15 [  220/ 1327], train_loss/perplexity = 4.22323179/68.2537079 secs/batch = 0.1992s, grad.norm=11.75449944
 20130: 15 [  225/ 1327], train_loss/perplexity = 4.44049358/84.8167953 secs/batch = 0.2001s, grad.norm=12.43212414
 20135: 15 [  230/ 1327], train_loss/perplexity = 4.33269453/76.1492004 secs/batch = 0.1995s, grad.norm=12.96631050
 20140: 15 [  235/ 1327], train_loss/perplexity = 4.15466785/63.7307930 secs/batch = 0.1978s, grad.norm=11.76624012
 20145: 15 [  240/ 1327], train_loss/perplexity = 3.93762779/51.2967720 secs/batch = 0.2009s, grad.norm=12.26836681
 20150: 15 [  245/ 1327], train_loss/perplexity = 4.20626736/67.1055908 secs/batch = 0.1992s, grad.norm=11.70230007
 20155: 15 [  250/ 1327], train_loss/perplexity = 4.03031969/56.2789001 secs/batch = 0.1993s, grad.norm=11.51402855
 20160: 15 [  255/ 1327], train_loss/perplexity = 4.08763361/59.5986900 secs/batch = 0.1998s, grad.norm=11.56174850
 20165: 15 [  260/ 1327], train_loss/perplexity = 4.27228355/71.6851425 secs/batch = 0.2000s, grad.norm=12.56999302
 20170: 15 [  265/ 1327], train_loss/perplexity = 4.44710302/85.3792419 secs/batch = 0.2001s, grad.norm=11.89196301
 20175: 15 [  270/ 1327], train_loss/perplexity = 4.54264402/93.9388504 secs/batch = 0.1995s, grad.norm=12.09715080
 20180: 15 [  275/ 1327], train_loss/perplexity = 4.45999050/86.4866867 secs/batch = 0.1986s, grad.norm=12.07027531
 20185: 15 [  280/ 1327], train_loss/perplexity = 4.27528143/71.9003677 secs/batch = 0.1931s, grad.norm=11.42065907
 20190: 15 [  285/ 1327], train_loss/perplexity = 4.49152708/89.2576447 secs/batch = 0.1928s, grad.norm=11.76826000
 20195: 15 [  290/ 1327], train_loss/perplexity = 4.19440985/66.3145828 secs/batch = 0.1946s, grad.norm=12.20036030
 20200: 15 [  295/ 1327], train_loss/perplexity = 4.01993513/55.6974945 secs/batch = 0.1989s, grad.norm=11.74132442
 20205: 15 [  300/ 1327], train_loss/perplexity = 3.57330346/35.6341133 secs/batch = 0.1954s, grad.norm=10.83641529
 20210: 15 [  305/ 1327], train_loss/perplexity = 4.01675940/55.5208931 secs/batch = 0.1996s, grad.norm=12.24258804
 20215: 15 [  310/ 1327], train_loss/perplexity = 4.07500792/58.8509445 secs/batch = 0.1974s, grad.norm=11.82784367
 20220: 15 [  315/ 1327], train_loss/perplexity = 3.52819276/34.0623512 secs/batch = 0.1993s, grad.norm=11.14184856
 20225: 15 [  320/ 1327], train_loss/perplexity = 3.56253529/35.2524605 secs/batch = 0.1999s, grad.norm=12.20049286
 20230: 15 [  325/ 1327], train_loss/perplexity = 3.61104155/37.0045738 secs/batch = 0.2008s, grad.norm=11.64746475
 20235: 15 [  330/ 1327], train_loss/perplexity = 4.16137600/64.1597443 secs/batch = 0.1991s, grad.norm=11.78104305
 20240: 15 [  335/ 1327], train_loss/perplexity = 3.57027102/35.5262184 secs/batch = 0.1999s, grad.norm=10.96072006
 20245: 15 [  340/ 1327], train_loss/perplexity = 4.33132458/76.0449448 secs/batch = 0.2003s, grad.norm=11.79765224
 20250: 15 [  345/ 1327], train_loss/perplexity = 4.10257530/60.4958839 secs/batch = 0.1998s, grad.norm=11.72317982
 20255: 15 [  350/ 1327], train_loss/perplexity = 4.11229801/61.0869331 secs/batch = 0.2002s, grad.norm=12.30445385
 20260: 15 [  355/ 1327], train_loss/perplexity = 4.12189817/61.6762047 secs/batch = 0.1990s, grad.norm=12.02222443
 20265: 15 [  360/ 1327], train_loss/perplexity = 4.26387215/71.0847015 secs/batch = 0.1995s, grad.norm=12.81016541
 20270: 15 [  365/ 1327], train_loss/perplexity = 4.27065325/71.5683746 secs/batch = 0.2003s, grad.norm=12.26774597
 20275: 15 [  370/ 1327], train_loss/perplexity = 4.27158213/71.6348801 secs/batch = 0.1984s, grad.norm=11.95907688
 20280: 15 [  375/ 1327], train_loss/perplexity = 3.72760725/41.5794983 secs/batch = 0.1983s, grad.norm=11.63336468
 20285: 15 [  380/ 1327], train_loss/perplexity = 3.78800154/44.1680450 secs/batch = 0.1999s, grad.norm=11.90474319
 20290: 15 [  385/ 1327], train_loss/perplexity = 4.02731514/56.1100616 secs/batch = 0.1997s, grad.norm=12.25093365
 20295: 15 [  390/ 1327], train_loss/perplexity = 4.17714930/65.1797791 secs/batch = 0.1990s, grad.norm=12.49684238
 20300: 15 [  395/ 1327], train_loss/perplexity = 4.25185108/70.2353058 secs/batch = 0.1994s, grad.norm=12.63307285
 20305: 15 [  400/ 1327], train_loss/perplexity = 4.14247799/62.9586372 secs/batch = 0.1994s, grad.norm=11.93671608
 20310: 15 [  405/ 1327], train_loss/perplexity = 4.37759256/79.6460571 secs/batch = 0.1996s, grad.norm=12.12569427
 20315: 15 [  410/ 1327], train_loss/perplexity = 4.06594992/58.3202820 secs/batch = 0.1995s, grad.norm=12.08823013
 20320: 15 [  415/ 1327], train_loss/perplexity = 4.01621723/55.4907990 secs/batch = 0.1991s, grad.norm=12.03145695
 20325: 15 [  420/ 1327], train_loss/perplexity = 3.71321964/40.9855537 secs/batch = 0.2005s, grad.norm=11.93458080
 20330: 15 [  425/ 1327], train_loss/perplexity = 3.98517370/53.7946320 secs/batch = 0.2005s, grad.norm=13.06042194
 20335: 15 [  430/ 1327], train_loss/perplexity = 4.20057154/66.7244568 secs/batch = 0.2005s, grad.norm=12.55136681
 20340: 15 [  435/ 1327], train_loss/perplexity = 4.21750355/67.8638535 secs/batch = 0.1995s, grad.norm=12.46835518
 20345: 15 [  440/ 1327], train_loss/perplexity = 3.80594683/44.9678078 secs/batch = 0.1950s, grad.norm=11.84180927
 20350: 15 [  445/ 1327], train_loss/perplexity = 4.12016678/61.5695114 secs/batch = 0.1987s, grad.norm=12.63607025
 20355: 15 [  450/ 1327], train_loss/perplexity = 4.04440308/57.0771065 secs/batch = 0.2002s, grad.norm=12.11450481
 20360: 15 [  455/ 1327], train_loss/perplexity = 4.02430630/55.9414902 secs/batch = 0.1997s, grad.norm=11.87663174
 20365: 15 [  460/ 1327], train_loss/perplexity = 4.03986645/56.8187523 secs/batch = 0.1991s, grad.norm=12.56317234
 20370: 15 [  465/ 1327], train_loss/perplexity = 3.72807217/41.5988350 secs/batch = 0.2003s, grad.norm=12.44432545
 20375: 15 [  470/ 1327], train_loss/perplexity = 4.49836731/89.8702774 secs/batch = 0.1996s, grad.norm=12.10762596
 20380: 15 [  475/ 1327], train_loss/perplexity = 3.87785006/48.3202171 secs/batch = 0.1999s, grad.norm=11.88655949
 20385: 15 [  480/ 1327], train_loss/perplexity = 3.95893049/52.4012527 secs/batch = 0.1993s, grad.norm=12.34110641
 20390: 15 [  485/ 1327], train_loss/perplexity = 4.03377247/56.4735527 secs/batch = 0.1996s, grad.norm=12.17588139
 20395: 15 [  490/ 1327], train_loss/perplexity = 3.84604597/46.8076172 secs/batch = 0.2008s, grad.norm=13.02920437
 20400: 15 [  495/ 1327], train_loss/perplexity = 3.95183921/52.0309753 secs/batch = 0.2005s, grad.norm=11.92821884
 20405: 15 [  500/ 1327], train_loss/perplexity = 4.13617849/62.5632782 secs/batch = 0.1991s, grad.norm=12.60503387
 20410: 15 [  505/ 1327], train_loss/perplexity = 4.22672606/68.4926224 secs/batch = 0.2000s, grad.norm=11.23945522
 20415: 15 [  510/ 1327], train_loss/perplexity = 4.51709652/91.5693436 secs/batch = 0.1987s, grad.norm=11.62098789
 20420: 15 [  515/ 1327], train_loss/perplexity = 4.18411493/65.6353836 secs/batch = 0.1998s, grad.norm=11.46765995
 20425: 15 [  520/ 1327], train_loss/perplexity = 4.31140089/74.5448456 secs/batch = 0.1992s, grad.norm=11.83223820
 20430: 15 [  525/ 1327], train_loss/perplexity = 3.97496939/53.2484856 secs/batch = 0.2002s, grad.norm=11.84792519
 20435: 15 [  530/ 1327], train_loss/perplexity = 4.02262545/55.8475380 secs/batch = 0.1993s, grad.norm=12.30951309
 20440: 15 [  535/ 1327], train_loss/perplexity = 4.13283920/62.3547096 secs/batch = 0.2004s, grad.norm=12.50929260
 20445: 15 [  540/ 1327], train_loss/perplexity = 4.18851471/65.9247971 secs/batch = 0.1991s, grad.norm=11.97414494
 20450: 15 [  545/ 1327], train_loss/perplexity = 4.16166401/64.1782303 secs/batch = 0.1996s, grad.norm=12.74108887
 20455: 15 [  550/ 1327], train_loss/perplexity = 4.14133167/62.8865089 secs/batch = 0.1999s, grad.norm=12.25037670
 20460: 15 [  555/ 1327], train_loss/perplexity = 4.04550886/57.1402550 secs/batch = 0.1993s, grad.norm=11.34105492
 20465: 15 [  560/ 1327], train_loss/perplexity = 4.09725428/60.1748352 secs/batch = 0.1999s, grad.norm=12.64858150
 20470: 15 [  565/ 1327], train_loss/perplexity = 3.98500824/53.7857323 secs/batch = 0.1971s, grad.norm=12.78128147
 20475: 15 [  570/ 1327], train_loss/perplexity = 3.98087263/53.5637550 secs/batch = 0.2002s, grad.norm=12.43996716
 20480: 15 [  575/ 1327], train_loss/perplexity = 3.70902133/40.8138428 secs/batch = 0.1987s, grad.norm=12.08713913
 20485: 15 [  580/ 1327], train_loss/perplexity = 4.19318628/66.2334900 secs/batch = 0.1995s, grad.norm=12.29433918
 20490: 15 [  585/ 1327], train_loss/perplexity = 3.79400206/44.4338722 secs/batch = 0.2006s, grad.norm=11.98605061
 20495: 15 [  590/ 1327], train_loss/perplexity = 4.20374727/66.9366913 secs/batch = 0.2006s, grad.norm=11.79859161
 20500: 15 [  595/ 1327], train_loss/perplexity = 4.16050959/64.1041794 secs/batch = 0.1990s, grad.norm=12.57076550
 20505: 15 [  600/ 1327], train_loss/perplexity = 4.18181801/65.4847946 secs/batch = 0.1995s, grad.norm=11.76914692
 20510: 15 [  605/ 1327], train_loss/perplexity = 4.20181370/66.8073883 secs/batch = 0.1988s, grad.norm=12.08966827
 20515: 15 [  610/ 1327], train_loss/perplexity = 4.38818121/80.4938812 secs/batch = 0.1994s, grad.norm=12.03632069
 20520: 15 [  615/ 1327], train_loss/perplexity = 3.93174028/50.9956474 secs/batch = 0.1936s, grad.norm=11.88594532
 20525: 15 [  620/ 1327], train_loss/perplexity = 4.35617447/77.9583282 secs/batch = 0.1993s, grad.norm=12.11014843
 20530: 15 [  625/ 1327], train_loss/perplexity = 4.36210966/78.4224014 secs/batch = 0.1996s, grad.norm=11.65813351
 20535: 15 [  630/ 1327], train_loss/perplexity = 4.43317032/84.1979294 secs/batch = 0.2001s, grad.norm=12.18717670
 20540: 15 [  635/ 1327], train_loss/perplexity = 4.05682182/57.7903519 secs/batch = 0.1989s, grad.norm=12.28944111
 20545: 15 [  640/ 1327], train_loss/perplexity = 4.10041332/60.3652344 secs/batch = 0.1987s, grad.norm=11.88540745
 20550: 15 [  645/ 1327], train_loss/perplexity = 4.41192818/82.4282455 secs/batch = 0.1990s, grad.norm=12.79794598
 20555: 15 [  650/ 1327], train_loss/perplexity = 3.88645363/48.7377357 secs/batch = 0.1996s, grad.norm=12.35294819
 20560: 15 [  655/ 1327], train_loss/perplexity = 4.04042339/56.8504066 secs/batch = 0.2004s, grad.norm=12.41372776
 20565: 15 [  660/ 1327], train_loss/perplexity = 3.91342878/50.0703392 secs/batch = 0.1988s, grad.norm=12.19768143
 20570: 15 [  665/ 1327], train_loss/perplexity = 4.05034494/57.4172592 secs/batch = 0.1984s, grad.norm=12.34049034
 20575: 15 [  670/ 1327], train_loss/perplexity = 3.96887445/52.9249268 secs/batch = 0.2003s, grad.norm=12.14713478
 20580: 15 [  675/ 1327], train_loss/perplexity = 3.90888476/49.8433342 secs/batch = 0.1997s, grad.norm=12.27271652
 20585: 15 [  680/ 1327], train_loss/perplexity = 3.99538994/54.3470268 secs/batch = 0.2000s, grad.norm=12.25347710
 20590: 15 [  685/ 1327], train_loss/perplexity = 3.82462335/45.8155403 secs/batch = 0.1980s, grad.norm=12.23303795
 20595: 15 [  690/ 1327], train_loss/perplexity = 4.22669172/68.4902725 secs/batch = 0.1993s, grad.norm=12.21750259
 20600: 15 [  695/ 1327], train_loss/perplexity = 4.00390482/54.8117638 secs/batch = 0.1997s, grad.norm=12.22532654
 20605: 15 [  700/ 1327], train_loss/perplexity = 4.28989029/72.9584656 secs/batch = 0.1954s, grad.norm=12.65308571
 20610: 15 [  705/ 1327], train_loss/perplexity = 4.02470207/55.9636345 secs/batch = 0.1993s, grad.norm=11.46116447
 20615: 15 [  710/ 1327], train_loss/perplexity = 4.00862885/55.0713081 secs/batch = 0.1998s, grad.norm=12.07121849
 20620: 15 [  715/ 1327], train_loss/perplexity = 3.84651804/46.8297195 secs/batch = 0.1953s, grad.norm=11.92789841
 20625: 15 [  720/ 1327], train_loss/perplexity = 3.83011198/46.0676956 secs/batch = 0.1994s, grad.norm=11.88607979
 20630: 15 [  725/ 1327], train_loss/perplexity = 3.89157963/48.9882088 secs/batch = 0.2010s, grad.norm=12.09915543
 20635: 15 [  730/ 1327], train_loss/perplexity = 4.13643503/62.5793304 secs/batch = 0.1960s, grad.norm=11.98691845
 20640: 15 [  735/ 1327], train_loss/perplexity = 4.16667652/64.5007324 secs/batch = 0.1997s, grad.norm=12.74480152
 20645: 15 [  740/ 1327], train_loss/perplexity = 3.56072474/35.1886902 secs/batch = 0.2001s, grad.norm=11.68926144
 20650: 15 [  745/ 1327], train_loss/perplexity = 4.09904385/60.2826195 secs/batch = 0.2009s, grad.norm=12.34856892
 20655: 15 [  750/ 1327], train_loss/perplexity = 3.93007588/50.9108391 secs/batch = 0.1956s, grad.norm=11.78115177
 20660: 15 [  755/ 1327], train_loss/perplexity = 3.85706806/47.3263893 secs/batch = 0.1996s, grad.norm=12.43957043
 20665: 15 [  760/ 1327], train_loss/perplexity = 3.69280505/40.1573334 secs/batch = 0.1998s, grad.norm=11.64082241
 20670: 15 [  765/ 1327], train_loss/perplexity = 3.77655363/43.6652946 secs/batch = 0.2002s, grad.norm=12.02292633
 20675: 15 [  770/ 1327], train_loss/perplexity = 3.77767158/43.7141380 secs/batch = 0.1929s, grad.norm=11.93260670
 20680: 15 [  775/ 1327], train_loss/perplexity = 3.81200027/45.2408409 secs/batch = 0.1989s, grad.norm=12.68842125
 20685: 15 [  780/ 1327], train_loss/perplexity = 4.19364882/66.2641373 secs/batch = 0.2000s, grad.norm=12.19003201
 20690: 15 [  785/ 1327], train_loss/perplexity = 4.10135174/60.4219055 secs/batch = 0.1992s, grad.norm=12.82759285
 20695: 15 [  790/ 1327], train_loss/perplexity = 3.83687711/46.3804054 secs/batch = 0.1998s, grad.norm=12.33382416
 20700: 15 [  795/ 1327], train_loss/perplexity = 4.25683975/70.5865631 secs/batch = 0.1968s, grad.norm=12.61354733
 20705: 15 [  800/ 1327], train_loss/perplexity = 4.10227680/60.4778252 secs/batch = 0.1998s, grad.norm=12.33485699
 20710: 15 [  805/ 1327], train_loss/perplexity = 4.41914940/83.0256348 secs/batch = 0.1994s, grad.norm=12.56117249
 20715: 15 [  810/ 1327], train_loss/perplexity = 4.05893326/57.9125023 secs/batch = 0.2001s, grad.norm=11.75736046
 20720: 15 [  815/ 1327], train_loss/perplexity = 3.86341500/47.6277237 secs/batch = 0.1996s, grad.norm=11.78001022
 20725: 15 [  820/ 1327], train_loss/perplexity = 3.81514049/45.3831329 secs/batch = 0.2002s, grad.norm=11.42035770
 20730: 15 [  825/ 1327], train_loss/perplexity = 3.96355510/52.6441498 secs/batch = 0.1987s, grad.norm=12.14960575
 20735: 15 [  830/ 1327], train_loss/perplexity = 3.69895339/40.4049950 secs/batch = 0.1992s, grad.norm=11.87115765
 20740: 15 [  835/ 1327], train_loss/perplexity = 3.99307346/54.2212791 secs/batch = 0.1993s, grad.norm=11.86941910
 20745: 15 [  840/ 1327], train_loss/perplexity = 4.05353737/57.6008530 secs/batch = 0.2006s, grad.norm=12.27262402
 20750: 15 [  845/ 1327], train_loss/perplexity = 3.97956038/53.4935112 secs/batch = 0.1991s, grad.norm=12.60951996
 20755: 15 [  850/ 1327], train_loss/perplexity = 3.92359281/50.5818481 secs/batch = 0.1985s, grad.norm=11.34384060
 20760: 15 [  855/ 1327], train_loss/perplexity = 3.99711561/54.4408951 secs/batch = 0.2008s, grad.norm=12.08824730
 20765: 15 [  860/ 1327], train_loss/perplexity = 3.68185663/39.7200699 secs/batch = 0.1986s, grad.norm=11.29169178
 20770: 15 [  865/ 1327], train_loss/perplexity = 4.14973021/63.4168892 secs/batch = 0.1990s, grad.norm=11.97283936
 20775: 15 [  870/ 1327], train_loss/perplexity = 4.01047468/55.1730537 secs/batch = 0.1985s, grad.norm=12.45200920
 20780: 15 [  875/ 1327], train_loss/perplexity = 3.68872261/39.9937286 secs/batch = 0.2017s, grad.norm=11.62561893
 20785: 15 [  880/ 1327], train_loss/perplexity = 3.84075308/46.5605240 secs/batch = 0.1990s, grad.norm=11.47710133
 20790: 15 [  885/ 1327], train_loss/perplexity = 4.04507780/57.1156311 secs/batch = 0.1963s, grad.norm=11.90052223
 20795: 15 [  890/ 1327], train_loss/perplexity = 4.13353825/62.3983116 secs/batch = 0.1994s, grad.norm=11.87152481
 20800: 15 [  895/ 1327], train_loss/perplexity = 4.13935423/62.7622795 secs/batch = 0.1991s, grad.norm=11.66432381
 20805: 15 [  900/ 1327], train_loss/perplexity = 3.98842192/53.9696541 secs/batch = 0.1996s, grad.norm=11.35328388
 20810: 15 [  905/ 1327], train_loss/perplexity = 3.89515591/49.1637192 secs/batch = 0.1997s, grad.norm=11.56193256
 20815: 15 [  910/ 1327], train_loss/perplexity = 3.92460155/50.6329002 secs/batch = 0.1999s, grad.norm=10.89392185
 20820: 15 [  915/ 1327], train_loss/perplexity = 4.08717108/59.5711327 secs/batch = 0.1952s, grad.norm=12.37510014
 20825: 15 [  920/ 1327], train_loss/perplexity = 4.28859568/72.8640747 secs/batch = 0.1989s, grad.norm=12.16536808
 20830: 15 [  925/ 1327], train_loss/perplexity = 4.06712198/58.3886757 secs/batch = 0.1986s, grad.norm=12.38371849
 20835: 15 [  930/ 1327], train_loss/perplexity = 4.05375528/57.6134071 secs/batch = 0.1999s, grad.norm=11.50305367
 20840: 15 [  935/ 1327], train_loss/perplexity = 4.22162199/68.1439209 secs/batch = 0.2006s, grad.norm=12.06130695
 20845: 15 [  940/ 1327], train_loss/perplexity = 4.12276649/61.7297821 secs/batch = 0.1923s, grad.norm=11.77397156
 20850: 15 [  945/ 1327], train_loss/perplexity = 4.33030033/75.9671021 secs/batch = 0.2002s, grad.norm=12.07433224
 20855: 15 [  950/ 1327], train_loss/perplexity = 4.08767080/59.6009064 secs/batch = 0.1981s, grad.norm=11.48286247
 20860: 15 [  955/ 1327], train_loss/perplexity = 4.03727341/56.6716118 secs/batch = 0.1996s, grad.norm=11.99389076
 20865: 15 [  960/ 1327], train_loss/perplexity = 4.40761852/82.0737762 secs/batch = 0.1991s, grad.norm=11.87994385
 20870: 15 [  965/ 1327], train_loss/perplexity = 4.05928755/57.9330215 secs/batch = 0.2000s, grad.norm=11.82050514
 20875: 15 [  970/ 1327], train_loss/perplexity = 4.34377337/76.9975281 secs/batch = 0.1993s, grad.norm=11.88347530
 20880: 15 [  975/ 1327], train_loss/perplexity = 4.02302933/55.8700981 secs/batch = 0.1992s, grad.norm=12.86860943
 20885: 15 [  980/ 1327], train_loss/perplexity = 3.85740495/47.3423347 secs/batch = 0.1999s, grad.norm=11.83697224
 20890: 15 [  985/ 1327], train_loss/perplexity = 3.95679617/52.2895317 secs/batch = 0.2000s, grad.norm=11.90357208
 20895: 15 [  990/ 1327], train_loss/perplexity = 4.17182446/64.8336334 secs/batch = 0.1990s, grad.norm=12.32528400
 20900: 15 [  995/ 1327], train_loss/perplexity = 4.20848656/67.2546768 secs/batch = 0.1989s, grad.norm=11.85693550
 20905: 15 [ 1000/ 1327], train_loss/perplexity = 3.76860523/43.3196030 secs/batch = 0.2002s, grad.norm=11.65203381
 20910: 15 [ 1005/ 1327], train_loss/perplexity = 4.25932503/70.7622070 secs/batch = 0.1998s, grad.norm=12.28595543
 20915: 15 [ 1010/ 1327], train_loss/perplexity = 3.78213263/43.9095840 secs/batch = 0.1984s, grad.norm=11.25962067
 20920: 15 [ 1015/ 1327], train_loss/perplexity = 4.31786537/75.0282974 secs/batch = 0.2001s, grad.norm=11.85817432
 20925: 15 [ 1020/ 1327], train_loss/perplexity = 4.30947685/74.4015579 secs/batch = 0.1930s, grad.norm=11.47133923
 20930: 15 [ 1025/ 1327], train_loss/perplexity = 4.23742247/69.2291794 secs/batch = 0.1993s, grad.norm=11.66847801
 20935: 15 [ 1030/ 1327], train_loss/perplexity = 4.01664209/55.5143814 secs/batch = 0.1982s, grad.norm=11.72669792
 20940: 15 [ 1035/ 1327], train_loss/perplexity = 3.94879436/51.8727913 secs/batch = 0.1988s, grad.norm=11.76799679
 20945: 15 [ 1040/ 1327], train_loss/perplexity = 4.24000835/69.4084320 secs/batch = 0.1939s, grad.norm=12.42425632
 20950: 15 [ 1045/ 1327], train_loss/perplexity = 3.75316000/42.6556625 secs/batch = 0.1994s, grad.norm=11.71476460
 20955: 15 [ 1050/ 1327], train_loss/perplexity = 3.84954786/46.9718208 secs/batch = 0.1984s, grad.norm=11.11768246
 20960: 15 [ 1055/ 1327], train_loss/perplexity = 3.97575951/53.2905769 secs/batch = 0.1959s, grad.norm=12.33475876
 20965: 15 [ 1060/ 1327], train_loss/perplexity = 3.57989454/35.8697586 secs/batch = 0.1995s, grad.norm=12.69972229
 20970: 15 [ 1065/ 1327], train_loss/perplexity = 3.71480775/41.0506935 secs/batch = 0.1984s, grad.norm=12.12614059
 20975: 15 [ 1070/ 1327], train_loss/perplexity = 4.03792906/56.7087822 secs/batch = 0.1990s, grad.norm=12.00596237
 20980: 15 [ 1075/ 1327], train_loss/perplexity = 3.79578137/44.5130043 secs/batch = 0.1996s, grad.norm=11.98622799
 20985: 15 [ 1080/ 1327], train_loss/perplexity = 3.78601360/44.0803261 secs/batch = 0.1995s, grad.norm=12.02702522
 20990: 15 [ 1085/ 1327], train_loss/perplexity = 3.63585711/37.9343529 secs/batch = 0.1999s, grad.norm=11.77504635
 20995: 15 [ 1090/ 1327], train_loss/perplexity = 3.91133666/49.9656944 secs/batch = 0.1935s, grad.norm=12.76933765
 21000: 15 [ 1095/ 1327], train_loss/perplexity = 4.02339745/55.8906708 secs/batch = 0.1982s, grad.norm=12.76914501
 21005: 15 [ 1100/ 1327], train_loss/perplexity = 3.68535757/39.8593712 secs/batch = 0.1996s, grad.norm=13.41620922
 21010: 15 [ 1105/ 1327], train_loss/perplexity = 3.68342471/39.7824059 secs/batch = 0.1989s, grad.norm=12.71008682
 21015: 15 [ 1110/ 1327], train_loss/perplexity = 4.05399370/57.6271439 secs/batch = 0.2002s, grad.norm=12.28211021
 21020: 15 [ 1115/ 1327], train_loss/perplexity = 3.77279449/43.5014610 secs/batch = 0.1994s, grad.norm=11.74913597
 21025: 15 [ 1120/ 1327], train_loss/perplexity = 4.05923605/57.9300385 secs/batch = 0.2004s, grad.norm=12.40573502
 21030: 15 [ 1125/ 1327], train_loss/perplexity = 4.22175026/68.1526642 secs/batch = 0.2006s, grad.norm=12.87170410
 21035: 15 [ 1130/ 1327], train_loss/perplexity = 3.87906694/48.3790550 secs/batch = 0.1990s, grad.norm=11.63924503
 21040: 15 [ 1135/ 1327], train_loss/perplexity = 3.90892553/49.8453674 secs/batch = 0.1989s, grad.norm=11.66289425
 21045: 15 [ 1140/ 1327], train_loss/perplexity = 4.17029762/64.7347183 secs/batch = 0.2006s, grad.norm=12.60421276
 21050: 15 [ 1145/ 1327], train_loss/perplexity = 3.96645689/52.7971344 secs/batch = 0.1991s, grad.norm=11.49535751
 21055: 15 [ 1150/ 1327], train_loss/perplexity = 3.88228106/48.5348015 secs/batch = 0.1999s, grad.norm=11.66145897
 21060: 15 [ 1155/ 1327], train_loss/perplexity = 4.04368496/57.0361328 secs/batch = 0.1997s, grad.norm=12.13945675
 21065: 15 [ 1160/ 1327], train_loss/perplexity = 3.90110064/49.4568520 secs/batch = 0.1993s, grad.norm=11.79635715
 21070: 15 [ 1165/ 1327], train_loss/perplexity = 3.95444965/52.1669769 secs/batch = 0.1994s, grad.norm=12.03276825
 21075: 15 [ 1170/ 1327], train_loss/perplexity = 3.85955501/47.4442329 secs/batch = 0.2000s, grad.norm=11.70266819
 21080: 15 [ 1175/ 1327], train_loss/perplexity = 3.69136167/40.0994110 secs/batch = 0.1993s, grad.norm=11.82847309
 21085: 15 [ 1180/ 1327], train_loss/perplexity = 3.73033214/41.6929550 secs/batch = 0.1997s, grad.norm=12.46462059
 21090: 15 [ 1185/ 1327], train_loss/perplexity = 3.91754317/50.2767715 secs/batch = 0.1998s, grad.norm=12.57395744
 21095: 15 [ 1190/ 1327], train_loss/perplexity = 3.91405106/50.1015053 secs/batch = 0.1952s, grad.norm=12.20312786
 21100: 15 [ 1195/ 1327], train_loss/perplexity = 3.75552583/42.7566948 secs/batch = 0.1993s, grad.norm=11.56690598
 21105: 15 [ 1200/ 1327], train_loss/perplexity = 3.77797771/43.7275238 secs/batch = 0.1999s, grad.norm=12.03254986
 21110: 15 [ 1205/ 1327], train_loss/perplexity = 3.76494479/43.1613235 secs/batch = 0.1948s, grad.norm=12.68298435
 21115: 15 [ 1210/ 1327], train_loss/perplexity = 3.41006589/30.2672386 secs/batch = 0.1994s, grad.norm=12.03002834
 21120: 15 [ 1215/ 1327], train_loss/perplexity = 3.64712214/38.3641014 secs/batch = 0.2007s, grad.norm=11.56888676
 21125: 15 [ 1220/ 1327], train_loss/perplexity = 3.77906322/43.7750168 secs/batch = 0.2004s, grad.norm=12.22125340
 21130: 15 [ 1225/ 1327], train_loss/perplexity = 3.59050989/36.2525558 secs/batch = 0.1995s, grad.norm=12.67193794
 21135: 15 [ 1230/ 1327], train_loss/perplexity = 3.77484894/43.5909233 secs/batch = 0.1999s, grad.norm=11.90584183
 21140: 15 [ 1235/ 1327], train_loss/perplexity = 3.81964636/45.5880852 secs/batch = 0.1995s, grad.norm=12.33542824
 21145: 15 [ 1240/ 1327], train_loss/perplexity = 3.97670913/53.3412056 secs/batch = 0.1989s, grad.norm=12.56824493
 21150: 15 [ 1245/ 1327], train_loss/perplexity = 3.89949894/49.3777008 secs/batch = 0.1958s, grad.norm=11.55654907
 21155: 15 [ 1250/ 1327], train_loss/perplexity = 3.99977589/54.5859146 secs/batch = 0.1995s, grad.norm=11.64231300
 21160: 15 [ 1255/ 1327], train_loss/perplexity = 4.04261684/56.9752426 secs/batch = 0.1991s, grad.norm=12.09703541
 21165: 15 [ 1260/ 1327], train_loss/perplexity = 3.78094292/43.8573761 secs/batch = 0.1996s, grad.norm=12.90286541
 21170: 15 [ 1265/ 1327], train_loss/perplexity = 3.97576380/53.2908058 secs/batch = 0.1986s, grad.norm=12.40410709
 21175: 15 [ 1270/ 1327], train_loss/perplexity = 3.77663684/43.6689301 secs/batch = 0.1993s, grad.norm=12.37403584
 21180: 15 [ 1275/ 1327], train_loss/perplexity = 3.93529987/51.1774940 secs/batch = 0.1992s, grad.norm=12.22969913
 21185: 15 [ 1280/ 1327], train_loss/perplexity = 3.78813481/44.1739311 secs/batch = 0.1996s, grad.norm=12.51431656
 21190: 15 [ 1285/ 1327], train_loss/perplexity = 3.71029401/40.8658180 secs/batch = 0.1988s, grad.norm=12.16109085
 21195: 15 [ 1290/ 1327], train_loss/perplexity = 3.93420887/51.1216888 secs/batch = 0.2003s, grad.norm=12.07878113
 21200: 15 [ 1295/ 1327], train_loss/perplexity = 3.90464997/49.6327057 secs/batch = 0.2002s, grad.norm=12.07091904
 21205: 15 [ 1300/ 1327], train_loss/perplexity = 4.04869747/57.3227425 secs/batch = 0.1995s, grad.norm=11.30951309
 21210: 15 [ 1305/ 1327], train_loss/perplexity = 4.16690874/64.5157089 secs/batch = 0.2003s, grad.norm=12.40652657
 21215: 15 [ 1310/ 1327], train_loss/perplexity = 4.47906971/88.1526260 secs/batch = 0.1994s, grad.norm=12.80295849
 21220: 15 [ 1315/ 1327], train_loss/perplexity = 4.22842693/68.6092224 secs/batch = 0.1993s, grad.norm=12.41980267
 21225: 15 [ 1320/ 1327], train_loss/perplexity = 4.23058891/68.7577133 secs/batch = 0.1998s, grad.norm=12.38969135
 21230: 15 [ 1325/ 1327], train_loss/perplexity = 4.19768476/66.5321121 secs/batch = 0.1942s, grad.norm=12.33791065
Epoch training time: 264.2126479148865
	> validation loss = 4.74606037, perplexity = 115.12982178
	> validation loss = 4.64624691, perplexity = 104.19320679
	> validation loss = 4.60568810, perplexity = 100.05180359
	> validation loss = 4.63616037, perplexity = 103.14753723
	> validation loss = 4.83211136, perplexity = 125.47560883
	> validation loss = 4.66010618, perplexity = 105.64730072
	> validation loss = 4.68860054, perplexity = 108.70095062
	> validation loss = 4.49444103, perplexity = 89.51811981
	> validation loss = 4.31105423, perplexity = 74.51900482
	> validation loss = 4.39636993, perplexity = 81.15573120
	> validation loss = 4.54500771, perplexity = 94.16115570
	> validation loss = 4.62721586, perplexity = 102.22904968
	> validation loss = 4.54676867, perplexity = 94.32711029
	> validation loss = 4.35504723, perplexity = 77.87050629
	> validation loss = 4.27094746, perplexity = 71.58943176
	> validation loss = 4.26756763, perplexity = 71.34787750
	> validation loss = 4.73390913, perplexity = 113.73931885
	> validation loss = 4.26299095, perplexity = 71.02208710
	> validation loss = 4.77841616, perplexity = 118.91585541
	> validation loss = 4.61855698, perplexity = 101.34767914
	> validation loss = 4.44148636, perplexity = 84.90103912
at the end of epoch: 15
train loss = 4.06786354, perplexity = 58.43199149
validation loss = 4.55180905, perplexity = 94.80375800
Saved model cv/epoch015_4.5518.model
 21237: 16 [    5/ 1327], train_loss/perplexity = 4.26813984/71.3887177 secs/batch = 0.1999s, grad.norm=12.25333691
 21242: 16 [   10/ 1327], train_loss/perplexity = 3.86657190/47.7783165 secs/batch = 0.2000s, grad.norm=11.85697269
 21247: 16 [   15/ 1327], train_loss/perplexity = 4.23009825/68.7239838 secs/batch = 0.2004s, grad.norm=11.88906956
 21252: 16 [   20/ 1327], train_loss/perplexity = 4.32894182/75.8639679 secs/batch = 0.1943s, grad.norm=11.72878551
 21257: 16 [   25/ 1327], train_loss/perplexity = 4.19288397/66.2134705 secs/batch = 0.1985s, grad.norm=12.11732388
 21262: 16 [   30/ 1327], train_loss/perplexity = 4.19407225/66.2921982 secs/batch = 0.1980s, grad.norm=12.12369633
 21267: 16 [   35/ 1327], train_loss/perplexity = 4.06120443/58.0441780 secs/batch = 0.1987s, grad.norm=12.22053719
 21272: 16 [   40/ 1327], train_loss/perplexity = 4.03761005/56.6906929 secs/batch = 0.1966s, grad.norm=12.84333420
 21277: 16 [   45/ 1327], train_loss/perplexity = 3.78714538/44.1302452 secs/batch = 0.1992s, grad.norm=11.83593845
 21282: 16 [   50/ 1327], train_loss/perplexity = 4.06526470/58.2803345 secs/batch = 0.1985s, grad.norm=12.49402332
 21287: 16 [   55/ 1327], train_loss/perplexity = 4.08057022/59.1792068 secs/batch = 0.1996s, grad.norm=12.06470203
 21292: 16 [   60/ 1327], train_loss/perplexity = 4.28486967/72.5930862 secs/batch = 0.2011s, grad.norm=12.48499966
 21297: 16 [   65/ 1327], train_loss/perplexity = 3.82579064/45.8690529 secs/batch = 0.1924s, grad.norm=11.81542206
 21302: 16 [   70/ 1327], train_loss/perplexity = 3.74806786/42.4390030 secs/batch = 0.1941s, grad.norm=11.85048580
 21307: 16 [   75/ 1327], train_loss/perplexity = 3.56419277/35.3109398 secs/batch = 0.1993s, grad.norm=11.15755081
 21312: 16 [   80/ 1327], train_loss/perplexity = 3.95329785/52.1069260 secs/batch = 0.1986s, grad.norm=12.32232666
 21317: 16 [   85/ 1327], train_loss/perplexity = 3.97712994/53.3636589 secs/batch = 0.1987s, grad.norm=12.19886303
 21322: 16 [   90/ 1327], train_loss/perplexity = 4.04757118/57.2582169 secs/batch = 0.1996s, grad.norm=12.58817768
 21327: 16 [   95/ 1327], train_loss/perplexity = 3.90986085/49.8920097 secs/batch = 0.1992s, grad.norm=12.01092052
 21332: 16 [  100/ 1327], train_loss/perplexity = 4.23271513/68.9040604 secs/batch = 0.2016s, grad.norm=12.57203865
 21337: 16 [  105/ 1327], train_loss/perplexity = 4.03600216/56.5996132 secs/batch = 0.1987s, grad.norm=13.23181725
 21342: 16 [  110/ 1327], train_loss/perplexity = 3.86926150/47.9069939 secs/batch = 0.1992s, grad.norm=11.70894527
 21347: 16 [  115/ 1327], train_loss/perplexity = 3.86738467/47.8171654 secs/batch = 0.1998s, grad.norm=12.44276524
 21352: 16 [  120/ 1327], train_loss/perplexity = 3.97259402/53.1221542 secs/batch = 0.1990s, grad.norm=12.45336246
 21357: 16 [  125/ 1327], train_loss/perplexity = 4.07724714/58.9828758 secs/batch = 0.1993s, grad.norm=12.70551682
 21362: 16 [  130/ 1327], train_loss/perplexity = 3.98374653/53.7179146 secs/batch = 0.1989s, grad.norm=12.61500359
 21367: 16 [  135/ 1327], train_loss/perplexity = 3.98991585/54.0503426 secs/batch = 0.1991s, grad.norm=12.31425762
 21372: 16 [  140/ 1327], train_loss/perplexity = 4.26944304/71.4818115 secs/batch = 0.1945s, grad.norm=12.53820801
 21377: 16 [  145/ 1327], train_loss/perplexity = 4.14419127/63.0665970 secs/batch = 0.1990s, grad.norm=13.44044018
 21382: 16 [  150/ 1327], train_loss/perplexity = 4.18902636/65.9585419 secs/batch = 0.1994s, grad.norm=12.56415367
 21387: 16 [  155/ 1327], train_loss/perplexity = 4.42016935/83.1103592 secs/batch = 0.2002s, grad.norm=12.26428413
 21392: 16 [  160/ 1327], train_loss/perplexity = 4.06996441/58.5548782 secs/batch = 0.2000s, grad.norm=11.50385094
 21397: 16 [  165/ 1327], train_loss/perplexity = 4.25748539/70.6321487 secs/batch = 0.1999s, grad.norm=12.05729294
 21402: 16 [  170/ 1327], train_loss/perplexity = 4.07963419/59.1238365 secs/batch = 0.1987s, grad.norm=12.09481716
 21407: 16 [  175/ 1327], train_loss/perplexity = 4.31544876/74.8472061 secs/batch = 0.2006s, grad.norm=12.54668045
 21412: 16 [  180/ 1327], train_loss/perplexity = 4.10308170/60.5265236 secs/batch = 0.1990s, grad.norm=11.72544575
 21417: 16 [  185/ 1327], train_loss/perplexity = 4.41713142/82.8582611 secs/batch = 0.1927s, grad.norm=12.26500702
 21422: 16 [  190/ 1327], train_loss/perplexity = 3.99620628/54.3914108 secs/batch = 0.2011s, grad.norm=11.98830414
 21427: 16 [  195/ 1327], train_loss/perplexity = 4.25815058/70.6791458 secs/batch = 0.1992s, grad.norm=11.97381592
 21432: 16 [  200/ 1327], train_loss/perplexity = 4.14398766/63.0537567 secs/batch = 0.1968s, grad.norm=12.33006763
 21437: 16 [  205/ 1327], train_loss/perplexity = 4.34804344/77.3270187 secs/batch = 0.1995s, grad.norm=12.25292301
 21442: 16 [  210/ 1327], train_loss/perplexity = 4.21519136/67.7071228 secs/batch = 0.1998s, grad.norm=12.18916798
 21447: 16 [  215/ 1327], train_loss/perplexity = 4.30414438/74.0058670 secs/batch = 0.2000s, grad.norm=12.17590618
 21452: 16 [  220/ 1327], train_loss/perplexity = 4.20664454/67.1309052 secs/batch = 0.1945s, grad.norm=11.95082855
 21457: 16 [  225/ 1327], train_loss/perplexity = 4.39927912/81.3921738 secs/batch = 0.1994s, grad.norm=12.62597752
 21462: 16 [  230/ 1327], train_loss/perplexity = 4.30191278/73.8409042 secs/batch = 0.1996s, grad.norm=13.03983212
 21467: 16 [  235/ 1327], train_loss/perplexity = 4.12254143/61.7158890 secs/batch = 0.1992s, grad.norm=12.03958702
 21472: 16 [  240/ 1327], train_loss/perplexity = 3.89373350/49.0938377 secs/batch = 0.1993s, grad.norm=12.27141380
 21477: 16 [  245/ 1327], train_loss/perplexity = 4.21810341/67.9045715 secs/batch = 0.2003s, grad.norm=11.99113178
 21482: 16 [  250/ 1327], train_loss/perplexity = 3.99116611/54.1179619 secs/batch = 0.1989s, grad.norm=11.73190784
 21487: 16 [  255/ 1327], train_loss/perplexity = 4.02884722/56.1960907 secs/batch = 0.1986s, grad.norm=11.62519360
 21492: 16 [  260/ 1327], train_loss/perplexity = 4.22961521/68.6907959 secs/batch = 0.2006s, grad.norm=12.29054642
 21497: 16 [  265/ 1327], train_loss/perplexity = 4.37565660/79.4920197 secs/batch = 0.1953s, grad.norm=11.91856670
 21502: 16 [  270/ 1327], train_loss/perplexity = 4.46054268/86.5344543 secs/batch = 0.1998s, grad.norm=12.15739346
 21507: 16 [  275/ 1327], train_loss/perplexity = 4.37354040/79.3239746 secs/batch = 0.1921s, grad.norm=11.96176147
 21512: 16 [  280/ 1327], train_loss/perplexity = 4.19829512/66.5727386 secs/batch = 0.1995s, grad.norm=11.89009762
 21517: 16 [  285/ 1327], train_loss/perplexity = 4.53514910/93.2374191 secs/batch = 0.1985s, grad.norm=12.07441235
 21522: 16 [  290/ 1327], train_loss/perplexity = 4.21048737/67.3893738 secs/batch = 0.1929s, grad.norm=11.99792480
 21527: 16 [  295/ 1327], train_loss/perplexity = 3.91871738/50.3358421 secs/batch = 0.2000s, grad.norm=11.83190727
 21532: 16 [  300/ 1327], train_loss/perplexity = 3.55971766/35.1532707 secs/batch = 0.2012s, grad.norm=11.31620502
 21537: 16 [  305/ 1327], train_loss/perplexity = 4.03843784/56.7376404 secs/batch = 0.2013s, grad.norm=11.92669106
 21542: 16 [  310/ 1327], train_loss/perplexity = 4.02633095/56.0548668 secs/batch = 0.1991s, grad.norm=11.90109158
 21547: 16 [  315/ 1327], train_loss/perplexity = 3.58662128/36.1118584 secs/batch = 0.1976s, grad.norm=11.57931423
 21552: 16 [  320/ 1327], train_loss/perplexity = 3.53141260/34.1722069 secs/batch = 0.1988s, grad.norm=12.21172810
 21557: 16 [  325/ 1327], train_loss/perplexity = 3.58479762/36.0460625 secs/batch = 0.1991s, grad.norm=11.55915165
 21562: 16 [  330/ 1327], train_loss/perplexity = 4.15208817/63.5666008 secs/batch = 0.1962s, grad.norm=12.26561928
 21567: 16 [  335/ 1327], train_loss/perplexity = 3.55856299/35.1127052 secs/batch = 0.1996s, grad.norm=11.57421970
 21572: 16 [  340/ 1327], train_loss/perplexity = 4.30758381/74.2608414 secs/batch = 0.1994s, grad.norm=12.02758598
 21577: 16 [  345/ 1327], train_loss/perplexity = 4.12399149/61.8054466 secs/batch = 0.1996s, grad.norm=12.23911572
 21582: 16 [  350/ 1327], train_loss/perplexity = 4.09830093/60.2378540 secs/batch = 0.1995s, grad.norm=12.13727665
 21587: 16 [  355/ 1327], train_loss/perplexity = 4.12612867/61.9376755 secs/batch = 0.1980s, grad.norm=12.13113308
 21592: 16 [  360/ 1327], train_loss/perplexity = 4.23560476/69.1034546 secs/batch = 0.1991s, grad.norm=12.87870789
 21597: 16 [  365/ 1327], train_loss/perplexity = 4.21237135/67.5164566 secs/batch = 0.1993s, grad.norm=12.09558582
 21602: 16 [  370/ 1327], train_loss/perplexity = 4.27848053/72.1307526 secs/batch = 0.1972s, grad.norm=12.07579613
 21607: 16 [  375/ 1327], train_loss/perplexity = 3.70215559/40.5345879 secs/batch = 0.1990s, grad.norm=11.89457321
 21612: 16 [  380/ 1327], train_loss/perplexity = 3.76915932/43.3436127 secs/batch = 0.1989s, grad.norm=12.13401413
 21617: 16 [  385/ 1327], train_loss/perplexity = 3.98568654/53.8222275 secs/batch = 0.1995s, grad.norm=12.51695824
 21622: 16 [  390/ 1327], train_loss/perplexity = 4.09851599/60.2508087 secs/batch = 0.1989s, grad.norm=12.07040882
 21627: 16 [  395/ 1327], train_loss/perplexity = 4.17746353/65.2002640 secs/batch = 0.1999s, grad.norm=12.27021122
 21632: 16 [  400/ 1327], train_loss/perplexity = 4.07692719/58.9640045 secs/batch = 0.1979s, grad.norm=11.84416008
 21637: 16 [  405/ 1327], train_loss/perplexity = 4.37398148/79.3589706 secs/batch = 0.1981s, grad.norm=11.99235630
 21642: 16 [  410/ 1327], train_loss/perplexity = 4.00000572/54.5984612 secs/batch = 0.2001s, grad.norm=12.17412376
 21647: 16 [  415/ 1327], train_loss/perplexity = 3.95532417/52.2126160 secs/batch = 0.1985s, grad.norm=12.42958546
 21652: 16 [  420/ 1327], train_loss/perplexity = 3.67932916/39.6198082 secs/batch = 0.1964s, grad.norm=11.74685574
 21657: 16 [  425/ 1327], train_loss/perplexity = 3.93349600/51.0852585 secs/batch = 0.1985s, grad.norm=12.86850739
 21662: 16 [  430/ 1327], train_loss/perplexity = 4.13163567/62.2797089 secs/batch = 0.1992s, grad.norm=12.70175743
 21667: 16 [  435/ 1327], train_loss/perplexity = 4.11605740/61.3170166 secs/batch = 0.1996s, grad.norm=12.25637627
 21672: 16 [  440/ 1327], train_loss/perplexity = 3.78225541/43.9149780 secs/batch = 0.2003s, grad.norm=12.49507999
 21677: 16 [  445/ 1327], train_loss/perplexity = 4.11242533/61.0947113 secs/batch = 0.1990s, grad.norm=12.48692226
 21682: 16 [  450/ 1327], train_loss/perplexity = 4.00564384/54.9071655 secs/batch = 0.1995s, grad.norm=11.81610870
 21687: 16 [  455/ 1327], train_loss/perplexity = 3.99941730/54.5663452 secs/batch = 0.2003s, grad.norm=11.60627937
 21692: 16 [  460/ 1327], train_loss/perplexity = 3.98639870/53.8605728 secs/batch = 0.2006s, grad.norm=12.49040604
 21697: 16 [  465/ 1327], train_loss/perplexity = 3.71566081/41.0857277 secs/batch = 0.1992s, grad.norm=13.08617878
 21702: 16 [  470/ 1327], train_loss/perplexity = 4.45958519/86.4516373 secs/batch = 0.1992s, grad.norm=12.07062340
 21707: 16 [  475/ 1327], train_loss/perplexity = 3.88522434/48.6778603 secs/batch = 0.1989s, grad.norm=12.37320328
 21712: 16 [  480/ 1327], train_loss/perplexity = 3.96794438/52.8757286 secs/batch = 0.1995s, grad.norm=12.15482616
 21717: 16 [  485/ 1327], train_loss/perplexity = 3.89128971/48.9740067 secs/batch = 0.1997s, grad.norm=12.37486267
 21722: 16 [  490/ 1327], train_loss/perplexity = 3.92726183/50.7677765 secs/batch = 0.1997s, grad.norm=13.22212601
 21727: 16 [  495/ 1327], train_loss/perplexity = 3.94649363/51.7535820 secs/batch = 0.1991s, grad.norm=12.05712223
 21732: 16 [  500/ 1327], train_loss/perplexity = 4.08796549/59.6184731 secs/batch = 0.1938s, grad.norm=15.22408199
 21737: 16 [  505/ 1327], train_loss/perplexity = 4.18089390/65.4243088 secs/batch = 0.1995s, grad.norm=11.34751701
 21742: 16 [  510/ 1327], train_loss/perplexity = 4.50203037/90.2000885 secs/batch = 0.1956s, grad.norm=11.38961792
 21747: 16 [  515/ 1327], train_loss/perplexity = 4.19015551/66.0330582 secs/batch = 0.1998s, grad.norm=11.85038948
 21752: 16 [  520/ 1327], train_loss/perplexity = 4.32990932/75.9374008 secs/batch = 0.1992s, grad.norm=12.23628330
 21757: 16 [  525/ 1327], train_loss/perplexity = 3.92264843/50.5341034 secs/batch = 0.1995s, grad.norm=11.86429691
 21762: 16 [  530/ 1327], train_loss/perplexity = 3.94799137/51.8311539 secs/batch = 0.1994s, grad.norm=12.61709213
 21767: 16 [  535/ 1327], train_loss/perplexity = 4.11261606/61.1063652 secs/batch = 0.1989s, grad.norm=12.30903912
 21772: 16 [  540/ 1327], train_loss/perplexity = 4.14771128/63.2889824 secs/batch = 0.1998s, grad.norm=12.27632999
 21777: 16 [  545/ 1327], train_loss/perplexity = 4.16863918/64.6274490 secs/batch = 0.2007s, grad.norm=12.41071892
 21782: 16 [  550/ 1327], train_loss/perplexity = 4.12613440/61.9380302 secs/batch = 0.1939s, grad.norm=11.97729206
 21787: 16 [  555/ 1327], train_loss/perplexity = 3.96931028/52.9479980 secs/batch = 0.1992s, grad.norm=11.75819111
 21792: 16 [  560/ 1327], train_loss/perplexity = 4.01883268/55.6361237 secs/batch = 0.1999s, grad.norm=12.53647995
 21797: 16 [  565/ 1327], train_loss/perplexity = 3.95563602/52.2289009 secs/batch = 0.1998s, grad.norm=12.81576252
 21802: 16 [  570/ 1327], train_loss/perplexity = 3.98178411/53.6125984 secs/batch = 0.2006s, grad.norm=12.82455444
 21807: 16 [  575/ 1327], train_loss/perplexity = 3.69790912/40.3628235 secs/batch = 0.1929s, grad.norm=12.28557587
 21812: 16 [  580/ 1327], train_loss/perplexity = 4.14201307/62.9293747 secs/batch = 0.2007s, grad.norm=12.47349644
 21817: 16 [  585/ 1327], train_loss/perplexity = 3.76706409/43.2528915 secs/batch = 0.1992s, grad.norm=11.76794624
 21822: 16 [  590/ 1327], train_loss/perplexity = 4.19846106/66.5837860 secs/batch = 0.1997s, grad.norm=12.04931831
 21827: 16 [  595/ 1327], train_loss/perplexity = 4.06662941/58.3599243 secs/batch = 0.2000s, grad.norm=12.56707096
 21832: 16 [  600/ 1327], train_loss/perplexity = 4.23031712/68.7390289 secs/batch = 0.1995s, grad.norm=11.66061401
 21837: 16 [  605/ 1327], train_loss/perplexity = 4.21931505/67.9869003 secs/batch = 0.1992s, grad.norm=12.30945969
 21842: 16 [  610/ 1327], train_loss/perplexity = 4.37130356/79.1467361 secs/batch = 0.1930s, grad.norm=11.99967289
 21847: 16 [  615/ 1327], train_loss/perplexity = 3.98860216/53.9793816 secs/batch = 0.1988s, grad.norm=12.21265411
 21852: 16 [  620/ 1327], train_loss/perplexity = 4.30670691/74.1957550 secs/batch = 0.1994s, grad.norm=12.10139370
 21857: 16 [  625/ 1327], train_loss/perplexity = 4.27355337/71.7762299 secs/batch = 0.1993s, grad.norm=11.84018421
 21862: 16 [  630/ 1327], train_loss/perplexity = 4.33860064/76.6002731 secs/batch = 0.1992s, grad.norm=12.54218388
 21867: 16 [  635/ 1327], train_loss/perplexity = 4.04340458/57.0201416 secs/batch = 0.2000s, grad.norm=11.91766739
 21872: 16 [  640/ 1327], train_loss/perplexity = 4.06053352/58.0052490 secs/batch = 0.1983s, grad.norm=12.15968513
 21877: 16 [  645/ 1327], train_loss/perplexity = 4.40633249/81.9682922 secs/batch = 0.1988s, grad.norm=12.82185555
 21882: 16 [  650/ 1327], train_loss/perplexity = 3.78032780/43.8304062 secs/batch = 0.1992s, grad.norm=12.33103943
 21887: 16 [  655/ 1327], train_loss/perplexity = 3.96020746/52.4682083 secs/batch = 0.1991s, grad.norm=12.34759140
 21892: 16 [  660/ 1327], train_loss/perplexity = 3.91293168/50.0454559 secs/batch = 0.1999s, grad.norm=12.04764271
 21897: 16 [  665/ 1327], train_loss/perplexity = 4.06664228/58.3606758 secs/batch = 0.1994s, grad.norm=12.65504837
 21902: 16 [  670/ 1327], train_loss/perplexity = 3.98336530/53.6974373 secs/batch = 0.1992s, grad.norm=12.46179676
 21907: 16 [  675/ 1327], train_loss/perplexity = 3.81298947/45.2856178 secs/batch = 0.1992s, grad.norm=12.24495888
 21912: 16 [  680/ 1327], train_loss/perplexity = 4.02567673/56.0182037 secs/batch = 0.1988s, grad.norm=12.50676918
 21917: 16 [  685/ 1327], train_loss/perplexity = 3.81532264/45.3913994 secs/batch = 0.1993s, grad.norm=11.75591087
 21922: 16 [  690/ 1327], train_loss/perplexity = 4.12388945/61.7991409 secs/batch = 0.1993s, grad.norm=11.97861385
 21927: 16 [  695/ 1327], train_loss/perplexity = 4.03973532/56.8113022 secs/batch = 0.2006s, grad.norm=12.35172272
 21932: 16 [  700/ 1327], train_loss/perplexity = 4.28538179/72.6302719 secs/batch = 0.1987s, grad.norm=12.50512791
 21937: 16 [  705/ 1327], train_loss/perplexity = 4.00178909/54.6959190 secs/batch = 0.2004s, grad.norm=11.64003944
 21942: 16 [  710/ 1327], train_loss/perplexity = 3.94994903/51.9327202 secs/batch = 0.1983s, grad.norm=12.32950878
 21947: 16 [  715/ 1327], train_loss/perplexity = 3.87809634/48.3321190 secs/batch = 0.1972s, grad.norm=12.51529503
 21952: 16 [  720/ 1327], train_loss/perplexity = 3.84693193/46.8491058 secs/batch = 0.1997s, grad.norm=12.20503902
 21957: 16 [  725/ 1327], train_loss/perplexity = 3.86075497/47.5011978 secs/batch = 0.2008s, grad.norm=12.05758095
 21962: 16 [  730/ 1327], train_loss/perplexity = 4.11071777/60.9904785 secs/batch = 0.1994s, grad.norm=12.59026814
 21967: 16 [  735/ 1327], train_loss/perplexity = 4.11815596/61.4458275 secs/batch = 0.1998s, grad.norm=12.73943520
 21972: 16 [  740/ 1327], train_loss/perplexity = 3.56225681/35.2426453 secs/batch = 0.1994s, grad.norm=11.72152805
 21977: 16 [  745/ 1327], train_loss/perplexity = 4.13157749/62.2760849 secs/batch = 0.1987s, grad.norm=13.15709686
 21982: 16 [  750/ 1327], train_loss/perplexity = 3.93644953/51.2363663 secs/batch = 0.1934s, grad.norm=12.38818073
 21987: 16 [  755/ 1327], train_loss/perplexity = 3.87130165/48.0048294 secs/batch = 0.1994s, grad.norm=12.25020599
 21992: 16 [  760/ 1327], train_loss/perplexity = 3.67514014/39.4541855 secs/batch = 0.1996s, grad.norm=11.82769775
 21997: 16 [  765/ 1327], train_loss/perplexity = 3.77221131/43.4760971 secs/batch = 0.2004s, grad.norm=11.49025249
 22002: 16 [  770/ 1327], train_loss/perplexity = 3.80263877/44.8192978 secs/batch = 0.2002s, grad.norm=12.31746960
 22007: 16 [  775/ 1327], train_loss/perplexity = 3.84614658/46.8123283 secs/batch = 0.1998s, grad.norm=12.92511940
 22012: 16 [  780/ 1327], train_loss/perplexity = 4.20262718/66.8617554 secs/batch = 0.1977s, grad.norm=12.44590187
 22017: 16 [  785/ 1327], train_loss/perplexity = 4.11895514/61.4949532 secs/batch = 0.1997s, grad.norm=12.31806850
 22022: 16 [  790/ 1327], train_loss/perplexity = 3.76373649/43.1092033 secs/batch = 0.1997s, grad.norm=12.16727638
 22027: 16 [  795/ 1327], train_loss/perplexity = 4.19587278/66.4116669 secs/batch = 0.1994s, grad.norm=12.83272552
 22032: 16 [  800/ 1327], train_loss/perplexity = 4.06751108/58.4113998 secs/batch = 0.1993s, grad.norm=12.75479889
 22037: 16 [  805/ 1327], train_loss/perplexity = 4.37340832/79.3134995 secs/batch = 0.1990s, grad.norm=12.49114895
 22042: 16 [  810/ 1327], train_loss/perplexity = 4.00131702/54.6701050 secs/batch = 0.1977s, grad.norm=11.41000271
 22047: 16 [  815/ 1327], train_loss/perplexity = 3.88156223/48.4999237 secs/batch = 0.2001s, grad.norm=11.87298679
 22052: 16 [  820/ 1327], train_loss/perplexity = 3.76161623/43.0178947 secs/batch = 0.1995s, grad.norm=11.73072910
 22057: 16 [  825/ 1327], train_loss/perplexity = 3.96998930/52.9839630 secs/batch = 0.2000s, grad.norm=12.12790203
 22062: 16 [  830/ 1327], train_loss/perplexity = 3.74481273/42.3010864 secs/batch = 0.1994s, grad.norm=12.47535801
 22067: 16 [  835/ 1327], train_loss/perplexity = 3.98917007/54.0100479 secs/batch = 0.1996s, grad.norm=12.86195183
 22072: 16 [  840/ 1327], train_loss/perplexity = 4.05545235/57.7112617 secs/batch = 0.1994s, grad.norm=12.93029213
 22077: 16 [  845/ 1327], train_loss/perplexity = 3.91773725/50.2865295 secs/batch = 0.1978s, grad.norm=12.66509438
 22082: 16 [  850/ 1327], train_loss/perplexity = 3.91480041/50.1390648 secs/batch = 0.1989s, grad.norm=11.99126148
 22087: 16 [  855/ 1327], train_loss/perplexity = 4.02169180/55.7954216 secs/batch = 0.1997s, grad.norm=12.22870445
 22092: 16 [  860/ 1327], train_loss/perplexity = 3.70862150/40.7975273 secs/batch = 0.1987s, grad.norm=11.52347946
 22097: 16 [  865/ 1327], train_loss/perplexity = 4.19108868/66.0947113 secs/batch = 0.1999s, grad.norm=11.87851906
 22102: 16 [  870/ 1327], train_loss/perplexity = 3.98335433/53.6968498 secs/batch = 0.1998s, grad.norm=12.54006672
 22107: 16 [  875/ 1327], train_loss/perplexity = 3.64389873/38.2406349 secs/batch = 0.1946s, grad.norm=12.16415310
 22112: 16 [  880/ 1327], train_loss/perplexity = 3.84226751/46.6310921 secs/batch = 0.1993s, grad.norm=11.95608139
 22117: 16 [  885/ 1327], train_loss/perplexity = 4.03823185/56.7259560 secs/batch = 0.1991s, grad.norm=12.18339729
 22122: 16 [  890/ 1327], train_loss/perplexity = 4.17109299/64.7862244 secs/batch = 0.1994s, grad.norm=12.00951004
 22127: 16 [  895/ 1327], train_loss/perplexity = 4.11232519/61.0885963 secs/batch = 0.1995s, grad.norm=12.08885002
 22132: 16 [  900/ 1327], train_loss/perplexity = 3.96606588/52.7764931 secs/batch = 0.2001s, grad.norm=11.58546066
 22137: 16 [  905/ 1327], train_loss/perplexity = 3.85826659/47.3831444 secs/batch = 0.1991s, grad.norm=11.32359886
 22142: 16 [  910/ 1327], train_loss/perplexity = 3.87946224/48.3981819 secs/batch = 0.1993s, grad.norm=11.36957169
 22147: 16 [  915/ 1327], train_loss/perplexity = 4.09156370/59.8333817 secs/batch = 0.1994s, grad.norm=12.18551254
 22152: 16 [  920/ 1327], train_loss/perplexity = 4.24569941/69.8045654 secs/batch = 0.1996s, grad.norm=12.24390507
 22157: 16 [  925/ 1327], train_loss/perplexity = 4.01720810/55.5458107 secs/batch = 0.1984s, grad.norm=11.79036522
 22162: 16 [  930/ 1327], train_loss/perplexity = 4.05343342/57.5948639 secs/batch = 0.1990s, grad.norm=11.92372131
 22167: 16 [  935/ 1327], train_loss/perplexity = 4.19772148/66.5345612 secs/batch = 0.1992s, grad.norm=12.16726875
 22172: 16 [  940/ 1327], train_loss/perplexity = 4.13888073/62.7325668 secs/batch = 0.2016s, grad.norm=12.40164757
 22177: 16 [  945/ 1327], train_loss/perplexity = 4.27944946/72.2006836 secs/batch = 0.1999s, grad.norm=12.05972385
 22182: 16 [  950/ 1327], train_loss/perplexity = 4.12993813/62.1740761 secs/batch = 0.1998s, grad.norm=12.04470444
 22187: 16 [  955/ 1327], train_loss/perplexity = 4.03105879/56.3205109 secs/batch = 0.1987s, grad.norm=12.45708656
 22192: 16 [  960/ 1327], train_loss/perplexity = 4.36173058/78.3926849 secs/batch = 0.1989s, grad.norm=12.51484489
 22197: 16 [  965/ 1327], train_loss/perplexity = 4.06083393/58.0226784 secs/batch = 0.1979s, grad.norm=12.30149174
 22202: 16 [  970/ 1327], train_loss/perplexity = 4.27576113/71.9348679 secs/batch = 0.1999s, grad.norm=11.93442535
 22207: 16 [  975/ 1327], train_loss/perplexity = 4.01943159/55.6694527 secs/batch = 0.1998s, grad.norm=12.90531445
 22212: 16 [  980/ 1327], train_loss/perplexity = 3.88009477/48.4288025 secs/batch = 0.1984s, grad.norm=11.86668682
 22217: 16 [  985/ 1327], train_loss/perplexity = 3.98588705/53.8330193 secs/batch = 0.1923s, grad.norm=12.20018768
 22222: 16 [  990/ 1327], train_loss/perplexity = 4.06997728/58.5556335 secs/batch = 0.1987s, grad.norm=12.30740356
 22227: 16 [  995/ 1327], train_loss/perplexity = 4.20454788/66.9903030 secs/batch = 0.1985s, grad.norm=12.06774426
 22232: 16 [ 1000/ 1327], train_loss/perplexity = 3.70369196/40.5969086 secs/batch = 0.2000s, grad.norm=12.05537319
 22237: 16 [ 1005/ 1327], train_loss/perplexity = 4.19107151/66.0935745 secs/batch = 0.2003s, grad.norm=12.17003250
 22242: 16 [ 1010/ 1327], train_loss/perplexity = 3.79171395/44.3323174 secs/batch = 0.1996s, grad.norm=11.60029888
 22247: 16 [ 1015/ 1327], train_loss/perplexity = 4.28354359/72.4968872 secs/batch = 0.2007s, grad.norm=12.22720051
 22252: 16 [ 1020/ 1327], train_loss/perplexity = 4.27069712/71.5715103 secs/batch = 0.1990s, grad.norm=11.87537479
 22257: 16 [ 1025/ 1327], train_loss/perplexity = 4.21197319/67.4895782 secs/batch = 0.1991s, grad.norm=12.05850410
 22262: 16 [ 1030/ 1327], train_loss/perplexity = 4.02939129/56.2266731 secs/batch = 0.1988s, grad.norm=11.86481476
 22267: 16 [ 1035/ 1327], train_loss/perplexity = 3.96494246/52.7172356 secs/batch = 0.1996s, grad.norm=11.68543720
 22272: 16 [ 1040/ 1327], train_loss/perplexity = 4.22237682/68.1953812 secs/batch = 0.2003s, grad.norm=12.46614456
 22277: 16 [ 1045/ 1327], train_loss/perplexity = 3.70194960/40.5262375 secs/batch = 0.2002s, grad.norm=11.70081329
 22282: 16 [ 1050/ 1327], train_loss/perplexity = 3.77630520/43.6544495 secs/batch = 0.2002s, grad.norm=11.35179520
 22287: 16 [ 1055/ 1327], train_loss/perplexity = 3.88881540/48.8529816 secs/batch = 0.1996s, grad.norm=12.25780678
 22292: 16 [ 1060/ 1327], train_loss/perplexity = 3.56140423/35.2126083 secs/batch = 0.1994s, grad.norm=12.55605030
 22297: 16 [ 1065/ 1327], train_loss/perplexity = 3.72311258/41.3930321 secs/batch = 0.1974s, grad.norm=11.88565254
 22302: 16 [ 1070/ 1327], train_loss/perplexity = 4.00208807/54.7122726 secs/batch = 0.1991s, grad.norm=12.27677059
 22307: 16 [ 1075/ 1327], train_loss/perplexity = 3.81412077/45.3368759 secs/batch = 0.1946s, grad.norm=11.89360905
 22312: 16 [ 1080/ 1327], train_loss/perplexity = 3.75386620/42.6857948 secs/batch = 0.1999s, grad.norm=12.64565372
 22317: 16 [ 1085/ 1327], train_loss/perplexity = 3.57020783/35.5239754 secs/batch = 0.2004s, grad.norm=12.47187233
 22322: 16 [ 1090/ 1327], train_loss/perplexity = 3.90588903/49.6942406 secs/batch = 0.2002s, grad.norm=12.85587120
 22327: 16 [ 1095/ 1327], train_loss/perplexity = 3.99087238/54.1020660 secs/batch = 0.1994s, grad.norm=12.76855755
 22332: 16 [ 1100/ 1327], train_loss/perplexity = 3.70464373/40.6355667 secs/batch = 0.1989s, grad.norm=12.92254925
 22337: 16 [ 1105/ 1327], train_loss/perplexity = 3.68055105/39.6682472 secs/batch = 0.2013s, grad.norm=12.53359032
 22342: 16 [ 1110/ 1327], train_loss/perplexity = 4.03745270/56.6817741 secs/batch = 0.1992s, grad.norm=12.57628250
 22347: 16 [ 1115/ 1327], train_loss/perplexity = 3.79028416/44.2689781 secs/batch = 0.1992s, grad.norm=11.95853806
 22352: 16 [ 1120/ 1327], train_loss/perplexity = 3.98426437/53.7457390 secs/batch = 0.1993s, grad.norm=12.06444359
 22357: 16 [ 1125/ 1327], train_loss/perplexity = 4.16568995/64.4371262 secs/batch = 0.1994s, grad.norm=12.61124039
 22362: 16 [ 1130/ 1327], train_loss/perplexity = 3.87724566/48.2910233 secs/batch = 0.1989s, grad.norm=12.02107525
 22367: 16 [ 1135/ 1327], train_loss/perplexity = 3.89570212/49.1905785 secs/batch = 0.1983s, grad.norm=12.33630371
 22372: 16 [ 1140/ 1327], train_loss/perplexity = 4.13174152/62.2863007 secs/batch = 0.1990s, grad.norm=12.79043674
 22377: 16 [ 1145/ 1327], train_loss/perplexity = 3.94176579/51.5094757 secs/batch = 0.2002s, grad.norm=11.69488335
 22382: 16 [ 1150/ 1327], train_loss/perplexity = 3.94843197/51.8539925 secs/batch = 0.1983s, grad.norm=12.14157867
 22387: 16 [ 1155/ 1327], train_loss/perplexity = 4.04908133/57.3447533 secs/batch = 0.1992s, grad.norm=12.44368076
 22392: 16 [ 1160/ 1327], train_loss/perplexity = 3.93179607/50.9984932 secs/batch = 0.2009s, grad.norm=12.03217220
 22397: 16 [ 1165/ 1327], train_loss/perplexity = 3.97070384/53.0218353 secs/batch = 0.1961s, grad.norm=12.24664974
 22402: 16 [ 1170/ 1327], train_loss/perplexity = 3.85716057/47.3307686 secs/batch = 0.1944s, grad.norm=11.87911224
 22407: 16 [ 1175/ 1327], train_loss/perplexity = 3.61877656/37.2919159 secs/batch = 0.1991s, grad.norm=12.24962425
 22412: 16 [ 1180/ 1327], train_loss/perplexity = 3.74832726/42.4500160 secs/batch = 0.1998s, grad.norm=12.25620079
 22417: 16 [ 1185/ 1327], train_loss/perplexity = 3.84911633/46.9515533 secs/batch = 0.1998s, grad.norm=12.15333271
 22422: 16 [ 1190/ 1327], train_loss/perplexity = 3.93158102/50.9875259 secs/batch = 0.2003s, grad.norm=12.61912918
 22427: 16 [ 1195/ 1327], train_loss/perplexity = 3.74807191/42.4391747 secs/batch = 0.1990s, grad.norm=12.06641579
 22432: 16 [ 1200/ 1327], train_loss/perplexity = 3.69434929/40.2193947 secs/batch = 0.1991s, grad.norm=12.61515808
 22437: 16 [ 1205/ 1327], train_loss/perplexity = 3.72003198/41.2657127 secs/batch = 0.1988s, grad.norm=12.46324444
 22442: 16 [ 1210/ 1327], train_loss/perplexity = 3.45666337/31.7109928 secs/batch = 0.1959s, grad.norm=11.79202271
 22447: 16 [ 1215/ 1327], train_loss/perplexity = 3.60132504/36.6467590 secs/batch = 0.2001s, grad.norm=11.78493118
 22452: 16 [ 1220/ 1327], train_loss/perplexity = 3.70951867/40.8341484 secs/batch = 0.1987s, grad.norm=12.51037693
 22457: 16 [ 1225/ 1327], train_loss/perplexity = 3.53488851/34.2911911 secs/batch = 0.1997s, grad.norm=12.60977745
 22462: 16 [ 1230/ 1327], train_loss/perplexity = 3.89374542/49.0944214 secs/batch = 0.1994s, grad.norm=12.34787846
 22467: 16 [ 1235/ 1327], train_loss/perplexity = 3.78880310/44.2034607 secs/batch = 0.1990s, grad.norm=12.13452911
 22472: 16 [ 1240/ 1327], train_loss/perplexity = 3.92608166/50.7078972 secs/batch = 0.1998s, grad.norm=12.78683662
 22477: 16 [ 1245/ 1327], train_loss/perplexity = 3.89740753/49.2745399 secs/batch = 0.1998s, grad.norm=11.82397461
 22482: 16 [ 1250/ 1327], train_loss/perplexity = 3.98862243/53.9804764 secs/batch = 0.1995s, grad.norm=11.92905903
 22487: 16 [ 1255/ 1327], train_loss/perplexity = 4.03010798/56.2669868 secs/batch = 0.1992s, grad.norm=12.11285782
 22492: 16 [ 1260/ 1327], train_loss/perplexity = 3.77287364/43.5049019 secs/batch = 0.1993s, grad.norm=12.78512001
 22497: 16 [ 1265/ 1327], train_loss/perplexity = 3.99574471/54.3663139 secs/batch = 0.1995s, grad.norm=12.59338665
 22502: 16 [ 1270/ 1327], train_loss/perplexity = 3.76782298/43.2857285 secs/batch = 0.1985s, grad.norm=12.45400620
 22507: 16 [ 1275/ 1327], train_loss/perplexity = 3.94491529/51.6719627 secs/batch = 0.1988s, grad.norm=12.50074291
 22512: 16 [ 1280/ 1327], train_loss/perplexity = 3.81185913/45.2344589 secs/batch = 0.1996s, grad.norm=12.07685280
 22517: 16 [ 1285/ 1327], train_loss/perplexity = 3.66180992/38.9317436 secs/batch = 0.2005s, grad.norm=12.57277012
 22522: 16 [ 1290/ 1327], train_loss/perplexity = 3.92599821/50.7036667 secs/batch = 0.1999s, grad.norm=11.96772766
 22527: 16 [ 1295/ 1327], train_loss/perplexity = 3.93176365/50.9968376 secs/batch = 0.1998s, grad.norm=12.62545109
 22532: 16 [ 1300/ 1327], train_loss/perplexity = 4.03992367/56.8220062 secs/batch = 0.1993s, grad.norm=11.82396793
 22537: 16 [ 1305/ 1327], train_loss/perplexity = 4.16202593/64.2014618 secs/batch = 0.2005s, grad.norm=12.35038662
 22542: 16 [ 1310/ 1327], train_loss/perplexity = 4.41713381/82.8584595 secs/batch = 0.1993s, grad.norm=13.34440041
 22547: 16 [ 1315/ 1327], train_loss/perplexity = 4.18437433/65.6524124 secs/batch = 0.1996s, grad.norm=12.45889759
 22552: 16 [ 1320/ 1327], train_loss/perplexity = 4.21657848/67.8011017 secs/batch = 0.2013s, grad.norm=11.99313068
 22557: 16 [ 1325/ 1327], train_loss/perplexity = 4.16694641/64.5181427 secs/batch = 0.1987s, grad.norm=12.36995125
Epoch training time: 264.1203227043152
	> validation loss = 4.73604631, perplexity = 113.98265839
	> validation loss = 4.64697790, perplexity = 104.26939392
	> validation loss = 4.59375238, perplexity = 98.86471558
	> validation loss = 4.62118816, perplexity = 101.61469269
	> validation loss = 4.82311630, perplexity = 124.35200500
	> validation loss = 4.65510368, perplexity = 105.12011719
	> validation loss = 4.69087601, perplexity = 108.94857788
	> validation loss = 4.48341751, perplexity = 88.53672791
	> validation loss = 4.30063009, perplexity = 73.74624634
	> validation loss = 4.39896488, perplexity = 81.36660004
	> validation loss = 4.54615259, perplexity = 94.26902008
	> validation loss = 4.62334824, perplexity = 101.83442688
	> validation loss = 4.53042459, perplexity = 92.79795074
	> validation loss = 4.34320879, perplexity = 76.95407104
	> validation loss = 4.27238655, perplexity = 71.69252777
	> validation loss = 4.26055908, perplexity = 70.84958649
	> validation loss = 4.72955132, perplexity = 113.24474335
	> validation loss = 4.26142645, perplexity = 70.91106415
	> validation loss = 4.76585817, perplexity = 117.43185425
	> validation loss = 4.61647463, perplexity = 101.13685608
	> validation loss = 4.42855835, perplexity = 83.81050110
at the end of epoch: 16
train loss = 4.04523330, perplexity = 57.12451171
validation loss = 4.54761186, perplexity = 94.40668261
Saved model cv/epoch016_4.5476.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.125
new learning rate is: 0.0625
 22564: 17 [    5/ 1327], train_loss/perplexity = 4.25825024/70.6861877 secs/batch = 0.1991s, grad.norm=12.34135914
 22569: 17 [   10/ 1327], train_loss/perplexity = 3.87480068/48.1730957 secs/batch = 0.2005s, grad.norm=11.90117645
 22574: 17 [   15/ 1327], train_loss/perplexity = 4.18848228/65.9226608 secs/batch = 0.1996s, grad.norm=12.03960228
 22579: 17 [   20/ 1327], train_loss/perplexity = 4.33060217/75.9900284 secs/batch = 0.1985s, grad.norm=12.15067768
 22584: 17 [   25/ 1327], train_loss/perplexity = 4.17815781/65.2455444 secs/batch = 0.1942s, grad.norm=12.19662571
 22589: 17 [   30/ 1327], train_loss/perplexity = 4.19900370/66.6199265 secs/batch = 0.1996s, grad.norm=12.33016396
 22594: 17 [   35/ 1327], train_loss/perplexity = 4.04825354/57.2973022 secs/batch = 0.1998s, grad.norm=12.13245487
 22599: 17 [   40/ 1327], train_loss/perplexity = 4.02544165/56.0050392 secs/batch = 0.1994s, grad.norm=12.19595146
 22604: 17 [   45/ 1327], train_loss/perplexity = 3.83962774/46.5081596 secs/batch = 0.1949s, grad.norm=11.50536633
 22609: 17 [   50/ 1327], train_loss/perplexity = 4.05897903/57.9151497 secs/batch = 0.1992s, grad.norm=12.11136436
 22614: 17 [   55/ 1327], train_loss/perplexity = 4.00324488/54.7756004 secs/batch = 0.1998s, grad.norm=12.50708675
 22619: 17 [   60/ 1327], train_loss/perplexity = 4.24502850/69.7577515 secs/batch = 0.1997s, grad.norm=12.47215176
 22624: 17 [   65/ 1327], train_loss/perplexity = 3.84566641/46.7898560 secs/batch = 0.2001s, grad.norm=12.05512619
 22629: 17 [   70/ 1327], train_loss/perplexity = 3.70660496/40.7153397 secs/batch = 0.1999s, grad.norm=12.21984291
 22634: 17 [   75/ 1327], train_loss/perplexity = 3.48029757/32.4693832 secs/batch = 0.2000s, grad.norm=11.61585522
 22639: 17 [   80/ 1327], train_loss/perplexity = 3.92739344/50.7744598 secs/batch = 0.2009s, grad.norm=12.23655128
 22644: 17 [   85/ 1327], train_loss/perplexity = 3.98182559/53.6148224 secs/batch = 0.1997s, grad.norm=12.25582600
 22649: 17 [   90/ 1327], train_loss/perplexity = 4.00353479/54.7914848 secs/batch = 0.1994s, grad.norm=12.45965672
 22654: 17 [   95/ 1327], train_loss/perplexity = 3.88130164/48.4872856 secs/batch = 0.1994s, grad.norm=12.08841419
 22659: 17 [  100/ 1327], train_loss/perplexity = 4.21253967/67.5278244 secs/batch = 0.1993s, grad.norm=12.40106869
 22664: 17 [  105/ 1327], train_loss/perplexity = 4.04114056/56.8911934 secs/batch = 0.1997s, grad.norm=13.21443081
 22669: 17 [  110/ 1327], train_loss/perplexity = 3.92178249/50.4903641 secs/batch = 0.2007s, grad.norm=12.57482624
 22674: 17 [  115/ 1327], train_loss/perplexity = 3.88218784/48.5302773 secs/batch = 0.1988s, grad.norm=12.64340210
 22679: 17 [  120/ 1327], train_loss/perplexity = 3.91985011/50.3928909 secs/batch = 0.1978s, grad.norm=12.76818180
 22684: 17 [  125/ 1327], train_loss/perplexity = 4.00477600/54.8595352 secs/batch = 0.1989s, grad.norm=13.01023769
 22689: 17 [  130/ 1327], train_loss/perplexity = 3.99922466/54.5558357 secs/batch = 0.1954s, grad.norm=12.85519409
 22694: 17 [  135/ 1327], train_loss/perplexity = 3.93798161/51.3149223 secs/batch = 0.2001s, grad.norm=12.50113869
 22699: 17 [  140/ 1327], train_loss/perplexity = 4.22264671/68.2137909 secs/batch = 0.1992s, grad.norm=12.89472961
 22704: 17 [  145/ 1327], train_loss/perplexity = 4.10098743/60.3998985 secs/batch = 0.1992s, grad.norm=13.20941067
 22709: 17 [  150/ 1327], train_loss/perplexity = 4.16334486/64.2861938 secs/batch = 0.1991s, grad.norm=12.78664207
 22714: 17 [  155/ 1327], train_loss/perplexity = 4.38169432/79.9734192 secs/batch = 0.1993s, grad.norm=12.75088406
 22719: 17 [  160/ 1327], train_loss/perplexity = 4.06226444/58.1057396 secs/batch = 0.1995s, grad.norm=11.93522453
 22724: 17 [  165/ 1327], train_loss/perplexity = 4.23762941/69.2435074 secs/batch = 0.1992s, grad.norm=12.71633816
 22729: 17 [  170/ 1327], train_loss/perplexity = 4.01234818/55.2765160 secs/batch = 0.1990s, grad.norm=12.32215786
 22734: 17 [  175/ 1327], train_loss/perplexity = 4.25257158/70.2859268 secs/batch = 0.1985s, grad.norm=12.54361629
 22739: 17 [  180/ 1327], train_loss/perplexity = 4.09203625/59.8616600 secs/batch = 0.1938s, grad.norm=12.55441475
 22744: 17 [  185/ 1327], train_loss/perplexity = 4.44258595/84.9944458 secs/batch = 0.1994s, grad.norm=12.45223045
 22749: 17 [  190/ 1327], train_loss/perplexity = 4.01457977/55.4000092 secs/batch = 0.2003s, grad.norm=11.97701645
 22754: 17 [  195/ 1327], train_loss/perplexity = 4.23153830/68.8230209 secs/batch = 0.1983s, grad.norm=12.23274326
 22759: 17 [  200/ 1327], train_loss/perplexity = 4.10862637/60.8630562 secs/batch = 0.1996s, grad.norm=12.38523197
 22764: 17 [  205/ 1327], train_loss/perplexity = 4.35259342/77.6796570 secs/batch = 0.2010s, grad.norm=12.41265774
 22769: 17 [  210/ 1327], train_loss/perplexity = 4.15086317/63.4887772 secs/batch = 0.1993s, grad.norm=11.66261864
 22774: 17 [  215/ 1327], train_loss/perplexity = 4.23760128/69.2415619 secs/batch = 0.1962s, grad.norm=12.06897736
 22779: 17 [  220/ 1327], train_loss/perplexity = 4.23317432/68.9357071 secs/batch = 0.1990s, grad.norm=12.28163052
 22784: 17 [  225/ 1327], train_loss/perplexity = 4.43949986/84.7325516 secs/batch = 0.1993s, grad.norm=12.61841393
 22789: 17 [  230/ 1327], train_loss/perplexity = 4.22736645/68.5364990 secs/batch = 0.2002s, grad.norm=13.34209061
 22794: 17 [  235/ 1327], train_loss/perplexity = 4.10334778/60.5426331 secs/batch = 0.1992s, grad.norm=12.23768425
 22799: 17 [  240/ 1327], train_loss/perplexity = 3.84514809/46.7656097 secs/batch = 0.1990s, grad.norm=12.66668320
 22804: 17 [  245/ 1327], train_loss/perplexity = 4.14300156/62.9916115 secs/batch = 0.1989s, grad.norm=12.14953041
 22809: 17 [  250/ 1327], train_loss/perplexity = 3.96726370/52.8397484 secs/batch = 0.1992s, grad.norm=11.88009262
 22814: 17 [  255/ 1327], train_loss/perplexity = 4.04939032/57.3624725 secs/batch = 0.2000s, grad.norm=12.39983654
 22819: 17 [  260/ 1327], train_loss/perplexity = 4.20481730/67.0083542 secs/batch = 0.1998s, grad.norm=12.88560581
 22824: 17 [  265/ 1327], train_loss/perplexity = 4.35165501/77.6067963 secs/batch = 0.1998s, grad.norm=11.81452274
 22829: 17 [  270/ 1327], train_loss/perplexity = 4.46210051/86.6693649 secs/batch = 0.2005s, grad.norm=12.50604916
 22834: 17 [  275/ 1327], train_loss/perplexity = 4.35606003/77.9494095 secs/batch = 0.2000s, grad.norm=12.11226940
 22839: 17 [  280/ 1327], train_loss/perplexity = 4.14359188/63.0288086 secs/batch = 0.1993s, grad.norm=12.21469593
 22844: 17 [  285/ 1327], train_loss/perplexity = 4.54421949/94.0869598 secs/batch = 0.1989s, grad.norm=12.09134007
 22849: 17 [  290/ 1327], train_loss/perplexity = 4.22361469/68.2798462 secs/batch = 0.1987s, grad.norm=12.10342884
 22854: 17 [  295/ 1327], train_loss/perplexity = 3.94905710/51.8864212 secs/batch = 0.1991s, grad.norm=12.19951725
 22859: 17 [  300/ 1327], train_loss/perplexity = 3.55174065/34.8739662 secs/batch = 0.2005s, grad.norm=11.25648594
 22864: 17 [  305/ 1327], train_loss/perplexity = 3.95551872/52.2227745 secs/batch = 0.1999s, grad.norm=12.10304737
 22869: 17 [  310/ 1327], train_loss/perplexity = 4.00482178/54.8620453 secs/batch = 0.1992s, grad.norm=12.20865917
 22874: 17 [  315/ 1327], train_loss/perplexity = 3.54810596/34.7474403 secs/batch = 0.1941s, grad.norm=11.34041214
 22879: 17 [  320/ 1327], train_loss/perplexity = 3.54388618/34.6011238 secs/batch = 0.1939s, grad.norm=12.52881432
 22884: 17 [  325/ 1327], train_loss/perplexity = 3.56774187/35.4364815 secs/batch = 0.2002s, grad.norm=11.76022625
 22889: 17 [  330/ 1327], train_loss/perplexity = 4.03639030/56.6215858 secs/batch = 0.1947s, grad.norm=12.22898579
 22894: 17 [  335/ 1327], train_loss/perplexity = 3.48953581/32.7707329 secs/batch = 0.1998s, grad.norm=11.57810020
 22899: 17 [  340/ 1327], train_loss/perplexity = 4.26455307/71.1331177 secs/batch = 0.1993s, grad.norm=12.08562183
 22904: 17 [  345/ 1327], train_loss/perplexity = 4.07158518/58.6498604 secs/batch = 0.1995s, grad.norm=11.93767357
 22909: 17 [  350/ 1327], train_loss/perplexity = 4.02027082/55.7161942 secs/batch = 0.2006s, grad.norm=12.37081051
 22914: 17 [  355/ 1327], train_loss/perplexity = 4.10665417/60.7431412 secs/batch = 0.1992s, grad.norm=12.16607094
 22919: 17 [  360/ 1327], train_loss/perplexity = 4.24491692/69.7499619 secs/batch = 0.1989s, grad.norm=13.56707096
 22924: 17 [  365/ 1327], train_loss/perplexity = 4.13664484/62.5924606 secs/batch = 0.1985s, grad.norm=11.89901161
 22929: 17 [  370/ 1327], train_loss/perplexity = 4.21863461/67.9406586 secs/batch = 0.2000s, grad.norm=12.17432404
 22934: 17 [  375/ 1327], train_loss/perplexity = 3.68870974/39.9932137 secs/batch = 0.1991s, grad.norm=12.05023003
 22939: 17 [  380/ 1327], train_loss/perplexity = 3.75159788/42.5890808 secs/batch = 0.1994s, grad.norm=12.25933170
 22944: 17 [  385/ 1327], train_loss/perplexity = 3.96104741/52.5122986 secs/batch = 0.1986s, grad.norm=12.44708729
 22949: 17 [  390/ 1327], train_loss/perplexity = 4.07654810/58.9416580 secs/batch = 0.1988s, grad.norm=12.60935783
 22954: 17 [  395/ 1327], train_loss/perplexity = 4.16486931/64.3842697 secs/batch = 0.2000s, grad.norm=12.42576313
 22959: 17 [  400/ 1327], train_loss/perplexity = 4.06542540/58.2896996 secs/batch = 0.2001s, grad.norm=12.13877106
 22964: 17 [  405/ 1327], train_loss/perplexity = 4.33755255/76.5200272 secs/batch = 0.1978s, grad.norm=12.51217937
 22969: 17 [  410/ 1327], train_loss/perplexity = 3.98493767/53.7819366 secs/batch = 0.1940s, grad.norm=12.50834846
 22974: 17 [  415/ 1327], train_loss/perplexity = 3.91141605/49.9696617 secs/batch = 0.1996s, grad.norm=12.32689857
 22979: 17 [  420/ 1327], train_loss/perplexity = 3.59987545/36.5936775 secs/batch = 0.1997s, grad.norm=11.96477604
 22984: 17 [  425/ 1327], train_loss/perplexity = 3.96446276/52.6919518 secs/batch = 0.1992s, grad.norm=13.17552948
 22989: 17 [  430/ 1327], train_loss/perplexity = 4.13439465/62.4517746 secs/batch = 0.1993s, grad.norm=12.67146111
 22994: 17 [  435/ 1327], train_loss/perplexity = 4.16475582/64.3769608 secs/batch = 0.1984s, grad.norm=12.73226738
 22999: 17 [  440/ 1327], train_loss/perplexity = 3.71545506/41.0772743 secs/batch = 0.1985s, grad.norm=12.49980450
 23004: 17 [  445/ 1327], train_loss/perplexity = 4.07812548/59.0347023 secs/batch = 0.1993s, grad.norm=12.62704754
 23009: 17 [  450/ 1327], train_loss/perplexity = 4.03579235/56.5877380 secs/batch = 0.2000s, grad.norm=12.86923218
 23014: 17 [  455/ 1327], train_loss/perplexity = 3.92545629/50.6761971 secs/batch = 0.1995s, grad.norm=11.78666878
 23019: 17 [  460/ 1327], train_loss/perplexity = 3.93961167/51.3986397 secs/batch = 0.1994s, grad.norm=13.07879925
 23024: 17 [  465/ 1327], train_loss/perplexity = 3.72473097/41.4600754 secs/batch = 0.1989s, grad.norm=13.20962143
 23029: 17 [  470/ 1327], train_loss/perplexity = 4.39268398/80.8571472 secs/batch = 0.1979s, grad.norm=12.04232407
 23034: 17 [  475/ 1327], train_loss/perplexity = 3.80077672/44.7359200 secs/batch = 0.1997s, grad.norm=12.07667542
 23039: 17 [  480/ 1327], train_loss/perplexity = 3.94906521/51.8868408 secs/batch = 0.1995s, grad.norm=12.61564922
 23044: 17 [  485/ 1327], train_loss/perplexity = 3.87547088/48.2053909 secs/batch = 0.1985s, grad.norm=12.58051777
 23049: 17 [  490/ 1327], train_loss/perplexity = 3.88047981/48.4474564 secs/batch = 0.1992s, grad.norm=13.55460548
 23054: 17 [  495/ 1327], train_loss/perplexity = 3.93466043/51.1447792 secs/batch = 0.1992s, grad.norm=12.48226643
 23059: 17 [  500/ 1327], train_loss/perplexity = 4.06449223/58.2353325 secs/batch = 0.1949s, grad.norm=12.56148434
 23064: 17 [  505/ 1327], train_loss/perplexity = 4.19818640/66.5654984 secs/batch = 0.1994s, grad.norm=11.45351505
 23069: 17 [  510/ 1327], train_loss/perplexity = 4.48196316/88.4080582 secs/batch = 0.2006s, grad.norm=11.91950798
 23074: 17 [  515/ 1327], train_loss/perplexity = 4.20261955/66.8612518 secs/batch = 0.1990s, grad.norm=11.70796680
 23079: 17 [  520/ 1327], train_loss/perplexity = 4.29557323/73.3742599 secs/batch = 0.1996s, grad.norm=12.03413200
 23084: 17 [  525/ 1327], train_loss/perplexity = 3.90478086/49.6392021 secs/batch = 0.1999s, grad.norm=11.91867256
 23089: 17 [  530/ 1327], train_loss/perplexity = 3.89898658/49.3524094 secs/batch = 0.1993s, grad.norm=12.28380394
 23094: 17 [  535/ 1327], train_loss/perplexity = 4.07742405/58.9933090 secs/batch = 0.1991s, grad.norm=12.17935276
 23099: 17 [  540/ 1327], train_loss/perplexity = 4.13023186/62.1923409 secs/batch = 0.2001s, grad.norm=12.15537167
 23104: 17 [  545/ 1327], train_loss/perplexity = 4.11185837/61.0600853 secs/batch = 0.1996s, grad.norm=12.23429298
 23109: 17 [  550/ 1327], train_loss/perplexity = 4.07848072/59.0556793 secs/batch = 0.2015s, grad.norm=12.36781406
 23114: 17 [  555/ 1327], train_loss/perplexity = 4.01036596/55.1670570 secs/batch = 0.1941s, grad.norm=11.99258709
 23119: 17 [  560/ 1327], train_loss/perplexity = 4.01282024/55.3026161 secs/batch = 0.1986s, grad.norm=12.72257900
 23124: 17 [  565/ 1327], train_loss/perplexity = 3.92082691/50.4421387 secs/batch = 0.1993s, grad.norm=12.63365746
 23129: 17 [  570/ 1327], train_loss/perplexity = 3.97779322/53.3990631 secs/batch = 0.1938s, grad.norm=13.09685040
 23134: 17 [  575/ 1327], train_loss/perplexity = 3.62092400/37.3720818 secs/batch = 0.1993s, grad.norm=12.11606312
 23139: 17 [  580/ 1327], train_loss/perplexity = 4.14100647/62.8660622 secs/batch = 0.1994s, grad.norm=12.59992981
 23144: 17 [  585/ 1327], train_loss/perplexity = 3.70423985/40.6191597 secs/batch = 0.1992s, grad.norm=12.09214592
 23149: 17 [  590/ 1327], train_loss/perplexity = 4.16763401/64.5625153 secs/batch = 0.1995s, grad.norm=12.40842533
 23154: 17 [  595/ 1327], train_loss/perplexity = 4.06030846/57.9921951 secs/batch = 0.1993s, grad.norm=12.68071270
 23159: 17 [  600/ 1327], train_loss/perplexity = 4.17443848/65.0033264 secs/batch = 0.2001s, grad.norm=11.89951897
 23164: 17 [  605/ 1327], train_loss/perplexity = 4.13661098/62.5903435 secs/batch = 0.1984s, grad.norm=12.14897633
 23169: 17 [  610/ 1327], train_loss/perplexity = 4.32248688/75.3758469 secs/batch = 0.1996s, grad.norm=12.39943123
 23174: 17 [  615/ 1327], train_loss/perplexity = 3.98451924/53.7594376 secs/batch = 0.1998s, grad.norm=12.18154430
 23179: 17 [  620/ 1327], train_loss/perplexity = 4.31392002/74.7328720 secs/batch = 0.1999s, grad.norm=12.14708233
 23184: 17 [  625/ 1327], train_loss/perplexity = 4.25882864/70.7270889 secs/batch = 0.1999s, grad.norm=11.89487267
 23189: 17 [  630/ 1327], train_loss/perplexity = 4.28110218/72.3201065 secs/batch = 0.2000s, grad.norm=12.42883968
 23194: 17 [  635/ 1327], train_loss/perplexity = 4.03026009/56.2755470 secs/batch = 0.2001s, grad.norm=11.89510155
 23199: 17 [  640/ 1327], train_loss/perplexity = 4.06332827/58.1675873 secs/batch = 0.1997s, grad.norm=12.33732319
 23204: 17 [  645/ 1327], train_loss/perplexity = 4.33281088/76.1580582 secs/batch = 0.1954s, grad.norm=13.30042171
 23209: 17 [  650/ 1327], train_loss/perplexity = 3.77376962/43.5438995 secs/batch = 0.1985s, grad.norm=12.13821793
 23214: 17 [  655/ 1327], train_loss/perplexity = 3.96355224/52.6439972 secs/batch = 0.1993s, grad.norm=12.94229221
 23219: 17 [  660/ 1327], train_loss/perplexity = 3.86396217/47.6537895 secs/batch = 0.1995s, grad.norm=12.51835823
 23224: 17 [  665/ 1327], train_loss/perplexity = 4.05403328/57.6294250 secs/batch = 0.2003s, grad.norm=12.51299095
 23229: 17 [  670/ 1327], train_loss/perplexity = 3.93333149/51.0768547 secs/batch = 0.1990s, grad.norm=12.53103352
 23234: 17 [  675/ 1327], train_loss/perplexity = 3.77753878/43.7083321 secs/batch = 0.1996s, grad.norm=12.69564438
 23239: 17 [  680/ 1327], train_loss/perplexity = 3.98340392/53.6995125 secs/batch = 0.1956s, grad.norm=12.62772846
 23244: 17 [  685/ 1327], train_loss/perplexity = 3.81241012/45.2593880 secs/batch = 0.2002s, grad.norm=12.04268837
 23249: 17 [  690/ 1327], train_loss/perplexity = 4.19529915/66.3735809 secs/batch = 0.1933s, grad.norm=12.05032825
 23254: 17 [  695/ 1327], train_loss/perplexity = 4.02323484/55.8815804 secs/batch = 0.1990s, grad.norm=11.95024586
 23259: 17 [  700/ 1327], train_loss/perplexity = 4.22021675/68.0482330 secs/batch = 0.1931s, grad.norm=12.68801498
 23264: 17 [  705/ 1327], train_loss/perplexity = 3.95008922/51.9399986 secs/batch = 0.2003s, grad.norm=11.82211018
 23269: 17 [  710/ 1327], train_loss/perplexity = 3.93049216/50.9320374 secs/batch = 0.1996s, grad.norm=12.37110329
 23274: 17 [  715/ 1327], train_loss/perplexity = 3.86357307/47.6352501 secs/batch = 0.1980s, grad.norm=12.43063354
 23279: 17 [  720/ 1327], train_loss/perplexity = 3.84209228/46.6229210 secs/batch = 0.1995s, grad.norm=12.38065052
 23284: 17 [  725/ 1327], train_loss/perplexity = 3.77685547/43.6784782 secs/batch = 0.1999s, grad.norm=12.15310574
 23289: 17 [  730/ 1327], train_loss/perplexity = 4.08218384/59.2747765 secs/batch = 0.1990s, grad.norm=12.08469772
 23294: 17 [  735/ 1327], train_loss/perplexity = 4.15673637/63.8627586 secs/batch = 0.1996s, grad.norm=13.04958344
 23299: 17 [  740/ 1327], train_loss/perplexity = 3.51951027/33.7678871 secs/batch = 0.1991s, grad.norm=11.49819565
 23304: 17 [  745/ 1327], train_loss/perplexity = 4.06402302/58.2080116 secs/batch = 0.1985s, grad.norm=13.10193157
 23309: 17 [  750/ 1327], train_loss/perplexity = 3.87863064/48.3579483 secs/batch = 0.1986s, grad.norm=12.23964119
 23314: 17 [  755/ 1327], train_loss/perplexity = 3.79878187/44.6467667 secs/batch = 0.1993s, grad.norm=11.64521027
 23319: 17 [  760/ 1327], train_loss/perplexity = 3.56746030/35.4265060 secs/batch = 0.1946s, grad.norm=11.86135006
 23324: 17 [  765/ 1327], train_loss/perplexity = 3.72659683/41.5375099 secs/batch = 0.1980s, grad.norm=11.65568542
 23329: 17 [  770/ 1327], train_loss/perplexity = 3.68941164/40.0212936 secs/batch = 0.2009s, grad.norm=12.12263966
 23334: 17 [  775/ 1327], train_loss/perplexity = 3.80558515/44.9515457 secs/batch = 0.1997s, grad.norm=12.54037285
 23339: 17 [  780/ 1327], train_loss/perplexity = 4.15888500/64.0001221 secs/batch = 0.2005s, grad.norm=12.32645130
 23344: 17 [  785/ 1327], train_loss/perplexity = 4.06027937/57.9905090 secs/batch = 0.2014s, grad.norm=12.86978531
 23349: 17 [  790/ 1327], train_loss/perplexity = 3.75278950/42.6398582 secs/batch = 0.1998s, grad.norm=12.08613300
 23354: 17 [  795/ 1327], train_loss/perplexity = 4.15858936/63.9812050 secs/batch = 0.1994s, grad.norm=12.51497173
 23359: 17 [  800/ 1327], train_loss/perplexity = 4.06354141/58.1799850 secs/batch = 0.2006s, grad.norm=13.00296783
 23364: 17 [  805/ 1327], train_loss/perplexity = 4.39195156/80.7979507 secs/batch = 0.1989s, grad.norm=12.92679691
 23369: 17 [  810/ 1327], train_loss/perplexity = 3.96222043/52.5739326 secs/batch = 0.1999s, grad.norm=11.49223232
 23374: 17 [  815/ 1327], train_loss/perplexity = 3.81494141/45.3740959 secs/batch = 0.1998s, grad.norm=11.79460621
 23379: 17 [  820/ 1327], train_loss/perplexity = 3.78774071/44.1565247 secs/batch = 0.1999s, grad.norm=11.71131325
 23384: 17 [  825/ 1327], train_loss/perplexity = 3.89750624/49.2794037 secs/batch = 0.2001s, grad.norm=12.41736507
 23389: 17 [  830/ 1327], train_loss/perplexity = 3.62044168/37.3540611 secs/batch = 0.2009s, grad.norm=12.67250252
 23394: 17 [  835/ 1327], train_loss/perplexity = 3.93886876/51.3604660 secs/batch = 0.2000s, grad.norm=12.71163559
 23399: 17 [  840/ 1327], train_loss/perplexity = 4.04500437/57.1114349 secs/batch = 0.1992s, grad.norm=12.53049946
 23404: 17 [  845/ 1327], train_loss/perplexity = 3.88310003/48.5745659 secs/batch = 0.1995s, grad.norm=12.99214363
 23409: 17 [  850/ 1327], train_loss/perplexity = 3.90453100/49.6268005 secs/batch = 0.1989s, grad.norm=11.93507385
 23414: 17 [  855/ 1327], train_loss/perplexity = 3.91293573/50.0456581 secs/batch = 0.2006s, grad.norm=12.52624130
 23419: 17 [  860/ 1327], train_loss/perplexity = 3.67663908/39.5133705 secs/batch = 0.1987s, grad.norm=11.99669933
 23424: 17 [  865/ 1327], train_loss/perplexity = 4.18121386/65.4452438 secs/batch = 0.1922s, grad.norm=12.25812817
 23429: 17 [  870/ 1327], train_loss/perplexity = 3.97455788/53.2265778 secs/batch = 0.1993s, grad.norm=12.69117928
 23434: 17 [  875/ 1327], train_loss/perplexity = 3.61254501/37.0602531 secs/batch = 0.1986s, grad.norm=11.77468586
 23439: 17 [  880/ 1327], train_loss/perplexity = 3.81136918/45.2122993 secs/batch = 0.1996s, grad.norm=11.98894501
 23444: 17 [  885/ 1327], train_loss/perplexity = 4.07962275/59.1231613 secs/batch = 0.1987s, grad.norm=11.98357201
 23449: 17 [  890/ 1327], train_loss/perplexity = 4.19098282/66.0877075 secs/batch = 0.1949s, grad.norm=12.18868446
 23454: 17 [  895/ 1327], train_loss/perplexity = 4.08892345/59.6756134 secs/batch = 0.1955s, grad.norm=11.94055080
 23459: 17 [  900/ 1327], train_loss/perplexity = 3.90531492/49.6657181 secs/batch = 0.1977s, grad.norm=11.88736725
 23464: 17 [  905/ 1327], train_loss/perplexity = 3.82598782/45.8780975 secs/batch = 0.1989s, grad.norm=11.50405502
 23469: 17 [  910/ 1327], train_loss/perplexity = 3.87558603/48.2109413 secs/batch = 0.1992s, grad.norm=11.22691154
 23474: 17 [  915/ 1327], train_loss/perplexity = 4.05746937/57.8277855 secs/batch = 0.1994s, grad.norm=12.11753559
 23479: 17 [  920/ 1327], train_loss/perplexity = 4.23878431/69.3235245 secs/batch = 0.1999s, grad.norm=12.31390572
 23484: 17 [  925/ 1327], train_loss/perplexity = 4.08110189/59.2106781 secs/batch = 0.1982s, grad.norm=12.00395679
 23489: 17 [  930/ 1327], train_loss/perplexity = 4.02884197/56.1957970 secs/batch = 0.1997s, grad.norm=11.96060848
 23494: 17 [  935/ 1327], train_loss/perplexity = 4.16009378/64.0775299 secs/batch = 0.2004s, grad.norm=12.05097198
 23499: 17 [  940/ 1327], train_loss/perplexity = 4.08257341/59.2978706 secs/batch = 0.1999s, grad.norm=11.65639210
 23504: 17 [  945/ 1327], train_loss/perplexity = 4.26494694/71.1611404 secs/batch = 0.1989s, grad.norm=12.26731014
 23509: 17 [  950/ 1327], train_loss/perplexity = 4.06949902/58.5276337 secs/batch = 0.2001s, grad.norm=11.94173241
 23514: 17 [  955/ 1327], train_loss/perplexity = 4.00561142/54.9053841 secs/batch = 0.1995s, grad.norm=12.13771439
 23519: 17 [  960/ 1327], train_loss/perplexity = 4.36893559/78.9595413 secs/batch = 0.2001s, grad.norm=12.52980423
 23524: 17 [  965/ 1327], train_loss/perplexity = 4.04084253/56.8742409 secs/batch = 0.1982s, grad.norm=12.29755402
 23529: 17 [  970/ 1327], train_loss/perplexity = 4.28391647/72.5239258 secs/batch = 0.1994s, grad.norm=12.05337334
 23534: 17 [  975/ 1327], train_loss/perplexity = 3.94497132/51.6748581 secs/batch = 0.1951s, grad.norm=12.76755047
 23539: 17 [  980/ 1327], train_loss/perplexity = 3.82540059/45.8511658 secs/batch = 0.1996s, grad.norm=11.77553558
 23544: 17 [  985/ 1327], train_loss/perplexity = 3.88052654/48.4497185 secs/batch = 0.2001s, grad.norm=12.37638855
 23549: 17 [  990/ 1327], train_loss/perplexity = 4.07207394/58.6785316 secs/batch = 0.2006s, grad.norm=12.70831966
 23554: 17 [  995/ 1327], train_loss/perplexity = 4.09493732/60.0355759 secs/batch = 0.1989s, grad.norm=12.12743568
 23559: 17 [ 1000/ 1327], train_loss/perplexity = 3.72038937/41.2804642 secs/batch = 0.1983s, grad.norm=11.60323620
 23564: 17 [ 1005/ 1327], train_loss/perplexity = 4.15090942/63.4917145 secs/batch = 0.1978s, grad.norm=11.98833561
 23569: 17 [ 1010/ 1327], train_loss/perplexity = 3.78586745/44.0738869 secs/batch = 0.2010s, grad.norm=11.72094250
 23574: 17 [ 1015/ 1327], train_loss/perplexity = 4.23527050/69.0803604 secs/batch = 0.1999s, grad.norm=12.34450817
 23579: 17 [ 1020/ 1327], train_loss/perplexity = 4.27295399/71.7332230 secs/batch = 0.2005s, grad.norm=11.82720089
 23584: 17 [ 1025/ 1327], train_loss/perplexity = 4.19702911/66.4885101 secs/batch = 0.1996s, grad.norm=12.01839828
 23589: 17 [ 1030/ 1327], train_loss/perplexity = 3.99022889/54.0672646 secs/batch = 0.1984s, grad.norm=11.97781086
 23594: 17 [ 1035/ 1327], train_loss/perplexity = 3.91023731/49.9107933 secs/batch = 0.1989s, grad.norm=11.75469971
 23599: 17 [ 1040/ 1327], train_loss/perplexity = 4.25820255/70.6828232 secs/batch = 0.1985s, grad.norm=12.83317852
 23604: 17 [ 1045/ 1327], train_loss/perplexity = 3.71530056/41.0709305 secs/batch = 0.2000s, grad.norm=11.93051338
 23609: 17 [ 1050/ 1327], train_loss/perplexity = 3.77002335/43.3810768 secs/batch = 0.1982s, grad.norm=11.57735443
 23614: 17 [ 1055/ 1327], train_loss/perplexity = 3.89557433/49.1842918 secs/batch = 0.1996s, grad.norm=12.51631641
 23619: 17 [ 1060/ 1327], train_loss/perplexity = 3.54261184/34.5570602 secs/batch = 0.1999s, grad.norm=12.27503967
 23624: 17 [ 1065/ 1327], train_loss/perplexity = 3.63379908/37.8563614 secs/batch = 0.1982s, grad.norm=12.25242710
 23629: 17 [ 1070/ 1327], train_loss/perplexity = 3.96111226/52.5157051 secs/batch = 0.1995s, grad.norm=12.29546070
 23634: 17 [ 1075/ 1327], train_loss/perplexity = 3.72014475/41.2703667 secs/batch = 0.2000s, grad.norm=13.24135590
 23639: 17 [ 1080/ 1327], train_loss/perplexity = 3.73032522/41.6926651 secs/batch = 0.1963s, grad.norm=11.93557644
 23644: 17 [ 1085/ 1327], train_loss/perplexity = 3.59508801/36.4189034 secs/batch = 0.2001s, grad.norm=12.25263214
 23649: 17 [ 1090/ 1327], train_loss/perplexity = 3.87595463/48.2287178 secs/batch = 0.1978s, grad.norm=12.58304310
 23654: 17 [ 1095/ 1327], train_loss/perplexity = 3.97275782/53.1308556 secs/batch = 0.1960s, grad.norm=12.29936790
 23659: 17 [ 1100/ 1327], train_loss/perplexity = 3.59009361/36.2374687 secs/batch = 0.1992s, grad.norm=12.85724545
 23664: 17 [ 1105/ 1327], train_loss/perplexity = 3.62321305/37.4577293 secs/batch = 0.2000s, grad.norm=12.34865475
 23669: 17 [ 1110/ 1327], train_loss/perplexity = 3.97720766/53.3678055 secs/batch = 0.1974s, grad.norm=12.91705513
 23674: 17 [ 1115/ 1327], train_loss/perplexity = 3.73489523/41.8836365 secs/batch = 0.1993s, grad.norm=11.70062160
 23679: 17 [ 1120/ 1327], train_loss/perplexity = 3.90757275/49.7779808 secs/batch = 0.1942s, grad.norm=12.20750427
 23684: 17 [ 1125/ 1327], train_loss/perplexity = 4.15486574/63.7434044 secs/batch = 0.1999s, grad.norm=12.95612812
 23689: 17 [ 1130/ 1327], train_loss/perplexity = 3.86335373/47.6248055 secs/batch = 0.1960s, grad.norm=12.31753826
 23694: 17 [ 1135/ 1327], train_loss/perplexity = 3.81364846/45.3154678 secs/batch = 0.1997s, grad.norm=11.92670059
 23699: 17 [ 1140/ 1327], train_loss/perplexity = 4.12069321/61.6019287 secs/batch = 0.1988s, grad.norm=12.62617970
 23704: 17 [ 1145/ 1327], train_loss/perplexity = 3.99146819/54.1343117 secs/batch = 0.1992s, grad.norm=11.95860577
 23709: 17 [ 1150/ 1327], train_loss/perplexity = 3.88333535/48.5859947 secs/batch = 0.1999s, grad.norm=12.34671688
 23714: 17 [ 1155/ 1327], train_loss/perplexity = 3.95259047/52.0700798 secs/batch = 0.2004s, grad.norm=12.41518402
 23719: 17 [ 1160/ 1327], train_loss/perplexity = 3.92018580/50.4098091 secs/batch = 0.2002s, grad.norm=12.25099182
 23724: 17 [ 1165/ 1327], train_loss/perplexity = 3.91643047/50.2208595 secs/batch = 0.1997s, grad.norm=12.47818089
 23729: 17 [ 1170/ 1327], train_loss/perplexity = 3.77878690/43.7629204 secs/batch = 0.1955s, grad.norm=12.37672520
 23734: 17 [ 1175/ 1327], train_loss/perplexity = 3.64315104/38.2120552 secs/batch = 0.1993s, grad.norm=12.08499146
 23739: 17 [ 1180/ 1327], train_loss/perplexity = 3.70631719/40.7036247 secs/batch = 0.2015s, grad.norm=12.42669392
 23744: 17 [ 1185/ 1327], train_loss/perplexity = 3.79818344/44.6200562 secs/batch = 0.1993s, grad.norm=12.24876881
 23749: 17 [ 1190/ 1327], train_loss/perplexity = 3.89476061/49.1442871 secs/batch = 0.1987s, grad.norm=12.52835083
 23754: 17 [ 1195/ 1327], train_loss/perplexity = 3.67698598/39.5270805 secs/batch = 0.1948s, grad.norm=11.92614174
 23759: 17 [ 1200/ 1327], train_loss/perplexity = 3.65979314/38.8533058 secs/batch = 0.1987s, grad.norm=11.99723625
 23764: 17 [ 1205/ 1327], train_loss/perplexity = 3.76665473/43.2351875 secs/batch = 0.2006s, grad.norm=12.83074665
 23769: 17 [ 1210/ 1327], train_loss/perplexity = 3.36974025/29.0709743 secs/batch = 0.1989s, grad.norm=12.52271461
 23774: 17 [ 1215/ 1327], train_loss/perplexity = 3.56714582/35.4153671 secs/batch = 0.1985s, grad.norm=12.11475849
 23779: 17 [ 1220/ 1327], train_loss/perplexity = 3.73612738/41.9352760 secs/batch = 0.1967s, grad.norm=12.31433678
 23784: 17 [ 1225/ 1327], train_loss/perplexity = 3.46310234/31.9158363 secs/batch = 0.1992s, grad.norm=12.96749687
 23789: 17 [ 1230/ 1327], train_loss/perplexity = 3.76914787/43.3431168 secs/batch = 0.1943s, grad.norm=12.07201290
 23794: 17 [ 1235/ 1327], train_loss/perplexity = 3.76674771/43.2392082 secs/batch = 0.1959s, grad.norm=12.20447445
 23799: 17 [ 1240/ 1327], train_loss/perplexity = 3.92109013/50.4554176 secs/batch = 0.1992s, grad.norm=12.50614357
 23804: 17 [ 1245/ 1327], train_loss/perplexity = 3.88559151/48.6957359 secs/batch = 0.1997s, grad.norm=12.10636044
 23809: 17 [ 1250/ 1327], train_loss/perplexity = 3.95413876/52.1507607 secs/batch = 0.1994s, grad.norm=12.03838444
 23814: 17 [ 1255/ 1327], train_loss/perplexity = 4.01309061/55.3175735 secs/batch = 0.2004s, grad.norm=12.01965237
 23819: 17 [ 1260/ 1327], train_loss/perplexity = 3.81067014/45.1807060 secs/batch = 0.1989s, grad.norm=12.93285084
 23824: 17 [ 1265/ 1327], train_loss/perplexity = 4.01983643/55.6919937 secs/batch = 0.1995s, grad.norm=13.07450199
 23829: 17 [ 1270/ 1327], train_loss/perplexity = 3.72076654/41.2960358 secs/batch = 0.1991s, grad.norm=12.55462551
 23834: 17 [ 1275/ 1327], train_loss/perplexity = 3.92898369/50.8552666 secs/batch = 0.2002s, grad.norm=13.67426395
 23839: 17 [ 1280/ 1327], train_loss/perplexity = 3.80589294/44.9653854 secs/batch = 0.1997s, grad.norm=12.60469627
 23844: 17 [ 1285/ 1327], train_loss/perplexity = 3.68494987/39.8431244 secs/batch = 0.1941s, grad.norm=12.37695408
 23849: 17 [ 1290/ 1327], train_loss/perplexity = 3.91465330/50.1316872 secs/batch = 0.1996s, grad.norm=12.68326855
 23854: 17 [ 1295/ 1327], train_loss/perplexity = 3.82707930/45.9281998 secs/batch = 0.2000s, grad.norm=12.29261303
 23859: 17 [ 1300/ 1327], train_loss/perplexity = 4.00699329/54.9813080 secs/batch = 0.2004s, grad.norm=11.56419849
 23864: 17 [ 1305/ 1327], train_loss/perplexity = 4.10880756/60.8740845 secs/batch = 0.1997s, grad.norm=12.43953133
 23869: 17 [ 1310/ 1327], train_loss/perplexity = 4.37534142/79.4669647 secs/batch = 0.1997s, grad.norm=12.65287399
 23874: 17 [ 1315/ 1327], train_loss/perplexity = 4.22556162/68.4129181 secs/batch = 0.1995s, grad.norm=12.82086754
 23879: 17 [ 1320/ 1327], train_loss/perplexity = 4.09674597/60.1442566 secs/batch = 0.1991s, grad.norm=12.00502586
 23884: 17 [ 1325/ 1327], train_loss/perplexity = 4.11618519/61.3248520 secs/batch = 0.1993s, grad.norm=12.29108429
Epoch training time: 264.22360467910767
	> validation loss = 4.73211765, perplexity = 113.53573608
	> validation loss = 4.63921070, perplexity = 103.46265411
	> validation loss = 4.57793665, perplexity = 97.31339264
	> validation loss = 4.61860180, perplexity = 101.35221863
	> validation loss = 4.81030035, perplexity = 122.76848602
	> validation loss = 4.64303160, perplexity = 103.85872650
	> validation loss = 4.69393063, perplexity = 109.28188324
	> validation loss = 4.48154831, perplexity = 88.37139130
	> validation loss = 4.29755783, perplexity = 73.52002716
	> validation loss = 4.39372826, perplexity = 80.94162750
	> validation loss = 4.53782368, perplexity = 93.48712158
	> validation loss = 4.62018347, perplexity = 101.51265717
	> validation loss = 4.53383255, perplexity = 93.11474609
	> validation loss = 4.33482313, perplexity = 76.31146240
	> validation loss = 4.26606941, perplexity = 71.24106598
	> validation loss = 4.26738119, perplexity = 71.33457947
	> validation loss = 4.72189331, perplexity = 112.38082123
	> validation loss = 4.24704790, perplexity = 69.89875793
	> validation loss = 4.76019335, perplexity = 116.76850128
	> validation loss = 4.61160755, perplexity = 100.64581299
	> validation loss = 4.42033100, perplexity = 83.12379456
at the end of epoch: 17
train loss = 4.01697252, perplexity = 55.53272703
validation loss = 4.54078402, perplexity = 93.76428444
Saved model cv/epoch017_4.5408.model
 23891: 18 [    5/ 1327], train_loss/perplexity = 4.25910378/70.7465515 secs/batch = 0.1988s, grad.norm=12.57610226
 23896: 18 [   10/ 1327], train_loss/perplexity = 3.79568672/44.5087929 secs/batch = 0.1994s, grad.norm=11.94568443
 23901: 18 [   15/ 1327], train_loss/perplexity = 4.14414358/63.0635910 secs/batch = 0.1999s, grad.norm=11.65029049
 23906: 18 [   20/ 1327], train_loss/perplexity = 4.36020184/78.2729340 secs/batch = 0.1992s, grad.norm=12.17726612
 23911: 18 [   25/ 1327], train_loss/perplexity = 4.16485929/64.3836212 secs/batch = 0.1983s, grad.norm=12.56166553
 23916: 18 [   30/ 1327], train_loss/perplexity = 4.21406317/67.6307755 secs/batch = 0.1998s, grad.norm=12.51308155
 23921: 18 [   35/ 1327], train_loss/perplexity = 3.97926569/53.4777489 secs/batch = 0.1995s, grad.norm=12.14543247
 23926: 18 [   40/ 1327], train_loss/perplexity = 4.03557682/56.5755463 secs/batch = 0.1938s, grad.norm=12.59406853
 23931: 18 [   45/ 1327], train_loss/perplexity = 3.76726651/43.2616463 secs/batch = 0.2001s, grad.norm=11.74199677
 23936: 18 [   50/ 1327], train_loss/perplexity = 4.02575111/56.0223732 secs/batch = 0.1996s, grad.norm=12.24621391
 23941: 18 [   55/ 1327], train_loss/perplexity = 4.01494980/55.4205132 secs/batch = 0.1992s, grad.norm=12.67353630
 23946: 18 [   60/ 1327], train_loss/perplexity = 4.27684689/72.0130157 secs/batch = 0.2005s, grad.norm=12.99714088
 23951: 18 [   65/ 1327], train_loss/perplexity = 3.82928705/46.0297089 secs/batch = 0.2008s, grad.norm=12.40435982
 23956: 18 [   70/ 1327], train_loss/perplexity = 3.67908049/39.6099548 secs/batch = 0.1997s, grad.norm=11.94888306
 23961: 18 [   75/ 1327], train_loss/perplexity = 3.47344327/32.2475891 secs/batch = 0.1991s, grad.norm=11.46690750
 23966: 18 [   80/ 1327], train_loss/perplexity = 3.91389418/50.0936470 secs/batch = 0.1990s, grad.norm=12.64830875
 23971: 18 [   85/ 1327], train_loss/perplexity = 3.92370462/50.5875053 secs/batch = 0.1962s, grad.norm=12.42308903
 23976: 18 [   90/ 1327], train_loss/perplexity = 4.05768871/57.8404694 secs/batch = 0.2004s, grad.norm=12.80186749
 23981: 18 [   95/ 1327], train_loss/perplexity = 3.83869672/46.4648781 secs/batch = 0.2013s, grad.norm=12.37915134
 23986: 18 [  100/ 1327], train_loss/perplexity = 4.18027687/65.3839569 secs/batch = 0.1989s, grad.norm=13.06923008
 23991: 18 [  105/ 1327], train_loss/perplexity = 3.99502707/54.3273125 secs/batch = 0.2016s, grad.norm=13.13465023
 23996: 18 [  110/ 1327], train_loss/perplexity = 3.82962155/46.0451088 secs/batch = 0.1992s, grad.norm=12.32411003
 24001: 18 [  115/ 1327], train_loss/perplexity = 3.81998873/45.6036949 secs/batch = 0.2010s, grad.norm=12.30050182
 24006: 18 [  120/ 1327], train_loss/perplexity = 3.93985271/51.4110298 secs/batch = 0.1987s, grad.norm=12.71963978
 24011: 18 [  125/ 1327], train_loss/perplexity = 4.03653336/56.6296883 secs/batch = 0.2003s, grad.norm=13.30590153
 24016: 18 [  130/ 1327], train_loss/perplexity = 3.95154738/52.0157928 secs/batch = 0.1989s, grad.norm=13.36534882
 24021: 18 [  135/ 1327], train_loss/perplexity = 3.89785767/49.2967262 secs/batch = 0.2007s, grad.norm=12.75411510
 24026: 18 [  140/ 1327], train_loss/perplexity = 4.25768185/70.6460266 secs/batch = 0.1992s, grad.norm=13.12051678
 24031: 18 [  145/ 1327], train_loss/perplexity = 4.10992622/60.9422226 secs/batch = 0.1934s, grad.norm=13.41463852
 24036: 18 [  150/ 1327], train_loss/perplexity = 4.15475845/63.7365685 secs/batch = 0.1993s, grad.norm=13.58445358
 24041: 18 [  155/ 1327], train_loss/perplexity = 4.32949209/75.9057236 secs/batch = 0.1994s, grad.norm=12.71357727
 24046: 18 [  160/ 1327], train_loss/perplexity = 4.09222603/59.8730240 secs/batch = 0.1993s, grad.norm=12.24688911
 24051: 18 [  165/ 1327], train_loss/perplexity = 4.22412443/68.3146667 secs/batch = 0.1995s, grad.norm=12.55247402
 24056: 18 [  170/ 1327], train_loss/perplexity = 3.98472071/53.7702713 secs/batch = 0.2006s, grad.norm=12.17037010
 24061: 18 [  175/ 1327], train_loss/perplexity = 4.26329470/71.0436630 secs/batch = 0.1995s, grad.norm=12.79538345
 24066: 18 [  180/ 1327], train_loss/perplexity = 4.15014696/63.4433250 secs/batch = 0.1985s, grad.norm=12.22493172
 24071: 18 [  185/ 1327], train_loss/perplexity = 4.38837051/80.5091248 secs/batch = 0.2000s, grad.norm=13.00202560
 24076: 18 [  190/ 1327], train_loss/perplexity = 3.99068165/54.0917473 secs/batch = 0.1987s, grad.norm=11.63226318
 24081: 18 [  195/ 1327], train_loss/perplexity = 4.25466633/70.4333115 secs/batch = 0.1934s, grad.norm=12.02629089
 24086: 18 [  200/ 1327], train_loss/perplexity = 4.08341980/59.3480835 secs/batch = 0.1990s, grad.norm=12.48326492
 24091: 18 [  205/ 1327], train_loss/perplexity = 4.33454084/76.2899246 secs/batch = 0.2002s, grad.norm=12.21323490
 24096: 18 [  210/ 1327], train_loss/perplexity = 4.22255182/68.2073135 secs/batch = 0.1949s, grad.norm=11.97252178
 24101: 18 [  215/ 1327], train_loss/perplexity = 4.18523455/65.7089081 secs/batch = 0.1995s, grad.norm=12.00962543
 24106: 18 [  220/ 1327], train_loss/perplexity = 4.21895313/67.9623032 secs/batch = 0.1993s, grad.norm=12.37608910
 24111: 18 [  225/ 1327], train_loss/perplexity = 4.42895126/83.8434448 secs/batch = 0.1991s, grad.norm=12.84779167
 24116: 18 [  230/ 1327], train_loss/perplexity = 4.25481510/70.4437943 secs/batch = 0.1993s, grad.norm=13.35108089
 24121: 18 [  235/ 1327], train_loss/perplexity = 4.09074831/59.7846107 secs/batch = 0.1952s, grad.norm=12.48648643
 24126: 18 [  240/ 1327], train_loss/perplexity = 3.89796734/49.3021317 secs/batch = 0.1994s, grad.norm=12.36017895
 24131: 18 [  245/ 1327], train_loss/perplexity = 4.13411427/62.4342651 secs/batch = 0.2000s, grad.norm=11.96654701
 24136: 18 [  250/ 1327], train_loss/perplexity = 3.99003029/54.0565262 secs/batch = 0.1997s, grad.norm=12.09171677
 24141: 18 [  255/ 1327], train_loss/perplexity = 4.03566170/56.5803452 secs/batch = 0.1992s, grad.norm=11.81498909
 24146: 18 [  260/ 1327], train_loss/perplexity = 4.13502216/62.4909744 secs/batch = 0.2000s, grad.norm=12.77445602
 24151: 18 [  265/ 1327], train_loss/perplexity = 4.30549908/74.1061935 secs/batch = 0.2001s, grad.norm=12.24815178
 24156: 18 [  270/ 1327], train_loss/perplexity = 4.40293598/81.6903610 secs/batch = 0.1993s, grad.norm=12.45281792
 24161: 18 [  275/ 1327], train_loss/perplexity = 4.27153111/71.6312256 secs/batch = 0.2000s, grad.norm=11.91626930
 24166: 18 [  280/ 1327], train_loss/perplexity = 4.17049217/64.7473145 secs/batch = 0.2012s, grad.norm=12.14630222
 24171: 18 [  285/ 1327], train_loss/perplexity = 4.43341351/84.2184067 secs/batch = 0.1998s, grad.norm=12.07505131
 24176: 18 [  290/ 1327], train_loss/perplexity = 4.17978334/65.3516922 secs/batch = 0.1986s, grad.norm=12.65072727
 24181: 18 [  295/ 1327], train_loss/perplexity = 3.92086887/50.4442558 secs/batch = 0.1987s, grad.norm=12.34728909
 24186: 18 [  300/ 1327], train_loss/perplexity = 3.52819586/34.0624580 secs/batch = 0.1990s, grad.norm=11.58493805
 24191: 18 [  305/ 1327], train_loss/perplexity = 3.96846199/52.9031029 secs/batch = 0.2005s, grad.norm=12.04259014
 24196: 18 [  310/ 1327], train_loss/perplexity = 4.01942348/55.6690025 secs/batch = 0.2000s, grad.norm=12.48771286
 24201: 18 [  315/ 1327], train_loss/perplexity = 3.45303297/31.5960770 secs/batch = 0.2002s, grad.norm=10.93431282
 24206: 18 [  320/ 1327], train_loss/perplexity = 3.49771357/33.0398216 secs/batch = 0.1986s, grad.norm=12.43128777
 24211: 18 [  325/ 1327], train_loss/perplexity = 3.50944638/33.4297562 secs/batch = 0.1931s, grad.norm=11.59970856
 24216: 18 [  330/ 1327], train_loss/perplexity = 4.10344696/60.5486374 secs/batch = 0.1982s, grad.norm=12.87164402
 24221: 18 [  335/ 1327], train_loss/perplexity = 3.53404593/34.2623100 secs/batch = 0.2004s, grad.norm=11.31830406
 24226: 18 [  340/ 1327], train_loss/perplexity = 4.27840805/72.1255264 secs/batch = 0.2008s, grad.norm=12.12657928
 24231: 18 [  345/ 1327], train_loss/perplexity = 4.06461096/58.2422447 secs/batch = 0.1992s, grad.norm=12.13242531
 24236: 18 [  350/ 1327], train_loss/perplexity = 4.07264709/58.7121735 secs/batch = 0.1997s, grad.norm=12.58924294
 24241: 18 [  355/ 1327], train_loss/perplexity = 4.06933832/58.5182304 secs/batch = 0.1991s, grad.norm=12.27923107
 24246: 18 [  360/ 1327], train_loss/perplexity = 4.20031023/66.7070236 secs/batch = 0.1998s, grad.norm=13.19961548
 24251: 18 [  365/ 1327], train_loss/perplexity = 4.15559864/63.7901421 secs/batch = 0.2001s, grad.norm=12.37773705
 24256: 18 [  370/ 1327], train_loss/perplexity = 4.22998142/68.7159576 secs/batch = 0.2000s, grad.norm=11.74188137
 24261: 18 [  375/ 1327], train_loss/perplexity = 3.66593933/39.0928383 secs/batch = 0.1984s, grad.norm=11.96582603
 24266: 18 [  380/ 1327], train_loss/perplexity = 3.74640179/42.3683586 secs/batch = 0.1995s, grad.norm=12.29237461
 24271: 18 [  385/ 1327], train_loss/perplexity = 3.94243598/51.5440102 secs/batch = 0.1986s, grad.norm=12.91005802
 24276: 18 [  390/ 1327], train_loss/perplexity = 4.06423664/58.2204475 secs/batch = 0.1991s, grad.norm=12.21195793
 24281: 18 [  395/ 1327], train_loss/perplexity = 4.11529255/61.2701378 secs/batch = 0.2000s, grad.norm=12.39664459
 24286: 18 [  400/ 1327], train_loss/perplexity = 4.07083035/58.6056061 secs/batch = 0.1997s, grad.norm=12.01736164
 24291: 18 [  405/ 1327], train_loss/perplexity = 4.32352924/75.4544525 secs/batch = 0.1996s, grad.norm=12.78191280
 24296: 18 [  410/ 1327], train_loss/perplexity = 4.01547146/55.4494324 secs/batch = 0.2001s, grad.norm=12.39358711
 24301: 18 [  415/ 1327], train_loss/perplexity = 3.89850354/49.3285751 secs/batch = 0.2001s, grad.norm=12.25216484
 24306: 18 [  420/ 1327], train_loss/perplexity = 3.58269095/35.9702034 secs/batch = 0.1994s, grad.norm=11.87136745
 24311: 18 [  425/ 1327], train_loss/perplexity = 3.89236355/49.0266266 secs/batch = 0.1952s, grad.norm=12.82184887
 24316: 18 [  430/ 1327], train_loss/perplexity = 4.12516022/61.8777237 secs/batch = 0.1992s, grad.norm=12.70850468
 24321: 18 [  435/ 1327], train_loss/perplexity = 4.08578110/59.4883842 secs/batch = 0.1995s, grad.norm=12.74880409
 24326: 18 [  440/ 1327], train_loss/perplexity = 3.72932243/41.6508789 secs/batch = 0.1953s, grad.norm=12.85661125
 24331: 18 [  445/ 1327], train_loss/perplexity = 4.02627182/56.0515518 secs/batch = 0.1977s, grad.norm=12.45149994
 24336: 18 [  450/ 1327], train_loss/perplexity = 3.94644690/51.7511635 secs/batch = 0.1996s, grad.norm=12.08616257
 24341: 18 [  455/ 1327], train_loss/perplexity = 3.95739746/52.3209801 secs/batch = 0.1945s, grad.norm=12.03473473
 24346: 18 [  460/ 1327], train_loss/perplexity = 3.95752430/52.3276176 secs/batch = 0.2006s, grad.norm=12.91644573
 24351: 18 [  465/ 1327], train_loss/perplexity = 3.68123388/39.6953430 secs/batch = 0.2004s, grad.norm=12.89574718
 24356: 18 [  470/ 1327], train_loss/perplexity = 4.34109449/76.7915421 secs/batch = 0.1987s, grad.norm=11.92527866
 24361: 18 [  475/ 1327], train_loss/perplexity = 3.80577850/44.9602394 secs/batch = 0.2001s, grad.norm=12.45480633
 24366: 18 [  480/ 1327], train_loss/perplexity = 3.94347048/51.5973587 secs/batch = 0.1993s, grad.norm=12.97750759
 24371: 18 [  485/ 1327], train_loss/perplexity = 3.94385982/51.6174507 secs/batch = 0.1992s, grad.norm=12.73738194
 24376: 18 [  490/ 1327], train_loss/perplexity = 3.86395431/47.6534157 secs/batch = 0.1993s, grad.norm=13.20245934
 24381: 18 [  495/ 1327], train_loss/perplexity = 3.86735010/47.8155136 secs/batch = 0.2000s, grad.norm=12.05570698
 24386: 18 [  500/ 1327], train_loss/perplexity = 4.05315113/57.5786095 secs/batch = 0.2008s, grad.norm=12.73488617
 24391: 18 [  505/ 1327], train_loss/perplexity = 4.15025520/63.4501915 secs/batch = 0.1995s, grad.norm=11.53108883
 24396: 18 [  510/ 1327], train_loss/perplexity = 4.45305681/85.8890915 secs/batch = 0.1999s, grad.norm=11.56356144
 24401: 18 [  515/ 1327], train_loss/perplexity = 4.15884495/63.9975586 secs/batch = 0.1994s, grad.norm=11.71912193
 24406: 18 [  520/ 1327], train_loss/perplexity = 4.27913237/72.1777878 secs/batch = 0.1992s, grad.norm=12.11141968
 24411: 18 [  525/ 1327], train_loss/perplexity = 3.88876653/48.8505936 secs/batch = 0.1968s, grad.norm=12.38314724
 24416: 18 [  530/ 1327], train_loss/perplexity = 3.98514700/53.7931976 secs/batch = 0.1990s, grad.norm=12.51960468
 24421: 18 [  535/ 1327], train_loss/perplexity = 4.06685495/58.3730888 secs/batch = 0.1991s, grad.norm=12.68084240
 24426: 18 [  540/ 1327], train_loss/perplexity = 4.16165257/64.1774902 secs/batch = 0.2001s, grad.norm=12.75039387
 24431: 18 [  545/ 1327], train_loss/perplexity = 4.13751221/62.6467743 secs/batch = 0.2014s, grad.norm=12.94283962
 24436: 18 [  550/ 1327], train_loss/perplexity = 4.12615061/61.9390373 secs/batch = 0.1992s, grad.norm=12.17438507
 24441: 18 [  555/ 1327], train_loss/perplexity = 3.93161011/50.9890099 secs/batch = 0.2001s, grad.norm=12.38251781
 24446: 18 [  560/ 1327], train_loss/perplexity = 4.02059650/55.7343407 secs/batch = 0.1990s, grad.norm=12.78783512
 24451: 18 [  565/ 1327], train_loss/perplexity = 3.88394308/48.6155319 secs/batch = 0.2002s, grad.norm=12.72333813
 24456: 18 [  570/ 1327], train_loss/perplexity = 3.92245770/50.5244675 secs/batch = 0.2009s, grad.norm=12.87945747
 24461: 18 [  575/ 1327], train_loss/perplexity = 3.67794800/39.5651245 secs/batch = 0.2009s, grad.norm=12.73990631
 24466: 18 [  580/ 1327], train_loss/perplexity = 4.14421082/63.0678291 secs/batch = 0.2000s, grad.norm=13.15470314
 24471: 18 [  585/ 1327], train_loss/perplexity = 3.72923279/41.6471443 secs/batch = 0.1979s, grad.norm=12.14694309
 24476: 18 [  590/ 1327], train_loss/perplexity = 4.12599754/61.9295540 secs/batch = 0.1999s, grad.norm=12.53650761
 24481: 18 [  595/ 1327], train_loss/perplexity = 4.02753830/56.1225853 secs/batch = 0.1995s, grad.norm=12.61716747
 24486: 18 [  600/ 1327], train_loss/perplexity = 4.18049526/65.3982315 secs/batch = 0.2017s, grad.norm=11.89446163
 24491: 18 [  605/ 1327], train_loss/perplexity = 4.09979534/60.3279381 secs/batch = 0.1980s, grad.norm=12.26129341
 24496: 18 [  610/ 1327], train_loss/perplexity = 4.33830643/76.5777359 secs/batch = 0.2003s, grad.norm=12.61359310
 24501: 18 [  615/ 1327], train_loss/perplexity = 3.93630266/51.2288399 secs/batch = 0.1998s, grad.norm=12.33784485
 24506: 18 [  620/ 1327], train_loss/perplexity = 4.25202703/70.2476654 secs/batch = 0.1995s, grad.norm=12.17158794
 24511: 18 [  625/ 1327], train_loss/perplexity = 4.25423479/70.4029236 secs/batch = 0.2003s, grad.norm=12.24359131
 24516: 18 [  630/ 1327], train_loss/perplexity = 4.26749134/71.3424377 secs/batch = 0.1993s, grad.norm=12.36935139
 24521: 18 [  635/ 1327], train_loss/perplexity = 4.00164986/54.6883049 secs/batch = 0.1991s, grad.norm=11.94041252
 24526: 18 [  640/ 1327], train_loss/perplexity = 4.05713940/57.8087082 secs/batch = 0.1936s, grad.norm=12.24979782
 24531: 18 [  645/ 1327], train_loss/perplexity = 4.35567331/77.9192734 secs/batch = 0.2007s, grad.norm=12.94678783
 24536: 18 [  650/ 1327], train_loss/perplexity = 3.77501249/43.5980530 secs/batch = 0.1992s, grad.norm=12.69852734
 24541: 18 [  655/ 1327], train_loss/perplexity = 3.92700052/50.7545128 secs/batch = 0.2003s, grad.norm=13.00071335
 24546: 18 [  660/ 1327], train_loss/perplexity = 3.83080196/46.0994949 secs/batch = 0.1987s, grad.norm=12.42007732
 24551: 18 [  665/ 1327], train_loss/perplexity = 3.93927240/51.3812027 secs/batch = 0.1994s, grad.norm=12.70042133
 24556: 18 [  670/ 1327], train_loss/perplexity = 3.98050356/53.5439911 secs/batch = 0.1993s, grad.norm=12.70947266
 24561: 18 [  675/ 1327], train_loss/perplexity = 3.78880167/44.2033997 secs/batch = 0.1996s, grad.norm=12.80228233
 24566: 18 [  680/ 1327], train_loss/perplexity = 4.01626301/55.4933395 secs/batch = 0.1994s, grad.norm=12.94921112
 24571: 18 [  685/ 1327], train_loss/perplexity = 3.74457884/42.2911911 secs/batch = 0.1997s, grad.norm=11.90835190
 24576: 18 [  690/ 1327], train_loss/perplexity = 4.21034908/67.3800583 secs/batch = 0.1999s, grad.norm=12.60758495
 24581: 18 [  695/ 1327], train_loss/perplexity = 3.96636248/52.7921486 secs/batch = 0.1993s, grad.norm=12.30335140
 24586: 18 [  700/ 1327], train_loss/perplexity = 4.26279449/71.0081406 secs/batch = 0.2001s, grad.norm=12.85624313
 24591: 18 [  705/ 1327], train_loss/perplexity = 3.96990204/52.9793396 secs/batch = 0.1998s, grad.norm=11.99130917
 24596: 18 [  710/ 1327], train_loss/perplexity = 3.87423563/48.1458817 secs/batch = 0.1996s, grad.norm=12.56172085
 24601: 18 [  715/ 1327], train_loss/perplexity = 3.82677984/45.9144478 secs/batch = 0.1994s, grad.norm=12.33943939
 24606: 18 [  720/ 1327], train_loss/perplexity = 3.78907704/44.2155724 secs/batch = 0.1993s, grad.norm=12.51266098
 24611: 18 [  725/ 1327], train_loss/perplexity = 3.79903316/44.6579857 secs/batch = 0.1992s, grad.norm=12.08176804
 24616: 18 [  730/ 1327], train_loss/perplexity = 4.03301716/56.4309158 secs/batch = 0.2005s, grad.norm=12.80636024
 24621: 18 [  735/ 1327], train_loss/perplexity = 4.09432840/59.9990311 secs/batch = 0.1939s, grad.norm=13.11803532
 24626: 18 [  740/ 1327], train_loss/perplexity = 3.55581975/35.0165138 secs/batch = 0.2008s, grad.norm=11.90543652
 24631: 18 [  745/ 1327], train_loss/perplexity = 4.07453966/58.8233948 secs/batch = 0.1997s, grad.norm=12.96504784
 24636: 18 [  750/ 1327], train_loss/perplexity = 3.88623381/48.7270241 secs/batch = 0.1993s, grad.norm=12.41289902
 24641: 18 [  755/ 1327], train_loss/perplexity = 3.78870463/44.1991081 secs/batch = 0.1991s, grad.norm=12.16761780
 24646: 18 [  760/ 1327], train_loss/perplexity = 3.59726214/36.4981689 secs/batch = 0.1985s, grad.norm=11.69353199
 24651: 18 [  765/ 1327], train_loss/perplexity = 3.69918251/40.4142532 secs/batch = 0.1999s, grad.norm=11.41130924
 24656: 18 [  770/ 1327], train_loss/perplexity = 3.70726871/40.7423744 secs/batch = 0.2006s, grad.norm=11.87003040
 24661: 18 [  775/ 1327], train_loss/perplexity = 3.76066065/42.9768105 secs/batch = 0.1999s, grad.norm=12.37212276
 24666: 18 [  780/ 1327], train_loss/perplexity = 4.12264013/61.7219810 secs/batch = 0.1941s, grad.norm=12.79565048
 24671: 18 [  785/ 1327], train_loss/perplexity = 4.07525206/58.8653145 secs/batch = 0.1924s, grad.norm=12.92715931
 24676: 18 [  790/ 1327], train_loss/perplexity = 3.75891447/42.9018288 secs/batch = 0.2001s, grad.norm=12.45346832
 24681: 18 [  795/ 1327], train_loss/perplexity = 4.13381481/62.4155731 secs/batch = 0.1997s, grad.norm=13.06212044
 24686: 18 [  800/ 1327], train_loss/perplexity = 4.05353785/57.6008797 secs/batch = 0.1991s, grad.norm=12.47549820
 24691: 18 [  805/ 1327], train_loss/perplexity = 4.34955454/77.4439545 secs/batch = 0.1994s, grad.norm=12.68427849
 24696: 18 [  810/ 1327], train_loss/perplexity = 3.91425705/50.1118279 secs/batch = 0.1995s, grad.norm=11.77368736
 24701: 18 [  815/ 1327], train_loss/perplexity = 3.82436657/45.8037758 secs/batch = 0.1994s, grad.norm=12.09342670
 24706: 18 [  820/ 1327], train_loss/perplexity = 3.73958778/42.0806389 secs/batch = 0.1989s, grad.norm=12.19454765
 24711: 18 [  825/ 1327], train_loss/perplexity = 3.91956782/50.3786659 secs/batch = 0.1988s, grad.norm=12.44111538
 24716: 18 [  830/ 1327], train_loss/perplexity = 3.59635639/36.4651260 secs/batch = 0.1994s, grad.norm=12.40165424
 24721: 18 [  835/ 1327], train_loss/perplexity = 3.92051435/50.4263763 secs/batch = 0.2011s, grad.norm=13.03020859
 24726: 18 [  840/ 1327], train_loss/perplexity = 4.12021112/61.5722389 secs/batch = 0.1994s, grad.norm=12.79383659
 24731: 18 [  845/ 1327], train_loss/perplexity = 3.87538290/48.2011528 secs/batch = 0.1999s, grad.norm=12.30830288
 24736: 18 [  850/ 1327], train_loss/perplexity = 3.88904428/48.8641624 secs/batch = 0.2000s, grad.norm=12.64416027
 24741: 18 [  855/ 1327], train_loss/perplexity = 3.92777491/50.7938309 secs/batch = 0.1986s, grad.norm=12.53355122
 24746: 18 [  860/ 1327], train_loss/perplexity = 3.68265295/39.7517128 secs/batch = 0.1931s, grad.norm=12.04666519
 24751: 18 [  865/ 1327], train_loss/perplexity = 4.13802528/62.6789246 secs/batch = 0.1943s, grad.norm=12.27256107
 24756: 18 [  870/ 1327], train_loss/perplexity = 3.96059895/52.4887543 secs/batch = 0.2002s, grad.norm=12.59746933
 24761: 18 [  875/ 1327], train_loss/perplexity = 3.64215398/38.1739731 secs/batch = 0.2003s, grad.norm=11.98562145
 24766: 18 [  880/ 1327], train_loss/perplexity = 3.79385042/44.4271355 secs/batch = 0.1995s, grad.norm=11.70920849
 24771: 18 [  885/ 1327], train_loss/perplexity = 4.02799606/56.1482811 secs/batch = 0.1993s, grad.norm=12.18238163
 24776: 18 [  890/ 1327], train_loss/perplexity = 4.17163229/64.8211746 secs/batch = 0.1999s, grad.norm=12.25548363
 24781: 18 [  895/ 1327], train_loss/perplexity = 4.03489351/56.5368996 secs/batch = 0.2011s, grad.norm=11.98914909
 24786: 18 [  900/ 1327], train_loss/perplexity = 3.90275359/49.5386696 secs/batch = 0.1981s, grad.norm=12.06830692
 24791: 18 [  905/ 1327], train_loss/perplexity = 3.84152913/46.5966721 secs/batch = 0.1987s, grad.norm=11.78062057
 24796: 18 [  910/ 1327], train_loss/perplexity = 3.89071441/48.9458427 secs/batch = 0.1996s, grad.norm=11.54549885
 24801: 18 [  915/ 1327], train_loss/perplexity = 4.06300211/58.1486168 secs/batch = 0.1995s, grad.norm=12.17132950
 24806: 18 [  920/ 1327], train_loss/perplexity = 4.23364925/68.9684601 secs/batch = 0.1940s, grad.norm=12.58334351
 24811: 18 [  925/ 1327], train_loss/perplexity = 4.03463936/56.5225334 secs/batch = 0.1992s, grad.norm=12.61438751
 24816: 18 [  930/ 1327], train_loss/perplexity = 3.94783592/51.8230972 secs/batch = 0.1996s, grad.norm=11.93997288
 24821: 18 [  935/ 1327], train_loss/perplexity = 4.16229296/64.2186050 secs/batch = 0.1989s, grad.norm=12.34311581
 24826: 18 [  940/ 1327], train_loss/perplexity = 4.06179476/58.0784531 secs/batch = 0.1989s, grad.norm=11.93826580
 24831: 18 [  945/ 1327], train_loss/perplexity = 4.26260328/70.9945602 secs/batch = 0.1998s, grad.norm=11.94759655
 24836: 18 [  950/ 1327], train_loss/perplexity = 4.07192850/58.6699982 secs/batch = 0.2015s, grad.norm=12.56266403
 24841: 18 [  955/ 1327], train_loss/perplexity = 3.94238281/51.5412674 secs/batch = 0.1984s, grad.norm=12.44223499
 24846: 18 [  960/ 1327], train_loss/perplexity = 4.33715439/76.4895706 secs/batch = 0.1998s, grad.norm=12.45370960
 24851: 18 [  965/ 1327], train_loss/perplexity = 4.05254078/57.5434761 secs/batch = 0.1994s, grad.norm=12.26947117
 24856: 18 [  970/ 1327], train_loss/perplexity = 4.30445480/74.0288467 secs/batch = 0.1994s, grad.norm=12.23346233
 24861: 18 [  975/ 1327], train_loss/perplexity = 3.88711548/48.7700043 secs/batch = 0.1999s, grad.norm=13.20599556
 24866: 18 [  980/ 1327], train_loss/perplexity = 3.77556777/43.6222687 secs/batch = 0.1994s, grad.norm=11.88098812
 24871: 18 [  985/ 1327], train_loss/perplexity = 3.87884021/48.3680840 secs/batch = 0.1989s, grad.norm=12.36218166
 24876: 18 [  990/ 1327], train_loss/perplexity = 4.07233572/58.6938934 secs/batch = 0.1942s, grad.norm=12.86081028
 24881: 18 [  995/ 1327], train_loss/perplexity = 4.12803650/62.0559578 secs/batch = 0.2006s, grad.norm=12.51492214
 24886: 18 [ 1000/ 1327], train_loss/perplexity = 3.67628098/39.4992218 secs/batch = 0.1993s, grad.norm=11.88208866
 24891: 18 [ 1005/ 1327], train_loss/perplexity = 4.13813066/62.6855316 secs/batch = 0.1935s, grad.norm=12.40898323
 24896: 18 [ 1010/ 1327], train_loss/perplexity = 3.82236505/45.7121925 secs/batch = 0.1931s, grad.norm=12.20952415
 24901: 18 [ 1015/ 1327], train_loss/perplexity = 4.25397015/70.3842926 secs/batch = 0.1992s, grad.norm=12.24413204
 24906: 18 [ 1020/ 1327], train_loss/perplexity = 4.22615671/68.4536362 secs/batch = 0.2001s, grad.norm=12.30583286
 24911: 18 [ 1025/ 1327], train_loss/perplexity = 4.15742493/63.9067459 secs/batch = 0.1992s, grad.norm=12.16174316
 24916: 18 [ 1030/ 1327], train_loss/perplexity = 3.99441981/54.2943306 secs/batch = 0.1992s, grad.norm=12.29597664
 24921: 18 [ 1035/ 1327], train_loss/perplexity = 3.91006589/49.9022408 secs/batch = 0.2001s, grad.norm=12.07870102
 24926: 18 [ 1040/ 1327], train_loss/perplexity = 4.16066980/64.1144485 secs/batch = 0.1995s, grad.norm=12.57989788
 24931: 18 [ 1045/ 1327], train_loss/perplexity = 3.67528272/39.4598122 secs/batch = 0.1997s, grad.norm=12.10380077
 24936: 18 [ 1050/ 1327], train_loss/perplexity = 3.72919226/41.6454544 secs/batch = 0.1993s, grad.norm=11.80757713
 24941: 18 [ 1055/ 1327], train_loss/perplexity = 3.85863447/47.4005814 secs/batch = 0.2004s, grad.norm=12.06357288
 24946: 18 [ 1060/ 1327], train_loss/perplexity = 3.53598142/34.3286896 secs/batch = 0.2000s, grad.norm=12.96384621
 24951: 18 [ 1065/ 1327], train_loss/perplexity = 3.62500286/37.5248299 secs/batch = 0.2007s, grad.norm=12.40255451
 24956: 18 [ 1070/ 1327], train_loss/perplexity = 3.94776154/51.8192406 secs/batch = 0.2004s, grad.norm=12.28873348
 24961: 18 [ 1075/ 1327], train_loss/perplexity = 3.74337530/42.2403221 secs/batch = 0.1994s, grad.norm=11.88259983
 24966: 18 [ 1080/ 1327], train_loss/perplexity = 3.70550990/40.6707802 secs/batch = 0.1927s, grad.norm=12.29002380
 24971: 18 [ 1085/ 1327], train_loss/perplexity = 3.54355145/34.5895424 secs/batch = 0.1980s, grad.norm=12.00313663
 24976: 18 [ 1090/ 1327], train_loss/perplexity = 3.83982325/46.5172501 secs/batch = 0.1992s, grad.norm=12.99727917
 24981: 18 [ 1095/ 1327], train_loss/perplexity = 3.92483854/50.6449013 secs/batch = 0.1990s, grad.norm=12.95272923
 24986: 18 [ 1100/ 1327], train_loss/perplexity = 3.59031725/36.2455711 secs/batch = 0.1932s, grad.norm=12.86124516
 24991: 18 [ 1105/ 1327], train_loss/perplexity = 3.63622093/37.9481583 secs/batch = 0.1946s, grad.norm=13.00721741
 24996: 18 [ 1110/ 1327], train_loss/perplexity = 3.94938421/51.9033966 secs/batch = 0.1995s, grad.norm=12.97688866
 25001: 18 [ 1115/ 1327], train_loss/perplexity = 3.71862912/41.2078629 secs/batch = 0.1995s, grad.norm=11.74498653
 25006: 18 [ 1120/ 1327], train_loss/perplexity = 4.00012589/54.6050224 secs/batch = 0.1996s, grad.norm=12.12010002
 25011: 18 [ 1125/ 1327], train_loss/perplexity = 4.12295341/61.7413216 secs/batch = 0.1963s, grad.norm=13.09744167
 25016: 18 [ 1130/ 1327], train_loss/perplexity = 3.81728601/45.4806061 secs/batch = 0.1995s, grad.norm=12.16086960
 25021: 18 [ 1135/ 1327], train_loss/perplexity = 3.73765707/41.9994736 secs/batch = 0.2003s, grad.norm=11.95241547
 25026: 18 [ 1140/ 1327], train_loss/perplexity = 4.11825609/61.4519806 secs/batch = 0.1965s, grad.norm=13.79957962
 25031: 18 [ 1145/ 1327], train_loss/perplexity = 3.93973041/51.4047394 secs/batch = 0.1993s, grad.norm=11.98293400
 25036: 18 [ 1150/ 1327], train_loss/perplexity = 3.81891584/45.5547943 secs/batch = 0.1995s, grad.norm=12.19998455
 25041: 18 [ 1155/ 1327], train_loss/perplexity = 4.02313757/55.8761482 secs/batch = 0.1945s, grad.norm=12.49456310
 25046: 18 [ 1160/ 1327], train_loss/perplexity = 3.87841201/48.3473778 secs/batch = 0.2003s, grad.norm=12.38174820
 25051: 18 [ 1165/ 1327], train_loss/perplexity = 3.90826440/49.8124237 secs/batch = 0.1971s, grad.norm=12.43705368
 25056: 18 [ 1170/ 1327], train_loss/perplexity = 3.86585522/47.7440872 secs/batch = 0.2002s, grad.norm=12.66114616
 25061: 18 [ 1175/ 1327], train_loss/perplexity = 3.63236833/37.8022385 secs/batch = 0.1993s, grad.norm=12.43455410
 25066: 18 [ 1180/ 1327], train_loss/perplexity = 3.69443226/40.2227287 secs/batch = 0.1924s, grad.norm=12.51643467
 25071: 18 [ 1185/ 1327], train_loss/perplexity = 3.79854321/44.6361122 secs/batch = 0.1931s, grad.norm=12.67364597
 25076: 18 [ 1190/ 1327], train_loss/perplexity = 3.87362337/48.1164131 secs/batch = 0.1997s, grad.norm=13.02025414
 25081: 18 [ 1195/ 1327], train_loss/perplexity = 3.65920544/38.8304787 secs/batch = 0.1992s, grad.norm=12.91976070
 25086: 18 [ 1200/ 1327], train_loss/perplexity = 3.65606356/38.7086678 secs/batch = 0.1996s, grad.norm=12.55001354
 25091: 18 [ 1205/ 1327], train_loss/perplexity = 3.69475555/40.2357368 secs/batch = 0.2006s, grad.norm=12.94483376
 25096: 18 [ 1210/ 1327], train_loss/perplexity = 3.34091616/28.2449913 secs/batch = 0.2000s, grad.norm=12.09523869
 25101: 18 [ 1215/ 1327], train_loss/perplexity = 3.57019472/35.5235100 secs/batch = 0.1995s, grad.norm=12.16163254
 25106: 18 [ 1220/ 1327], train_loss/perplexity = 3.75003481/42.5225639 secs/batch = 0.2004s, grad.norm=12.95665455
 25111: 18 [ 1225/ 1327], train_loss/perplexity = 3.45226169/31.5717163 secs/batch = 0.2003s, grad.norm=12.72179413
 25116: 18 [ 1230/ 1327], train_loss/perplexity = 3.80593729/44.9673767 secs/batch = 0.2000s, grad.norm=12.40664959
 25121: 18 [ 1235/ 1327], train_loss/perplexity = 3.68767452/39.9518318 secs/batch = 0.1929s, grad.norm=12.09937382
 25126: 18 [ 1240/ 1327], train_loss/perplexity = 3.92259979/50.5316467 secs/batch = 0.1990s, grad.norm=12.95850754
 25131: 18 [ 1245/ 1327], train_loss/perplexity = 3.83209825/46.1592903 secs/batch = 0.1995s, grad.norm=12.03899574
 25136: 18 [ 1250/ 1327], train_loss/perplexity = 3.95842934/52.3749962 secs/batch = 0.2000s, grad.norm=12.18481922
 25141: 18 [ 1255/ 1327], train_loss/perplexity = 3.97533846/53.2681427 secs/batch = 0.1995s, grad.norm=12.49391365
 25146: 18 [ 1260/ 1327], train_loss/perplexity = 3.78406262/43.9944115 secs/batch = 0.1978s, grad.norm=13.40739155
 25151: 18 [ 1265/ 1327], train_loss/perplexity = 3.91967106/50.3838692 secs/batch = 0.1997s, grad.norm=13.00375938
 25156: 18 [ 1270/ 1327], train_loss/perplexity = 3.70825601/40.7826195 secs/batch = 0.1993s, grad.norm=12.80364609
 25161: 18 [ 1275/ 1327], train_loss/perplexity = 3.90238738/49.5205307 secs/batch = 0.1985s, grad.norm=12.51037693
 25166: 18 [ 1280/ 1327], train_loss/perplexity = 3.70210743/40.5326347 secs/batch = 0.1929s, grad.norm=12.85446739
 25171: 18 [ 1285/ 1327], train_loss/perplexity = 3.65425324/38.6386566 secs/batch = 0.1991s, grad.norm=12.35668087
 25176: 18 [ 1290/ 1327], train_loss/perplexity = 3.92992449/50.9031334 secs/batch = 0.1973s, grad.norm=12.46805000
 25181: 18 [ 1295/ 1327], train_loss/perplexity = 3.85542679/47.2487793 secs/batch = 0.1997s, grad.norm=12.84144020
 25186: 18 [ 1300/ 1327], train_loss/perplexity = 3.97692513/53.3527298 secs/batch = 0.1955s, grad.norm=12.05022049
 25191: 18 [ 1305/ 1327], train_loss/perplexity = 4.11361170/61.1672363 secs/batch = 0.1991s, grad.norm=13.21623230
 25196: 18 [ 1310/ 1327], train_loss/perplexity = 4.37517786/79.4539719 secs/batch = 0.1996s, grad.norm=12.72102547
 25201: 18 [ 1315/ 1327], train_loss/perplexity = 4.19640636/66.4471130 secs/batch = 0.1990s, grad.norm=12.89766216
 25206: 18 [ 1320/ 1327], train_loss/perplexity = 4.11836433/61.4586334 secs/batch = 0.1993s, grad.norm=12.12820816
 25211: 18 [ 1325/ 1327], train_loss/perplexity = 4.07639599/58.9326935 secs/batch = 0.1928s, grad.norm=12.66618156
Epoch training time: 264.23464131355286
	> validation loss = 4.72747850, perplexity = 113.01024628
	> validation loss = 4.63643312, perplexity = 103.17567444
	> validation loss = 4.58888054, perplexity = 98.38423157
	> validation loss = 4.61124706, perplexity = 100.60953522
	> validation loss = 4.81493139, perplexity = 123.33834839
	> validation loss = 4.64469290, perplexity = 104.03141022
	> validation loss = 4.69668007, perplexity = 109.58276367
	> validation loss = 4.47667265, perplexity = 87.94157410
	> validation loss = 4.29930353, perplexity = 73.64848328
	> validation loss = 4.38641548, perplexity = 80.35188293
	> validation loss = 4.54215527, perplexity = 93.89294434
	> validation loss = 4.61433363, perplexity = 100.92055511
	> validation loss = 4.53172636, perplexity = 92.91883087
	> validation loss = 4.32419729, perplexity = 75.50488281
	> validation loss = 4.26137352, perplexity = 70.90731049
	> validation loss = 4.26108885, perplexity = 70.88712311
	> validation loss = 4.72529936, perplexity = 112.76425171
	> validation loss = 4.24708700, perplexity = 69.90148926
	> validation loss = 4.76148605, perplexity = 116.91954803
	> validation loss = 4.61410522, perplexity = 100.89750671
	> validation loss = 4.42001009, perplexity = 83.09712219
at the end of epoch: 18
train loss = 4.00118864, perplexity = 54.66308618
validation loss = 4.53993780, perplexity = 93.68497320
Saved model cv/epoch018_4.5399.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.0625
new learning rate is: 0.03125
 25218: 19 [    5/ 1327], train_loss/perplexity = 4.21612406/67.7703018 secs/batch = 0.1995s, grad.norm=12.61134529
 25223: 19 [   10/ 1327], train_loss/perplexity = 3.81868339/45.5442047 secs/batch = 0.1999s, grad.norm=12.74899197
 25228: 19 [   15/ 1327], train_loss/perplexity = 4.18185329/65.4871063 secs/batch = 0.1986s, grad.norm=12.62745953
 25233: 19 [   20/ 1327], train_loss/perplexity = 4.32086086/75.2533798 secs/batch = 0.2007s, grad.norm=12.06993198
 25238: 19 [   25/ 1327], train_loss/perplexity = 4.15326500/63.6414490 secs/batch = 0.1993s, grad.norm=12.77066612
 25243: 19 [   30/ 1327], train_loss/perplexity = 4.16658783/64.4950104 secs/batch = 0.1995s, grad.norm=12.47588158
 25248: 19 [   35/ 1327], train_loss/perplexity = 4.04736662/57.2465057 secs/batch = 0.1995s, grad.norm=12.52762985
 25253: 19 [   40/ 1327], train_loss/perplexity = 3.99910736/54.5494347 secs/batch = 0.1986s, grad.norm=12.75568199
 25258: 19 [   45/ 1327], train_loss/perplexity = 3.75799274/42.8623047 secs/batch = 0.2006s, grad.norm=11.94431210
 25263: 19 [   50/ 1327], train_loss/perplexity = 3.97841620/53.4323425 secs/batch = 0.2001s, grad.norm=12.55482864
 25268: 19 [   55/ 1327], train_loss/perplexity = 3.95195436/52.0369682 secs/batch = 0.1996s, grad.norm=12.52267933
 25273: 19 [   60/ 1327], train_loss/perplexity = 4.24377251/69.6701889 secs/batch = 0.2002s, grad.norm=13.04774189
 25278: 19 [   65/ 1327], train_loss/perplexity = 3.86020494/47.4750786 secs/batch = 0.2001s, grad.norm=12.33354759
 25283: 19 [   70/ 1327], train_loss/perplexity = 3.66079044/38.8920746 secs/batch = 0.1956s, grad.norm=12.09932899
 25288: 19 [   75/ 1327], train_loss/perplexity = 3.50776863/33.3737144 secs/batch = 0.1987s, grad.norm=11.68995190
 25293: 19 [   80/ 1327], train_loss/perplexity = 3.95646811/52.2723808 secs/batch = 0.2002s, grad.norm=12.70025826
 25298: 19 [   85/ 1327], train_loss/perplexity = 3.94082785/51.4611855 secs/batch = 0.1934s, grad.norm=12.41839695
 25303: 19 [   90/ 1327], train_loss/perplexity = 4.06117487/58.0424652 secs/batch = 0.1991s, grad.norm=12.74166298
 25308: 19 [   95/ 1327], train_loss/perplexity = 3.88287878/48.5638199 secs/batch = 0.1998s, grad.norm=12.37585163
 25313: 19 [  100/ 1327], train_loss/perplexity = 4.11728525/61.3923492 secs/batch = 0.1951s, grad.norm=13.05202007
 25318: 19 [  105/ 1327], train_loss/perplexity = 3.94390869/51.6199722 secs/batch = 0.2004s, grad.norm=13.58694077
 25323: 19 [  110/ 1327], train_loss/perplexity = 3.89363384/49.0889435 secs/batch = 0.1997s, grad.norm=12.49817467
 25328: 19 [  115/ 1327], train_loss/perplexity = 3.81126714/45.2076874 secs/batch = 0.1979s, grad.norm=12.82320690
 25333: 19 [  120/ 1327], train_loss/perplexity = 3.93750358/51.2903976 secs/batch = 0.1995s, grad.norm=13.12347794
 25338: 19 [  125/ 1327], train_loss/perplexity = 3.98607159/53.8429565 secs/batch = 0.1997s, grad.norm=12.93099403
 25343: 19 [  130/ 1327], train_loss/perplexity = 3.94412112/51.6309395 secs/batch = 0.1993s, grad.norm=13.34997177
 25348: 19 [  135/ 1327], train_loss/perplexity = 3.96011496/52.4633560 secs/batch = 0.1984s, grad.norm=13.06057262
 25353: 19 [  140/ 1327], train_loss/perplexity = 4.20821047/67.2361145 secs/batch = 0.1938s, grad.norm=12.99876308
 25358: 19 [  145/ 1327], train_loss/perplexity = 4.00096321/54.6507645 secs/batch = 0.1983s, grad.norm=13.54860878
 25363: 19 [  150/ 1327], train_loss/perplexity = 4.18076754/65.4160461 secs/batch = 0.1985s, grad.norm=13.30992603
 25368: 19 [  155/ 1327], train_loss/perplexity = 4.36860132/78.9331512 secs/batch = 0.1981s, grad.norm=12.94370079
 25373: 19 [  160/ 1327], train_loss/perplexity = 4.05243778/57.5375519 secs/batch = 0.2000s, grad.norm=12.38222885
 25378: 19 [  165/ 1327], train_loss/perplexity = 4.13440704/62.4525490 secs/batch = 0.1994s, grad.norm=12.84516430
 25383: 19 [  170/ 1327], train_loss/perplexity = 3.95596194/52.2459259 secs/batch = 0.1992s, grad.norm=12.04504967
 25388: 19 [  175/ 1327], train_loss/perplexity = 4.25640345/70.5557709 secs/batch = 0.1990s, grad.norm=13.13032722
 25393: 19 [  180/ 1327], train_loss/perplexity = 4.06625175/58.3378868 secs/batch = 0.2006s, grad.norm=12.40824127
 25398: 19 [  185/ 1327], train_loss/perplexity = 4.38397694/80.1561737 secs/batch = 0.1999s, grad.norm=12.82212067
 25403: 19 [  190/ 1327], train_loss/perplexity = 4.05463362/57.6640320 secs/batch = 0.1997s, grad.norm=11.90487480
 25408: 19 [  195/ 1327], train_loss/perplexity = 4.16423512/64.3434448 secs/batch = 0.1994s, grad.norm=12.43030167
 25413: 19 [  200/ 1327], train_loss/perplexity = 4.05916882/57.9261436 secs/batch = 0.1994s, grad.norm=12.61390877
 25418: 19 [  205/ 1327], train_loss/perplexity = 4.31440401/74.7690506 secs/batch = 0.1989s, grad.norm=12.62625408
 25423: 19 [  210/ 1327], train_loss/perplexity = 4.16713333/64.5301971 secs/batch = 0.1992s, grad.norm=11.93564606
 25428: 19 [  215/ 1327], train_loss/perplexity = 4.23983383/69.3963165 secs/batch = 0.1989s, grad.norm=11.94997501
 25433: 19 [  220/ 1327], train_loss/perplexity = 4.18548489/65.7253647 secs/batch = 0.1988s, grad.norm=12.19494057
 25438: 19 [  225/ 1327], train_loss/perplexity = 4.39507771/81.0509262 secs/batch = 0.1979s, grad.norm=13.23317337
 25443: 19 [  230/ 1327], train_loss/perplexity = 4.14497089/63.1157837 secs/batch = 0.1983s, grad.norm=13.19083023
 25448: 19 [  235/ 1327], train_loss/perplexity = 4.03608847/56.6044998 secs/batch = 0.1961s, grad.norm=12.65750122
 25453: 19 [  240/ 1327], train_loss/perplexity = 3.87368798/48.1195221 secs/batch = 0.1969s, grad.norm=13.06455517
 25458: 19 [  245/ 1327], train_loss/perplexity = 4.14826679/63.3241501 secs/batch = 0.1979s, grad.norm=12.46058750
 25463: 19 [  250/ 1327], train_loss/perplexity = 3.97120094/53.0481987 secs/batch = 0.1990s, grad.norm=12.05240536
 25468: 19 [  255/ 1327], train_loss/perplexity = 3.94060898/51.4499245 secs/batch = 0.2010s, grad.norm=11.98191643
 25473: 19 [  260/ 1327], train_loss/perplexity = 4.17907000/65.3050919 secs/batch = 0.1987s, grad.norm=12.82287216
 25478: 19 [  265/ 1327], train_loss/perplexity = 4.35245895/77.6692123 secs/batch = 0.2010s, grad.norm=12.37309074
 25483: 19 [  270/ 1327], train_loss/perplexity = 4.46555710/86.9694672 secs/batch = 0.2007s, grad.norm=12.74546432
 25488: 19 [  275/ 1327], train_loss/perplexity = 4.31325579/74.6832428 secs/batch = 0.2003s, grad.norm=12.56570816
 25493: 19 [  280/ 1327], train_loss/perplexity = 4.18913078/65.9654312 secs/batch = 0.1997s, grad.norm=12.59802532
 25498: 19 [  285/ 1327], train_loss/perplexity = 4.50585651/90.5458679 secs/batch = 0.1993s, grad.norm=12.40721512
 25503: 19 [  290/ 1327], train_loss/perplexity = 4.16511250/64.3999252 secs/batch = 0.1989s, grad.norm=12.73651028
 25508: 19 [  295/ 1327], train_loss/perplexity = 3.90956402/49.8772011 secs/batch = 0.2005s, grad.norm=12.17270470
 25513: 19 [  300/ 1327], train_loss/perplexity = 3.50116110/33.1539230 secs/batch = 0.1990s, grad.norm=11.76807976
 25518: 19 [  305/ 1327], train_loss/perplexity = 3.94104528/51.4723778 secs/batch = 0.1985s, grad.norm=12.08257771
 25523: 19 [  310/ 1327], train_loss/perplexity = 4.00705767/54.9848480 secs/batch = 0.2000s, grad.norm=12.71870041
 25528: 19 [  315/ 1327], train_loss/perplexity = 3.46952748/32.1215591 secs/batch = 0.1996s, grad.norm=11.39948273
 25533: 19 [  320/ 1327], train_loss/perplexity = 3.54580688/34.6676483 secs/batch = 0.1986s, grad.norm=12.57250214
 25538: 19 [  325/ 1327], train_loss/perplexity = 3.52467132/33.9426155 secs/batch = 0.1980s, grad.norm=11.67628384
 25543: 19 [  330/ 1327], train_loss/perplexity = 4.06143188/58.0573845 secs/batch = 0.2010s, grad.norm=12.51821613
 25548: 19 [  335/ 1327], train_loss/perplexity = 3.53023243/34.1319008 secs/batch = 0.2004s, grad.norm=11.60842991
 25553: 19 [  340/ 1327], train_loss/perplexity = 4.23662472/69.1739731 secs/batch = 0.1998s, grad.norm=12.46701241
 25558: 19 [  345/ 1327], train_loss/perplexity = 4.06317139/58.1584625 secs/batch = 0.1945s, grad.norm=11.98367310
 25563: 19 [  350/ 1327], train_loss/perplexity = 4.04409027/57.0592537 secs/batch = 0.1994s, grad.norm=12.17255020
 25568: 19 [  355/ 1327], train_loss/perplexity = 4.07064581/58.5947914 secs/batch = 0.2006s, grad.norm=12.51337147
 25573: 19 [  360/ 1327], train_loss/perplexity = 4.18884706/65.9467163 secs/batch = 0.1991s, grad.norm=13.55668831
 25578: 19 [  365/ 1327], train_loss/perplexity = 4.14424801/63.0701752 secs/batch = 0.2007s, grad.norm=12.33067799
 25583: 19 [  370/ 1327], train_loss/perplexity = 4.16304731/64.2670670 secs/batch = 0.1996s, grad.norm=12.85481930
 25588: 19 [  375/ 1327], train_loss/perplexity = 3.61597586/37.1876183 secs/batch = 0.1998s, grad.norm=12.00650883
 25593: 19 [  380/ 1327], train_loss/perplexity = 3.74658132/42.3759651 secs/batch = 0.1998s, grad.norm=12.19351959
 25598: 19 [  385/ 1327], train_loss/perplexity = 3.88111830/48.4783974 secs/batch = 0.1932s, grad.norm=13.13703918
 25603: 19 [  390/ 1327], train_loss/perplexity = 4.08045721/59.1725197 secs/batch = 0.1995s, grad.norm=12.53114986
 25608: 19 [  395/ 1327], train_loss/perplexity = 4.09109068/59.8050842 secs/batch = 0.2006s, grad.norm=12.54167461
 25613: 19 [  400/ 1327], train_loss/perplexity = 4.07050705/58.5866623 secs/batch = 0.1993s, grad.norm=12.28283024
 25618: 19 [  405/ 1327], train_loss/perplexity = 4.30160189/73.8179474 secs/batch = 0.1990s, grad.norm=12.82269478
 25623: 19 [  410/ 1327], train_loss/perplexity = 3.97750163/53.3834953 secs/batch = 0.1991s, grad.norm=13.04038811
 25628: 19 [  415/ 1327], train_loss/perplexity = 3.88173389/48.5082512 secs/batch = 0.1989s, grad.norm=12.48648357
 25633: 19 [  420/ 1327], train_loss/perplexity = 3.56297088/35.2678185 secs/batch = 0.1992s, grad.norm=12.33287239
 25638: 19 [  425/ 1327], train_loss/perplexity = 3.88929343/48.8763390 secs/batch = 0.1990s, grad.norm=13.16386986
 25643: 19 [  430/ 1327], train_loss/perplexity = 4.15679836/63.8667183 secs/batch = 0.1997s, grad.norm=13.26944828
 25648: 19 [  435/ 1327], train_loss/perplexity = 4.12591028/61.9241524 secs/batch = 0.1994s, grad.norm=12.67687702
 25653: 19 [  440/ 1327], train_loss/perplexity = 3.70854139/40.7942619 secs/batch = 0.2002s, grad.norm=12.76282787
 25658: 19 [  445/ 1327], train_loss/perplexity = 3.98795271/53.9443359 secs/batch = 0.1991s, grad.norm=12.86635113
 25663: 19 [  450/ 1327], train_loss/perplexity = 3.97157121/53.0678444 secs/batch = 0.1948s, grad.norm=12.60867214
 25668: 19 [  455/ 1327], train_loss/perplexity = 3.96241474/52.5841484 secs/batch = 0.1979s, grad.norm=12.11078835
 25673: 19 [  460/ 1327], train_loss/perplexity = 3.91318703/50.0582352 secs/batch = 0.1997s, grad.norm=12.63952446
 25678: 19 [  465/ 1327], train_loss/perplexity = 3.66360235/39.0015869 secs/batch = 0.1995s, grad.norm=13.41370869
 25683: 19 [  470/ 1327], train_loss/perplexity = 4.40077543/81.5140533 secs/batch = 0.1994s, grad.norm=12.17799950
 25688: 19 [  475/ 1327], train_loss/perplexity = 3.84211874/46.6241531 secs/batch = 0.2001s, grad.norm=12.47642994
 25693: 19 [  480/ 1327], train_loss/perplexity = 3.93661690/51.2449417 secs/batch = 0.1951s, grad.norm=12.57413006
 25698: 19 [  485/ 1327], train_loss/perplexity = 3.93679929/51.2542877 secs/batch = 0.1978s, grad.norm=12.53661633
 25703: 19 [  490/ 1327], train_loss/perplexity = 3.82478642/45.8230133 secs/batch = 0.1989s, grad.norm=13.40506840
 25708: 19 [  495/ 1327], train_loss/perplexity = 3.88974571/48.8984489 secs/batch = 0.1997s, grad.norm=12.49439716
 25713: 19 [  500/ 1327], train_loss/perplexity = 4.06290531/58.1429901 secs/batch = 0.1983s, grad.norm=13.33461380
 25718: 19 [  505/ 1327], train_loss/perplexity = 4.11985493/61.5503120 secs/batch = 0.1996s, grad.norm=11.80262566
 25723: 19 [  510/ 1327], train_loss/perplexity = 4.46550560/86.9649887 secs/batch = 0.1987s, grad.norm=12.42739201
 25728: 19 [  515/ 1327], train_loss/perplexity = 4.20373535/66.9358902 secs/batch = 0.2004s, grad.norm=12.09045696
 25733: 19 [  520/ 1327], train_loss/perplexity = 4.26363754/71.0680237 secs/batch = 0.2009s, grad.norm=12.43905258
 25738: 19 [  525/ 1327], train_loss/perplexity = 3.91526222/50.1622238 secs/batch = 0.1995s, grad.norm=12.34848976
 25743: 19 [  530/ 1327], train_loss/perplexity = 3.91217995/50.0078468 secs/batch = 0.1989s, grad.norm=13.06638908
 25748: 19 [  535/ 1327], train_loss/perplexity = 4.13168335/62.2826767 secs/batch = 0.2010s, grad.norm=12.57972240
 25753: 19 [  540/ 1327], train_loss/perplexity = 4.06705856/58.3849754 secs/batch = 0.1996s, grad.norm=12.44936848
 25758: 19 [  545/ 1327], train_loss/perplexity = 4.08254910/59.2964287 secs/batch = 0.1997s, grad.norm=12.45396328
 25763: 19 [  550/ 1327], train_loss/perplexity = 4.09681273/60.1482735 secs/batch = 0.1999s, grad.norm=12.30382824
 25768: 19 [  555/ 1327], train_loss/perplexity = 3.93545604/51.1854858 secs/batch = 0.1997s, grad.norm=11.88322353
 25773: 19 [  560/ 1327], train_loss/perplexity = 3.95862007/52.3849869 secs/batch = 0.2004s, grad.norm=13.06607628
 25778: 19 [  565/ 1327], train_loss/perplexity = 3.85229969/47.1012573 secs/batch = 0.2001s, grad.norm=12.84450722
 25783: 19 [  570/ 1327], train_loss/perplexity = 3.85231113/47.1017952 secs/batch = 0.1956s, grad.norm=12.68105888
 25788: 19 [  575/ 1327], train_loss/perplexity = 3.58456087/36.0375290 secs/batch = 0.1991s, grad.norm=12.47103596
 25793: 19 [  580/ 1327], train_loss/perplexity = 4.15748787/63.9107704 secs/batch = 0.1995s, grad.norm=12.97041512
 25798: 19 [  585/ 1327], train_loss/perplexity = 3.75763035/42.8467751 secs/batch = 0.1995s, grad.norm=12.36322784
 25803: 19 [  590/ 1327], train_loss/perplexity = 4.16459703/64.3667374 secs/batch = 0.1999s, grad.norm=12.54721832
 25808: 19 [  595/ 1327], train_loss/perplexity = 3.98280048/53.6671181 secs/batch = 0.2002s, grad.norm=13.31255245
 25813: 19 [  600/ 1327], train_loss/perplexity = 4.16641092/64.4835968 secs/batch = 0.1986s, grad.norm=11.97051334
 25818: 19 [  605/ 1327], train_loss/perplexity = 4.11570740/61.2955589 secs/batch = 0.1967s, grad.norm=12.52035332
 25823: 19 [  610/ 1327], train_loss/perplexity = 4.30398750/73.9942551 secs/batch = 0.1999s, grad.norm=12.39933491
 25828: 19 [  615/ 1327], train_loss/perplexity = 3.91911936/50.3560791 secs/batch = 0.1987s, grad.norm=12.24862289
 25833: 19 [  620/ 1327], train_loss/perplexity = 4.27209949/71.6719513 secs/batch = 0.1983s, grad.norm=12.70169353
 25838: 19 [  625/ 1327], train_loss/perplexity = 4.26657200/71.2768784 secs/batch = 0.2012s, grad.norm=12.35167408
 25843: 19 [  630/ 1327], train_loss/perplexity = 4.30321312/73.9369812 secs/batch = 0.1991s, grad.norm=12.92534161
 25848: 19 [  635/ 1327], train_loss/perplexity = 3.94203019/51.5230980 secs/batch = 0.1996s, grad.norm=12.75983047
 25853: 19 [  640/ 1327], train_loss/perplexity = 4.06146097/58.0590706 secs/batch = 0.1993s, grad.norm=12.33915234
 25858: 19 [  645/ 1327], train_loss/perplexity = 4.40110540/81.5409546 secs/batch = 0.2007s, grad.norm=12.68742752
 25863: 19 [  650/ 1327], train_loss/perplexity = 3.72463059/41.4559174 secs/batch = 0.1997s, grad.norm=13.35625553
 25868: 19 [  655/ 1327], train_loss/perplexity = 3.95637226/52.2673683 secs/batch = 0.2007s, grad.norm=12.92345047
 25873: 19 [  660/ 1327], train_loss/perplexity = 3.84920859/46.9558868 secs/batch = 0.2004s, grad.norm=12.91220570
 25878: 19 [  665/ 1327], train_loss/perplexity = 3.95745206/52.3238373 secs/batch = 0.1932s, grad.norm=12.82374668
 25883: 19 [  670/ 1327], train_loss/perplexity = 3.93447280/51.1351852 secs/batch = 0.1997s, grad.norm=12.74886990
 25888: 19 [  675/ 1327], train_loss/perplexity = 3.78814912/44.1745644 secs/batch = 0.1995s, grad.norm=12.68826866
 25893: 19 [  680/ 1327], train_loss/perplexity = 3.96117306/52.5188980 secs/batch = 0.2003s, grad.norm=12.76535606
 25898: 19 [  685/ 1327], train_loss/perplexity = 3.74064040/42.1249580 secs/batch = 0.2000s, grad.norm=11.77752876
 25903: 19 [  690/ 1327], train_loss/perplexity = 4.16356421/64.3002930 secs/batch = 0.1993s, grad.norm=12.52812099
 25908: 19 [  695/ 1327], train_loss/perplexity = 3.98702383/53.8942528 secs/batch = 0.1964s, grad.norm=13.39818001
 25913: 19 [  700/ 1327], train_loss/perplexity = 4.19958401/66.6585922 secs/batch = 0.1998s, grad.norm=12.76487446
 25918: 19 [  705/ 1327], train_loss/perplexity = 3.93489075/51.1565590 secs/batch = 0.1995s, grad.norm=12.10997581
 25923: 19 [  710/ 1327], train_loss/perplexity = 3.90110636/49.4571342 secs/batch = 0.1986s, grad.norm=12.33138657
 25928: 19 [  715/ 1327], train_loss/perplexity = 3.79291677/44.3856735 secs/batch = 0.1999s, grad.norm=12.30784035
 25933: 19 [  720/ 1327], train_loss/perplexity = 3.78453016/44.0149841 secs/batch = 0.1993s, grad.norm=12.61855125
 25938: 19 [  725/ 1327], train_loss/perplexity = 3.78858042/44.1936188 secs/batch = 0.1954s, grad.norm=12.63515282
 25943: 19 [  730/ 1327], train_loss/perplexity = 4.03287315/56.4227905 secs/batch = 0.1989s, grad.norm=12.52900982
 25948: 19 [  735/ 1327], train_loss/perplexity = 4.08540440/59.4659805 secs/batch = 0.2006s, grad.norm=13.39577007
 25953: 19 [  740/ 1327], train_loss/perplexity = 3.48663163/32.6756973 secs/batch = 0.1994s, grad.norm=11.68110943
 25958: 19 [  745/ 1327], train_loss/perplexity = 4.05958462/57.9502335 secs/batch = 0.2000s, grad.norm=12.68722630
 25963: 19 [  750/ 1327], train_loss/perplexity = 3.87375283/48.1226425 secs/batch = 0.1991s, grad.norm=12.29808617
 25968: 19 [  755/ 1327], train_loss/perplexity = 3.76969314/43.3667564 secs/batch = 0.1971s, grad.norm=12.22604084
 25973: 19 [  760/ 1327], train_loss/perplexity = 3.58442259/36.0325470 secs/batch = 0.1987s, grad.norm=11.53037167
 25978: 19 [  765/ 1327], train_loss/perplexity = 3.70898247/40.8122559 secs/batch = 0.1989s, grad.norm=11.88817787
 25983: 19 [  770/ 1327], train_loss/perplexity = 3.67543983/39.4660110 secs/batch = 0.1981s, grad.norm=12.51930141
 25988: 19 [  775/ 1327], train_loss/perplexity = 3.79753399/44.5910873 secs/batch = 0.1937s, grad.norm=12.84912491
 25993: 19 [  780/ 1327], train_loss/perplexity = 4.13280964/62.3528671 secs/batch = 0.1980s, grad.norm=13.03637314
 25998: 19 [  785/ 1327], train_loss/perplexity = 3.97178221/53.0790443 secs/batch = 0.1997s, grad.norm=12.89653492
 26003: 19 [  790/ 1327], train_loss/perplexity = 3.72685575/41.5482635 secs/batch = 0.1990s, grad.norm=12.69673634
 26008: 19 [  795/ 1327], train_loss/perplexity = 4.13435173/62.4490929 secs/batch = 0.1994s, grad.norm=12.77903652
 26013: 19 [  800/ 1327], train_loss/perplexity = 3.97565794/53.2851639 secs/batch = 0.2006s, grad.norm=13.50465298
 26018: 19 [  805/ 1327], train_loss/perplexity = 4.33367157/76.2236328 secs/batch = 0.1995s, grad.norm=12.94115925
 26023: 19 [  810/ 1327], train_loss/perplexity = 3.93826342/51.3293877 secs/batch = 0.1989s, grad.norm=11.57649517
 26028: 19 [  815/ 1327], train_loss/perplexity = 3.80796623/45.0587082 secs/batch = 0.1995s, grad.norm=11.95461559
 26033: 19 [  820/ 1327], train_loss/perplexity = 3.74362469/42.2508583 secs/batch = 0.1993s, grad.norm=11.77030182
 26038: 19 [  825/ 1327], train_loss/perplexity = 3.92159390/50.4808426 secs/batch = 0.1936s, grad.norm=12.52898788
 26043: 19 [  830/ 1327], train_loss/perplexity = 3.61441255/37.1295280 secs/batch = 0.1992s, grad.norm=12.56596565
 26048: 19 [  835/ 1327], train_loss/perplexity = 3.93258452/51.0387192 secs/batch = 0.2007s, grad.norm=12.68311214
 26053: 19 [  840/ 1327], train_loss/perplexity = 4.01058674/55.1792374 secs/batch = 0.1994s, grad.norm=12.84166145
 26058: 19 [  845/ 1327], train_loss/perplexity = 3.87581944/48.2221985 secs/batch = 0.2001s, grad.norm=12.67915535
 26063: 19 [  850/ 1327], train_loss/perplexity = 3.82153916/45.6744537 secs/batch = 0.1999s, grad.norm=12.22296810
 26068: 19 [  855/ 1327], train_loss/perplexity = 3.86525416/47.7153969 secs/batch = 0.1999s, grad.norm=12.66711044
 26073: 19 [  860/ 1327], train_loss/perplexity = 3.62248445/37.4304466 secs/batch = 0.1998s, grad.norm=11.89527225
 26078: 19 [  865/ 1327], train_loss/perplexity = 4.08529472/59.4594574 secs/batch = 0.1993s, grad.norm=12.54800701
 26083: 19 [  870/ 1327], train_loss/perplexity = 3.90756559/49.7776260 secs/batch = 0.1933s, grad.norm=12.44928074
 26088: 19 [  875/ 1327], train_loss/perplexity = 3.59090328/36.2668190 secs/batch = 0.1991s, grad.norm=12.13864613
 26093: 19 [  880/ 1327], train_loss/perplexity = 3.82157707/45.6761856 secs/batch = 0.1988s, grad.norm=12.13329983
 26098: 19 [  885/ 1327], train_loss/perplexity = 4.00444508/54.8413849 secs/batch = 0.1996s, grad.norm=11.97438240
 26103: 19 [  890/ 1327], train_loss/perplexity = 4.15021229/63.4474678 secs/batch = 0.1995s, grad.norm=12.36449909
 26108: 19 [  895/ 1327], train_loss/perplexity = 4.02495527/55.9778061 secs/batch = 0.2004s, grad.norm=11.72345829
 26113: 19 [  900/ 1327], train_loss/perplexity = 3.93816853/51.3245163 secs/batch = 0.1990s, grad.norm=11.62382126
 26118: 19 [  905/ 1327], train_loss/perplexity = 3.78601313/44.0803070 secs/batch = 0.1993s, grad.norm=11.54686832
 26123: 19 [  910/ 1327], train_loss/perplexity = 3.93062878/50.9389954 secs/batch = 0.1993s, grad.norm=11.86353111
 26128: 19 [  915/ 1327], train_loss/perplexity = 4.04059315/56.8600578 secs/batch = 0.1990s, grad.norm=12.52194214
 26133: 19 [  920/ 1327], train_loss/perplexity = 4.18764019/65.8671722 secs/batch = 0.2001s, grad.norm=12.60789776
 26138: 19 [  925/ 1327], train_loss/perplexity = 4.00822830/55.0492516 secs/batch = 0.2003s, grad.norm=12.42454433
 26143: 19 [  930/ 1327], train_loss/perplexity = 3.94396949/51.6231117 secs/batch = 0.1990s, grad.norm=11.93931007
 26148: 19 [  935/ 1327], train_loss/perplexity = 4.11755514/61.4089241 secs/batch = 0.1997s, grad.norm=11.98171329
 26153: 19 [  940/ 1327], train_loss/perplexity = 4.02812386/56.1554565 secs/batch = 0.1955s, grad.norm=12.02449226
 26158: 19 [  945/ 1327], train_loss/perplexity = 4.24758720/69.9364624 secs/batch = 0.1994s, grad.norm=12.06093597
 26163: 19 [  950/ 1327], train_loss/perplexity = 4.06675100/58.3670197 secs/batch = 0.1981s, grad.norm=12.70320702
 26168: 19 [  955/ 1327], train_loss/perplexity = 3.98224163/53.6371346 secs/batch = 0.1997s, grad.norm=12.55770683
 26173: 19 [  960/ 1327], train_loss/perplexity = 4.29936790/73.6532211 secs/batch = 0.1994s, grad.norm=12.89411545
 26178: 19 [  965/ 1327], train_loss/perplexity = 4.04489899/57.1054192 secs/batch = 0.1958s, grad.norm=12.51274490
 26183: 19 [  970/ 1327], train_loss/perplexity = 4.22623444/68.4589615 secs/batch = 0.1996s, grad.norm=12.29191399
 26188: 19 [  975/ 1327], train_loss/perplexity = 3.91651106/50.2249069 secs/batch = 0.1997s, grad.norm=13.25575542
 26193: 19 [  980/ 1327], train_loss/perplexity = 3.80239677/44.8084526 secs/batch = 0.1984s, grad.norm=12.23659134
 26198: 19 [  985/ 1327], train_loss/perplexity = 3.86555839/47.7299156 secs/batch = 0.1988s, grad.norm=12.57824707
 26203: 19 [  990/ 1327], train_loss/perplexity = 4.02604198/56.0386696 secs/batch = 0.2001s, grad.norm=12.87633610
 26208: 19 [  995/ 1327], train_loss/perplexity = 4.11560869/61.2895088 secs/batch = 0.1919s, grad.norm=12.57856560
 26213: 19 [ 1000/ 1327], train_loss/perplexity = 3.67401171/39.4096909 secs/batch = 0.1983s, grad.norm=11.96601868
 26218: 19 [ 1005/ 1327], train_loss/perplexity = 4.14983797/63.4237213 secs/batch = 0.1990s, grad.norm=12.39125824
 26223: 19 [ 1010/ 1327], train_loss/perplexity = 3.72425795/41.4404716 secs/batch = 0.1990s, grad.norm=12.18937969
 26228: 19 [ 1015/ 1327], train_loss/perplexity = 4.23845243/69.3005219 secs/batch = 0.1990s, grad.norm=12.44315434
 26233: 19 [ 1020/ 1327], train_loss/perplexity = 4.26043129/70.8405304 secs/batch = 0.1998s, grad.norm=12.25248337
 26238: 19 [ 1025/ 1327], train_loss/perplexity = 4.14703941/63.2464752 secs/batch = 0.2001s, grad.norm=12.43930626
 26243: 19 [ 1030/ 1327], train_loss/perplexity = 3.91872311/50.3361282 secs/batch = 0.2001s, grad.norm=12.42523479
 26248: 19 [ 1035/ 1327], train_loss/perplexity = 3.85312104/47.1399612 secs/batch = 0.2004s, grad.norm=12.58932972
 26253: 19 [ 1040/ 1327], train_loss/perplexity = 4.16146469/64.1654358 secs/batch = 0.1999s, grad.norm=13.01520348
 26258: 19 [ 1045/ 1327], train_loss/perplexity = 3.64619827/38.3286743 secs/batch = 0.1989s, grad.norm=11.67188263
 26263: 19 [ 1050/ 1327], train_loss/perplexity = 3.78749514/44.1456833 secs/batch = 0.1987s, grad.norm=11.91822052
 26268: 19 [ 1055/ 1327], train_loss/perplexity = 3.85677052/47.3123093 secs/batch = 0.1991s, grad.norm=12.51656532
 26273: 19 [ 1060/ 1327], train_loss/perplexity = 3.44684005/31.4010105 secs/batch = 0.2011s, grad.norm=12.51880550
 26278: 19 [ 1065/ 1327], train_loss/perplexity = 3.62895775/37.6735306 secs/batch = 0.1989s, grad.norm=12.83278084
 26283: 19 [ 1070/ 1327], train_loss/perplexity = 3.91523576/50.1608963 secs/batch = 0.1994s, grad.norm=12.58298397
 26288: 19 [ 1075/ 1327], train_loss/perplexity = 3.71041441/40.8707390 secs/batch = 0.1993s, grad.norm=12.28223896
 26293: 19 [ 1080/ 1327], train_loss/perplexity = 3.67097235/39.2900925 secs/batch = 0.1999s, grad.norm=12.36955547
 26298: 19 [ 1085/ 1327], train_loss/perplexity = 3.55201912/34.8836823 secs/batch = 0.1998s, grad.norm=12.69922543
 26303: 19 [ 1090/ 1327], train_loss/perplexity = 3.79193425/44.3420868 secs/batch = 0.1996s, grad.norm=12.58254814
 26308: 19 [ 1095/ 1327], train_loss/perplexity = 3.90822411/49.8104172 secs/batch = 0.1994s, grad.norm=13.24115372
 26313: 19 [ 1100/ 1327], train_loss/perplexity = 3.62508869/37.5280533 secs/batch = 0.2006s, grad.norm=13.54550457
 26318: 19 [ 1105/ 1327], train_loss/perplexity = 3.61512566/37.1560135 secs/batch = 0.2011s, grad.norm=12.57433319
 26323: 19 [ 1110/ 1327], train_loss/perplexity = 3.99388576/54.2653427 secs/batch = 0.2006s, grad.norm=12.92294979
 26328: 19 [ 1115/ 1327], train_loss/perplexity = 3.71437597/41.0329742 secs/batch = 0.1994s, grad.norm=11.71654701
 26333: 19 [ 1120/ 1327], train_loss/perplexity = 3.90001225/49.4030533 secs/batch = 0.2004s, grad.norm=12.07479668
 26338: 19 [ 1125/ 1327], train_loss/perplexity = 4.08916950/59.6903000 secs/batch = 0.2009s, grad.norm=13.06463051
 26343: 19 [ 1130/ 1327], train_loss/perplexity = 3.76507854/43.1670952 secs/batch = 0.1995s, grad.norm=12.56487274
 26348: 19 [ 1135/ 1327], train_loss/perplexity = 3.77186441/43.4610176 secs/batch = 0.1947s, grad.norm=12.38582039
 26353: 19 [ 1140/ 1327], train_loss/perplexity = 4.11283112/61.1195107 secs/batch = 0.2000s, grad.norm=13.05054665
 26358: 19 [ 1145/ 1327], train_loss/perplexity = 3.91567326/50.1828461 secs/batch = 0.1997s, grad.norm=12.34285355
 26363: 19 [ 1150/ 1327], train_loss/perplexity = 3.86554813/47.7294273 secs/batch = 0.1998s, grad.norm=12.23270226
 26368: 19 [ 1155/ 1327], train_loss/perplexity = 3.94847989/51.8564796 secs/batch = 0.1993s, grad.norm=12.93087006
 26373: 19 [ 1160/ 1327], train_loss/perplexity = 3.81092119/45.1920509 secs/batch = 0.1952s, grad.norm=12.30495167
 26378: 19 [ 1165/ 1327], train_loss/perplexity = 3.88293958/48.5667725 secs/batch = 0.1997s, grad.norm=12.63445759
 26383: 19 [ 1170/ 1327], train_loss/perplexity = 3.80638981/44.9877319 secs/batch = 0.2001s, grad.norm=12.49774647
 26388: 19 [ 1175/ 1327], train_loss/perplexity = 3.59254861/36.3265419 secs/batch = 0.2000s, grad.norm=12.22120857
 26393: 19 [ 1180/ 1327], train_loss/perplexity = 3.64015961/38.0979156 secs/batch = 0.2007s, grad.norm=12.48657990
 26398: 19 [ 1185/ 1327], train_loss/perplexity = 3.79565382/44.5073280 secs/batch = 0.1994s, grad.norm=12.68769646
 26403: 19 [ 1190/ 1327], train_loss/perplexity = 3.91124630/49.9611816 secs/batch = 0.1995s, grad.norm=12.88112545
 26408: 19 [ 1195/ 1327], train_loss/perplexity = 3.63071728/37.7398758 secs/batch = 0.1998s, grad.norm=11.80184555
 26413: 19 [ 1200/ 1327], train_loss/perplexity = 3.67322397/39.3786583 secs/batch = 0.1988s, grad.norm=12.11129761
 26418: 19 [ 1205/ 1327], train_loss/perplexity = 3.68729615/39.9367180 secs/batch = 0.1993s, grad.norm=12.86672115
 26423: 19 [ 1210/ 1327], train_loss/perplexity = 3.39194202/29.7236195 secs/batch = 0.2005s, grad.norm=12.14551163
 26428: 19 [ 1215/ 1327], train_loss/perplexity = 3.54900503/34.7786980 secs/batch = 0.1989s, grad.norm=12.18848038
 26433: 19 [ 1220/ 1327], train_loss/perplexity = 3.70200086/40.5283165 secs/batch = 0.1985s, grad.norm=12.56824589
 26438: 19 [ 1225/ 1327], train_loss/perplexity = 3.47165680/32.1900291 secs/batch = 0.1995s, grad.norm=12.60383129
 26443: 19 [ 1230/ 1327], train_loss/perplexity = 3.73995423/42.0960617 secs/batch = 0.1998s, grad.norm=12.01057816
 26448: 19 [ 1235/ 1327], train_loss/perplexity = 3.73890972/42.0521164 secs/batch = 0.1995s, grad.norm=12.31154346
 26453: 19 [ 1240/ 1327], train_loss/perplexity = 3.86034179/47.4815788 secs/batch = 0.1992s, grad.norm=14.30404663
 26458: 19 [ 1245/ 1327], train_loss/perplexity = 3.82811785/45.9759216 secs/batch = 0.1993s, grad.norm=11.96807480
 26463: 19 [ 1250/ 1327], train_loss/perplexity = 3.93948483/51.3921204 secs/batch = 0.1941s, grad.norm=12.09990788
 26468: 19 [ 1255/ 1327], train_loss/perplexity = 3.90786266/49.7924156 secs/batch = 0.2000s, grad.norm=12.61808586
 26473: 19 [ 1260/ 1327], train_loss/perplexity = 3.74752665/42.4160423 secs/batch = 0.2003s, grad.norm=12.85143566
 26478: 19 [ 1265/ 1327], train_loss/perplexity = 3.97457027/53.2272377 secs/batch = 0.1991s, grad.norm=13.18620777
 26483: 19 [ 1270/ 1327], train_loss/perplexity = 3.68845630/39.9830780 secs/batch = 0.2014s, grad.norm=12.60846233
 26488: 19 [ 1275/ 1327], train_loss/perplexity = 3.90070391/49.4372368 secs/batch = 0.1947s, grad.norm=12.77914047
 26493: 19 [ 1280/ 1327], train_loss/perplexity = 3.80280972/44.8269577 secs/batch = 0.1996s, grad.norm=12.51677704
 26498: 19 [ 1285/ 1327], train_loss/perplexity = 3.59202361/36.3074722 secs/batch = 0.1993s, grad.norm=12.42030907
 26503: 19 [ 1290/ 1327], train_loss/perplexity = 3.88880062/48.8522606 secs/batch = 0.1998s, grad.norm=12.17908859
 26508: 19 [ 1295/ 1327], train_loss/perplexity = 3.82326818/45.7534943 secs/batch = 0.1997s, grad.norm=12.50239182
 26513: 19 [ 1300/ 1327], train_loss/perplexity = 4.00561905/54.9058037 secs/batch = 0.2000s, grad.norm=11.91608047
 26518: 19 [ 1305/ 1327], train_loss/perplexity = 4.10572004/60.6864243 secs/batch = 0.1996s, grad.norm=12.29152012
 26523: 19 [ 1310/ 1327], train_loss/perplexity = 4.37990904/79.8307724 secs/batch = 0.1994s, grad.norm=12.62555122
 26528: 19 [ 1315/ 1327], train_loss/perplexity = 4.15602875/63.8175812 secs/batch = 0.2003s, grad.norm=12.70033169
 26533: 19 [ 1320/ 1327], train_loss/perplexity = 4.12357330/61.7796059 secs/batch = 0.1989s, grad.norm=12.82027817
 26538: 19 [ 1325/ 1327], train_loss/perplexity = 4.05752659/57.8310928 secs/batch = 0.2008s, grad.norm=12.60690403
Epoch training time: 264.33331847190857
	> validation loss = 4.72442865, perplexity = 112.66610718
	> validation loss = 4.63419676, perplexity = 102.94519806
	> validation loss = 4.58640862, perplexity = 98.14133453
	> validation loss = 4.60979462, perplexity = 100.46351624
	> validation loss = 4.81139421, perplexity = 122.90284729
	> validation loss = 4.64282751, perplexity = 103.83753204
	> validation loss = 4.68625736, perplexity = 108.44654083
	> validation loss = 4.47434425, perplexity = 87.73704529
	> validation loss = 4.29489279, perplexity = 73.32435608
	> validation loss = 4.38288784, perplexity = 80.06892395
	> validation loss = 4.54131317, perplexity = 93.81391144
	> validation loss = 4.61228609, perplexity = 100.71412659
	> validation loss = 4.53529596, perplexity = 93.25111389
	> validation loss = 4.32645178, perplexity = 75.67530060
	> validation loss = 4.25875568, perplexity = 70.72193146
	> validation loss = 4.25473738, perplexity = 70.43831635
	> validation loss = 4.72390938, perplexity = 112.60762024
	> validation loss = 4.24141502, perplexity = 69.50613403
	> validation loss = 4.75854349, perplexity = 116.57601166
	> validation loss = 4.61502171, perplexity = 100.99002075
	> validation loss = 4.41967535, perplexity = 83.06931305
at the end of epoch: 19
train loss = 3.98424416, perplexity = 53.74465200
validation loss = 4.53685440, perplexity = 93.39654906
Saved model cv/epoch019_4.5369.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.03125
new learning rate is: 0.015625
 26545: 20 [    5/ 1327], train_loss/perplexity = 4.19729090/66.5059128 secs/batch = 0.1989s, grad.norm=12.37465191
 26550: 20 [   10/ 1327], train_loss/perplexity = 3.80614686/44.9768028 secs/batch = 0.2003s, grad.norm=12.21849918
 26555: 20 [   15/ 1327], train_loss/perplexity = 4.17140961/64.8067398 secs/batch = 0.1996s, grad.norm=12.27249622
 26560: 20 [   20/ 1327], train_loss/perplexity = 4.32161236/75.3099594 secs/batch = 0.2016s, grad.norm=12.01585293
 26565: 20 [   25/ 1327], train_loss/perplexity = 4.17051458/64.7487640 secs/batch = 0.1994s, grad.norm=13.18475723
 26570: 20 [   30/ 1327], train_loss/perplexity = 4.21519279/67.7072220 secs/batch = 0.1994s, grad.norm=12.56971741
 26575: 20 [   35/ 1327], train_loss/perplexity = 4.00740814/55.0041237 secs/batch = 0.1995s, grad.norm=12.51766968
 26580: 20 [   40/ 1327], train_loss/perplexity = 3.94472027/51.6618843 secs/batch = 0.1998s, grad.norm=12.54903698
 26585: 20 [   45/ 1327], train_loss/perplexity = 3.73623800/41.9399147 secs/batch = 0.2012s, grad.norm=11.86290264
 26590: 20 [   50/ 1327], train_loss/perplexity = 4.03084278/56.3083458 secs/batch = 0.1958s, grad.norm=12.70325375
 26595: 20 [   55/ 1327], train_loss/perplexity = 3.92396426/50.6006432 secs/batch = 0.1943s, grad.norm=12.67129898
 26600: 20 [   60/ 1327], train_loss/perplexity = 4.23690891/69.1936340 secs/batch = 0.1987s, grad.norm=13.34100437
 26605: 20 [   65/ 1327], train_loss/perplexity = 3.78046393/43.8363724 secs/batch = 0.1996s, grad.norm=12.12026978
 26610: 20 [   70/ 1327], train_loss/perplexity = 3.68157125/39.7087364 secs/batch = 0.1988s, grad.norm=12.32910442
 26615: 20 [   75/ 1327], train_loss/perplexity = 3.49339867/32.8975639 secs/batch = 0.2006s, grad.norm=11.66937256
 26620: 20 [   80/ 1327], train_loss/perplexity = 3.87021255/47.9525757 secs/batch = 0.1992s, grad.norm=12.43770504
 26625: 20 [   85/ 1327], train_loss/perplexity = 3.89633393/49.2216682 secs/batch = 0.1999s, grad.norm=12.37473488
 26630: 20 [   90/ 1327], train_loss/perplexity = 3.97493029/53.2464066 secs/batch = 0.1989s, grad.norm=12.85745525
 26635: 20 [   95/ 1327], train_loss/perplexity = 3.88170123/48.5066643 secs/batch = 0.2000s, grad.norm=12.66592598
 26640: 20 [  100/ 1327], train_loss/perplexity = 4.15140915/63.5234528 secs/batch = 0.2003s, grad.norm=12.94811249
 26645: 20 [  105/ 1327], train_loss/perplexity = 3.96893716/52.9282455 secs/batch = 0.2003s, grad.norm=13.60187626
 26650: 20 [  110/ 1327], train_loss/perplexity = 3.84398437/46.7112198 secs/batch = 0.2000s, grad.norm=12.48232651
 26655: 20 [  115/ 1327], train_loss/perplexity = 3.84271955/46.6521759 secs/batch = 0.1994s, grad.norm=13.07420254
 26660: 20 [  120/ 1327], train_loss/perplexity = 3.91346502/50.0721512 secs/batch = 0.1993s, grad.norm=12.90729046
 26665: 20 [  125/ 1327], train_loss/perplexity = 3.97450542/53.2237854 secs/batch = 0.1992s, grad.norm=13.32259750
 26670: 20 [  130/ 1327], train_loss/perplexity = 3.95452785/52.1710548 secs/batch = 0.2000s, grad.norm=13.47101021
 26675: 20 [  135/ 1327], train_loss/perplexity = 3.93333888/51.0772324 secs/batch = 0.1995s, grad.norm=13.25063705
 26680: 20 [  140/ 1327], train_loss/perplexity = 4.23163748/68.8298492 secs/batch = 0.1992s, grad.norm=14.29021835
 26685: 20 [  145/ 1327], train_loss/perplexity = 4.05492353/57.6807518 secs/batch = 0.1996s, grad.norm=13.33369541
 26690: 20 [  150/ 1327], train_loss/perplexity = 4.15491152/63.7463226 secs/batch = 0.2004s, grad.norm=13.19280434
 26695: 20 [  155/ 1327], train_loss/perplexity = 4.35975885/78.2382660 secs/batch = 0.1991s, grad.norm=12.75012779
 26700: 20 [  160/ 1327], train_loss/perplexity = 4.04314566/57.0053787 secs/batch = 0.2004s, grad.norm=12.36594582
 26705: 20 [  165/ 1327], train_loss/perplexity = 4.16662836/64.4976196 secs/batch = 0.1994s, grad.norm=12.75043201
 26710: 20 [  170/ 1327], train_loss/perplexity = 3.91253352/50.0255318 secs/batch = 0.1994s, grad.norm=12.35042095
 26715: 20 [  175/ 1327], train_loss/perplexity = 4.23926258/69.3566895 secs/batch = 0.2000s, grad.norm=13.42868710
 26720: 20 [  180/ 1327], train_loss/perplexity = 4.04038048/56.8479691 secs/batch = 0.1996s, grad.norm=12.13248730
 26725: 20 [  185/ 1327], train_loss/perplexity = 4.38237333/80.0277405 secs/batch = 0.2001s, grad.norm=13.76163578
 26730: 20 [  190/ 1327], train_loss/perplexity = 3.96067929/52.4929733 secs/batch = 0.1996s, grad.norm=12.34752083
 26735: 20 [  195/ 1327], train_loss/perplexity = 4.21832561/67.9196625 secs/batch = 0.1984s, grad.norm=12.87224102
 26740: 20 [  200/ 1327], train_loss/perplexity = 4.09099388/59.7992973 secs/batch = 0.1990s, grad.norm=12.81221390
 26745: 20 [  205/ 1327], train_loss/perplexity = 4.30208588/73.8536835 secs/batch = 0.1998s, grad.norm=12.96369457
 26750: 20 [  210/ 1327], train_loss/perplexity = 4.10224104/60.4756660 secs/batch = 0.1996s, grad.norm=11.85880756
 26755: 20 [  215/ 1327], train_loss/perplexity = 4.21188021/67.4833069 secs/batch = 0.1936s, grad.norm=12.53904915
 26760: 20 [  220/ 1327], train_loss/perplexity = 4.19248486/66.1870499 secs/batch = 0.2006s, grad.norm=12.35616684
 26765: 20 [  225/ 1327], train_loss/perplexity = 4.35354233/77.7534027 secs/batch = 0.1996s, grad.norm=12.73581600
 26770: 20 [  230/ 1327], train_loss/perplexity = 4.19626284/66.4375763 secs/batch = 0.1997s, grad.norm=13.47621632
 26775: 20 [  235/ 1327], train_loss/perplexity = 4.05800819/57.8589516 secs/batch = 0.2003s, grad.norm=13.06256962
 26780: 20 [  240/ 1327], train_loss/perplexity = 3.86772156/47.8332748 secs/batch = 0.2000s, grad.norm=13.43557072
 26785: 20 [  245/ 1327], train_loss/perplexity = 4.14531469/63.1374893 secs/batch = 0.2001s, grad.norm=12.63512707
 26790: 20 [  250/ 1327], train_loss/perplexity = 3.92935705/50.8742561 secs/batch = 0.1997s, grad.norm=12.13081741
 26795: 20 [  255/ 1327], train_loss/perplexity = 3.99956441/54.5743713 secs/batch = 0.2004s, grad.norm=12.43045998
 26800: 20 [  260/ 1327], train_loss/perplexity = 4.16493082/64.3882294 secs/batch = 0.1997s, grad.norm=13.00565529
 26805: 20 [  265/ 1327], train_loss/perplexity = 4.31971025/75.1668472 secs/batch = 0.2004s, grad.norm=12.44122410
 26810: 20 [  270/ 1327], train_loss/perplexity = 4.44833469/85.4844666 secs/batch = 0.1999s, grad.norm=13.06974697
 26815: 20 [  275/ 1327], train_loss/perplexity = 4.35755014/78.0656509 secs/batch = 0.1990s, grad.norm=12.22647476
 26820: 20 [  280/ 1327], train_loss/perplexity = 4.11388016/61.1836586 secs/batch = 0.2005s, grad.norm=12.52715683
 26825: 20 [  285/ 1327], train_loss/perplexity = 4.47246933/87.5727005 secs/batch = 0.1994s, grad.norm=12.47043037
 26830: 20 [  290/ 1327], train_loss/perplexity = 4.14044285/62.8306389 secs/batch = 0.2001s, grad.norm=12.37864780
 26835: 20 [  295/ 1327], train_loss/perplexity = 3.90649724/49.7244720 secs/batch = 0.2000s, grad.norm=12.69883251
 26840: 20 [  300/ 1327], train_loss/perplexity = 3.54098105/34.5007515 secs/batch = 0.1925s, grad.norm=12.21141815
 26845: 20 [  305/ 1327], train_loss/perplexity = 3.93857026/51.3451385 secs/batch = 0.1998s, grad.norm=12.58797264
 26850: 20 [  310/ 1327], train_loss/perplexity = 3.99969625/54.5815697 secs/batch = 0.2002s, grad.norm=12.69006348
 26855: 20 [  315/ 1327], train_loss/perplexity = 3.47339106/32.2459068 secs/batch = 0.1995s, grad.norm=11.55384731
 26860: 20 [  320/ 1327], train_loss/perplexity = 3.50536728/33.2936707 secs/batch = 0.2001s, grad.norm=13.22631073
 26865: 20 [  325/ 1327], train_loss/perplexity = 3.51090050/33.4784012 secs/batch = 0.2013s, grad.norm=11.89967060
 26870: 20 [  330/ 1327], train_loss/perplexity = 4.07869387/59.0682678 secs/batch = 0.1992s, grad.norm=12.25272179
 26875: 20 [  335/ 1327], train_loss/perplexity = 3.51138330/33.4945679 secs/batch = 0.1955s, grad.norm=12.06652069
 26880: 20 [  340/ 1327], train_loss/perplexity = 4.24519539/69.7693939 secs/batch = 0.1998s, grad.norm=12.64326859
 26885: 20 [  345/ 1327], train_loss/perplexity = 4.10268784/60.5026894 secs/batch = 0.1994s, grad.norm=12.77204990
 26890: 20 [  350/ 1327], train_loss/perplexity = 4.02580595/56.0254440 secs/batch = 0.1937s, grad.norm=12.79322147
 26895: 20 [  355/ 1327], train_loss/perplexity = 4.05354261/57.6011543 secs/batch = 0.1990s, grad.norm=12.21008015
 26900: 20 [  360/ 1327], train_loss/perplexity = 4.18022442/65.3805237 secs/batch = 0.1999s, grad.norm=13.25751495
 26905: 20 [  365/ 1327], train_loss/perplexity = 4.20830107/67.2422028 secs/batch = 0.1986s, grad.norm=12.75689793
 26910: 20 [  370/ 1327], train_loss/perplexity = 4.21008778/67.3624496 secs/batch = 0.1985s, grad.norm=12.90766907
 26915: 20 [  375/ 1327], train_loss/perplexity = 3.64362645/38.2302246 secs/batch = 0.1999s, grad.norm=12.29790688
 26920: 20 [  380/ 1327], train_loss/perplexity = 3.73749614/41.9927139 secs/batch = 0.1989s, grad.norm=12.47187519
 26925: 20 [  385/ 1327], train_loss/perplexity = 3.91477442/50.1377602 secs/batch = 0.2001s, grad.norm=12.97167778
 26930: 20 [  390/ 1327], train_loss/perplexity = 4.05507994/57.6897736 secs/batch = 0.1995s, grad.norm=12.34671783
 26935: 20 [  395/ 1327], train_loss/perplexity = 4.11792088/61.4313850 secs/batch = 0.2003s, grad.norm=12.80816269
 26940: 20 [  400/ 1327], train_loss/perplexity = 4.06249237/58.1189842 secs/batch = 0.1998s, grad.norm=12.40258789
 26945: 20 [  405/ 1327], train_loss/perplexity = 4.26359367/71.0649109 secs/batch = 0.1993s, grad.norm=12.90537548
 26950: 20 [  410/ 1327], train_loss/perplexity = 3.95507240/52.1994743 secs/batch = 0.1992s, grad.norm=12.74928188
 26955: 20 [  415/ 1327], train_loss/perplexity = 3.89644766/49.2272644 secs/batch = 0.2004s, grad.norm=12.57393360
 26960: 20 [  420/ 1327], train_loss/perplexity = 3.60595155/36.8167000 secs/batch = 0.1999s, grad.norm=12.11136818
 26965: 20 [  425/ 1327], train_loss/perplexity = 3.85937667/47.4357758 secs/batch = 0.1995s, grad.norm=13.01992416
 26970: 20 [  430/ 1327], train_loss/perplexity = 4.11128712/61.0252151 secs/batch = 0.2004s, grad.norm=13.19232655
 26975: 20 [  435/ 1327], train_loss/perplexity = 4.08904314/59.6827545 secs/batch = 0.1919s, grad.norm=13.07104492
 26980: 20 [  440/ 1327], train_loss/perplexity = 3.69910383/40.4110718 secs/batch = 0.1997s, grad.norm=12.47711468
 26985: 20 [  445/ 1327], train_loss/perplexity = 4.03542519/56.5669670 secs/batch = 0.1992s, grad.norm=12.84860706
 26990: 20 [  450/ 1327], train_loss/perplexity = 3.94701052/51.7803383 secs/batch = 0.1983s, grad.norm=12.51012707
 26995: 20 [  455/ 1327], train_loss/perplexity = 3.95494151/52.1926422 secs/batch = 0.1990s, grad.norm=12.24767303
 27000: 20 [  460/ 1327], train_loss/perplexity = 3.90953159/49.8755836 secs/batch = 0.1999s, grad.norm=13.05147648
 27005: 20 [  465/ 1327], train_loss/perplexity = 3.62480378/37.5173607 secs/batch = 0.1980s, grad.norm=13.29978752
 27010: 20 [  470/ 1327], train_loss/perplexity = 4.37854958/79.7223206 secs/batch = 0.1996s, grad.norm=12.44049358
 27015: 20 [  475/ 1327], train_loss/perplexity = 3.77689433/43.6801758 secs/batch = 0.2004s, grad.norm=12.48995972
 27020: 20 [  480/ 1327], train_loss/perplexity = 3.90395546/49.5982437 secs/batch = 0.1953s, grad.norm=12.75203800
 27025: 20 [  485/ 1327], train_loss/perplexity = 3.93360162/51.0906563 secs/batch = 0.1998s, grad.norm=13.24462414
 27030: 20 [  490/ 1327], train_loss/perplexity = 3.83531570/46.3080444 secs/batch = 0.1996s, grad.norm=13.74432373
 27035: 20 [  495/ 1327], train_loss/perplexity = 3.87837696/48.3456841 secs/batch = 0.2004s, grad.norm=12.46470737
 27040: 20 [  500/ 1327], train_loss/perplexity = 4.05215406/57.5212288 secs/batch = 0.1992s, grad.norm=12.82449150
 27045: 20 [  505/ 1327], train_loss/perplexity = 4.17621708/65.1190491 secs/batch = 0.1923s, grad.norm=11.67770576
 27050: 20 [  510/ 1327], train_loss/perplexity = 4.44683838/85.3566513 secs/batch = 0.1989s, grad.norm=12.07372475
 27055: 20 [  515/ 1327], train_loss/perplexity = 4.09191084/59.8541527 secs/batch = 0.1993s, grad.norm=11.96608353
 27060: 20 [  520/ 1327], train_loss/perplexity = 4.27798605/72.0951004 secs/batch = 0.2001s, grad.norm=12.45197964
 27065: 20 [  525/ 1327], train_loss/perplexity = 3.86005330/47.4678802 secs/batch = 0.1953s, grad.norm=12.25879478
 27070: 20 [  530/ 1327], train_loss/perplexity = 3.88945484/48.8842278 secs/batch = 0.2009s, grad.norm=12.67734909
 27075: 20 [  535/ 1327], train_loss/perplexity = 4.07470798/58.8332977 secs/batch = 0.2003s, grad.norm=12.53465748
 27080: 20 [  540/ 1327], train_loss/perplexity = 4.07577276/58.8959770 secs/batch = 0.1980s, grad.norm=12.41464710
 27085: 20 [  545/ 1327], train_loss/perplexity = 4.06127596/58.0483322 secs/batch = 0.1998s, grad.norm=12.58704472
 27090: 20 [  550/ 1327], train_loss/perplexity = 4.02704000/56.0946236 secs/batch = 0.1951s, grad.norm=12.52417088
 27095: 20 [  555/ 1327], train_loss/perplexity = 3.98896265/53.9988441 secs/batch = 0.1998s, grad.norm=13.02415466
 27100: 20 [  560/ 1327], train_loss/perplexity = 3.97821212/53.4214363 secs/batch = 0.2002s, grad.norm=12.98201942
 27105: 20 [  565/ 1327], train_loss/perplexity = 3.87340593/48.1059532 secs/batch = 0.2003s, grad.norm=13.02048016
 27110: 20 [  570/ 1327], train_loss/perplexity = 3.92155147/50.4786987 secs/batch = 0.2013s, grad.norm=13.25788403
 27115: 20 [  575/ 1327], train_loss/perplexity = 3.60644460/36.8348579 secs/batch = 0.1990s, grad.norm=12.82453918
 27120: 20 [  580/ 1327], train_loss/perplexity = 4.12879753/62.1031990 secs/batch = 0.2010s, grad.norm=13.26140213
 27125: 20 [  585/ 1327], train_loss/perplexity = 3.73552036/41.9098282 secs/batch = 0.1943s, grad.norm=12.98706150
 27130: 20 [  590/ 1327], train_loss/perplexity = 4.15051508/63.4666824 secs/batch = 0.2001s, grad.norm=12.67349911
 27135: 20 [  595/ 1327], train_loss/perplexity = 3.95370841/52.1283226 secs/batch = 0.1997s, grad.norm=12.73554325
 27140: 20 [  600/ 1327], train_loss/perplexity = 4.15910864/64.0144348 secs/batch = 0.2000s, grad.norm=12.00680351
 27145: 20 [  605/ 1327], train_loss/perplexity = 4.09116602/59.8095894 secs/batch = 0.1992s, grad.norm=12.04525471
 27150: 20 [  610/ 1327], train_loss/perplexity = 4.31585169/74.8773651 secs/batch = 0.1997s, grad.norm=12.65674114
 27155: 20 [  615/ 1327], train_loss/perplexity = 3.89072371/48.9462967 secs/batch = 0.2001s, grad.norm=12.34688759
 27160: 20 [  620/ 1327], train_loss/perplexity = 4.25829697/70.6894913 secs/batch = 0.2005s, grad.norm=12.50584030
 27165: 20 [  625/ 1327], train_loss/perplexity = 4.23417997/69.0050659 secs/batch = 0.1987s, grad.norm=12.53337574
 27170: 20 [  630/ 1327], train_loss/perplexity = 4.27372599/71.7886200 secs/batch = 0.2002s, grad.norm=12.60732937
 27175: 20 [  635/ 1327], train_loss/perplexity = 3.94815850/51.8398170 secs/batch = 0.2000s, grad.norm=12.41102219
 27180: 20 [  640/ 1327], train_loss/perplexity = 4.06355858/58.1809845 secs/batch = 0.1998s, grad.norm=12.64826584
 27185: 20 [  645/ 1327], train_loss/perplexity = 4.29213667/73.1225433 secs/batch = 0.1984s, grad.norm=13.24263954
 27190: 20 [  650/ 1327], train_loss/perplexity = 3.76503992/43.1654282 secs/batch = 0.1996s, grad.norm=12.64310265
 27195: 20 [  655/ 1327], train_loss/perplexity = 3.83983326/46.5177193 secs/batch = 0.1994s, grad.norm=12.52179909
 27200: 20 [  660/ 1327], train_loss/perplexity = 3.81907248/45.5619278 secs/batch = 0.2003s, grad.norm=12.67671967
 27205: 20 [  665/ 1327], train_loss/perplexity = 3.92799258/50.8048897 secs/batch = 0.1986s, grad.norm=13.05554199
 27210: 20 [  670/ 1327], train_loss/perplexity = 3.98024750/53.5302811 secs/batch = 0.2003s, grad.norm=12.73334026
 27215: 20 [  675/ 1327], train_loss/perplexity = 3.78319716/43.9563522 secs/batch = 0.2000s, grad.norm=13.07295609
 27220: 20 [  680/ 1327], train_loss/perplexity = 3.90762043/49.7803535 secs/batch = 0.1992s, grad.norm=13.05446815
 27225: 20 [  685/ 1327], train_loss/perplexity = 3.74132347/42.1537437 secs/batch = 0.1995s, grad.norm=12.12410259
 27230: 20 [  690/ 1327], train_loss/perplexity = 4.16903400/64.6529694 secs/batch = 0.2009s, grad.norm=12.12372875
 27235: 20 [  695/ 1327], train_loss/perplexity = 3.98292494/53.6737976 secs/batch = 0.1992s, grad.norm=12.67443752
 27240: 20 [  700/ 1327], train_loss/perplexity = 4.21591997/67.7564697 secs/batch = 0.1994s, grad.norm=13.10474777
 27245: 20 [  705/ 1327], train_loss/perplexity = 3.87742519/48.2996941 secs/batch = 0.1993s, grad.norm=12.11025810
 27250: 20 [  710/ 1327], train_loss/perplexity = 3.88301396/48.5703850 secs/batch = 0.1972s, grad.norm=12.79194450
 27255: 20 [  715/ 1327], train_loss/perplexity = 3.81773353/45.5009651 secs/batch = 0.2001s, grad.norm=12.44336796
 27260: 20 [  720/ 1327], train_loss/perplexity = 3.75744605/42.8388786 secs/batch = 0.1994s, grad.norm=12.50133801
 27265: 20 [  725/ 1327], train_loss/perplexity = 3.84140062/46.5906830 secs/batch = 0.1999s, grad.norm=12.33854961
 27270: 20 [  730/ 1327], train_loss/perplexity = 4.01530743/55.4403381 secs/batch = 0.2003s, grad.norm=12.54278660
 27275: 20 [  735/ 1327], train_loss/perplexity = 4.09275341/59.9046059 secs/batch = 0.1992s, grad.norm=13.27653980
 27280: 20 [  740/ 1327], train_loss/perplexity = 3.50458908/33.2677689 secs/batch = 0.1990s, grad.norm=11.98013020
 27285: 20 [  745/ 1327], train_loss/perplexity = 4.06697130/58.3798790 secs/batch = 0.1992s, grad.norm=13.08653259
 27290: 20 [  750/ 1327], train_loss/perplexity = 3.85180998/47.0781975 secs/batch = 0.1993s, grad.norm=12.73274040
 27295: 20 [  755/ 1327], train_loss/perplexity = 3.69409037/40.2089806 secs/batch = 0.2004s, grad.norm=11.87938499
 27300: 20 [  760/ 1327], train_loss/perplexity = 3.59404516/36.3809471 secs/batch = 0.1993s, grad.norm=11.89868927
 27305: 20 [  765/ 1327], train_loss/perplexity = 3.69563103/40.2709770 secs/batch = 0.2004s, grad.norm=11.95658875
 27310: 20 [  770/ 1327], train_loss/perplexity = 3.67079830/39.2832527 secs/batch = 0.1992s, grad.norm=12.23699284
 27315: 20 [  775/ 1327], train_loss/perplexity = 3.76706219/43.2528076 secs/batch = 0.1962s, grad.norm=12.45653152
 27320: 20 [  780/ 1327], train_loss/perplexity = 4.08843708/59.6465950 secs/batch = 0.2002s, grad.norm=12.53617096
 27325: 20 [  785/ 1327], train_loss/perplexity = 4.02786636/56.1409988 secs/batch = 0.1997s, grad.norm=12.91236115
 27330: 20 [  790/ 1327], train_loss/perplexity = 3.75921488/42.9147186 secs/batch = 0.1977s, grad.norm=12.98924065
 27335: 20 [  795/ 1327], train_loss/perplexity = 4.11131668/61.0270195 secs/batch = 0.1987s, grad.norm=13.13597870
 27340: 20 [  800/ 1327], train_loss/perplexity = 4.00186491/54.7000656 secs/batch = 0.1992s, grad.norm=13.19800663
 27345: 20 [  805/ 1327], train_loss/perplexity = 4.32097864/75.2622452 secs/batch = 0.1993s, grad.norm=12.83671951
 27350: 20 [  810/ 1327], train_loss/perplexity = 3.94748712/51.8050232 secs/batch = 0.1989s, grad.norm=11.78041935
 27355: 20 [  815/ 1327], train_loss/perplexity = 3.81488848/45.3716965 secs/batch = 0.1991s, grad.norm=12.26857567
 27360: 20 [  820/ 1327], train_loss/perplexity = 3.75572014/42.7650070 secs/batch = 0.1987s, grad.norm=12.05543137
 27365: 20 [  825/ 1327], train_loss/perplexity = 3.88731003/48.7794952 secs/batch = 0.1952s, grad.norm=12.56827164
 27370: 20 [  830/ 1327], train_loss/perplexity = 3.63371062/37.8530159 secs/batch = 0.1977s, grad.norm=12.78380775
 27375: 20 [  835/ 1327], train_loss/perplexity = 3.91283369/50.0405502 secs/batch = 0.2004s, grad.norm=13.08227348
 27380: 20 [  840/ 1327], train_loss/perplexity = 4.01473284/55.4084892 secs/batch = 0.1987s, grad.norm=12.77262211
 27385: 20 [  845/ 1327], train_loss/perplexity = 3.86927986/47.9078751 secs/batch = 0.1994s, grad.norm=12.77873898
 27390: 20 [  850/ 1327], train_loss/perplexity = 3.86488008/47.6975517 secs/batch = 0.1986s, grad.norm=12.70252228
 27395: 20 [  855/ 1327], train_loss/perplexity = 3.89539433/49.1754417 secs/batch = 0.1995s, grad.norm=12.99826717
 27400: 20 [  860/ 1327], train_loss/perplexity = 3.62953520/37.6952934 secs/batch = 0.1999s, grad.norm=11.86409664
 27405: 20 [  865/ 1327], train_loss/perplexity = 4.09701872/60.1606636 secs/batch = 0.1986s, grad.norm=12.66746998
 27410: 20 [  870/ 1327], train_loss/perplexity = 3.93818951/51.3255920 secs/batch = 0.1984s, grad.norm=12.58214474
 27415: 20 [  875/ 1327], train_loss/perplexity = 3.61671400/37.2150764 secs/batch = 0.1982s, grad.norm=12.62510109
 27420: 20 [  880/ 1327], train_loss/perplexity = 3.80005360/44.7035789 secs/batch = 0.1992s, grad.norm=12.25689030
 27425: 20 [  885/ 1327], train_loss/perplexity = 3.96377730/52.6558495 secs/batch = 0.1998s, grad.norm=12.20659351
 27430: 20 [  890/ 1327], train_loss/perplexity = 4.09098387/59.7986984 secs/batch = 0.1989s, grad.norm=12.33310986
 27435: 20 [  895/ 1327], train_loss/perplexity = 4.07172918/58.6583061 secs/batch = 0.1988s, grad.norm=12.04886055
 27440: 20 [  900/ 1327], train_loss/perplexity = 3.97850370/53.4370155 secs/batch = 0.2007s, grad.norm=12.53074074
 27445: 20 [  905/ 1327], train_loss/perplexity = 3.81443024/45.3509102 secs/batch = 0.2000s, grad.norm=11.91212559
 27450: 20 [  910/ 1327], train_loss/perplexity = 3.91058159/49.9279823 secs/batch = 0.2011s, grad.norm=11.51286030
 27455: 20 [  915/ 1327], train_loss/perplexity = 4.03727436/56.6716652 secs/batch = 0.2002s, grad.norm=12.37268543
 27460: 20 [  920/ 1327], train_loss/perplexity = 4.23573589/69.1125183 secs/batch = 0.1932s, grad.norm=12.71675968
 27465: 20 [  925/ 1327], train_loss/perplexity = 3.98770332/53.9308853 secs/batch = 0.1992s, grad.norm=12.95910168
 27470: 20 [  930/ 1327], train_loss/perplexity = 3.96555042/52.7492943 secs/batch = 0.1977s, grad.norm=12.35657406
 27475: 20 [  935/ 1327], train_loss/perplexity = 4.14199400/62.9281769 secs/batch = 0.1928s, grad.norm=12.01516247
 27480: 20 [  940/ 1327], train_loss/perplexity = 4.04034090/56.8457184 secs/batch = 0.1995s, grad.norm=12.11884212
 27485: 20 [  945/ 1327], train_loss/perplexity = 4.25101757/70.1767883 secs/batch = 0.2002s, grad.norm=12.70613384
 27490: 20 [  950/ 1327], train_loss/perplexity = 4.02616215/56.0454025 secs/batch = 0.1982s, grad.norm=11.97813606
 27495: 20 [  955/ 1327], train_loss/perplexity = 3.98114967/53.5785980 secs/batch = 0.1980s, grad.norm=12.49666214
 27500: 20 [  960/ 1327], train_loss/perplexity = 4.27859211/72.1388016 secs/batch = 0.2007s, grad.norm=12.17174339
 27505: 20 [  965/ 1327], train_loss/perplexity = 3.98412585/53.7382927 secs/batch = 0.1991s, grad.norm=12.38524151
 27510: 20 [  970/ 1327], train_loss/perplexity = 4.21946526/67.9971161 secs/batch = 0.2008s, grad.norm=12.14400768
 27515: 20 [  975/ 1327], train_loss/perplexity = 3.99043989/54.0786743 secs/batch = 0.1980s, grad.norm=13.59801292
 27520: 20 [  980/ 1327], train_loss/perplexity = 3.78270245/43.9346123 secs/batch = 0.1985s, grad.norm=12.29836845
 27525: 20 [  985/ 1327], train_loss/perplexity = 3.84428787/46.7253990 secs/batch = 0.1995s, grad.norm=12.35277653
 27530: 20 [  990/ 1327], train_loss/perplexity = 4.08395147/59.3796425 secs/batch = 0.2000s, grad.norm=12.82238197
 27535: 20 [  995/ 1327], train_loss/perplexity = 4.11574125/61.2976341 secs/batch = 0.1995s, grad.norm=12.42223358
 27540: 20 [ 1000/ 1327], train_loss/perplexity = 3.67933464/39.6200256 secs/batch = 0.1982s, grad.norm=12.03837776
 27545: 20 [ 1005/ 1327], train_loss/perplexity = 4.10160637/60.4372940 secs/batch = 0.2001s, grad.norm=12.56610680
 27550: 20 [ 1010/ 1327], train_loss/perplexity = 3.71190238/40.9315987 secs/batch = 0.1951s, grad.norm=11.73858356
 27555: 20 [ 1015/ 1327], train_loss/perplexity = 4.18433857/65.6500626 secs/batch = 0.1999s, grad.norm=12.04936409
 27560: 20 [ 1020/ 1327], train_loss/perplexity = 4.25291920/70.3103638 secs/batch = 0.1995s, grad.norm=12.36232376
 27565: 20 [ 1025/ 1327], train_loss/perplexity = 4.14974594/63.4178848 secs/batch = 0.1989s, grad.norm=12.45162010
 27570: 20 [ 1030/ 1327], train_loss/perplexity = 3.88593340/48.7123909 secs/batch = 0.2002s, grad.norm=11.91443729
 27575: 20 [ 1035/ 1327], train_loss/perplexity = 3.84133720/46.5877304 secs/batch = 0.2000s, grad.norm=12.29749680
 27580: 20 [ 1040/ 1327], train_loss/perplexity = 4.21505880/67.6981430 secs/batch = 0.1999s, grad.norm=13.09185886
 27585: 20 [ 1045/ 1327], train_loss/perplexity = 3.66193867/38.9367561 secs/batch = 0.1996s, grad.norm=12.35302162
 27590: 20 [ 1050/ 1327], train_loss/perplexity = 3.73222065/41.7717667 secs/batch = 0.2003s, grad.norm=11.72003174
 27595: 20 [ 1055/ 1327], train_loss/perplexity = 3.87436104/48.1519203 secs/batch = 0.1949s, grad.norm=12.97209167
 27600: 20 [ 1060/ 1327], train_loss/perplexity = 3.45430374/31.6362534 secs/batch = 0.2002s, grad.norm=12.97541904
 27605: 20 [ 1065/ 1327], train_loss/perplexity = 3.59223604/36.3151894 secs/batch = 0.1994s, grad.norm=12.51577759
 27610: 20 [ 1070/ 1327], train_loss/perplexity = 3.87783432/48.3194580 secs/batch = 0.1993s, grad.norm=12.99142551
 27615: 20 [ 1075/ 1327], train_loss/perplexity = 3.66082406/38.8933792 secs/batch = 0.1996s, grad.norm=12.37731647
 27620: 20 [ 1080/ 1327], train_loss/perplexity = 3.71520281/41.0669136 secs/batch = 0.1989s, grad.norm=12.67125416
 27625: 20 [ 1085/ 1327], train_loss/perplexity = 3.57140589/35.5665588 secs/batch = 0.1950s, grad.norm=12.43712711
 27630: 20 [ 1090/ 1327], train_loss/perplexity = 3.78364778/43.9761658 secs/batch = 0.1950s, grad.norm=12.98447609
 27635: 20 [ 1095/ 1327], train_loss/perplexity = 3.92574644/50.6909027 secs/batch = 0.2002s, grad.norm=13.10790157
 27640: 20 [ 1100/ 1327], train_loss/perplexity = 3.62804699/37.6392365 secs/batch = 0.1952s, grad.norm=13.06541157
 27645: 20 [ 1105/ 1327], train_loss/perplexity = 3.57739401/35.7801781 secs/batch = 0.1999s, grad.norm=12.65565395
 27650: 20 [ 1110/ 1327], train_loss/perplexity = 3.90999818/49.8988609 secs/batch = 0.1980s, grad.norm=13.06971836
 27655: 20 [ 1115/ 1327], train_loss/perplexity = 3.71183181/40.9287109 secs/batch = 0.1996s, grad.norm=12.03899765
 27660: 20 [ 1120/ 1327], train_loss/perplexity = 3.89268303/49.0422935 secs/batch = 0.2006s, grad.norm=12.42080402
 27665: 20 [ 1125/ 1327], train_loss/perplexity = 4.18076372/65.4157944 secs/batch = 0.1996s, grad.norm=12.90202999
 27670: 20 [ 1130/ 1327], train_loss/perplexity = 3.78731585/44.1377678 secs/batch = 0.2018s, grad.norm=12.24979973
 27675: 20 [ 1135/ 1327], train_loss/perplexity = 3.76359606/43.1031494 secs/batch = 0.1989s, grad.norm=12.23670483
 27680: 20 [ 1140/ 1327], train_loss/perplexity = 4.10066795/60.3806038 secs/batch = 0.1947s, grad.norm=13.41075802
 27685: 20 [ 1145/ 1327], train_loss/perplexity = 3.93252945/51.0359077 secs/batch = 0.2003s, grad.norm=12.50334835
 27690: 20 [ 1150/ 1327], train_loss/perplexity = 3.82613564/45.8848801 secs/batch = 0.2001s, grad.norm=12.57723045
 27695: 20 [ 1155/ 1327], train_loss/perplexity = 3.92614269/50.7109909 secs/batch = 0.2006s, grad.norm=12.67068386
 27700: 20 [ 1160/ 1327], train_loss/perplexity = 3.93421364/51.1219330 secs/batch = 0.1987s, grad.norm=12.56755161
 27705: 20 [ 1165/ 1327], train_loss/perplexity = 3.90881324/49.8397675 secs/batch = 0.1999s, grad.norm=12.43766212
 27710: 20 [ 1170/ 1327], train_loss/perplexity = 3.79458523/44.4597931 secs/batch = 0.1988s, grad.norm=12.22078991
 27715: 20 [ 1175/ 1327], train_loss/perplexity = 3.58948040/36.2152519 secs/batch = 0.1994s, grad.norm=12.09738541
 27720: 20 [ 1180/ 1327], train_loss/perplexity = 3.61704159/37.2272720 secs/batch = 0.1998s, grad.norm=12.64928818
 27725: 20 [ 1185/ 1327], train_loss/perplexity = 3.75753593/42.8427277 secs/batch = 0.1996s, grad.norm=12.58735561
 27730: 20 [ 1190/ 1327], train_loss/perplexity = 3.88915586/48.8696175 secs/batch = 0.1991s, grad.norm=12.84837437
 27735: 20 [ 1195/ 1327], train_loss/perplexity = 3.67628741/39.4994774 secs/batch = 0.1993s, grad.norm=11.99703217
 27740: 20 [ 1200/ 1327], train_loss/perplexity = 3.68507600/39.8481522 secs/batch = 0.1992s, grad.norm=12.40489101
 27745: 20 [ 1205/ 1327], train_loss/perplexity = 3.74292254/42.2212029 secs/batch = 0.1995s, grad.norm=12.83956242
 27750: 20 [ 1210/ 1327], train_loss/perplexity = 3.33873272/28.1833878 secs/batch = 0.1930s, grad.norm=12.32466984
 27755: 20 [ 1215/ 1327], train_loss/perplexity = 3.48840117/32.7335701 secs/batch = 0.1987s, grad.norm=11.95972443
 27760: 20 [ 1220/ 1327], train_loss/perplexity = 3.72617841/41.5201302 secs/batch = 0.1992s, grad.norm=12.97119236
 27765: 20 [ 1225/ 1327], train_loss/perplexity = 3.40419793/30.0901508 secs/batch = 0.2001s, grad.norm=13.02591228
 27770: 20 [ 1230/ 1327], train_loss/perplexity = 3.71333313/40.9902039 secs/batch = 0.1988s, grad.norm=12.13603878
 27775: 20 [ 1235/ 1327], train_loss/perplexity = 3.72767687/41.5823936 secs/batch = 0.2000s, grad.norm=12.60549736
 27780: 20 [ 1240/ 1327], train_loss/perplexity = 3.88949847/48.8863640 secs/batch = 0.1989s, grad.norm=12.94940662
 27785: 20 [ 1245/ 1327], train_loss/perplexity = 3.80803061/45.0616074 secs/batch = 0.2010s, grad.norm=11.99661636
 27790: 20 [ 1250/ 1327], train_loss/perplexity = 3.91854811/50.3273201 secs/batch = 0.1989s, grad.norm=12.23604584
 27795: 20 [ 1255/ 1327], train_loss/perplexity = 3.95655203/52.2767677 secs/batch = 0.2010s, grad.norm=12.01023483
 27800: 20 [ 1260/ 1327], train_loss/perplexity = 3.74841309/42.4536591 secs/batch = 0.1994s, grad.norm=13.29549313
 27805: 20 [ 1265/ 1327], train_loss/perplexity = 3.98792648/53.9429207 secs/batch = 0.1997s, grad.norm=12.96074772
 27810: 20 [ 1270/ 1327], train_loss/perplexity = 3.64209914/38.1718826 secs/batch = 0.1999s, grad.norm=12.69259167
 27815: 20 [ 1275/ 1327], train_loss/perplexity = 3.80519867/44.9341774 secs/batch = 0.1954s, grad.norm=12.66645527
 27820: 20 [ 1280/ 1327], train_loss/perplexity = 3.69217706/40.1321220 secs/batch = 0.1999s, grad.norm=12.68459988
 27825: 20 [ 1285/ 1327], train_loss/perplexity = 3.61161399/37.0257645 secs/batch = 0.1992s, grad.norm=12.35787201
 27830: 20 [ 1290/ 1327], train_loss/perplexity = 3.90734768/49.7667809 secs/batch = 0.1989s, grad.norm=12.43485546
 27835: 20 [ 1295/ 1327], train_loss/perplexity = 3.82498169/45.8319626 secs/batch = 0.1989s, grad.norm=12.49235821
 27840: 20 [ 1300/ 1327], train_loss/perplexity = 3.98308492/53.6823845 secs/batch = 0.1994s, grad.norm=11.86633587
 27845: 20 [ 1305/ 1327], train_loss/perplexity = 4.05234814/57.5323906 secs/batch = 0.2001s, grad.norm=12.60986614
 27850: 20 [ 1310/ 1327], train_loss/perplexity = 4.32468510/75.5417175 secs/batch = 0.2004s, grad.norm=13.02580547
 27855: 20 [ 1315/ 1327], train_loss/perplexity = 4.10287952/60.5142899 secs/batch = 0.2004s, grad.norm=13.52727413
 27860: 20 [ 1320/ 1327], train_loss/perplexity = 4.16531277/64.4128265 secs/batch = 0.1987s, grad.norm=12.90017128
 27865: 20 [ 1325/ 1327], train_loss/perplexity = 4.03484583/56.5342026 secs/batch = 0.1995s, grad.norm=12.74032021
Epoch training time: 264.23686933517456
	> validation loss = 4.72057629, perplexity = 112.23291016
	> validation loss = 4.63280201, perplexity = 102.80171204
	> validation loss = 4.58306408, perplexity = 97.81364441
	> validation loss = 4.60659409, perplexity = 100.14249420
	> validation loss = 4.80902243, perplexity = 122.61169434
	> validation loss = 4.64076662, perplexity = 103.62375641
	> validation loss = 4.68634224, perplexity = 108.45574951
	> validation loss = 4.47270012, perplexity = 87.59291840
	> validation loss = 4.29435682, perplexity = 73.28506470
	> validation loss = 4.38244104, perplexity = 80.03315735
	> validation loss = 4.53946638, perplexity = 93.64081573
	> validation loss = 4.61200142, perplexity = 100.68546295
	> validation loss = 4.53157043, perplexity = 92.90435028
	> validation loss = 4.31989193, perplexity = 75.18050385
	> validation loss = 4.26194143, perplexity = 70.94758606
	> validation loss = 4.25292444, perplexity = 70.31072998
	> validation loss = 4.72424459, perplexity = 112.64537048
	> validation loss = 4.24176073, perplexity = 69.53016663
	> validation loss = 4.75296736, perplexity = 115.92777252
	> validation loss = 4.61304379, perplexity = 100.79046631
	> validation loss = 4.41665745, perplexity = 82.81899261
at the end of epoch: 20
train loss = 3.96760818, perplexity = 52.85795304
validation loss = 4.53459302, perplexity = 93.18558262
Saved model cv/epoch020_4.5346.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.015625
new learning rate is: 0.0078125
 27872: 21 [    5/ 1327], train_loss/perplexity = 4.24049091/69.4419327 secs/batch = 0.1995s, grad.norm=12.66658497
 27877: 21 [   10/ 1327], train_loss/perplexity = 3.81366014/45.3159981 secs/batch = 0.2002s, grad.norm=12.34683037
 27882: 21 [   15/ 1327], train_loss/perplexity = 4.13458824/62.4638672 secs/batch = 0.1990s, grad.norm=12.49577236
 27887: 21 [   20/ 1327], train_loss/perplexity = 4.28214169/72.3953247 secs/batch = 0.1961s, grad.norm=11.93414593
 27892: 21 [   25/ 1327], train_loss/perplexity = 4.10633755/60.7239113 secs/batch = 0.1931s, grad.norm=12.88989258
 27897: 21 [   30/ 1327], train_loss/perplexity = 4.22621822/68.4578476 secs/batch = 0.1985s, grad.norm=12.63105774
 27902: 21 [   35/ 1327], train_loss/perplexity = 4.00441551/54.8397636 secs/batch = 0.2002s, grad.norm=12.44060612
 27907: 21 [   40/ 1327], train_loss/perplexity = 3.98117065/53.5797195 secs/batch = 0.1988s, grad.norm=12.57866478
 27912: 21 [   45/ 1327], train_loss/perplexity = 3.75617933/42.7846489 secs/batch = 0.1984s, grad.norm=11.72971058
 27917: 21 [   50/ 1327], train_loss/perplexity = 4.02101326/55.7575760 secs/batch = 0.2000s, grad.norm=12.64463234
 27922: 21 [   55/ 1327], train_loss/perplexity = 3.95783329/52.3437881 secs/batch = 0.1920s, grad.norm=12.53432560
 27927: 21 [   60/ 1327], train_loss/perplexity = 4.28723907/72.7652893 secs/batch = 0.1989s, grad.norm=13.27036190
 27932: 21 [   65/ 1327], train_loss/perplexity = 3.79892421/44.6531219 secs/batch = 0.1982s, grad.norm=12.62485123
 27937: 21 [   70/ 1327], train_loss/perplexity = 3.62587285/37.5574913 secs/batch = 0.1987s, grad.norm=12.17939949
 27942: 21 [   75/ 1327], train_loss/perplexity = 3.48813820/32.7249641 secs/batch = 0.2003s, grad.norm=11.39692116
 27947: 21 [   80/ 1327], train_loss/perplexity = 3.90892029/49.8451042 secs/batch = 0.1996s, grad.norm=12.56535625
 27952: 21 [   85/ 1327], train_loss/perplexity = 3.92230988/50.5169983 secs/batch = 0.2001s, grad.norm=12.84024429
 27957: 21 [   90/ 1327], train_loss/perplexity = 3.95843554/52.3753242 secs/batch = 0.1995s, grad.norm=12.65281200
 27962: 21 [   95/ 1327], train_loss/perplexity = 3.86605406/47.7535820 secs/batch = 0.1989s, grad.norm=12.88550663
 27967: 21 [  100/ 1327], train_loss/perplexity = 4.11004162/60.9492531 secs/batch = 0.1997s, grad.norm=13.28999519
 27972: 21 [  105/ 1327], train_loss/perplexity = 3.96477818/52.7085762 secs/batch = 0.1994s, grad.norm=13.59602547
 27977: 21 [  110/ 1327], train_loss/perplexity = 3.85871458/47.4043770 secs/batch = 0.1999s, grad.norm=12.70883751
 27982: 21 [  115/ 1327], train_loss/perplexity = 3.84292960/46.6619759 secs/batch = 0.1998s, grad.norm=12.88037872
 27987: 21 [  120/ 1327], train_loss/perplexity = 3.88869619/48.8471565 secs/batch = 0.1993s, grad.norm=13.12387753
 27992: 21 [  125/ 1327], train_loss/perplexity = 4.01655531/55.5095634 secs/batch = 0.1986s, grad.norm=13.54033852
 27997: 21 [  130/ 1327], train_loss/perplexity = 3.92853999/50.8327065 secs/batch = 0.2004s, grad.norm=13.37613869
 28002: 21 [  135/ 1327], train_loss/perplexity = 3.91021824/49.9098434 secs/batch = 0.2000s, grad.norm=13.00142860
 28007: 21 [  140/ 1327], train_loss/perplexity = 4.21567822/67.7400894 secs/batch = 0.2006s, grad.norm=13.05769539
 28012: 21 [  145/ 1327], train_loss/perplexity = 4.03796244/56.7106743 secs/batch = 0.1996s, grad.norm=13.48143864
 28017: 21 [  150/ 1327], train_loss/perplexity = 4.09426260/59.9950829 secs/batch = 0.1983s, grad.norm=13.23762512
 28022: 21 [  155/ 1327], train_loss/perplexity = 4.33305550/76.1766891 secs/batch = 0.1995s, grad.norm=13.29545784
 28027: 21 [  160/ 1327], train_loss/perplexity = 4.00751448/55.0099716 secs/batch = 0.1992s, grad.norm=12.38265610
 28032: 21 [  165/ 1327], train_loss/perplexity = 4.16363430/64.3048019 secs/batch = 0.2004s, grad.norm=12.89685440
 28037: 21 [  170/ 1327], train_loss/perplexity = 4.00650787/54.9546280 secs/batch = 0.1994s, grad.norm=12.34388065
 28042: 21 [  175/ 1327], train_loss/perplexity = 4.27110481/71.6007004 secs/batch = 0.1997s, grad.norm=13.52107811
 28047: 21 [  180/ 1327], train_loss/perplexity = 4.06454659/58.2384949 secs/batch = 0.2001s, grad.norm=12.53209782
 28052: 21 [  185/ 1327], train_loss/perplexity = 4.38651562/80.3599243 secs/batch = 0.1988s, grad.norm=13.39565945
 28057: 21 [  190/ 1327], train_loss/perplexity = 3.99186707/54.1559067 secs/batch = 0.1990s, grad.norm=12.22472763
 28062: 21 [  195/ 1327], train_loss/perplexity = 4.20646906/67.1191254 secs/batch = 0.1997s, grad.norm=12.50928879
 28067: 21 [  200/ 1327], train_loss/perplexity = 4.05292654/57.5656776 secs/batch = 0.1996s, grad.norm=12.86119080
 28072: 21 [  205/ 1327], train_loss/perplexity = 4.31863403/75.0859909 secs/batch = 0.1993s, grad.norm=12.75349903
 28077: 21 [  210/ 1327], train_loss/perplexity = 4.16065693/64.1136246 secs/batch = 0.1995s, grad.norm=11.96567726
 28082: 21 [  215/ 1327], train_loss/perplexity = 4.25798178/70.6672134 secs/batch = 0.1988s, grad.norm=12.22417355
 28087: 21 [  220/ 1327], train_loss/perplexity = 4.16976070/64.6999664 secs/batch = 0.1933s, grad.norm=12.53913689
 28092: 21 [  225/ 1327], train_loss/perplexity = 4.40346575/81.7336502 secs/batch = 0.2002s, grad.norm=12.98403263
 28097: 21 [  230/ 1327], train_loss/perplexity = 4.24948168/70.0690842 secs/batch = 0.1993s, grad.norm=13.33221817
 28102: 21 [  235/ 1327], train_loss/perplexity = 4.08345509/59.3501778 secs/batch = 0.2000s, grad.norm=13.05752659
 28107: 21 [  240/ 1327], train_loss/perplexity = 3.85418439/47.1901131 secs/batch = 0.2012s, grad.norm=14.10662556
 28112: 21 [  245/ 1327], train_loss/perplexity = 4.10795403/60.8221512 secs/batch = 0.1991s, grad.norm=12.49966526
 28117: 21 [  250/ 1327], train_loss/perplexity = 3.94636559/51.7469559 secs/batch = 0.1996s, grad.norm=12.31519794
 28122: 21 [  255/ 1327], train_loss/perplexity = 3.98587489/53.8323669 secs/batch = 0.1989s, grad.norm=12.30235100
 28127: 21 [  260/ 1327], train_loss/perplexity = 4.13855982/62.7124405 secs/batch = 0.1992s, grad.norm=13.17202282
 28132: 21 [  265/ 1327], train_loss/perplexity = 4.31411982/74.7478027 secs/batch = 0.2003s, grad.norm=12.68169594
 28137: 21 [  270/ 1327], train_loss/perplexity = 4.47644711/87.9217377 secs/batch = 0.1994s, grad.norm=12.89324570
 28142: 21 [  275/ 1327], train_loss/perplexity = 4.32320595/75.4300690 secs/batch = 0.1987s, grad.norm=12.92629337
 28147: 21 [  280/ 1327], train_loss/perplexity = 4.17226410/64.8621368 secs/batch = 0.2000s, grad.norm=12.17648315
 28152: 21 [  285/ 1327], train_loss/perplexity = 4.43458080/84.3167725 secs/batch = 0.2010s, grad.norm=13.15191936
 28157: 21 [  290/ 1327], train_loss/perplexity = 4.20415115/66.9637299 secs/batch = 0.1994s, grad.norm=12.63282490
 28162: 21 [  295/ 1327], train_loss/perplexity = 3.89416790/49.1151695 secs/batch = 0.1975s, grad.norm=12.42839336
 28167: 21 [  300/ 1327], train_loss/perplexity = 3.50685525/33.3432465 secs/batch = 0.2010s, grad.norm=11.76649189
 28172: 21 [  305/ 1327], train_loss/perplexity = 3.95467901/52.1789436 secs/batch = 0.1994s, grad.norm=12.33408546
 28177: 21 [  310/ 1327], train_loss/perplexity = 4.02201462/55.8134346 secs/batch = 0.1989s, grad.norm=12.89585876
 28182: 21 [  315/ 1327], train_loss/perplexity = 3.44415092/31.3166828 secs/batch = 0.2000s, grad.norm=11.42807293
 28187: 21 [  320/ 1327], train_loss/perplexity = 3.47924590/32.4352531 secs/batch = 0.1939s, grad.norm=13.45979023
 28192: 21 [  325/ 1327], train_loss/perplexity = 3.47426581/32.2741241 secs/batch = 0.1987s, grad.norm=11.46311569
 28197: 21 [  330/ 1327], train_loss/perplexity = 4.05753756/57.8317299 secs/batch = 0.1970s, grad.norm=12.42842388
 28202: 21 [  335/ 1327], train_loss/perplexity = 3.52770162/34.0456276 secs/batch = 0.2004s, grad.norm=11.87258625
 28207: 21 [  340/ 1327], train_loss/perplexity = 4.22100067/68.1016006 secs/batch = 0.1997s, grad.norm=12.53305435
 28212: 21 [  345/ 1327], train_loss/perplexity = 4.01820421/55.6011696 secs/batch = 0.1991s, grad.norm=12.11409664
 28217: 21 [  350/ 1327], train_loss/perplexity = 4.02822828/56.1613197 secs/batch = 0.1990s, grad.norm=12.56278896
 28222: 21 [  355/ 1327], train_loss/perplexity = 4.01101542/55.2028961 secs/batch = 0.1994s, grad.norm=12.23404598
 28227: 21 [  360/ 1327], train_loss/perplexity = 4.11747599/61.4040604 secs/batch = 0.1992s, grad.norm=13.28607559
 28232: 21 [  365/ 1327], train_loss/perplexity = 4.16960478/64.6898804 secs/batch = 0.1930s, grad.norm=12.53125381
 28237: 21 [  370/ 1327], train_loss/perplexity = 4.21434021/67.6495132 secs/batch = 0.1992s, grad.norm=12.45709705
 28242: 21 [  375/ 1327], train_loss/perplexity = 3.62304664/37.4514961 secs/batch = 0.2000s, grad.norm=12.47322845
 28247: 21 [  380/ 1327], train_loss/perplexity = 3.68792510/39.9618454 secs/batch = 0.1992s, grad.norm=12.36216450
 28252: 21 [  385/ 1327], train_loss/perplexity = 3.94664097/51.7612076 secs/batch = 0.1995s, grad.norm=13.15136719
 28257: 21 [  390/ 1327], train_loss/perplexity = 4.01991844/55.6965637 secs/batch = 0.1998s, grad.norm=12.42383671
 28262: 21 [  395/ 1327], train_loss/perplexity = 4.06393862/58.2030983 secs/batch = 0.1993s, grad.norm=12.74240589
 28267: 21 [  400/ 1327], train_loss/perplexity = 4.09004211/59.7424088 secs/batch = 0.1989s, grad.norm=12.47588444
 28272: 21 [  405/ 1327], train_loss/perplexity = 4.26528549/71.1852417 secs/batch = 0.1994s, grad.norm=12.95178318
 28277: 21 [  410/ 1327], train_loss/perplexity = 3.97446489/53.2216301 secs/batch = 0.1976s, grad.norm=12.40957451
 28282: 21 [  415/ 1327], train_loss/perplexity = 3.94580054/51.7177238 secs/batch = 0.1989s, grad.norm=12.34819889
 28287: 21 [  420/ 1327], train_loss/perplexity = 3.57279944/35.6161575 secs/batch = 0.1996s, grad.norm=11.88381386
 28292: 21 [  425/ 1327], train_loss/perplexity = 3.89087963/48.9539299 secs/batch = 0.2001s, grad.norm=13.16946983
 28297: 21 [  430/ 1327], train_loss/perplexity = 4.09385681/59.9707413 secs/batch = 0.1990s, grad.norm=13.17374992
 28302: 21 [  435/ 1327], train_loss/perplexity = 4.06675196/58.3670769 secs/batch = 0.1944s, grad.norm=13.08679771
 28307: 21 [  440/ 1327], train_loss/perplexity = 3.66275263/38.9684601 secs/batch = 0.2004s, grad.norm=12.91413593
 28312: 21 [  445/ 1327], train_loss/perplexity = 4.03013420/56.2684631 secs/batch = 0.1996s, grad.norm=12.97469902
 28317: 21 [  450/ 1327], train_loss/perplexity = 3.92418242/50.6116829 secs/batch = 0.1964s, grad.norm=12.57341480
 28322: 21 [  455/ 1327], train_loss/perplexity = 3.97973156/53.5026703 secs/batch = 0.1988s, grad.norm=12.55141449
 28327: 21 [  460/ 1327], train_loss/perplexity = 3.94085312/51.4624863 secs/batch = 0.2000s, grad.norm=13.22305393
 28332: 21 [  465/ 1327], train_loss/perplexity = 3.62979937/37.7052498 secs/batch = 0.2000s, grad.norm=13.04849911
 28337: 21 [  470/ 1327], train_loss/perplexity = 4.39844990/81.3247070 secs/batch = 0.1976s, grad.norm=11.98775005
 28342: 21 [  475/ 1327], train_loss/perplexity = 3.78313994/43.9538383 secs/batch = 0.1993s, grad.norm=12.66876602
 28347: 21 [  480/ 1327], train_loss/perplexity = 3.91578865/50.1886368 secs/batch = 0.1998s, grad.norm=12.52127552
 28352: 21 [  485/ 1327], train_loss/perplexity = 3.92622375/50.7151031 secs/batch = 0.2007s, grad.norm=12.82794857
 28357: 21 [  490/ 1327], train_loss/perplexity = 3.78911400/44.2172050 secs/batch = 0.1988s, grad.norm=13.78705311
 28362: 21 [  495/ 1327], train_loss/perplexity = 3.84132767/46.5872841 secs/batch = 0.1998s, grad.norm=12.73144722
 28367: 21 [  500/ 1327], train_loss/perplexity = 4.02333927/55.8874168 secs/batch = 0.1954s, grad.norm=13.67255783
 28372: 21 [  505/ 1327], train_loss/perplexity = 4.09501028/60.0399551 secs/batch = 0.2010s, grad.norm=11.89216137
 28377: 21 [  510/ 1327], train_loss/perplexity = 4.42506981/83.5186386 secs/batch = 0.2005s, grad.norm=12.01596165
 28382: 21 [  515/ 1327], train_loss/perplexity = 4.22597790/68.4413986 secs/batch = 0.1991s, grad.norm=12.86311436
 28387: 21 [  520/ 1327], train_loss/perplexity = 4.20082903/66.7416382 secs/batch = 0.1992s, grad.norm=12.41442490
 28392: 21 [  525/ 1327], train_loss/perplexity = 3.86178732/47.5502625 secs/batch = 0.1996s, grad.norm=12.59301949
 28397: 21 [  530/ 1327], train_loss/perplexity = 3.91599751/50.1991196 secs/batch = 0.2002s, grad.norm=13.15996933
 28402: 21 [  535/ 1327], train_loss/perplexity = 4.03149223/56.3449287 secs/batch = 0.2010s, grad.norm=13.01207161
 28407: 21 [  540/ 1327], train_loss/perplexity = 4.14716721/63.2545586 secs/batch = 0.1995s, grad.norm=12.68324280
 28412: 21 [  545/ 1327], train_loss/perplexity = 4.06087351/58.0249748 secs/batch = 0.1995s, grad.norm=12.47772026
 28417: 21 [  550/ 1327], train_loss/perplexity = 4.09667635/60.1400719 secs/batch = 0.1940s, grad.norm=12.57925320
 28422: 21 [  555/ 1327], train_loss/perplexity = 3.96997786/52.9833565 secs/batch = 0.2009s, grad.norm=12.06195450
 28427: 21 [  560/ 1327], train_loss/perplexity = 4.00844479/55.0611725 secs/batch = 0.1998s, grad.norm=13.00256634
 28432: 21 [  565/ 1327], train_loss/perplexity = 3.84195518/46.6165276 secs/batch = 0.1991s, grad.norm=12.84452248
 28437: 21 [  570/ 1327], train_loss/perplexity = 3.87636876/48.2486954 secs/batch = 0.1997s, grad.norm=13.16644955
 28442: 21 [  575/ 1327], train_loss/perplexity = 3.56106901/35.2008057 secs/batch = 0.1997s, grad.norm=12.53741169
 28447: 21 [  580/ 1327], train_loss/perplexity = 4.15997458/64.0698929 secs/batch = 0.2010s, grad.norm=13.04335499
 28452: 21 [  585/ 1327], train_loss/perplexity = 3.65415335/38.6347961 secs/batch = 0.1994s, grad.norm=12.41594696
 28457: 21 [  590/ 1327], train_loss/perplexity = 4.10417747/60.5928841 secs/batch = 0.2001s, grad.norm=12.28214073
 28462: 21 [  595/ 1327], train_loss/perplexity = 3.99946737/54.5690765 secs/batch = 0.2004s, grad.norm=13.15858078
 28467: 21 [  600/ 1327], train_loss/perplexity = 4.11036015/60.9686699 secs/batch = 0.2003s, grad.norm=12.13420963
 28472: 21 [  605/ 1327], train_loss/perplexity = 4.08673906/59.5454025 secs/batch = 0.1993s, grad.norm=12.34608936
 28477: 21 [  610/ 1327], train_loss/perplexity = 4.26565933/71.2118530 secs/batch = 0.1993s, grad.norm=12.44621563
 28482: 21 [  615/ 1327], train_loss/perplexity = 3.96337533/52.6346855 secs/batch = 0.2002s, grad.norm=12.82625103
 28487: 21 [  620/ 1327], train_loss/perplexity = 4.25436115/70.4118195 secs/batch = 0.1955s, grad.norm=12.52289867
 28492: 21 [  625/ 1327], train_loss/perplexity = 4.19872904/66.6016312 secs/batch = 0.2006s, grad.norm=12.16837978
 28497: 21 [  630/ 1327], train_loss/perplexity = 4.32696152/75.7138824 secs/batch = 0.1998s, grad.norm=13.15353394
 28502: 21 [  635/ 1327], train_loss/perplexity = 4.04790258/57.2771950 secs/batch = 0.1994s, grad.norm=12.97587204
 28507: 21 [  640/ 1327], train_loss/perplexity = 4.05313063/57.5774269 secs/batch = 0.1990s, grad.norm=12.39530277
 28512: 21 [  645/ 1327], train_loss/perplexity = 4.32864857/75.8417206 secs/batch = 0.1993s, grad.norm=12.93012619
 28517: 21 [  650/ 1327], train_loss/perplexity = 3.80279112/44.8261261 secs/batch = 0.1989s, grad.norm=13.04513836
 28522: 21 [  655/ 1327], train_loss/perplexity = 3.88343763/48.5909653 secs/batch = 0.1997s, grad.norm=12.59349918
 28527: 21 [  660/ 1327], train_loss/perplexity = 3.81687260/45.4618073 secs/batch = 0.1992s, grad.norm=12.50196838
 28532: 21 [  665/ 1327], train_loss/perplexity = 3.94504595/51.6787109 secs/batch = 0.1993s, grad.norm=12.96521187
 28537: 21 [  670/ 1327], train_loss/perplexity = 3.89703417/49.2561455 secs/batch = 0.2001s, grad.norm=12.75637913
 28542: 21 [  675/ 1327], train_loss/perplexity = 3.73077440/41.7113953 secs/batch = 0.2009s, grad.norm=13.01118183
 28547: 21 [  680/ 1327], train_loss/perplexity = 3.94997644/51.9341431 secs/batch = 0.1983s, grad.norm=13.05443287
 28552: 21 [  685/ 1327], train_loss/perplexity = 3.73483944/41.8813019 secs/batch = 0.1994s, grad.norm=11.89379978
 28557: 21 [  690/ 1327], train_loss/perplexity = 4.15977097/64.0568466 secs/batch = 0.1989s, grad.norm=12.39149475
 28562: 21 [  695/ 1327], train_loss/perplexity = 3.99767447/54.4713287 secs/batch = 0.1950s, grad.norm=12.74623775
 28567: 21 [  700/ 1327], train_loss/perplexity = 4.17684603/65.1600189 secs/batch = 0.2010s, grad.norm=13.43383884
 28572: 21 [  705/ 1327], train_loss/perplexity = 3.91048360/49.9230881 secs/batch = 0.1999s, grad.norm=12.25496387
 28577: 21 [  710/ 1327], train_loss/perplexity = 3.87604260/48.2329597 secs/batch = 0.1931s, grad.norm=12.78091526
 28582: 21 [  715/ 1327], train_loss/perplexity = 3.74473524/42.2978058 secs/batch = 0.2007s, grad.norm=12.80226898
 28587: 21 [  720/ 1327], train_loss/perplexity = 3.81328917/45.2991905 secs/batch = 0.1991s, grad.norm=12.45547199
 28592: 21 [  725/ 1327], train_loss/perplexity = 3.75884151/42.8987007 secs/batch = 0.1994s, grad.norm=12.52080059
 28597: 21 [  730/ 1327], train_loss/perplexity = 4.01098919/55.2014465 secs/batch = 0.1944s, grad.norm=12.86885643
 28602: 21 [  735/ 1327], train_loss/perplexity = 4.00940037/55.1138115 secs/batch = 0.1985s, grad.norm=13.21418381
 28607: 21 [  740/ 1327], train_loss/perplexity = 3.46693587/32.0384216 secs/batch = 0.2013s, grad.norm=11.63717365
 28612: 21 [  745/ 1327], train_loss/perplexity = 4.07103205/58.6174278 secs/batch = 0.1996s, grad.norm=13.06988430
 28617: 21 [  750/ 1327], train_loss/perplexity = 3.86371946/47.6422272 secs/batch = 0.1994s, grad.norm=12.80615711
 28622: 21 [  755/ 1327], train_loss/perplexity = 3.71004987/40.8558426 secs/batch = 0.1985s, grad.norm=12.15763092
 28627: 21 [  760/ 1327], train_loss/perplexity = 3.59079981/36.2630692 secs/batch = 0.2000s, grad.norm=11.87093830
 28632: 21 [  765/ 1327], train_loss/perplexity = 3.72227049/41.3581924 secs/batch = 0.1997s, grad.norm=12.03826237
 28637: 21 [  770/ 1327], train_loss/perplexity = 3.72410989/41.4343338 secs/batch = 0.1934s, grad.norm=12.79231930
 28642: 21 [  775/ 1327], train_loss/perplexity = 3.77892232/43.7688484 secs/batch = 0.2005s, grad.norm=12.49402428
 28647: 21 [  780/ 1327], train_loss/perplexity = 4.07359934/58.7681084 secs/batch = 0.1999s, grad.norm=12.50144100
 28652: 21 [  785/ 1327], train_loss/perplexity = 3.98902941/54.0024490 secs/batch = 0.1999s, grad.norm=13.13028526
 28657: 21 [  790/ 1327], train_loss/perplexity = 3.70133352/40.5012779 secs/batch = 0.1994s, grad.norm=12.84759998
 28662: 21 [  795/ 1327], train_loss/perplexity = 4.11234713/61.0899353 secs/batch = 0.1949s, grad.norm=13.07463169
 28667: 21 [  800/ 1327], train_loss/perplexity = 4.00651598/54.9550705 secs/batch = 0.1999s, grad.norm=13.02028179
 28672: 21 [  805/ 1327], train_loss/perplexity = 4.38072729/79.8961182 secs/batch = 0.1991s, grad.norm=12.78528690
 28677: 21 [  810/ 1327], train_loss/perplexity = 3.91069221/49.9335060 secs/batch = 0.1988s, grad.norm=11.92777920
 28682: 21 [  815/ 1327], train_loss/perplexity = 3.75517416/42.7416649 secs/batch = 0.2001s, grad.norm=12.18982506
 28687: 21 [  820/ 1327], train_loss/perplexity = 3.67028236/39.2629890 secs/batch = 0.1989s, grad.norm=11.76693630
 28692: 21 [  825/ 1327], train_loss/perplexity = 3.88941050/48.8820610 secs/batch = 0.1958s, grad.norm=12.73869705
 28697: 21 [  830/ 1327], train_loss/perplexity = 3.61364460/37.1010246 secs/batch = 0.1997s, grad.norm=12.95987225
 28702: 21 [  835/ 1327], train_loss/perplexity = 3.90600324/49.6999168 secs/batch = 0.1937s, grad.norm=12.84689999
 28707: 21 [  840/ 1327], train_loss/perplexity = 4.02933741/56.2236443 secs/batch = 0.2011s, grad.norm=12.61823463
 28712: 21 [  845/ 1327], train_loss/perplexity = 3.87151670/48.0151558 secs/batch = 0.1999s, grad.norm=12.83693695
 28717: 21 [  850/ 1327], train_loss/perplexity = 3.83859348/46.4600830 secs/batch = 0.2006s, grad.norm=12.42425632
 28722: 21 [  855/ 1327], train_loss/perplexity = 3.88429642/48.6327133 secs/batch = 0.1979s, grad.norm=13.22228718
 28727: 21 [  860/ 1327], train_loss/perplexity = 3.60197639/36.6706390 secs/batch = 0.1998s, grad.norm=12.12536621
 28732: 21 [  865/ 1327], train_loss/perplexity = 4.09756374/60.1934624 secs/batch = 0.1998s, grad.norm=12.86235428
 28737: 21 [  870/ 1327], train_loss/perplexity = 3.88167882/48.5055771 secs/batch = 0.1951s, grad.norm=12.77951431
 28742: 21 [  875/ 1327], train_loss/perplexity = 3.61039925/36.9808159 secs/batch = 0.2001s, grad.norm=12.22209930
 28747: 21 [  880/ 1327], train_loss/perplexity = 3.80335283/44.8513107 secs/batch = 0.1930s, grad.norm=12.13904858
 28752: 21 [  885/ 1327], train_loss/perplexity = 3.99764252/54.4695892 secs/batch = 0.2002s, grad.norm=12.09908485
 28757: 21 [  890/ 1327], train_loss/perplexity = 4.19289255/66.2140427 secs/batch = 0.2004s, grad.norm=12.83379364
 28762: 21 [  895/ 1327], train_loss/perplexity = 4.09112835/59.8073387 secs/batch = 0.2010s, grad.norm=12.44084740
 28767: 21 [  900/ 1327], train_loss/perplexity = 3.88834667/48.8300858 secs/batch = 0.1995s, grad.norm=11.74528503
 28772: 21 [  905/ 1327], train_loss/perplexity = 3.77318478/43.5184402 secs/batch = 0.2012s, grad.norm=11.63918591
 28777: 21 [  910/ 1327], train_loss/perplexity = 3.86134243/47.5291138 secs/batch = 0.2006s, grad.norm=11.39744186
 28782: 21 [  915/ 1327], train_loss/perplexity = 4.03919458/56.7805939 secs/batch = 0.2001s, grad.norm=12.49645138
 28787: 21 [  920/ 1327], train_loss/perplexity = 4.24030876/69.4292831 secs/batch = 0.1989s, grad.norm=12.68050480
 28792: 21 [  925/ 1327], train_loss/perplexity = 4.04670238/57.2084923 secs/batch = 0.1981s, grad.norm=12.52299213
 28797: 21 [  930/ 1327], train_loss/perplexity = 4.01954365/55.6756935 secs/batch = 0.1987s, grad.norm=12.01087189
 28802: 21 [  935/ 1327], train_loss/perplexity = 4.10639906/60.7276459 secs/batch = 0.1944s, grad.norm=12.02017212
 28807: 21 [  940/ 1327], train_loss/perplexity = 4.06559992/58.2998734 secs/batch = 0.1992s, grad.norm=12.20084763
 28812: 21 [  945/ 1327], train_loss/perplexity = 4.20981789/67.3442764 secs/batch = 0.1993s, grad.norm=12.32345581
 28817: 21 [  950/ 1327], train_loss/perplexity = 4.00763702/55.0167122 secs/batch = 0.1995s, grad.norm=12.59105492
 28822: 21 [  955/ 1327], train_loss/perplexity = 3.92269611/50.5365143 secs/batch = 0.1979s, grad.norm=12.28040218
 28827: 21 [  960/ 1327], train_loss/perplexity = 4.29406643/73.2637863 secs/batch = 0.1989s, grad.norm=12.65150166
 28832: 21 [  965/ 1327], train_loss/perplexity = 3.96571970/52.7582245 secs/batch = 0.1985s, grad.norm=12.70752144
 28837: 21 [  970/ 1327], train_loss/perplexity = 4.28332329/72.4809189 secs/batch = 0.2008s, grad.norm=12.48808384
 28842: 21 [  975/ 1327], train_loss/perplexity = 3.92262411/50.5328751 secs/batch = 0.2001s, grad.norm=13.67552090
 28847: 21 [  980/ 1327], train_loss/perplexity = 3.76987553/43.3746643 secs/batch = 0.2003s, grad.norm=12.25849438
 28852: 21 [  985/ 1327], train_loss/perplexity = 3.87921667/48.3862991 secs/batch = 0.1999s, grad.norm=12.61568451
 28857: 21 [  990/ 1327], train_loss/perplexity = 4.01946640/55.6713905 secs/batch = 0.2004s, grad.norm=12.82292175
 28862: 21 [  995/ 1327], train_loss/perplexity = 4.13683748/62.6045189 secs/batch = 0.1995s, grad.norm=12.47733688
 28867: 21 [ 1000/ 1327], train_loss/perplexity = 3.65603709/38.7076454 secs/batch = 0.1959s, grad.norm=12.28605938
 28872: 21 [ 1005/ 1327], train_loss/perplexity = 4.12608528/61.9349899 secs/batch = 0.1997s, grad.norm=12.77634430
 28877: 21 [ 1010/ 1327], train_loss/perplexity = 3.70333481/40.5824127 secs/batch = 0.1959s, grad.norm=11.56185913
 28882: 21 [ 1015/ 1327], train_loss/perplexity = 4.24075937/69.4605789 secs/batch = 0.1933s, grad.norm=12.44357872
 28887: 21 [ 1020/ 1327], train_loss/perplexity = 4.23494291/69.0577393 secs/batch = 0.2002s, grad.norm=12.72365189
 28892: 21 [ 1025/ 1327], train_loss/perplexity = 4.15106678/63.5017052 secs/batch = 0.1987s, grad.norm=12.05116940
 28897: 21 [ 1030/ 1327], train_loss/perplexity = 3.90220666/49.5115852 secs/batch = 0.2009s, grad.norm=12.19004822
 28902: 21 [ 1035/ 1327], train_loss/perplexity = 3.86644650/47.7723236 secs/batch = 0.2014s, grad.norm=12.37102318
 28907: 21 [ 1040/ 1327], train_loss/perplexity = 4.11984253/61.5495491 secs/batch = 0.1992s, grad.norm=12.72297668
 28912: 21 [ 1045/ 1327], train_loss/perplexity = 3.60416746/36.7510757 secs/batch = 0.1988s, grad.norm=11.94594002
 28917: 21 [ 1050/ 1327], train_loss/perplexity = 3.75121784/42.5728989 secs/batch = 0.1997s, grad.norm=12.53366852
 28922: 21 [ 1055/ 1327], train_loss/perplexity = 3.82084250/45.6426468 secs/batch = 0.1995s, grad.norm=13.02153683
 28927: 21 [ 1060/ 1327], train_loss/perplexity = 3.43522644/31.0384407 secs/batch = 0.1998s, grad.norm=12.44094181
 28932: 21 [ 1065/ 1327], train_loss/perplexity = 3.61400700/37.1144714 secs/batch = 0.2004s, grad.norm=12.70278645
 28937: 21 [ 1070/ 1327], train_loss/perplexity = 3.90525413/49.6627007 secs/batch = 0.2000s, grad.norm=12.52299500
 28942: 21 [ 1075/ 1327], train_loss/perplexity = 3.64000940/38.0921936 secs/batch = 0.1995s, grad.norm=12.32010651
 28947: 21 [ 1080/ 1327], train_loss/perplexity = 3.65779543/38.7757645 secs/batch = 0.2006s, grad.norm=12.55279350
 28952: 21 [ 1085/ 1327], train_loss/perplexity = 3.58569884/36.0785637 secs/batch = 0.1992s, grad.norm=12.40758991
 28957: 21 [ 1090/ 1327], train_loss/perplexity = 3.79642439/44.5416374 secs/batch = 0.2002s, grad.norm=13.40593624
 28962: 21 [ 1095/ 1327], train_loss/perplexity = 3.91576958/50.1876793 secs/batch = 0.1991s, grad.norm=12.91416264
 28967: 21 [ 1100/ 1327], train_loss/perplexity = 3.59096217/36.2689552 secs/batch = 0.1935s, grad.norm=13.15689468
 28972: 21 [ 1105/ 1327], train_loss/perplexity = 3.59032989/36.2460327 secs/batch = 0.1997s, grad.norm=12.56748295
 28977: 21 [ 1110/ 1327], train_loss/perplexity = 3.93169022/50.9930954 secs/batch = 0.1999s, grad.norm=12.96856689
 28982: 21 [ 1115/ 1327], train_loss/perplexity = 3.75351000/42.6705933 secs/batch = 0.1994s, grad.norm=12.22557831
 28987: 21 [ 1120/ 1327], train_loss/perplexity = 3.93711567/51.2705078 secs/batch = 0.2011s, grad.norm=12.16871166
 28992: 21 [ 1125/ 1327], train_loss/perplexity = 4.13615608/62.5618744 secs/batch = 0.2002s, grad.norm=12.80356407
 28997: 21 [ 1130/ 1327], train_loss/perplexity = 3.80582666/44.9624023 secs/batch = 0.2004s, grad.norm=12.69520950
 29002: 21 [ 1135/ 1327], train_loss/perplexity = 3.76943254/43.3554573 secs/batch = 0.2000s, grad.norm=12.22007465
 29007: 21 [ 1140/ 1327], train_loss/perplexity = 4.07715797/58.9776154 secs/batch = 0.1995s, grad.norm=13.04224110
 29012: 21 [ 1145/ 1327], train_loss/perplexity = 3.87658644/48.2591972 secs/batch = 0.2003s, grad.norm=12.26108265
 29017: 21 [ 1150/ 1327], train_loss/perplexity = 3.80751467/45.0383644 secs/batch = 0.1993s, grad.norm=12.47028637
 29022: 21 [ 1155/ 1327], train_loss/perplexity = 3.93182468/50.9999504 secs/batch = 0.1990s, grad.norm=12.37844753
 29027: 21 [ 1160/ 1327], train_loss/perplexity = 3.83222246/46.1650238 secs/batch = 0.2001s, grad.norm=12.70516014
 29032: 21 [ 1165/ 1327], train_loss/perplexity = 3.88519502/48.6764336 secs/batch = 0.1997s, grad.norm=12.80004978
 29037: 21 [ 1170/ 1327], train_loss/perplexity = 3.75303078/42.6501503 secs/batch = 0.2002s, grad.norm=12.26735401
 29042: 21 [ 1175/ 1327], train_loss/perplexity = 3.61434102/37.1268730 secs/batch = 0.2003s, grad.norm=12.38657284
 29047: 21 [ 1180/ 1327], train_loss/perplexity = 3.64454484/38.2653503 secs/batch = 0.1994s, grad.norm=12.71794224
 29052: 21 [ 1185/ 1327], train_loss/perplexity = 3.79987955/44.6958008 secs/batch = 0.1999s, grad.norm=12.91063976
 29057: 21 [ 1190/ 1327], train_loss/perplexity = 3.81743598/45.4874268 secs/batch = 0.1998s, grad.norm=12.83575344
 29062: 21 [ 1195/ 1327], train_loss/perplexity = 3.70411706/40.6141701 secs/batch = 0.1988s, grad.norm=12.32924747
 29067: 21 [ 1200/ 1327], train_loss/perplexity = 3.61490417/37.1477852 secs/batch = 0.1998s, grad.norm=12.30501175
 29072: 21 [ 1205/ 1327], train_loss/perplexity = 3.66934919/39.2263680 secs/batch = 0.1999s, grad.norm=12.49080944
 29077: 21 [ 1210/ 1327], train_loss/perplexity = 3.33877850/28.1846771 secs/batch = 0.2007s, grad.norm=12.28696346
 29082: 21 [ 1215/ 1327], train_loss/perplexity = 3.52338290/33.8989105 secs/batch = 0.2011s, grad.norm=11.72106171
 29087: 21 [ 1220/ 1327], train_loss/perplexity = 3.73069215/41.7079659 secs/batch = 0.1997s, grad.norm=12.84433842
 29092: 21 [ 1225/ 1327], train_loss/perplexity = 3.45880008/31.7788219 secs/batch = 0.1947s, grad.norm=12.90055466
 29097: 21 [ 1230/ 1327], train_loss/perplexity = 3.69957924/40.4302902 secs/batch = 0.1992s, grad.norm=12.16696548
 29102: 21 [ 1235/ 1327], train_loss/perplexity = 3.72025251/41.2748146 secs/batch = 0.1995s, grad.norm=12.17228889
 29107: 21 [ 1240/ 1327], train_loss/perplexity = 3.89807558/49.3074684 secs/batch = 0.1989s, grad.norm=13.05618382
 29112: 21 [ 1245/ 1327], train_loss/perplexity = 3.79750323/44.5897141 secs/batch = 0.2001s, grad.norm=12.34719372
 29117: 21 [ 1250/ 1327], train_loss/perplexity = 3.89891052/49.3486557 secs/batch = 0.1996s, grad.norm=12.13545322
 29122: 21 [ 1255/ 1327], train_loss/perplexity = 3.96667576/52.8086891 secs/batch = 0.1994s, grad.norm=12.20554924
 29127: 21 [ 1260/ 1327], train_loss/perplexity = 3.73321390/41.8132744 secs/batch = 0.1998s, grad.norm=13.07176781
 29132: 21 [ 1265/ 1327], train_loss/perplexity = 3.99968147/54.5807610 secs/batch = 0.1954s, grad.norm=12.99238682
 29137: 21 [ 1270/ 1327], train_loss/perplexity = 3.71161079/40.9196663 secs/batch = 0.1974s, grad.norm=13.06696320
 29142: 21 [ 1275/ 1327], train_loss/perplexity = 3.81665969/45.4521294 secs/batch = 0.1997s, grad.norm=12.77472115
 29147: 21 [ 1280/ 1327], train_loss/perplexity = 3.72404265/41.4315491 secs/batch = 0.2005s, grad.norm=12.86396408
 29152: 21 [ 1285/ 1327], train_loss/perplexity = 3.61262894/37.0633621 secs/batch = 0.2010s, grad.norm=12.62786484
 29157: 21 [ 1290/ 1327], train_loss/perplexity = 3.84403968/46.7138023 secs/batch = 0.1941s, grad.norm=12.04354286
 29162: 21 [ 1295/ 1327], train_loss/perplexity = 3.86531448/47.7182770 secs/batch = 0.1995s, grad.norm=12.63401985
 29167: 21 [ 1300/ 1327], train_loss/perplexity = 4.02003765/55.7032013 secs/batch = 0.2001s, grad.norm=12.03428364
 29172: 21 [ 1305/ 1327], train_loss/perplexity = 4.06870890/58.4814072 secs/batch = 0.1992s, grad.norm=12.71080589
 29177: 21 [ 1310/ 1327], train_loss/perplexity = 4.26753473/71.3455353 secs/batch = 0.2002s, grad.norm=12.85876846
 29182: 21 [ 1315/ 1327], train_loss/perplexity = 4.10355949/60.5554504 secs/batch = 0.2000s, grad.norm=12.67074585
 29187: 21 [ 1320/ 1327], train_loss/perplexity = 4.11717176/61.3853836 secs/batch = 0.1947s, grad.norm=12.36161995
 29192: 21 [ 1325/ 1327], train_loss/perplexity = 4.01550627/55.4513626 secs/batch = 0.1991s, grad.norm=12.58041763
Epoch training time: 264.2424244880676
	> validation loss = 4.71840239, perplexity = 111.98919678
	> validation loss = 4.63184929, perplexity = 102.70381927
	> validation loss = 4.58140564, perplexity = 97.65155792
	> validation loss = 4.60542107, perplexity = 100.02509308
	> validation loss = 4.80761671, perplexity = 122.43946075
	> validation loss = 4.63977909, perplexity = 103.52147675
	> validation loss = 4.68528128, perplexity = 108.34074402
	> validation loss = 4.47186089, perplexity = 87.51943207
	> validation loss = 4.29445457, perplexity = 73.29222870
	> validation loss = 4.38021517, perplexity = 79.85521698
	> validation loss = 4.53824854, perplexity = 93.52684784
	> validation loss = 4.61266994, perplexity = 100.75279236
	> validation loss = 4.53060818, perplexity = 92.81499481
	> validation loss = 4.31828737, perplexity = 75.05996704
	> validation loss = 4.26226330, perplexity = 70.97042847
	> validation loss = 4.25218296, perplexity = 70.25861359
	> validation loss = 4.72312641, perplexity = 112.51948547
	> validation loss = 4.24186563, perplexity = 69.53746033
	> validation loss = 4.75148439, perplexity = 115.75598145
	> validation loss = 4.61285448, perplexity = 100.77139282
	> validation loss = 4.41665220, perplexity = 82.81855774
at the end of epoch: 21
train loss = 3.96851151, perplexity = 52.90572253
validation loss = 4.53342428, perplexity = 93.07673722
Saved model cv/epoch021_4.5334.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.0078125
new learning rate is: 0.00390625
 29199: 22 [    5/ 1327], train_loss/perplexity = 4.16922379/64.6652374 secs/batch = 0.1984s, grad.norm=12.44516087
 29204: 22 [   10/ 1327], train_loss/perplexity = 3.75053763/42.5439491 secs/batch = 0.1942s, grad.norm=12.30487442
 29209: 22 [   15/ 1327], train_loss/perplexity = 4.18811989/65.8987808 secs/batch = 0.1989s, grad.norm=12.27730751
 29214: 22 [   20/ 1327], train_loss/perplexity = 4.29227829/73.1328964 secs/batch = 0.2005s, grad.norm=12.11601353
 29219: 22 [   25/ 1327], train_loss/perplexity = 4.13648319/62.5823441 secs/batch = 0.2000s, grad.norm=12.99711609
 29224: 22 [   30/ 1327], train_loss/perplexity = 4.22086573/68.0924072 secs/batch = 0.1998s, grad.norm=12.55944157
 29229: 22 [   35/ 1327], train_loss/perplexity = 4.02378511/55.9123421 secs/batch = 0.1998s, grad.norm=12.53085327
 29234: 22 [   40/ 1327], train_loss/perplexity = 3.97068071/53.0206108 secs/batch = 0.1992s, grad.norm=13.13839054
 29239: 22 [   45/ 1327], train_loss/perplexity = 3.71733689/41.1546478 secs/batch = 0.2002s, grad.norm=11.83434677
 29244: 22 [   50/ 1327], train_loss/perplexity = 4.03573847/56.5846901 secs/batch = 0.1998s, grad.norm=13.06985664
 29249: 22 [   55/ 1327], train_loss/perplexity = 3.97190785/53.0857124 secs/batch = 0.1928s, grad.norm=12.55291367
 29254: 22 [   60/ 1327], train_loss/perplexity = 4.25895262/70.7358551 secs/batch = 0.1992s, grad.norm=12.97682190
 29259: 22 [   65/ 1327], train_loss/perplexity = 3.80141330/44.7644043 secs/batch = 0.2005s, grad.norm=12.42252827
 29264: 22 [   70/ 1327], train_loss/perplexity = 3.63455343/37.8849297 secs/batch = 0.1989s, grad.norm=12.07518387
 29269: 22 [   75/ 1327], train_loss/perplexity = 3.44941616/31.4820061 secs/batch = 0.1998s, grad.norm=12.11638069
 29274: 22 [   80/ 1327], train_loss/perplexity = 3.89341855/49.0783768 secs/batch = 0.1989s, grad.norm=12.56220627
 29279: 22 [   85/ 1327], train_loss/perplexity = 3.89570093/49.1905212 secs/batch = 0.1986s, grad.norm=12.59894753
 29284: 22 [   90/ 1327], train_loss/perplexity = 4.02921057/56.2165146 secs/batch = 0.1987s, grad.norm=13.39389420
 29289: 22 [   95/ 1327], train_loss/perplexity = 3.87807679/48.3311729 secs/batch = 0.1999s, grad.norm=12.47395325
 29294: 22 [  100/ 1327], train_loss/perplexity = 4.12427092/61.8227196 secs/batch = 0.1989s, grad.norm=13.12503242
 29299: 22 [  105/ 1327], train_loss/perplexity = 3.95065761/51.9695320 secs/batch = 0.1998s, grad.norm=13.31059837
 29304: 22 [  110/ 1327], train_loss/perplexity = 3.85149050/47.0631599 secs/batch = 0.1994s, grad.norm=12.87974548
 29309: 22 [  115/ 1327], train_loss/perplexity = 3.86418629/47.6644707 secs/batch = 0.1990s, grad.norm=13.10487556
 29314: 22 [  120/ 1327], train_loss/perplexity = 3.98528814/53.8007889 secs/batch = 0.1943s, grad.norm=13.24809074
 29319: 22 [  125/ 1327], train_loss/perplexity = 3.97820997/53.4213219 secs/batch = 0.1996s, grad.norm=13.37122917
 29324: 22 [  130/ 1327], train_loss/perplexity = 3.91434979/50.1164742 secs/batch = 0.2003s, grad.norm=13.37837124
 29329: 22 [  135/ 1327], train_loss/perplexity = 3.85176516/47.0760880 secs/batch = 0.1995s, grad.norm=12.69060516
 29334: 22 [  140/ 1327], train_loss/perplexity = 4.18094540/65.4276810 secs/batch = 0.2023s, grad.norm=13.05631924
 29339: 22 [  145/ 1327], train_loss/perplexity = 4.03909111/56.7747192 secs/batch = 0.2000s, grad.norm=13.43714523
 29344: 22 [  150/ 1327], train_loss/perplexity = 4.07787514/59.0199280 secs/batch = 0.1988s, grad.norm=13.09735775
 29349: 22 [  155/ 1327], train_loss/perplexity = 4.31378365/74.7226791 secs/batch = 0.2008s, grad.norm=13.19658756
 29354: 22 [  160/ 1327], train_loss/perplexity = 4.00008011/54.6025238 secs/batch = 0.1940s, grad.norm=12.37988663
 29359: 22 [  165/ 1327], train_loss/perplexity = 4.22076654/68.0856552 secs/batch = 0.2002s, grad.norm=13.26139450
 29364: 22 [  170/ 1327], train_loss/perplexity = 3.95596099/52.2458763 secs/batch = 0.1997s, grad.norm=12.57194233
 29369: 22 [  175/ 1327], train_loss/perplexity = 4.27671528/72.0035400 secs/batch = 0.1989s, grad.norm=13.38732910
 29374: 22 [  180/ 1327], train_loss/perplexity = 4.10331059/60.5403786 secs/batch = 0.1987s, grad.norm=12.93732166
 29379: 22 [  185/ 1327], train_loss/perplexity = 4.33266115/76.1466522 secs/batch = 0.1998s, grad.norm=13.11951160
 29384: 22 [  190/ 1327], train_loss/perplexity = 3.97648525/53.3292656 secs/batch = 0.1991s, grad.norm=12.68831348
 29389: 22 [  195/ 1327], train_loss/perplexity = 4.21486330/67.6849136 secs/batch = 0.1989s, grad.norm=12.34443474
 29394: 22 [  200/ 1327], train_loss/perplexity = 4.08175039/59.2490883 secs/batch = 0.2002s, grad.norm=12.92519665
 29399: 22 [  205/ 1327], train_loss/perplexity = 4.34991741/77.4720612 secs/batch = 0.1992s, grad.norm=12.85380459
 29404: 22 [  210/ 1327], train_loss/perplexity = 4.12110710/61.6274338 secs/batch = 0.1996s, grad.norm=12.49255753
 29409: 22 [  215/ 1327], train_loss/perplexity = 4.16420460/64.3414841 secs/batch = 0.2001s, grad.norm=12.28860092
 29414: 22 [  220/ 1327], train_loss/perplexity = 4.16910934/64.6578369 secs/batch = 0.2000s, grad.norm=12.32726574
 29419: 22 [  225/ 1327], train_loss/perplexity = 4.40379524/81.7605820 secs/batch = 0.1993s, grad.norm=13.13012981
 29424: 22 [  230/ 1327], train_loss/perplexity = 4.19615221/66.4302292 secs/batch = 0.2011s, grad.norm=13.72915649
 29429: 22 [  235/ 1327], train_loss/perplexity = 4.00437546/54.8375664 secs/batch = 0.1990s, grad.norm=12.67189884
 29434: 22 [  240/ 1327], train_loss/perplexity = 3.79303145/44.3907661 secs/batch = 0.1996s, grad.norm=12.67217350
 29439: 22 [  245/ 1327], train_loss/perplexity = 4.05582380/57.7327042 secs/batch = 0.1926s, grad.norm=12.51162148
 29444: 22 [  250/ 1327], train_loss/perplexity = 3.94166303/51.5041847 secs/batch = 0.1989s, grad.norm=12.32292938
 29449: 22 [  255/ 1327], train_loss/perplexity = 3.97787714/53.4035454 secs/batch = 0.2003s, grad.norm=12.25861645
 29454: 22 [  260/ 1327], train_loss/perplexity = 4.09926033/60.2956734 secs/batch = 0.2001s, grad.norm=13.23325062
 29459: 22 [  265/ 1327], train_loss/perplexity = 4.28793335/72.8158264 secs/batch = 0.2000s, grad.norm=12.76270008
 29464: 22 [  270/ 1327], train_loss/perplexity = 4.43672085/84.4974060 secs/batch = 0.1990s, grad.norm=13.20112133
 29469: 22 [  275/ 1327], train_loss/perplexity = 4.33680391/76.4627686 secs/batch = 0.1989s, grad.norm=12.79845428
 29474: 22 [  280/ 1327], train_loss/perplexity = 4.12126350/61.6370735 secs/batch = 0.1991s, grad.norm=12.41849995
 29479: 22 [  285/ 1327], train_loss/perplexity = 4.42517090/83.5270844 secs/batch = 0.1987s, grad.norm=12.67821407
 29484: 22 [  290/ 1327], train_loss/perplexity = 4.13126945/62.2569046 secs/batch = 0.1996s, grad.norm=12.97029781
 29489: 22 [  295/ 1327], train_loss/perplexity = 3.87757015/48.3066940 secs/batch = 0.1947s, grad.norm=12.16543007
 29494: 22 [  300/ 1327], train_loss/perplexity = 3.50452638/33.2656860 secs/batch = 0.1994s, grad.norm=11.82184505
 29499: 22 [  305/ 1327], train_loss/perplexity = 3.98518920/53.7954674 secs/batch = 0.1990s, grad.norm=12.65708923
 29504: 22 [  310/ 1327], train_loss/perplexity = 3.97202897/53.0921440 secs/batch = 0.1997s, grad.norm=13.12528229
 29509: 22 [  315/ 1327], train_loss/perplexity = 3.49165130/32.8401299 secs/batch = 0.1938s, grad.norm=11.62244511
 29514: 22 [  320/ 1327], train_loss/perplexity = 3.51448536/33.5986328 secs/batch = 0.1995s, grad.norm=13.04949951
 29519: 22 [  325/ 1327], train_loss/perplexity = 3.47190905/32.1981506 secs/batch = 0.1950s, grad.norm=11.70657444
 29524: 22 [  330/ 1327], train_loss/perplexity = 4.07081175/58.6045151 secs/batch = 0.2013s, grad.norm=12.67021465
 29529: 22 [  335/ 1327], train_loss/perplexity = 3.46353555/31.9296665 secs/batch = 0.1977s, grad.norm=11.82360554
 29534: 22 [  340/ 1327], train_loss/perplexity = 4.21082926/67.4124222 secs/batch = 0.1942s, grad.norm=12.68032074
 29539: 22 [  345/ 1327], train_loss/perplexity = 4.07213306/58.6820030 secs/batch = 0.2001s, grad.norm=12.37031937
 29544: 22 [  350/ 1327], train_loss/perplexity = 4.00473595/54.8573380 secs/batch = 0.2011s, grad.norm=12.72621346
 29549: 22 [  355/ 1327], train_loss/perplexity = 4.03394127/56.4830894 secs/batch = 0.2004s, grad.norm=12.80055618
 29554: 22 [  360/ 1327], train_loss/perplexity = 4.15955019/64.0427094 secs/batch = 0.1976s, grad.norm=13.85491848
 29559: 22 [  365/ 1327], train_loss/perplexity = 4.14191818/62.9234047 secs/batch = 0.1991s, grad.norm=12.61045647
 29564: 22 [  370/ 1327], train_loss/perplexity = 4.22750854/68.5462418 secs/batch = 0.1989s, grad.norm=12.81245613
 29569: 22 [  375/ 1327], train_loss/perplexity = 3.64926934/38.4465637 secs/batch = 0.1998s, grad.norm=12.31667709
 29574: 22 [  380/ 1327], train_loss/perplexity = 3.73628688/41.9419632 secs/batch = 0.2003s, grad.norm=12.47156715
 29579: 22 [  385/ 1327], train_loss/perplexity = 3.93402576/51.1123314 secs/batch = 0.2000s, grad.norm=13.19483566
 29584: 22 [  390/ 1327], train_loss/perplexity = 4.03758144/56.6890717 secs/batch = 0.1984s, grad.norm=12.61808681
 29589: 22 [  395/ 1327], train_loss/perplexity = 4.14444685/63.0827179 secs/batch = 0.2001s, grad.norm=13.12063313
 29594: 22 [  400/ 1327], train_loss/perplexity = 4.03409958/56.4920311 secs/batch = 0.1951s, grad.norm=12.38342762
 29599: 22 [  405/ 1327], train_loss/perplexity = 4.25774956/70.6508102 secs/batch = 0.1989s, grad.norm=12.65528393
 29604: 22 [  410/ 1327], train_loss/perplexity = 4.00522614/54.8842354 secs/batch = 0.2000s, grad.norm=12.55938530
 29609: 22 [  415/ 1327], train_loss/perplexity = 3.87426257/48.1471786 secs/batch = 0.1996s, grad.norm=12.38264847
 29614: 22 [  420/ 1327], train_loss/perplexity = 3.58054590/35.8931313 secs/batch = 0.1993s, grad.norm=11.97433567
 29619: 22 [  425/ 1327], train_loss/perplexity = 3.94596076/51.7260094 secs/batch = 0.1927s, grad.norm=13.36066532
 29624: 22 [  430/ 1327], train_loss/perplexity = 4.08342075/59.3481369 secs/batch = 0.1998s, grad.norm=13.70673084
 29629: 22 [  435/ 1327], train_loss/perplexity = 4.06667662/58.3626785 secs/batch = 0.1998s, grad.norm=13.13814068
 29634: 22 [  440/ 1327], train_loss/perplexity = 3.65740275/38.7605400 secs/batch = 0.1952s, grad.norm=12.41088200
 29639: 22 [  445/ 1327], train_loss/perplexity = 4.03002453/56.2622910 secs/batch = 0.1997s, grad.norm=13.17040730
 29644: 22 [  450/ 1327], train_loss/perplexity = 3.98951554/54.0287094 secs/batch = 0.1991s, grad.norm=12.58628941
 29649: 22 [  455/ 1327], train_loss/perplexity = 3.90610361/49.7049026 secs/batch = 0.1988s, grad.norm=12.32779121
 29654: 22 [  460/ 1327], train_loss/perplexity = 3.89832354/49.3196983 secs/batch = 0.1991s, grad.norm=13.11325645
 29659: 22 [  465/ 1327], train_loss/perplexity = 3.68253779/39.7471352 secs/batch = 0.1988s, grad.norm=13.35356903
 29664: 22 [  470/ 1327], train_loss/perplexity = 4.40702868/82.0253754 secs/batch = 0.1986s, grad.norm=12.26609421
 29669: 22 [  475/ 1327], train_loss/perplexity = 3.76890421/43.3325539 secs/batch = 0.1983s, grad.norm=12.31281185
 29674: 22 [  480/ 1327], train_loss/perplexity = 3.91006565/49.9022293 secs/batch = 0.1982s, grad.norm=12.65997219
 29679: 22 [  485/ 1327], train_loss/perplexity = 3.95619297/52.2579994 secs/batch = 0.1995s, grad.norm=13.07684422
 29684: 22 [  490/ 1327], train_loss/perplexity = 3.81200266/45.2409515 secs/batch = 0.1922s, grad.norm=13.54974651
 29689: 22 [  495/ 1327], train_loss/perplexity = 3.86907148/47.8978920 secs/batch = 0.1976s, grad.norm=12.45147800
 29694: 22 [  500/ 1327], train_loss/perplexity = 4.02037907/55.7222252 secs/batch = 0.1991s, grad.norm=12.97000504
 29699: 22 [  505/ 1327], train_loss/perplexity = 4.07936573/59.1079674 secs/batch = 0.1921s, grad.norm=11.72906494
 29704: 22 [  510/ 1327], train_loss/perplexity = 4.41308975/82.5240479 secs/batch = 0.1986s, grad.norm=12.04882717
 29709: 22 [  515/ 1327], train_loss/perplexity = 4.12084723/61.6114197 secs/batch = 0.1988s, grad.norm=12.45278072
 29714: 22 [  520/ 1327], train_loss/perplexity = 4.26875496/71.4326401 secs/batch = 0.1992s, grad.norm=12.41813087
 29719: 22 [  525/ 1327], train_loss/perplexity = 3.83851552/46.4564590 secs/batch = 0.1992s, grad.norm=12.31492138
 29724: 22 [  530/ 1327], train_loss/perplexity = 3.89737916/49.2731438 secs/batch = 0.1992s, grad.norm=12.95669270
 29729: 22 [  535/ 1327], train_loss/perplexity = 4.04241419/56.9636993 secs/batch = 0.1976s, grad.norm=12.67516232
 29734: 22 [  540/ 1327], train_loss/perplexity = 4.14399004/63.0539093 secs/batch = 0.1921s, grad.norm=12.62243271
 29739: 22 [  545/ 1327], train_loss/perplexity = 4.06318283/58.1591263 secs/batch = 0.1992s, grad.norm=12.78352833
 29744: 22 [  550/ 1327], train_loss/perplexity = 4.08401251/59.3832664 secs/batch = 0.1984s, grad.norm=12.57635307
 29749: 22 [  555/ 1327], train_loss/perplexity = 3.96591115/52.7683258 secs/batch = 0.1996s, grad.norm=12.24444771
 29754: 22 [  560/ 1327], train_loss/perplexity = 3.92342854/50.5735397 secs/batch = 0.1978s, grad.norm=13.04400253
 29759: 22 [  565/ 1327], train_loss/perplexity = 3.86978984/47.9323120 secs/batch = 0.1993s, grad.norm=12.70728302
 29764: 22 [  570/ 1327], train_loss/perplexity = 3.91013074/49.9054756 secs/batch = 0.1915s, grad.norm=13.35289192
 29769: 22 [  575/ 1327], train_loss/perplexity = 3.59767699/36.5133133 secs/batch = 0.1989s, grad.norm=12.86776733
 29774: 22 [  580/ 1327], train_loss/perplexity = 4.13675356/62.5992661 secs/batch = 0.1977s, grad.norm=13.11329269
 29779: 22 [  585/ 1327], train_loss/perplexity = 3.73996878/42.0966759 secs/batch = 0.1988s, grad.norm=13.00699520
 29784: 22 [  590/ 1327], train_loss/perplexity = 4.10695314/60.7613029 secs/batch = 0.1977s, grad.norm=12.40107632
 29789: 22 [  595/ 1327], train_loss/perplexity = 4.04738379/57.2474899 secs/batch = 0.1928s, grad.norm=13.20343876
 29794: 22 [  600/ 1327], train_loss/perplexity = 4.12892199/62.1109314 secs/batch = 0.1985s, grad.norm=12.20561028
 29799: 22 [  605/ 1327], train_loss/perplexity = 4.09604740/60.1022568 secs/batch = 0.1971s, grad.norm=12.31106186
 29804: 22 [  610/ 1327], train_loss/perplexity = 4.31033087/74.4651260 secs/batch = 0.1997s, grad.norm=12.62566185
 29809: 22 [  615/ 1327], train_loss/perplexity = 3.89367747/49.0910873 secs/batch = 0.1945s, grad.norm=12.32626438
 29814: 22 [  620/ 1327], train_loss/perplexity = 4.26535320/71.1900635 secs/batch = 0.1989s, grad.norm=12.71014977
 29819: 22 [  625/ 1327], train_loss/perplexity = 4.21894693/67.9618759 secs/batch = 0.2003s, grad.norm=12.24026108
 29824: 22 [  630/ 1327], train_loss/perplexity = 4.27979231/72.2254410 secs/batch = 0.1986s, grad.norm=12.47144890
 29829: 22 [  635/ 1327], train_loss/perplexity = 4.00034475/54.6169777 secs/batch = 0.1986s, grad.norm=12.53910065
 29834: 22 [  640/ 1327], train_loss/perplexity = 4.01778603/55.5779228 secs/batch = 0.1962s, grad.norm=12.53564835
 29839: 22 [  645/ 1327], train_loss/perplexity = 4.29234648/73.1378860 secs/batch = 0.1985s, grad.norm=13.43099499
 29844: 22 [  650/ 1327], train_loss/perplexity = 3.76393366/43.1177025 secs/batch = 0.1991s, grad.norm=12.70002937
 29849: 22 [  655/ 1327], train_loss/perplexity = 3.81835985/45.5294724 secs/batch = 0.1987s, grad.norm=12.85766029
 29854: 22 [  660/ 1327], train_loss/perplexity = 3.82883310/46.0088196 secs/batch = 0.1990s, grad.norm=12.93843365
 29859: 22 [  665/ 1327], train_loss/perplexity = 3.92536020/50.6713257 secs/batch = 0.1992s, grad.norm=14.16131783
 29864: 22 [  670/ 1327], train_loss/perplexity = 3.95923471/52.4171982 secs/batch = 0.1998s, grad.norm=13.07358742
 29869: 22 [  675/ 1327], train_loss/perplexity = 3.71989846/41.2602043 secs/batch = 0.1986s, grad.norm=12.67212582
 29874: 22 [  680/ 1327], train_loss/perplexity = 3.90162611/49.4828491 secs/batch = 0.1971s, grad.norm=12.79523373
 29879: 22 [  685/ 1327], train_loss/perplexity = 3.74168062/42.1688004 secs/batch = 0.1997s, grad.norm=11.99488831
 29884: 22 [  690/ 1327], train_loss/perplexity = 4.20664406/67.1308746 secs/batch = 0.1986s, grad.norm=12.49639130
 29889: 22 [  695/ 1327], train_loss/perplexity = 3.94763279/51.8125687 secs/batch = 0.1984s, grad.norm=12.81552696
 29894: 22 [  700/ 1327], train_loss/perplexity = 4.21028900/67.3760071 secs/batch = 0.1972s, grad.norm=13.15092659
 29899: 22 [  705/ 1327], train_loss/perplexity = 3.92742467/50.7760429 secs/batch = 0.1981s, grad.norm=12.29807949
 29904: 22 [  710/ 1327], train_loss/perplexity = 3.83097553/46.1074944 secs/batch = 0.1929s, grad.norm=12.27946854
 29909: 22 [  715/ 1327], train_loss/perplexity = 3.78638506/44.0967064 secs/batch = 0.1950s, grad.norm=12.32184696
 29914: 22 [  720/ 1327], train_loss/perplexity = 3.72842598/41.6135559 secs/batch = 0.1955s, grad.norm=12.61242294
 29919: 22 [  725/ 1327], train_loss/perplexity = 3.78961420/44.2393303 secs/batch = 0.1999s, grad.norm=12.59474277
 29924: 22 [  730/ 1327], train_loss/perplexity = 4.01144648/55.2266960 secs/batch = 0.1987s, grad.norm=12.55763340
 29929: 22 [  735/ 1327], train_loss/perplexity = 4.11766529/61.4156876 secs/batch = 0.1983s, grad.norm=13.07006359
 29934: 22 [  740/ 1327], train_loss/perplexity = 3.47010422/32.1400909 secs/batch = 0.1988s, grad.norm=11.71610832
 29939: 22 [  745/ 1327], train_loss/perplexity = 4.09782553/60.2092209 secs/batch = 0.1987s, grad.norm=13.02858829
 29944: 22 [  750/ 1327], train_loss/perplexity = 3.84244633/46.6394310 secs/batch = 0.1984s, grad.norm=12.43701267
 29949: 22 [  755/ 1327], train_loss/perplexity = 3.78523326/44.0459442 secs/batch = 0.1988s, grad.norm=12.28982067
 29954: 22 [  760/ 1327], train_loss/perplexity = 3.57361960/35.6453819 secs/batch = 0.1996s, grad.norm=12.10239315
 29959: 22 [  765/ 1327], train_loss/perplexity = 3.69321513/40.1738052 secs/batch = 0.1986s, grad.norm=12.05122471
 29964: 22 [  770/ 1327], train_loss/perplexity = 3.60479450/36.7741280 secs/batch = 0.1979s, grad.norm=11.91525269
 29969: 22 [  775/ 1327], train_loss/perplexity = 3.76038408/42.9649239 secs/batch = 0.1991s, grad.norm=12.87396812
 29974: 22 [  780/ 1327], train_loss/perplexity = 4.10240364/60.4854965 secs/batch = 0.1982s, grad.norm=12.54547787
 29979: 22 [  785/ 1327], train_loss/perplexity = 3.98664236/53.8736954 secs/batch = 0.1991s, grad.norm=13.27668476
 29984: 22 [  790/ 1327], train_loss/perplexity = 3.68260217/39.7496948 secs/batch = 0.1988s, grad.norm=12.96433926
 29989: 22 [  795/ 1327], train_loss/perplexity = 4.08831739/59.6394577 secs/batch = 0.1934s, grad.norm=13.14525223
 29994: 22 [  800/ 1327], train_loss/perplexity = 3.97687960/53.3502998 secs/batch = 0.1988s, grad.norm=12.62115383
 29999: 22 [  805/ 1327], train_loss/perplexity = 4.32515192/75.5769958 secs/batch = 0.1990s, grad.norm=13.08362389
 30004: 22 [  810/ 1327], train_loss/perplexity = 3.89791584/49.2995949 secs/batch = 0.1987s, grad.norm=12.04884911
 30009: 22 [  815/ 1327], train_loss/perplexity = 3.80247736/44.8120613 secs/batch = 0.1993s, grad.norm=12.45346355
 30014: 22 [  820/ 1327], train_loss/perplexity = 3.65784788/38.7777977 secs/batch = 0.2005s, grad.norm=11.97505760
 30019: 22 [  825/ 1327], train_loss/perplexity = 3.85257649/47.1142960 secs/batch = 0.1979s, grad.norm=12.47079468
 30024: 22 [  830/ 1327], train_loss/perplexity = 3.59275794/36.3341446 secs/batch = 0.1986s, grad.norm=12.70220470
 30029: 22 [  835/ 1327], train_loss/perplexity = 3.89108562/48.9640160 secs/batch = 0.1984s, grad.norm=12.98748207
 30034: 22 [  840/ 1327], train_loss/perplexity = 4.00562525/54.9061432 secs/batch = 0.1988s, grad.norm=12.80548859
 30039: 22 [  845/ 1327], train_loss/perplexity = 3.85911131/47.4231873 secs/batch = 0.1987s, grad.norm=13.31596565
 30044: 22 [  850/ 1327], train_loss/perplexity = 3.87540460/48.2021980 secs/batch = 0.1973s, grad.norm=12.18437576
 30049: 22 [  855/ 1327], train_loss/perplexity = 3.93528461/51.1767120 secs/batch = 0.1949s, grad.norm=13.15122032
 30054: 22 [  860/ 1327], train_loss/perplexity = 3.57553530/35.7137337 secs/batch = 0.1988s, grad.norm=11.85488796
 30059: 22 [  865/ 1327], train_loss/perplexity = 4.07597828/58.9080811 secs/batch = 0.1984s, grad.norm=12.80255795
 30064: 22 [  870/ 1327], train_loss/perplexity = 3.90999889/49.8988953 secs/batch = 0.1985s, grad.norm=12.58723927
 30069: 22 [  875/ 1327], train_loss/perplexity = 3.59957910/36.5828323 secs/batch = 0.1938s, grad.norm=12.43733883
 30074: 22 [  880/ 1327], train_loss/perplexity = 3.73635340/41.9447556 secs/batch = 0.1979s, grad.norm=12.17139339
 30079: 22 [  885/ 1327], train_loss/perplexity = 3.97991180/53.5123138 secs/batch = 0.1998s, grad.norm=12.12914371
 30084: 22 [  890/ 1327], train_loss/perplexity = 4.06565428/58.3030434 secs/batch = 0.1991s, grad.norm=12.35104465
 30089: 22 [  895/ 1327], train_loss/perplexity = 4.01594448/55.4756660 secs/batch = 0.1984s, grad.norm=12.32085800
 30094: 22 [  900/ 1327], train_loss/perplexity = 3.93889618/51.3618774 secs/batch = 0.1986s, grad.norm=11.99388504
 30099: 22 [  905/ 1327], train_loss/perplexity = 3.78859854/44.1944199 secs/batch = 0.1991s, grad.norm=12.13115597
 30104: 22 [  910/ 1327], train_loss/perplexity = 3.87851524/48.3523712 secs/batch = 0.1988s, grad.norm=11.65220737
 30109: 22 [  915/ 1327], train_loss/perplexity = 4.04342604/57.0213661 secs/batch = 0.1920s, grad.norm=12.39087772
 30114: 22 [  920/ 1327], train_loss/perplexity = 4.20265675/66.8637390 secs/batch = 0.1990s, grad.norm=12.87803745
 30119: 22 [  925/ 1327], train_loss/perplexity = 3.99717021/54.4438667 secs/batch = 0.1991s, grad.norm=12.53022861
 30124: 22 [  930/ 1327], train_loss/perplexity = 4.00972366/55.1316338 secs/batch = 0.1978s, grad.norm=12.57661533
 30129: 22 [  935/ 1327], train_loss/perplexity = 4.10049915/60.3704147 secs/batch = 0.1980s, grad.norm=12.22944450
 30134: 22 [  940/ 1327], train_loss/perplexity = 4.02919197/56.2154694 secs/batch = 0.1991s, grad.norm=11.97178364
 30139: 22 [  945/ 1327], train_loss/perplexity = 4.23646975/69.1632538 secs/batch = 0.1994s, grad.norm=12.39724636
 30144: 22 [  950/ 1327], train_loss/perplexity = 4.01335669/55.3322906 secs/batch = 0.1983s, grad.norm=12.58572960
 30149: 22 [  955/ 1327], train_loss/perplexity = 3.94483995/51.6680679 secs/batch = 0.1983s, grad.norm=12.61575699
 30154: 22 [  960/ 1327], train_loss/perplexity = 4.31719208/74.9777985 secs/batch = 0.1989s, grad.norm=12.45514774
 30159: 22 [  965/ 1327], train_loss/perplexity = 3.97367644/53.1796837 secs/batch = 0.1925s, grad.norm=12.52264214
 30164: 22 [  970/ 1327], train_loss/perplexity = 4.24833632/69.9888763 secs/batch = 0.1951s, grad.norm=12.34277058
 30169: 22 [  975/ 1327], train_loss/perplexity = 3.86251259/47.5847626 secs/batch = 0.1991s, grad.norm=13.44755554
 30174: 22 [  980/ 1327], train_loss/perplexity = 3.77743888/43.7039680 secs/batch = 0.1966s, grad.norm=12.56053066
 30179: 22 [  985/ 1327], train_loss/perplexity = 3.81086850/45.1896706 secs/batch = 0.1985s, grad.norm=12.50173187
 30184: 22 [  990/ 1327], train_loss/perplexity = 4.03966618/56.8073769 secs/batch = 0.1978s, grad.norm=12.95932198
 30189: 22 [  995/ 1327], train_loss/perplexity = 4.14764833/63.2849998 secs/batch = 0.1944s, grad.norm=12.90959167
 30194: 22 [ 1000/ 1327], train_loss/perplexity = 3.64758348/38.3818054 secs/batch = 0.1990s, grad.norm=12.03402138
 30199: 22 [ 1005/ 1327], train_loss/perplexity = 4.07637405/58.9314003 secs/batch = 0.1984s, grad.norm=12.65598488
 30204: 22 [ 1010/ 1327], train_loss/perplexity = 3.71432161/41.0307426 secs/batch = 0.1975s, grad.norm=11.87594795
 30209: 22 [ 1015/ 1327], train_loss/perplexity = 4.19827032/66.5710831 secs/batch = 0.1981s, grad.norm=12.21874142
 30214: 22 [ 1020/ 1327], train_loss/perplexity = 4.23616982/69.1425171 secs/batch = 0.1991s, grad.norm=12.49390888
 30219: 22 [ 1025/ 1327], train_loss/perplexity = 4.13368273/62.4073296 secs/batch = 0.1988s, grad.norm=12.51017380
 30224: 22 [ 1030/ 1327], train_loss/perplexity = 3.96222162/52.5739975 secs/batch = 0.1986s, grad.norm=12.30319977
 30229: 22 [ 1035/ 1327], train_loss/perplexity = 3.88357592/48.5976868 secs/batch = 0.1966s, grad.norm=12.20312500
 30234: 22 [ 1040/ 1327], train_loss/perplexity = 4.10221863/60.4743080 secs/batch = 0.1993s, grad.norm=13.04525757
 30239: 22 [ 1045/ 1327], train_loss/perplexity = 3.68162107/39.7107162 secs/batch = 0.1988s, grad.norm=11.88900471
 30244: 22 [ 1050/ 1327], train_loss/perplexity = 3.73819208/42.0219498 secs/batch = 0.1985s, grad.norm=12.46450424
 30249: 22 [ 1055/ 1327], train_loss/perplexity = 3.85184288/47.0797462 secs/batch = 0.1990s, grad.norm=12.81799030
 30254: 22 [ 1060/ 1327], train_loss/perplexity = 3.46323705/31.9201355 secs/batch = 0.1984s, grad.norm=12.57508945
 30259: 22 [ 1065/ 1327], train_loss/perplexity = 3.57532406/35.7061882 secs/batch = 0.1990s, grad.norm=12.56055832
 30264: 22 [ 1070/ 1327], train_loss/perplexity = 3.90300989/49.5513687 secs/batch = 0.1979s, grad.norm=12.67896938
 30269: 22 [ 1075/ 1327], train_loss/perplexity = 3.64084244/38.1239395 secs/batch = 0.2011s, grad.norm=12.40420246
 30274: 22 [ 1080/ 1327], train_loss/perplexity = 3.71336937/40.9916916 secs/batch = 0.1959s, grad.norm=12.41586018
 30279: 22 [ 1085/ 1327], train_loss/perplexity = 3.56572556/35.3651047 secs/batch = 0.1913s, grad.norm=12.51178551
 30284: 22 [ 1090/ 1327], train_loss/perplexity = 3.77871442/43.7597504 secs/batch = 0.1981s, grad.norm=13.20401287
 30289: 22 [ 1095/ 1327], train_loss/perplexity = 3.92479539/50.6427155 secs/batch = 0.1985s, grad.norm=12.83974171
 30294: 22 [ 1100/ 1327], train_loss/perplexity = 3.61928940/37.3110466 secs/batch = 0.1980s, grad.norm=13.07922077
 30299: 22 [ 1105/ 1327], train_loss/perplexity = 3.58869410/36.1867905 secs/batch = 0.1993s, grad.norm=12.66551018
 30304: 22 [ 1110/ 1327], train_loss/perplexity = 3.93674660/51.2515869 secs/batch = 0.1984s, grad.norm=12.93293762
 30309: 22 [ 1115/ 1327], train_loss/perplexity = 3.66861129/39.1974335 secs/batch = 0.1928s, grad.norm=12.09896851
 30314: 22 [ 1120/ 1327], train_loss/perplexity = 3.88493443/48.6637497 secs/batch = 0.1942s, grad.norm=12.17379189
 30319: 22 [ 1125/ 1327], train_loss/perplexity = 4.07342243/58.7577133 secs/batch = 0.1985s, grad.norm=13.13268089
 30324: 22 [ 1130/ 1327], train_loss/perplexity = 3.80437708/44.8972740 secs/batch = 0.1936s, grad.norm=12.39819050
 30329: 22 [ 1135/ 1327], train_loss/perplexity = 3.78699780/44.1237335 secs/batch = 0.1984s, grad.norm=12.46688843
 30334: 22 [ 1140/ 1327], train_loss/perplexity = 4.05987120/57.9668427 secs/batch = 0.1929s, grad.norm=13.01378632
 30339: 22 [ 1145/ 1327], train_loss/perplexity = 3.89184928/49.0014191 secs/batch = 0.1988s, grad.norm=12.11396599
 30344: 22 [ 1150/ 1327], train_loss/perplexity = 3.83094597/46.1061325 secs/batch = 0.1992s, grad.norm=12.42058659
 30349: 22 [ 1155/ 1327], train_loss/perplexity = 3.91059971/49.9288864 secs/batch = 0.1979s, grad.norm=12.84379005
 30354: 22 [ 1160/ 1327], train_loss/perplexity = 3.87827611/48.3408089 secs/batch = 0.1989s, grad.norm=12.31364155
 30359: 22 [ 1165/ 1327], train_loss/perplexity = 3.89366984/49.0907135 secs/batch = 0.1991s, grad.norm=12.13450909
 30364: 22 [ 1170/ 1327], train_loss/perplexity = 3.73786926/42.0083847 secs/batch = 0.1986s, grad.norm=12.18900681
 30369: 22 [ 1175/ 1327], train_loss/perplexity = 3.59296656/36.3417244 secs/batch = 0.1983s, grad.norm=12.17230892
 30374: 22 [ 1180/ 1327], train_loss/perplexity = 3.65221310/38.5599098 secs/batch = 0.1986s, grad.norm=12.60564327
 30379: 22 [ 1185/ 1327], train_loss/perplexity = 3.66931772/39.2251358 secs/batch = 0.1985s, grad.norm=12.45349121
 30384: 22 [ 1190/ 1327], train_loss/perplexity = 3.81045890/45.1711617 secs/batch = 0.1991s, grad.norm=12.74798489
 30389: 22 [ 1195/ 1327], train_loss/perplexity = 3.68250895/39.7459908 secs/batch = 0.1990s, grad.norm=12.32645702
 30394: 22 [ 1200/ 1327], train_loss/perplexity = 3.62648630/37.5805359 secs/batch = 0.1984s, grad.norm=12.66110992
 30399: 22 [ 1205/ 1327], train_loss/perplexity = 3.66710162/39.1383018 secs/batch = 0.1995s, grad.norm=13.02732468
 30404: 22 [ 1210/ 1327], train_loss/perplexity = 3.34275031/28.2968445 secs/batch = 0.1987s, grad.norm=12.65392876
 30409: 22 [ 1215/ 1327], train_loss/perplexity = 3.49849176/33.0655441 secs/batch = 0.1996s, grad.norm=11.88631725
 30414: 22 [ 1220/ 1327], train_loss/perplexity = 3.66416240/39.0234375 secs/batch = 0.1997s, grad.norm=12.87668419
 30419: 22 [ 1225/ 1327], train_loss/perplexity = 3.43821001/31.1311836 secs/batch = 0.1942s, grad.norm=13.17160702
 30424: 22 [ 1230/ 1327], train_loss/perplexity = 3.76684856/43.2435722 secs/batch = 0.1985s, grad.norm=12.25418663
 30429: 22 [ 1235/ 1327], train_loss/perplexity = 3.70884323/40.8065758 secs/batch = 0.2004s, grad.norm=12.18611717
 30434: 22 [ 1240/ 1327], train_loss/perplexity = 3.88841057/48.8332062 secs/batch = 0.1986s, grad.norm=13.42904472
 30439: 22 [ 1245/ 1327], train_loss/perplexity = 3.80543113/44.9446220 secs/batch = 0.1987s, grad.norm=12.26326942
 30444: 22 [ 1250/ 1327], train_loss/perplexity = 3.89226699/49.0218925 secs/batch = 0.2002s, grad.norm=12.05437183
 30449: 22 [ 1255/ 1327], train_loss/perplexity = 3.95970058/52.4416199 secs/batch = 0.1971s, grad.norm=12.15466785
 30454: 22 [ 1260/ 1327], train_loss/perplexity = 3.68559694/39.8689156 secs/batch = 0.1931s, grad.norm=13.15693855
 30459: 22 [ 1265/ 1327], train_loss/perplexity = 3.92732906/50.7711906 secs/batch = 0.1987s, grad.norm=12.72299194
 30464: 22 [ 1270/ 1327], train_loss/perplexity = 3.66788077/39.1688118 secs/batch = 0.1998s, grad.norm=12.53132725
 30469: 22 [ 1275/ 1327], train_loss/perplexity = 3.82053161/45.6284599 secs/batch = 0.1993s, grad.norm=12.63355923
 30474: 22 [ 1280/ 1327], train_loss/perplexity = 3.72349691/41.4089432 secs/batch = 0.1993s, grad.norm=12.57512474
 30479: 22 [ 1285/ 1327], train_loss/perplexity = 3.61836338/37.2765121 secs/batch = 0.1997s, grad.norm=12.29822731
 30484: 22 [ 1290/ 1327], train_loss/perplexity = 3.83688784/46.3809052 secs/batch = 0.1988s, grad.norm=12.02565289
 30489: 22 [ 1295/ 1327], train_loss/perplexity = 3.82343888/45.7613068 secs/batch = 0.1943s, grad.norm=12.51757336
 30494: 22 [ 1300/ 1327], train_loss/perplexity = 4.00387812/54.8102989 secs/batch = 0.1981s, grad.norm=12.16252518
 30499: 22 [ 1305/ 1327], train_loss/perplexity = 4.05960369/57.9513397 secs/batch = 0.1987s, grad.norm=12.83390903
 30504: 22 [ 1310/ 1327], train_loss/perplexity = 4.35822392/78.1182632 secs/batch = 0.1986s, grad.norm=13.06084251
 30509: 22 [ 1315/ 1327], train_loss/perplexity = 4.17244768/64.8740463 secs/batch = 0.1941s, grad.norm=13.06942940
 30514: 22 [ 1320/ 1327], train_loss/perplexity = 4.08846903/59.6485023 secs/batch = 0.1988s, grad.norm=12.06120968
 30519: 22 [ 1325/ 1327], train_loss/perplexity = 4.05919886/57.9278831 secs/batch = 0.1980s, grad.norm=12.48798180
Epoch training time: 263.3516206741333
	> validation loss = 4.71832943, perplexity = 111.98102570
	> validation loss = 4.63074636, perplexity = 102.59060669
	> validation loss = 4.58059883, perplexity = 97.57280731
	> validation loss = 4.60538149, perplexity = 100.02113342
	> validation loss = 4.80693626, perplexity = 122.35617828
	> validation loss = 4.63851738, perplexity = 103.39094543
	> validation loss = 4.68338728, perplexity = 108.13574219
	> validation loss = 4.47178936, perplexity = 87.51317596
	> validation loss = 4.29497385, perplexity = 73.33029938
	> validation loss = 4.37986135, perplexity = 79.82696533
	> validation loss = 4.53876352, perplexity = 93.57502747
	> validation loss = 4.61163855, perplexity = 100.64893341
	> validation loss = 4.52964306, perplexity = 92.72545624
	> validation loss = 4.31740904, perplexity = 74.99407196
	> validation loss = 4.26098204, perplexity = 70.87955475
	> validation loss = 4.25123262, perplexity = 70.19187927
	> validation loss = 4.72291088, perplexity = 112.49523926
	> validation loss = 4.24236822, perplexity = 69.57241821
	> validation loss = 4.75102425, perplexity = 115.70272827
	> validation loss = 4.61154032, perplexity = 100.63904572
	> validation loss = 4.41625690, perplexity = 82.78582764
at the end of epoch: 22
train loss = 3.96208556, perplexity = 52.56684291
validation loss = 4.53286495, perplexity = 93.02469032
Saved model cv/epoch022_4.5329.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.00390625
new learning rate is: 0.001953125
 30526: 23 [    5/ 1327], train_loss/perplexity = 4.16549206/64.4243774 secs/batch = 0.1996s, grad.norm=12.59135151
 30531: 23 [   10/ 1327], train_loss/perplexity = 3.73564887/41.9152145 secs/batch = 0.1982s, grad.norm=12.19181252
 30536: 23 [   15/ 1327], train_loss/perplexity = 4.20166636/66.7975464 secs/batch = 0.1985s, grad.norm=12.70983219
 30541: 23 [   20/ 1327], train_loss/perplexity = 4.26003170/70.8122253 secs/batch = 0.1983s, grad.norm=12.23845482
 30546: 23 [   25/ 1327], train_loss/perplexity = 4.13483715/62.4794159 secs/batch = 0.1994s, grad.norm=12.94111061
 30551: 23 [   30/ 1327], train_loss/perplexity = 4.17053795/64.7502747 secs/batch = 0.1987s, grad.norm=12.70353317
 30556: 23 [   35/ 1327], train_loss/perplexity = 3.99765921/54.4704971 secs/batch = 0.1982s, grad.norm=12.63335800
 30561: 23 [   40/ 1327], train_loss/perplexity = 3.99094677/54.1060905 secs/batch = 0.1929s, grad.norm=12.54268265
 30566: 23 [   45/ 1327], train_loss/perplexity = 3.77166867/43.4525108 secs/batch = 0.1989s, grad.norm=12.14398575
 30571: 23 [   50/ 1327], train_loss/perplexity = 3.99701977/54.4356766 secs/batch = 0.1977s, grad.norm=12.94677162
 30576: 23 [   55/ 1327], train_loss/perplexity = 3.97383380/53.1880531 secs/batch = 0.1994s, grad.norm=12.87573528
 30581: 23 [   60/ 1327], train_loss/perplexity = 4.21505022/67.6975632 secs/batch = 0.1981s, grad.norm=12.87268543
 30586: 23 [   65/ 1327], train_loss/perplexity = 3.81743383/45.4873314 secs/batch = 0.1992s, grad.norm=12.39386845
 30591: 23 [   70/ 1327], train_loss/perplexity = 3.61393404/37.1117668 secs/batch = 0.1994s, grad.norm=12.57673740
 30596: 23 [   75/ 1327], train_loss/perplexity = 3.48662043/32.6753311 secs/batch = 0.1987s, grad.norm=11.84392834
 30601: 23 [   80/ 1327], train_loss/perplexity = 3.86919999/47.9040489 secs/batch = 0.1990s, grad.norm=12.50548458
 30606: 23 [   85/ 1327], train_loss/perplexity = 3.91600227/50.1993599 secs/batch = 0.1986s, grad.norm=13.44027328
 30611: 23 [   90/ 1327], train_loss/perplexity = 4.02270508/55.8519859 secs/batch = 0.1994s, grad.norm=12.89503574
 30616: 23 [   95/ 1327], train_loss/perplexity = 3.81704855/45.4698067 secs/batch = 0.1977s, grad.norm=12.76890182
 30621: 23 [  100/ 1327], train_loss/perplexity = 4.10000706/60.3407135 secs/batch = 0.1989s, grad.norm=12.87302113
 30626: 23 [  105/ 1327], train_loss/perplexity = 4.02447224/55.9507713 secs/batch = 0.1999s, grad.norm=13.71949196
 30631: 23 [  110/ 1327], train_loss/perplexity = 3.83254576/46.1799507 secs/batch = 0.1992s, grad.norm=12.60676479
 30636: 23 [  115/ 1327], train_loss/perplexity = 3.87257648/48.0660667 secs/batch = 0.1991s, grad.norm=13.44945145
 30641: 23 [  120/ 1327], train_loss/perplexity = 3.86966181/47.9261742 secs/batch = 0.1992s, grad.norm=13.23618031
 30646: 23 [  125/ 1327], train_loss/perplexity = 4.01437759/55.3888092 secs/batch = 0.1984s, grad.norm=13.07638359
 30651: 23 [  130/ 1327], train_loss/perplexity = 3.94802690/51.8329926 secs/batch = 0.1970s, grad.norm=13.22799110
 30656: 23 [  135/ 1327], train_loss/perplexity = 3.93090367/50.9530029 secs/batch = 0.1995s, grad.norm=12.73820210
 30661: 23 [  140/ 1327], train_loss/perplexity = 4.17131042/64.8003159 secs/batch = 0.1934s, grad.norm=12.99102783
 30666: 23 [  145/ 1327], train_loss/perplexity = 4.07159185/58.6502495 secs/batch = 0.1986s, grad.norm=13.67020893
 30671: 23 [  150/ 1327], train_loss/perplexity = 4.14525175/63.1335144 secs/batch = 0.1982s, grad.norm=13.07587624
 30676: 23 [  155/ 1327], train_loss/perplexity = 4.35119390/77.5710220 secs/batch = 0.1980s, grad.norm=13.21475506
 30681: 23 [  160/ 1327], train_loss/perplexity = 4.01450300/55.3957558 secs/batch = 0.1985s, grad.norm=12.57184505
 30686: 23 [  165/ 1327], train_loss/perplexity = 4.18188620/65.4892654 secs/batch = 0.1985s, grad.norm=12.94463634
 30691: 23 [  170/ 1327], train_loss/perplexity = 3.93485188/51.1545715 secs/batch = 0.1951s, grad.norm=12.21069050
 30696: 23 [  175/ 1327], train_loss/perplexity = 4.25303030/70.3181763 secs/batch = 0.1990s, grad.norm=12.74901390
 30701: 23 [  180/ 1327], train_loss/perplexity = 4.00757742/55.0134354 secs/batch = 0.1989s, grad.norm=12.54488850
 30706: 23 [  185/ 1327], train_loss/perplexity = 4.36854267/78.9285202 secs/batch = 0.1985s, grad.norm=13.13414955
 30711: 23 [  190/ 1327], train_loss/perplexity = 3.89950275/49.3778915 secs/batch = 0.1986s, grad.norm=12.10381985
 30716: 23 [  195/ 1327], train_loss/perplexity = 4.19995928/66.6836166 secs/batch = 0.1970s, grad.norm=12.73195648
 30721: 23 [  200/ 1327], train_loss/perplexity = 4.06146097/58.0590706 secs/batch = 0.1985s, grad.norm=13.51018715
 30726: 23 [  205/ 1327], train_loss/perplexity = 4.28000307/72.2406616 secs/batch = 0.1910s, grad.norm=12.67027473
 30731: 23 [  210/ 1327], train_loss/perplexity = 4.13902855/62.7418404 secs/batch = 0.1986s, grad.norm=12.06421185
 30736: 23 [  215/ 1327], train_loss/perplexity = 4.22057724/68.0727692 secs/batch = 0.1994s, grad.norm=12.12740993
 30741: 23 [  220/ 1327], train_loss/perplexity = 4.18904448/65.9597321 secs/batch = 0.1981s, grad.norm=12.61950302
 30746: 23 [  225/ 1327], train_loss/perplexity = 4.40731907/82.0492020 secs/batch = 0.1988s, grad.norm=12.87183857
 30751: 23 [  230/ 1327], train_loss/perplexity = 4.15201950/63.5622330 secs/batch = 0.1987s, grad.norm=13.39701271
 30756: 23 [  235/ 1327], train_loss/perplexity = 4.07239819/58.6975632 secs/batch = 0.1984s, grad.norm=12.81186390
 30761: 23 [  240/ 1327], train_loss/perplexity = 3.85204983/47.0894890 secs/batch = 0.1984s, grad.norm=13.30988026
 30766: 23 [  245/ 1327], train_loss/perplexity = 4.04526663/57.1264153 secs/batch = 0.1942s, grad.norm=12.50597477
 30771: 23 [  250/ 1327], train_loss/perplexity = 3.88851261/48.8381920 secs/batch = 0.1989s, grad.norm=12.50649834
 30776: 23 [  255/ 1327], train_loss/perplexity = 3.99206758/54.1667671 secs/batch = 0.1996s, grad.norm=12.35513783
 30781: 23 [  260/ 1327], train_loss/perplexity = 4.11829376/61.4542961 secs/batch = 0.1990s, grad.norm=13.16752625
 30786: 23 [  265/ 1327], train_loss/perplexity = 4.32009697/75.1959229 secs/batch = 0.1995s, grad.norm=12.63268566
 30791: 23 [  270/ 1327], train_loss/perplexity = 4.40308666/81.7026672 secs/batch = 0.1986s, grad.norm=12.87684059
 30796: 23 [  275/ 1327], train_loss/perplexity = 4.28003836/72.2432098 secs/batch = 0.2012s, grad.norm=12.67683220
 30801: 23 [  280/ 1327], train_loss/perplexity = 4.11741877/61.4005470 secs/batch = 0.1944s, grad.norm=12.69490719
 30806: 23 [  285/ 1327], train_loss/perplexity = 4.43463182/84.3210754 secs/batch = 0.1994s, grad.norm=12.46238136
 30811: 23 [  290/ 1327], train_loss/perplexity = 4.19244432/66.1843719 secs/batch = 0.1988s, grad.norm=13.04932308
 30816: 23 [  295/ 1327], train_loss/perplexity = 3.87307906/48.0902328 secs/batch = 0.1987s, grad.norm=12.53611469
 30821: 23 [  300/ 1327], train_loss/perplexity = 3.53798866/34.3976631 secs/batch = 0.1982s, grad.norm=12.15859985
 30826: 23 [  305/ 1327], train_loss/perplexity = 3.94882679/51.8744698 secs/batch = 0.1967s, grad.norm=12.06918144
 30831: 23 [  310/ 1327], train_loss/perplexity = 3.94799376/51.8312759 secs/batch = 0.1987s, grad.norm=12.54744625
 30836: 23 [  315/ 1327], train_loss/perplexity = 3.47608566/32.3329124 secs/batch = 0.1991s, grad.norm=11.72229767
 30841: 23 [  320/ 1327], train_loss/perplexity = 3.48997855/32.7852440 secs/batch = 0.1969s, grad.norm=13.14812279
 30846: 23 [  325/ 1327], train_loss/perplexity = 3.50320911/33.2218933 secs/batch = 0.1985s, grad.norm=11.70196819
 30851: 23 [  330/ 1327], train_loss/perplexity = 4.07655239/58.9419098 secs/batch = 0.1987s, grad.norm=12.67531013
 30856: 23 [  335/ 1327], train_loss/perplexity = 3.51789427/33.7133636 secs/batch = 0.1968s, grad.norm=11.89591885
 30861: 23 [  340/ 1327], train_loss/perplexity = 4.28763771/72.7943039 secs/batch = 0.1989s, grad.norm=12.88315392
 30866: 23 [  345/ 1327], train_loss/perplexity = 4.01831913/55.6075592 secs/batch = 0.1970s, grad.norm=12.92052460
 30871: 23 [  350/ 1327], train_loss/perplexity = 4.02732801/56.1107826 secs/batch = 0.1926s, grad.norm=13.19670773
 30876: 23 [  355/ 1327], train_loss/perplexity = 4.07041311/58.5811577 secs/batch = 0.1983s, grad.norm=12.69265175
 30881: 23 [  360/ 1327], train_loss/perplexity = 4.14167118/62.9078636 secs/batch = 0.1996s, grad.norm=13.70853424
 30886: 23 [  365/ 1327], train_loss/perplexity = 4.16243982/64.2280350 secs/batch = 0.1996s, grad.norm=12.48973370
 30891: 23 [  370/ 1327], train_loss/perplexity = 4.25715733/70.6089783 secs/batch = 0.1980s, grad.norm=12.60925865
 30896: 23 [  375/ 1327], train_loss/perplexity = 3.64099836/38.1298866 secs/batch = 0.1983s, grad.norm=12.51050663
 30901: 23 [  380/ 1327], train_loss/perplexity = 3.72742128/41.5717697 secs/batch = 0.1993s, grad.norm=12.67584038
 30906: 23 [  385/ 1327], train_loss/perplexity = 3.92391491/50.5981445 secs/batch = 0.1985s, grad.norm=13.13337994
 30911: 23 [  390/ 1327], train_loss/perplexity = 4.01784420/55.5811539 secs/batch = 0.1923s, grad.norm=12.65116215
 30916: 23 [  395/ 1327], train_loss/perplexity = 4.09459925/60.0152817 secs/batch = 0.1988s, grad.norm=12.38897610
 30921: 23 [  400/ 1327], train_loss/perplexity = 4.05947399/57.9438248 secs/batch = 0.1994s, grad.norm=12.44303036
 30926: 23 [  405/ 1327], train_loss/perplexity = 4.25878954/70.7243195 secs/batch = 0.1984s, grad.norm=13.09151077
 30931: 23 [  410/ 1327], train_loss/perplexity = 3.93169546/50.9933624 secs/batch = 0.1991s, grad.norm=12.92445183
 30936: 23 [  415/ 1327], train_loss/perplexity = 3.86293435/47.6048355 secs/batch = 0.1992s, grad.norm=12.74455452
 30941: 23 [  420/ 1327], train_loss/perplexity = 3.58393002/36.0148010 secs/batch = 0.1998s, grad.norm=11.99049950
 30946: 23 [  425/ 1327], train_loss/perplexity = 3.87880874/48.3665619 secs/batch = 0.1980s, grad.norm=13.78369617
 30951: 23 [  430/ 1327], train_loss/perplexity = 4.09469414/60.0209770 secs/batch = 0.1981s, grad.norm=13.16513252
 30956: 23 [  435/ 1327], train_loss/perplexity = 4.10574865/60.6881638 secs/batch = 0.1989s, grad.norm=12.87114620
 30961: 23 [  440/ 1327], train_loss/perplexity = 3.70015454/40.4535561 secs/batch = 0.1991s, grad.norm=12.52118778
 30966: 23 [  445/ 1327], train_loss/perplexity = 3.97543097/53.2730713 secs/batch = 0.1993s, grad.norm=13.16434193
 30971: 23 [  450/ 1327], train_loss/perplexity = 3.96199751/52.5622139 secs/batch = 0.1995s, grad.norm=12.89088821
 30976: 23 [  455/ 1327], train_loss/perplexity = 3.96743393/52.8487434 secs/batch = 0.1983s, grad.norm=12.33547592
 30981: 23 [  460/ 1327], train_loss/perplexity = 3.90887976/49.8430824 secs/batch = 0.1972s, grad.norm=13.37686062
 30986: 23 [  465/ 1327], train_loss/perplexity = 3.64269781/38.1947403 secs/batch = 0.1983s, grad.norm=13.33647060
 30991: 23 [  470/ 1327], train_loss/perplexity = 4.40386534/81.7663116 secs/batch = 0.1991s, grad.norm=12.71888161
 30996: 23 [  475/ 1327], train_loss/perplexity = 3.81373024/45.3191757 secs/batch = 0.1991s, grad.norm=12.46930599
 31001: 23 [  480/ 1327], train_loss/perplexity = 3.89748526/49.2783699 secs/batch = 0.1965s, grad.norm=12.87604332
 31006: 23 [  485/ 1327], train_loss/perplexity = 3.91428375/50.1131668 secs/batch = 0.1990s, grad.norm=12.53018761
 31011: 23 [  490/ 1327], train_loss/perplexity = 3.83650517/46.3631592 secs/batch = 0.1993s, grad.norm=13.60287857
 31016: 23 [  495/ 1327], train_loss/perplexity = 3.89845896/49.3263779 secs/batch = 0.1983s, grad.norm=12.81579685
 31021: 23 [  500/ 1327], train_loss/perplexity = 4.01126385/55.2166100 secs/batch = 0.1941s, grad.norm=13.41587067
 31026: 23 [  505/ 1327], train_loss/perplexity = 4.11218500/61.0800323 secs/batch = 0.1988s, grad.norm=11.90931320
 31031: 23 [  510/ 1327], train_loss/perplexity = 4.44113350/84.8710861 secs/batch = 0.1986s, grad.norm=12.59849739
 31036: 23 [  515/ 1327], train_loss/perplexity = 4.11664486/61.3530502 secs/batch = 0.2000s, grad.norm=12.14014149
 31041: 23 [  520/ 1327], train_loss/perplexity = 4.28517246/72.6150665 secs/batch = 0.1985s, grad.norm=12.47690582
 31046: 23 [  525/ 1327], train_loss/perplexity = 3.91160631/49.9791679 secs/batch = 0.1993s, grad.norm=12.43980408
 31051: 23 [  530/ 1327], train_loss/perplexity = 3.90733480/49.7661400 secs/batch = 0.1984s, grad.norm=13.21189117
 31056: 23 [  535/ 1327], train_loss/perplexity = 4.03101397/56.3179855 secs/batch = 0.1988s, grad.norm=12.81061459
 31061: 23 [  540/ 1327], train_loss/perplexity = 4.07956314/59.1196365 secs/batch = 0.1962s, grad.norm=13.10643196
 31066: 23 [  545/ 1327], train_loss/perplexity = 4.13589287/62.5454102 secs/batch = 0.1998s, grad.norm=12.39884186
 31071: 23 [  550/ 1327], train_loss/perplexity = 4.07657385/58.9431763 secs/batch = 0.1988s, grad.norm=12.37394238
 31076: 23 [  555/ 1327], train_loss/perplexity = 3.94627690/51.7423668 secs/batch = 0.2008s, grad.norm=12.06843281
 31081: 23 [  560/ 1327], train_loss/perplexity = 3.99152422/54.1373444 secs/batch = 0.1929s, grad.norm=13.35254383
 31086: 23 [  565/ 1327], train_loss/perplexity = 3.83436346/46.2639694 secs/batch = 0.1988s, grad.norm=13.17308235
 31091: 23 [  570/ 1327], train_loss/perplexity = 3.82630801/45.8927879 secs/batch = 0.1954s, grad.norm=12.99912167
 31096: 23 [  575/ 1327], train_loss/perplexity = 3.65110159/38.5170746 secs/batch = 0.1986s, grad.norm=13.23289680
 31101: 23 [  580/ 1327], train_loss/perplexity = 4.06321955/58.1612625 secs/batch = 0.1923s, grad.norm=12.81536770
 31106: 23 [  585/ 1327], train_loss/perplexity = 3.73385572/41.8401222 secs/batch = 0.1926s, grad.norm=12.72372437
 31111: 23 [  590/ 1327], train_loss/perplexity = 4.09921408/60.2928848 secs/batch = 0.1990s, grad.norm=12.95992756
 31116: 23 [  595/ 1327], train_loss/perplexity = 3.99783707/54.4801865 secs/batch = 0.1990s, grad.norm=12.88394642
 31121: 23 [  600/ 1327], train_loss/perplexity = 4.22195864/68.1668701 secs/batch = 0.2000s, grad.norm=12.36830902
 31126: 23 [  605/ 1327], train_loss/perplexity = 4.05831242/57.8765564 secs/batch = 0.1988s, grad.norm=12.89833641
 31131: 23 [  610/ 1327], train_loss/perplexity = 4.25839472/70.6964035 secs/batch = 0.1987s, grad.norm=12.49630070
 31136: 23 [  615/ 1327], train_loss/perplexity = 3.92597318/50.7023964 secs/batch = 0.2001s, grad.norm=12.24967957
 31141: 23 [  620/ 1327], train_loss/perplexity = 4.24279833/69.6023483 secs/batch = 0.1941s, grad.norm=12.67101002
 31146: 23 [  625/ 1327], train_loss/perplexity = 4.16762447/64.5619049 secs/batch = 0.1997s, grad.norm=12.18755245
 31151: 23 [  630/ 1327], train_loss/perplexity = 4.25350189/70.3513412 secs/batch = 0.1978s, grad.norm=12.54363632
 31156: 23 [  635/ 1327], train_loss/perplexity = 3.98272848/53.6632538 secs/batch = 0.1978s, grad.norm=12.35086727
 31161: 23 [  640/ 1327], train_loss/perplexity = 4.04759979/57.2598572 secs/batch = 0.1993s, grad.norm=12.21344566
 31166: 23 [  645/ 1327], train_loss/perplexity = 4.29590321/73.3984756 secs/batch = 0.1952s, grad.norm=12.99972343
 31171: 23 [  650/ 1327], train_loss/perplexity = 3.75498557/42.7336044 secs/batch = 0.1995s, grad.norm=13.19196415
 31176: 23 [  655/ 1327], train_loss/perplexity = 3.93045807/50.9303017 secs/batch = 0.1985s, grad.norm=12.96169949
 31181: 23 [  660/ 1327], train_loss/perplexity = 3.82517004/45.8405952 secs/batch = 0.1989s, grad.norm=13.11479855
 31186: 23 [  665/ 1327], train_loss/perplexity = 3.88387084/48.6120224 secs/batch = 0.1996s, grad.norm=12.51567650
 31191: 23 [  670/ 1327], train_loss/perplexity = 3.89810848/49.3090935 secs/batch = 0.2000s, grad.norm=12.90458584
 31196: 23 [  675/ 1327], train_loss/perplexity = 3.70741558/40.7483597 secs/batch = 0.1990s, grad.norm=12.59250832
 31201: 23 [  680/ 1327], train_loss/perplexity = 3.93984270/51.4105148 secs/batch = 0.1999s, grad.norm=12.89196110
 31206: 23 [  685/ 1327], train_loss/perplexity = 3.70362663/40.5942574 secs/batch = 0.2005s, grad.norm=12.08476925
 31211: 23 [  690/ 1327], train_loss/perplexity = 4.14502048/63.1189156 secs/batch = 0.1989s, grad.norm=12.55171585
 31216: 23 [  695/ 1327], train_loss/perplexity = 3.90437341/49.6189804 secs/batch = 0.1934s, grad.norm=12.51445580
 31221: 23 [  700/ 1327], train_loss/perplexity = 4.22641659/68.4714279 secs/batch = 0.1986s, grad.norm=13.52738667
 31226: 23 [  705/ 1327], train_loss/perplexity = 3.91207957/50.0028267 secs/batch = 0.1965s, grad.norm=12.23104477
 31231: 23 [  710/ 1327], train_loss/perplexity = 3.83488750/46.2882195 secs/batch = 0.1989s, grad.norm=12.35396385
 31236: 23 [  715/ 1327], train_loss/perplexity = 3.83147645/46.1305962 secs/batch = 0.1986s, grad.norm=13.03565502
 31241: 23 [  720/ 1327], train_loss/perplexity = 3.75062346/42.5475998 secs/batch = 0.1968s, grad.norm=12.63735294
 31246: 23 [  725/ 1327], train_loss/perplexity = 3.78333926/43.9626007 secs/batch = 0.1997s, grad.norm=12.79507923
 31251: 23 [  730/ 1327], train_loss/perplexity = 3.98197484/53.6228256 secs/batch = 0.1977s, grad.norm=12.50295353
 31256: 23 [  735/ 1327], train_loss/perplexity = 4.11982965/61.5487556 secs/batch = 0.1987s, grad.norm=13.61180782
 31261: 23 [  740/ 1327], train_loss/perplexity = 3.49244428/32.8661842 secs/batch = 0.1990s, grad.norm=12.04636478
 31266: 23 [  745/ 1327], train_loss/perplexity = 4.07842207/59.0522156 secs/batch = 0.1983s, grad.norm=13.03421307
 31271: 23 [  750/ 1327], train_loss/perplexity = 3.84270906/46.6516876 secs/batch = 0.1916s, grad.norm=12.85423946
 31276: 23 [  755/ 1327], train_loss/perplexity = 3.78601694/44.0804749 secs/batch = 0.1995s, grad.norm=12.37247658
 31281: 23 [  760/ 1327], train_loss/perplexity = 3.55498695/34.9873619 secs/batch = 0.1988s, grad.norm=12.21406174
 31286: 23 [  765/ 1327], train_loss/perplexity = 3.67728138/39.5387573 secs/batch = 0.2000s, grad.norm=12.33452988
 31291: 23 [  770/ 1327], train_loss/perplexity = 3.61844587/37.2795868 secs/batch = 0.1988s, grad.norm=12.83987904
 31296: 23 [  775/ 1327], train_loss/perplexity = 3.76269007/43.0641174 secs/batch = 0.1991s, grad.norm=12.72538471
 31301: 23 [  780/ 1327], train_loss/perplexity = 4.01876688/55.6324615 secs/batch = 0.1992s, grad.norm=12.64626217
 31306: 23 [  785/ 1327], train_loss/perplexity = 3.95219564/52.0495224 secs/batch = 0.1984s, grad.norm=13.04930592
 31311: 23 [  790/ 1327], train_loss/perplexity = 3.72662592/41.5387154 secs/batch = 0.1990s, grad.norm=12.93349552
 31316: 23 [  795/ 1327], train_loss/perplexity = 4.05695868/57.7982597 secs/batch = 0.1981s, grad.norm=12.88903713
 31321: 23 [  800/ 1327], train_loss/perplexity = 4.03374290/56.4718857 secs/batch = 0.1985s, grad.norm=12.70389843
 31326: 23 [  805/ 1327], train_loss/perplexity = 4.38677502/80.3807755 secs/batch = 0.1922s, grad.norm=13.12457752
 31331: 23 [  810/ 1327], train_loss/perplexity = 3.90195417/49.4990845 secs/batch = 0.1991s, grad.norm=12.07127476
 31336: 23 [  815/ 1327], train_loss/perplexity = 3.77012897/43.3856583 secs/batch = 0.1992s, grad.norm=12.16284466
 31341: 23 [  820/ 1327], train_loss/perplexity = 3.75162435/42.5902061 secs/batch = 0.1996s, grad.norm=12.00595474
 31346: 23 [  825/ 1327], train_loss/perplexity = 3.87637901/48.2491875 secs/batch = 0.1940s, grad.norm=12.49179935
 31351: 23 [  830/ 1327], train_loss/perplexity = 3.64207435/38.1709328 secs/batch = 0.1983s, grad.norm=12.74308968
 31356: 23 [  835/ 1327], train_loss/perplexity = 3.94991875/51.9311485 secs/batch = 0.1998s, grad.norm=13.00788784
 31361: 23 [  840/ 1327], train_loss/perplexity = 3.97522855/53.2622871 secs/batch = 0.1985s, grad.norm=12.50692558
 31366: 23 [  845/ 1327], train_loss/perplexity = 3.82144642/45.6702194 secs/batch = 0.1976s, grad.norm=12.88983631
 31371: 23 [  850/ 1327], train_loss/perplexity = 3.88886333/48.8553238 secs/batch = 0.1995s, grad.norm=13.02448654
 31376: 23 [  855/ 1327], train_loss/perplexity = 3.89507985/49.1599770 secs/batch = 0.1986s, grad.norm=12.98641109
 31381: 23 [  860/ 1327], train_loss/perplexity = 3.62330627/37.4612198 secs/batch = 0.1963s, grad.norm=11.98280239
 31386: 23 [  865/ 1327], train_loss/perplexity = 4.07163429/58.6527405 secs/batch = 0.1985s, grad.norm=12.38585949
 31391: 23 [  870/ 1327], train_loss/perplexity = 3.86326790/47.6207161 secs/batch = 0.1993s, grad.norm=12.62281418
 31396: 23 [  875/ 1327], train_loss/perplexity = 3.64058304/38.1140518 secs/batch = 0.1990s, grad.norm=12.60708332
 31401: 23 [  880/ 1327], train_loss/perplexity = 3.77619505/43.6496391 secs/batch = 0.1993s, grad.norm=11.95776653
 31406: 23 [  885/ 1327], train_loss/perplexity = 3.99215746/54.1716347 secs/batch = 0.1984s, grad.norm=12.51865101
 31411: 23 [  890/ 1327], train_loss/perplexity = 4.04478598/57.0989647 secs/batch = 0.1986s, grad.norm=12.45255184
 31416: 23 [  895/ 1327], train_loss/perplexity = 4.07609463/58.9149361 secs/batch = 0.1997s, grad.norm=12.19709682
 31421: 23 [  900/ 1327], train_loss/perplexity = 3.89638209/49.2240372 secs/batch = 0.1982s, grad.norm=12.35669327
 31426: 23 [  905/ 1327], train_loss/perplexity = 3.75965166/42.9334679 secs/batch = 0.1983s, grad.norm=11.97861576
 31431: 23 [  910/ 1327], train_loss/perplexity = 3.83802509/46.4336815 secs/batch = 0.1993s, grad.norm=11.45924377
 31436: 23 [  915/ 1327], train_loss/perplexity = 4.01534843/55.4426117 secs/batch = 0.1990s, grad.norm=12.37182426
 31441: 23 [  920/ 1327], train_loss/perplexity = 4.21635342/67.7858429 secs/batch = 0.1992s, grad.norm=12.81567860
 31446: 23 [  925/ 1327], train_loss/perplexity = 4.04962254/57.3757973 secs/batch = 0.1991s, grad.norm=13.37659168
 31451: 23 [  930/ 1327], train_loss/perplexity = 3.90108585/49.4561234 secs/batch = 0.1916s, grad.norm=12.36489773
 31456: 23 [  935/ 1327], train_loss/perplexity = 4.13384056/62.4171791 secs/batch = 0.1991s, grad.norm=12.21084213
 31461: 23 [  940/ 1327], train_loss/perplexity = 3.99647903/54.4062500 secs/batch = 0.1988s, grad.norm=12.19846725
 31466: 23 [  945/ 1327], train_loss/perplexity = 4.19048500/66.0548172 secs/batch = 0.1984s, grad.norm=12.21303082
 31471: 23 [  950/ 1327], train_loss/perplexity = 4.03908396/56.7743111 secs/batch = 0.1989s, grad.norm=12.57882118
 31476: 23 [  955/ 1327], train_loss/perplexity = 3.92668915/50.7387123 secs/batch = 0.1922s, grad.norm=12.52972031
 31481: 23 [  960/ 1327], train_loss/perplexity = 4.29696321/73.4763260 secs/batch = 0.1993s, grad.norm=12.24010563
 31486: 23 [  965/ 1327], train_loss/perplexity = 3.94041181/51.4397812 secs/batch = 0.1944s, grad.norm=11.96911335
 31491: 23 [  970/ 1327], train_loss/perplexity = 4.21623850/67.7780533 secs/batch = 0.1994s, grad.norm=12.15055084
 31496: 23 [  975/ 1327], train_loss/perplexity = 3.90824509/49.8114586 secs/batch = 0.1992s, grad.norm=13.10276794
 31501: 23 [  980/ 1327], train_loss/perplexity = 3.78333187/43.9622765 secs/batch = 0.1987s, grad.norm=12.59447670
 31506: 23 [  985/ 1327], train_loss/perplexity = 3.84538460/46.7766724 secs/batch = 0.1995s, grad.norm=12.36322784
 31511: 23 [  990/ 1327], train_loss/perplexity = 4.09410810/59.9858131 secs/batch = 0.1979s, grad.norm=13.09240150
 31516: 23 [  995/ 1327], train_loss/perplexity = 4.09215164/59.8685684 secs/batch = 0.1986s, grad.norm=12.35469341
 31521: 23 [ 1000/ 1327], train_loss/perplexity = 3.65997100/38.8602142 secs/batch = 0.2000s, grad.norm=12.54422855
 31526: 23 [ 1005/ 1327], train_loss/perplexity = 4.09968758/60.3214378 secs/batch = 0.1991s, grad.norm=12.76982880
 31531: 23 [ 1010/ 1327], train_loss/perplexity = 3.67589808/39.4841003 secs/batch = 0.1973s, grad.norm=11.71038532
 31536: 23 [ 1015/ 1327], train_loss/perplexity = 4.22884512/68.6379166 secs/batch = 0.1981s, grad.norm=12.41019344
 31541: 23 [ 1020/ 1327], train_loss/perplexity = 4.16972971/64.6979599 secs/batch = 0.1997s, grad.norm=12.43468475
 31546: 23 [ 1025/ 1327], train_loss/perplexity = 4.16022444/64.0859070 secs/batch = 0.1963s, grad.norm=12.54298973
 31551: 23 [ 1030/ 1327], train_loss/perplexity = 3.96341658/52.6368561 secs/batch = 0.1984s, grad.norm=11.99489403
 31556: 23 [ 1035/ 1327], train_loss/perplexity = 3.90591383/49.6954727 secs/batch = 0.1989s, grad.norm=12.67871284
 31561: 23 [ 1040/ 1327], train_loss/perplexity = 4.14494801/63.1143417 secs/batch = 0.1986s, grad.norm=12.96166325
 31566: 23 [ 1045/ 1327], train_loss/perplexity = 3.60644603/36.8349113 secs/batch = 0.1983s, grad.norm=12.20870018
 31571: 23 [ 1050/ 1327], train_loss/perplexity = 3.79800415/44.6120567 secs/batch = 0.1982s, grad.norm=12.30413914
 31576: 23 [ 1055/ 1327], train_loss/perplexity = 3.78927445/44.2243004 secs/batch = 0.1989s, grad.norm=12.98796463
 31581: 23 [ 1060/ 1327], train_loss/perplexity = 3.46786761/32.0682869 secs/batch = 0.1985s, grad.norm=12.83557224
 31586: 23 [ 1065/ 1327], train_loss/perplexity = 3.64467001/38.2701416 secs/batch = 0.1989s, grad.norm=12.53200150
 31591: 23 [ 1070/ 1327], train_loss/perplexity = 3.91525960/50.1620903 secs/batch = 0.1993s, grad.norm=12.69094563
 31596: 23 [ 1075/ 1327], train_loss/perplexity = 3.59856439/36.5457306 secs/batch = 0.1980s, grad.norm=12.75641537
 31601: 23 [ 1080/ 1327], train_loss/perplexity = 3.68441701/39.8218994 secs/batch = 0.1985s, grad.norm=12.57179451
 31606: 23 [ 1085/ 1327], train_loss/perplexity = 3.58044910/35.8896561 secs/batch = 0.1987s, grad.norm=12.30398846
 31611: 23 [ 1090/ 1327], train_loss/perplexity = 3.77443457/43.5728645 secs/batch = 0.1985s, grad.norm=13.18393707
 31616: 23 [ 1095/ 1327], train_loss/perplexity = 3.92083979/50.4427872 secs/batch = 0.1985s, grad.norm=12.83382034
 31621: 23 [ 1100/ 1327], train_loss/perplexity = 3.57151318/35.5703773 secs/batch = 0.1919s, grad.norm=13.36432076
 31626: 23 [ 1105/ 1327], train_loss/perplexity = 3.59230876/36.3178291 secs/batch = 0.1987s, grad.norm=12.58857059
 31631: 23 [ 1110/ 1327], train_loss/perplexity = 3.94869924/51.8678551 secs/batch = 0.1986s, grad.norm=13.56764793
 31636: 23 [ 1115/ 1327], train_loss/perplexity = 3.63989902/38.0879898 secs/batch = 0.1981s, grad.norm=11.89013100
 31641: 23 [ 1120/ 1327], train_loss/perplexity = 3.91689491/50.2441902 secs/batch = 0.1981s, grad.norm=12.25575256
 31646: 23 [ 1125/ 1327], train_loss/perplexity = 4.15664625/63.8570023 secs/batch = 0.1994s, grad.norm=13.14618397
 31651: 23 [ 1130/ 1327], train_loss/perplexity = 3.76106763/42.9943047 secs/batch = 0.1985s, grad.norm=12.42177200
 31656: 23 [ 1135/ 1327], train_loss/perplexity = 3.76177311/43.0246468 secs/batch = 0.1982s, grad.norm=12.46322823
 31661: 23 [ 1140/ 1327], train_loss/perplexity = 4.07573938/58.8940086 secs/batch = 0.1927s, grad.norm=13.24584103
 31666: 23 [ 1145/ 1327], train_loss/perplexity = 3.92395830/50.6003418 secs/batch = 0.1999s, grad.norm=11.87486172
 31671: 23 [ 1150/ 1327], train_loss/perplexity = 3.83831286/46.4470444 secs/batch = 0.1990s, grad.norm=12.24302673
 31676: 23 [ 1155/ 1327], train_loss/perplexity = 3.92076898/50.4392166 secs/batch = 0.1994s, grad.norm=12.70705318
 31681: 23 [ 1160/ 1327], train_loss/perplexity = 3.84954691/46.9717751 secs/batch = 0.1991s, grad.norm=12.54536915
 31686: 23 [ 1165/ 1327], train_loss/perplexity = 3.91796565/50.2980156 secs/batch = 0.1987s, grad.norm=12.17843437
 31691: 23 [ 1170/ 1327], train_loss/perplexity = 3.75322270/42.6583366 secs/batch = 0.1987s, grad.norm=12.09078979
 31696: 23 [ 1175/ 1327], train_loss/perplexity = 3.54598761/34.6739120 secs/batch = 0.1989s, grad.norm=12.31911087
 31701: 23 [ 1180/ 1327], train_loss/perplexity = 3.59240651/36.3213768 secs/batch = 0.1948s, grad.norm=12.42598343
 31706: 23 [ 1185/ 1327], train_loss/perplexity = 3.76305866/43.0799904 secs/batch = 0.1986s, grad.norm=12.62065029
 31711: 23 [ 1190/ 1327], train_loss/perplexity = 3.83279467/46.1914482 secs/batch = 0.1965s, grad.norm=12.57536602
 31716: 23 [ 1195/ 1327], train_loss/perplexity = 3.68723869/39.9344215 secs/batch = 0.1940s, grad.norm=12.41155338
 31721: 23 [ 1200/ 1327], train_loss/perplexity = 3.60764408/36.8790665 secs/batch = 0.1985s, grad.norm=12.47441578
 31726: 23 [ 1205/ 1327], train_loss/perplexity = 3.72303820/41.3899536 secs/batch = 0.1970s, grad.norm=12.60258007
 31731: 23 [ 1210/ 1327], train_loss/perplexity = 3.37792730/29.3099575 secs/batch = 0.1993s, grad.norm=12.54031277
 31736: 23 [ 1215/ 1327], train_loss/perplexity = 3.54352975/34.5887947 secs/batch = 0.1994s, grad.norm=11.84196663
 31741: 23 [ 1220/ 1327], train_loss/perplexity = 3.72176743/41.3373909 secs/batch = 0.1994s, grad.norm=13.01228142
 31746: 23 [ 1225/ 1327], train_loss/perplexity = 3.43745422/31.1076641 secs/batch = 0.2003s, grad.norm=12.82937050
 31751: 23 [ 1230/ 1327], train_loss/perplexity = 3.73188448/41.7577248 secs/batch = 0.1981s, grad.norm=12.42622662
 31756: 23 [ 1235/ 1327], train_loss/perplexity = 3.67548227/39.4676857 secs/batch = 0.1981s, grad.norm=12.14484215
 31761: 23 [ 1240/ 1327], train_loss/perplexity = 3.92198658/50.5006676 secs/batch = 0.1989s, grad.norm=13.02192688
 31766: 23 [ 1245/ 1327], train_loss/perplexity = 3.83518887/46.3021736 secs/batch = 0.1986s, grad.norm=12.46791458
 31771: 23 [ 1250/ 1327], train_loss/perplexity = 3.91499496/50.1488190 secs/batch = 0.1981s, grad.norm=12.07557869
 31776: 23 [ 1255/ 1327], train_loss/perplexity = 3.91068006/49.9328995 secs/batch = 0.1988s, grad.norm=12.37335777
 31781: 23 [ 1260/ 1327], train_loss/perplexity = 3.70070982/40.4760246 secs/batch = 0.1995s, grad.norm=12.75492573
 31786: 23 [ 1265/ 1327], train_loss/perplexity = 3.89331031/49.0730667 secs/batch = 0.1980s, grad.norm=12.81332874
 31791: 23 [ 1270/ 1327], train_loss/perplexity = 3.67556667/39.4710159 secs/batch = 0.1971s, grad.norm=12.62577629
 31796: 23 [ 1275/ 1327], train_loss/perplexity = 3.79416656/44.4411812 secs/batch = 0.1982s, grad.norm=12.61496353
 31801: 23 [ 1280/ 1327], train_loss/perplexity = 3.72907734/41.6406708 secs/batch = 0.1982s, grad.norm=12.74885273
 31806: 23 [ 1285/ 1327], train_loss/perplexity = 3.54142618/34.5161095 secs/batch = 0.1990s, grad.norm=12.75817776
 31811: 23 [ 1290/ 1327], train_loss/perplexity = 3.90031195/49.4178619 secs/batch = 0.1988s, grad.norm=12.34988689
 31816: 23 [ 1295/ 1327], train_loss/perplexity = 3.82798266/45.9697075 secs/batch = 0.1991s, grad.norm=12.32604218
 31821: 23 [ 1300/ 1327], train_loss/perplexity = 3.97320724/53.1547394 secs/batch = 0.1945s, grad.norm=11.81030273
 31826: 23 [ 1305/ 1327], train_loss/perplexity = 4.09516287/60.0491180 secs/batch = 0.1997s, grad.norm=13.34043312
 31831: 23 [ 1310/ 1327], train_loss/perplexity = 4.33768463/76.5301361 secs/batch = 0.1996s, grad.norm=13.42232990
 31836: 23 [ 1315/ 1327], train_loss/perplexity = 4.10422277/60.5956306 secs/batch = 0.1988s, grad.norm=12.97104645
 31841: 23 [ 1320/ 1327], train_loss/perplexity = 4.04926443/57.3552513 secs/batch = 0.1990s, grad.norm=12.19953823
 31846: 23 [ 1325/ 1327], train_loss/perplexity = 4.07741117/58.9925499 secs/batch = 0.1989s, grad.norm=12.70468521
Epoch training time: 263.1197397708893
	> validation loss = 4.71847105, perplexity = 111.99688721
	> validation loss = 4.63023472, perplexity = 102.53813171
	> validation loss = 4.57965708, perplexity = 97.48095703
	> validation loss = 4.60478020, perplexity = 99.96100616
	> validation loss = 4.80608177, perplexity = 122.25167084
	> validation loss = 4.63791084, perplexity = 103.32825470
	> validation loss = 4.68364859, perplexity = 108.16400146
	> validation loss = 4.47164059, perplexity = 87.50016022
	> validation loss = 4.29493046, perplexity = 73.32711792
	> validation loss = 4.37946177, perplexity = 79.79507446
	> validation loss = 4.53888464, perplexity = 93.58635712
	> validation loss = 4.61113358, perplexity = 100.59812164
	> validation loss = 4.52923632, perplexity = 92.68775177
	> validation loss = 4.31661034, perplexity = 74.93419647
	> validation loss = 4.26034975, perplexity = 70.83475494
	> validation loss = 4.25107479, perplexity = 70.18080139
	> validation loss = 4.72256088, perplexity = 112.45587158
	> validation loss = 4.24243736, perplexity = 69.57723236
	> validation loss = 4.75100374, perplexity = 115.70036316
	> validation loss = 4.61023903, perplexity = 100.50817108
	> validation loss = 4.41586494, perplexity = 82.75338745
at the end of epoch: 23
train loss = 3.96240080, perplexity = 52.58341655
validation loss = 4.53245566, perplexity = 92.98662448
Saved model cv/epoch023_4.5325.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.00195312
new learning rate is: 0.0009765625
 31853: 24 [    5/ 1327], train_loss/perplexity = 4.22072268/68.0826645 secs/batch = 0.1991s, grad.norm=12.97419262
 31858: 24 [   10/ 1327], train_loss/perplexity = 3.73509312/41.8919258 secs/batch = 0.1990s, grad.norm=12.37381935
 31863: 24 [   15/ 1327], train_loss/perplexity = 4.14190388/62.9225044 secs/batch = 0.1983s, grad.norm=12.26855087
 31868: 24 [   20/ 1327], train_loss/perplexity = 4.29806614/73.5574036 secs/batch = 0.1989s, grad.norm=12.23670197
 31873: 24 [   25/ 1327], train_loss/perplexity = 4.19155407/66.1254730 secs/batch = 0.1995s, grad.norm=13.11192989
 31878: 24 [   30/ 1327], train_loss/perplexity = 4.15545082/63.7807121 secs/batch = 0.1992s, grad.norm=12.79041100
 31883: 24 [   35/ 1327], train_loss/perplexity = 3.97836280/53.4294891 secs/batch = 0.1985s, grad.norm=12.77466393
 31888: 24 [   40/ 1327], train_loss/perplexity = 3.99854946/54.5190125 secs/batch = 0.1991s, grad.norm=13.02673149
 31893: 24 [   45/ 1327], train_loss/perplexity = 3.72280145/41.3801575 secs/batch = 0.1990s, grad.norm=12.12196064
 31898: 24 [   50/ 1327], train_loss/perplexity = 4.04848337/57.3104706 secs/batch = 0.2000s, grad.norm=12.88507366
 31903: 24 [   55/ 1327], train_loss/perplexity = 3.98130369/53.5868492 secs/batch = 0.1983s, grad.norm=13.18651962
 31908: 24 [   60/ 1327], train_loss/perplexity = 4.23903370/69.3408127 secs/batch = 0.1923s, grad.norm=13.51936722
 31913: 24 [   65/ 1327], train_loss/perplexity = 3.76361895/43.1041374 secs/batch = 0.1931s, grad.norm=12.58276081
 31918: 24 [   70/ 1327], train_loss/perplexity = 3.60335016/36.7210503 secs/batch = 0.1996s, grad.norm=12.33987427
 31923: 24 [   75/ 1327], train_loss/perplexity = 3.44504857/31.3448067 secs/batch = 0.1993s, grad.norm=11.81891537
 31928: 24 [   80/ 1327], train_loss/perplexity = 3.90735483/49.7671356 secs/batch = 0.1982s, grad.norm=12.81467342
 31933: 24 [   85/ 1327], train_loss/perplexity = 3.94238615/51.5414391 secs/batch = 0.1995s, grad.norm=13.67164707
 31938: 24 [   90/ 1327], train_loss/perplexity = 3.99758172/54.4662743 secs/batch = 0.1983s, grad.norm=12.64975452
 31943: 24 [   95/ 1327], train_loss/perplexity = 3.86031199/47.4801636 secs/batch = 0.1990s, grad.norm=12.69329929
 31948: 24 [  100/ 1327], train_loss/perplexity = 4.14254284/62.9627228 secs/batch = 0.1942s, grad.norm=13.36303425
 31953: 24 [  105/ 1327], train_loss/perplexity = 3.93843317/51.3381004 secs/batch = 0.1998s, grad.norm=13.23421955
 31958: 24 [  110/ 1327], train_loss/perplexity = 3.85154891/47.0659065 secs/batch = 0.1927s, grad.norm=12.91908360
 31963: 24 [  115/ 1327], train_loss/perplexity = 3.86825252/47.8586807 secs/batch = 0.1990s, grad.norm=13.13146973
 31968: 24 [  120/ 1327], train_loss/perplexity = 3.87630486/48.2456131 secs/batch = 0.1984s, grad.norm=13.05614281
 31973: 24 [  125/ 1327], train_loss/perplexity = 4.00201988/54.7085419 secs/batch = 0.1991s, grad.norm=13.11531830
 31978: 24 [  130/ 1327], train_loss/perplexity = 3.90173411/49.4881935 secs/batch = 0.1946s, grad.norm=13.20004845
 31983: 24 [  135/ 1327], train_loss/perplexity = 3.83585072/46.3328285 secs/batch = 0.1935s, grad.norm=13.02617741
 31988: 24 [  140/ 1327], train_loss/perplexity = 4.22537708/68.4002914 secs/batch = 0.1989s, grad.norm=12.99503422
 31993: 24 [  145/ 1327], train_loss/perplexity = 4.11047554/60.9757080 secs/batch = 0.1940s, grad.norm=13.56677914
 31998: 24 [  150/ 1327], train_loss/perplexity = 4.14591885/63.1756439 secs/batch = 0.1979s, grad.norm=13.09686470
 32003: 24 [  155/ 1327], train_loss/perplexity = 4.32672882/75.6962662 secs/batch = 0.1986s, grad.norm=12.93281651
 32008: 24 [  160/ 1327], train_loss/perplexity = 3.98500085/53.7853355 secs/batch = 0.1915s, grad.norm=12.09000206
 32013: 24 [  165/ 1327], train_loss/perplexity = 4.19462442/66.3288116 secs/batch = 0.1936s, grad.norm=13.03946686
 32018: 24 [  170/ 1327], train_loss/perplexity = 3.90911937/49.8550301 secs/batch = 0.1990s, grad.norm=12.64975739
 32023: 24 [  175/ 1327], train_loss/perplexity = 4.28610182/72.6825867 secs/batch = 0.1986s, grad.norm=13.24466991
 32028: 24 [  180/ 1327], train_loss/perplexity = 4.04395866/57.0517464 secs/batch = 0.1985s, grad.norm=12.40617847
 32033: 24 [  185/ 1327], train_loss/perplexity = 4.33624649/76.4201584 secs/batch = 0.1985s, grad.norm=12.89231777
 32038: 24 [  190/ 1327], train_loss/perplexity = 3.95780158/52.3421288 secs/batch = 0.1990s, grad.norm=12.07258511
 32043: 24 [  195/ 1327], train_loss/perplexity = 4.20971632/67.3374329 secs/batch = 0.1955s, grad.norm=12.87709427
 32048: 24 [  200/ 1327], train_loss/perplexity = 4.10621357/60.7163849 secs/batch = 0.1987s, grad.norm=12.83619404
 32053: 24 [  205/ 1327], train_loss/perplexity = 4.34129000/76.8065567 secs/batch = 0.1995s, grad.norm=13.09791565
 32058: 24 [  210/ 1327], train_loss/perplexity = 4.11907482/61.5023155 secs/batch = 0.1992s, grad.norm=12.00802231
 32063: 24 [  215/ 1327], train_loss/perplexity = 4.18318558/65.5744095 secs/batch = 0.1988s, grad.norm=12.49440384
 32068: 24 [  220/ 1327], train_loss/perplexity = 4.14396763/63.0524940 secs/batch = 0.1981s, grad.norm=12.36862946
 32073: 24 [  225/ 1327], train_loss/perplexity = 4.34265947/76.9118118 secs/batch = 0.1991s, grad.norm=12.71242428
 32078: 24 [  230/ 1327], train_loss/perplexity = 4.23045635/68.7485962 secs/batch = 0.1940s, grad.norm=13.65495872
 32083: 24 [  235/ 1327], train_loss/perplexity = 4.06578255/58.3105202 secs/batch = 0.1997s, grad.norm=12.85442352
 32088: 24 [  240/ 1327], train_loss/perplexity = 3.81334257/45.3016090 secs/batch = 0.1985s, grad.norm=13.09580040
 32093: 24 [  245/ 1327], train_loss/perplexity = 4.08851528/59.6512604 secs/batch = 0.1999s, grad.norm=12.61858749
 32098: 24 [  250/ 1327], train_loss/perplexity = 3.93000889/50.9074287 secs/batch = 0.1996s, grad.norm=12.32321739
 32103: 24 [  255/ 1327], train_loss/perplexity = 3.97124863/53.0507317 secs/batch = 0.1999s, grad.norm=12.45976162
 32108: 24 [  260/ 1327], train_loss/perplexity = 4.18496895/65.6914597 secs/batch = 0.1952s, grad.norm=13.02923203
 32113: 24 [  265/ 1327], train_loss/perplexity = 4.32042933/75.2209167 secs/batch = 0.1993s, grad.norm=12.58478451
 32118: 24 [  270/ 1327], train_loss/perplexity = 4.42722416/83.6987610 secs/batch = 0.1996s, grad.norm=12.78522015
 32123: 24 [  275/ 1327], train_loss/perplexity = 4.29059792/73.0101089 secs/batch = 0.1959s, grad.norm=12.38276768
 32128: 24 [  280/ 1327], train_loss/perplexity = 4.12811089/62.0605736 secs/batch = 0.1992s, grad.norm=12.43765926
 32133: 24 [  285/ 1327], train_loss/perplexity = 4.42442513/83.4648132 secs/batch = 0.1994s, grad.norm=12.46986580
 32138: 24 [  290/ 1327], train_loss/perplexity = 4.08841181/59.6450882 secs/batch = 0.1998s, grad.norm=12.42602158
 32143: 24 [  295/ 1327], train_loss/perplexity = 3.87343001/48.1071129 secs/batch = 0.2003s, grad.norm=12.22267914
 32148: 24 [  300/ 1327], train_loss/perplexity = 3.48734021/32.6988602 secs/batch = 0.1944s, grad.norm=11.73163128
 32153: 24 [  305/ 1327], train_loss/perplexity = 3.91700482/50.2497139 secs/batch = 0.1987s, grad.norm=12.41042709
 32158: 24 [  310/ 1327], train_loss/perplexity = 3.98760748/53.9257164 secs/batch = 0.1995s, grad.norm=12.74391079
 32163: 24 [  315/ 1327], train_loss/perplexity = 3.47616553/32.3354950 secs/batch = 0.1998s, grad.norm=11.84656048
 32168: 24 [  320/ 1327], train_loss/perplexity = 3.52356482/33.9050789 secs/batch = 0.1992s, grad.norm=12.96540070
 32173: 24 [  325/ 1327], train_loss/perplexity = 3.45125294/31.5398846 secs/batch = 0.1940s, grad.norm=11.69128799
 32178: 24 [  330/ 1327], train_loss/perplexity = 4.04943657/57.3651276 secs/batch = 0.1978s, grad.norm=12.63637066
 32183: 24 [  335/ 1327], train_loss/perplexity = 3.45057154/31.5184002 secs/batch = 0.1995s, grad.norm=11.68028736
 32188: 24 [  340/ 1327], train_loss/perplexity = 4.22762632/68.5543137 secs/batch = 0.1993s, grad.norm=12.46537018
 32193: 24 [  345/ 1327], train_loss/perplexity = 4.01636362/55.4989243 secs/batch = 0.1991s, grad.norm=12.10999298
 32198: 24 [  350/ 1327], train_loss/perplexity = 4.02830172/56.1654434 secs/batch = 0.1955s, grad.norm=13.06292248
 32203: 24 [  355/ 1327], train_loss/perplexity = 4.01263952/55.2926254 secs/batch = 0.1996s, grad.norm=12.39201260
 32208: 24 [  360/ 1327], train_loss/perplexity = 4.09862804/60.2575607 secs/batch = 0.1997s, grad.norm=13.49743557
 32213: 24 [  365/ 1327], train_loss/perplexity = 4.11856461/61.4709435 secs/batch = 0.1993s, grad.norm=12.80786419
 32218: 24 [  370/ 1327], train_loss/perplexity = 4.18990326/66.0164032 secs/batch = 0.2000s, grad.norm=12.33860111
 32223: 24 [  375/ 1327], train_loss/perplexity = 3.59985757/36.5930214 secs/batch = 0.1997s, grad.norm=12.44562912
 32228: 24 [  380/ 1327], train_loss/perplexity = 3.69726276/40.3367424 secs/batch = 0.1997s, grad.norm=12.92745495
 32233: 24 [  385/ 1327], train_loss/perplexity = 3.89391255/49.1026268 secs/batch = 0.1965s, grad.norm=12.91029167
 32238: 24 [  390/ 1327], train_loss/perplexity = 3.99614000/54.3878059 secs/batch = 0.1993s, grad.norm=12.49305630
 32243: 24 [  395/ 1327], train_loss/perplexity = 4.07223797/58.6881561 secs/batch = 0.2001s, grad.norm=12.28704357
 32248: 24 [  400/ 1327], train_loss/perplexity = 4.03636551/56.6201820 secs/batch = 0.1993s, grad.norm=12.58785248
 32253: 24 [  405/ 1327], train_loss/perplexity = 4.20674086/67.1373749 secs/batch = 0.1990s, grad.norm=12.28662872
 32258: 24 [  410/ 1327], train_loss/perplexity = 3.95826554/52.3664207 secs/batch = 0.1951s, grad.norm=12.68066311
 32263: 24 [  415/ 1327], train_loss/perplexity = 3.89162874/48.9906158 secs/batch = 0.1996s, grad.norm=12.79149818
 32268: 24 [  420/ 1327], train_loss/perplexity = 3.55720592/35.0650864 secs/batch = 0.1973s, grad.norm=12.16100502
 32273: 24 [  425/ 1327], train_loss/perplexity = 3.88167953/48.5056152 secs/batch = 0.1941s, grad.norm=13.79206944
 32278: 24 [  430/ 1327], train_loss/perplexity = 3.99596071/54.3780556 secs/batch = 0.2005s, grad.norm=12.75278187
 32283: 24 [  435/ 1327], train_loss/perplexity = 4.07389545/58.7855148 secs/batch = 0.1986s, grad.norm=13.24542904
 32288: 24 [  440/ 1327], train_loss/perplexity = 3.64999962/38.4746513 secs/batch = 0.1950s, grad.norm=13.18092060
 32293: 24 [  445/ 1327], train_loss/perplexity = 4.02112818/55.7639809 secs/batch = 0.1984s, grad.norm=12.73961449
 32298: 24 [  450/ 1327], train_loss/perplexity = 3.96369743/52.6516418 secs/batch = 0.1990s, grad.norm=12.74209213
 32303: 24 [  455/ 1327], train_loss/perplexity = 3.92079711/50.4406357 secs/batch = 0.1992s, grad.norm=12.57654190
 32308: 24 [  460/ 1327], train_loss/perplexity = 3.82304025/45.7430687 secs/batch = 0.1949s, grad.norm=13.05692577
 32313: 24 [  465/ 1327], train_loss/perplexity = 3.65329480/38.6016426 secs/batch = 0.1995s, grad.norm=13.56007671
 32318: 24 [  470/ 1327], train_loss/perplexity = 4.39726210/81.2281723 secs/batch = 0.1988s, grad.norm=12.48069859
 32323: 24 [  475/ 1327], train_loss/perplexity = 3.75164294/42.5909996 secs/batch = 0.1995s, grad.norm=12.53615379
 32328: 24 [  480/ 1327], train_loss/perplexity = 3.88976002/48.8991508 secs/batch = 0.1995s, grad.norm=12.91962528
 32333: 24 [  485/ 1327], train_loss/perplexity = 3.85582352/47.2675247 secs/batch = 0.1987s, grad.norm=12.99469662
 32338: 24 [  490/ 1327], train_loss/perplexity = 3.80982590/45.1425781 secs/batch = 0.1992s, grad.norm=13.48870564
 32343: 24 [  495/ 1327], train_loss/perplexity = 3.85911083/47.4231644 secs/batch = 0.2000s, grad.norm=12.55310631
 32348: 24 [  500/ 1327], train_loss/perplexity = 3.99849248/54.5159035 secs/batch = 0.1989s, grad.norm=13.04589939
 32353: 24 [  505/ 1327], train_loss/perplexity = 4.08795214/59.6176796 secs/batch = 0.1998s, grad.norm=11.92625523
 32358: 24 [  510/ 1327], train_loss/perplexity = 4.43997288/84.7726440 secs/batch = 0.1993s, grad.norm=12.49186039
 32363: 24 [  515/ 1327], train_loss/perplexity = 4.14924765/63.3862953 secs/batch = 0.1986s, grad.norm=12.39253616
 32368: 24 [  520/ 1327], train_loss/perplexity = 4.19591475/66.4144592 secs/batch = 0.1941s, grad.norm=12.56264210
 32373: 24 [  525/ 1327], train_loss/perplexity = 3.86859131/47.8748970 secs/batch = 0.1983s, grad.norm=12.73035240
 32378: 24 [  530/ 1327], train_loss/perplexity = 3.88947821/48.8853722 secs/batch = 0.2006s, grad.norm=13.17148876
 32383: 24 [  535/ 1327], train_loss/perplexity = 4.08493614/59.4381409 secs/batch = 0.1997s, grad.norm=13.32101154
 32388: 24 [  540/ 1327], train_loss/perplexity = 4.08293152/59.3191109 secs/batch = 0.1991s, grad.norm=12.77263260
 32393: 24 [  545/ 1327], train_loss/perplexity = 4.08264303/59.3019981 secs/batch = 0.1998s, grad.norm=12.68528175
 32398: 24 [  550/ 1327], train_loss/perplexity = 4.02649307/56.0639534 secs/batch = 0.2004s, grad.norm=12.32875824
 32403: 24 [  555/ 1327], train_loss/perplexity = 3.96887612/52.9250145 secs/batch = 0.1999s, grad.norm=12.34110355
 32408: 24 [  560/ 1327], train_loss/perplexity = 4.00094128/54.6495667 secs/batch = 0.1994s, grad.norm=13.07164478
 32413: 24 [  565/ 1327], train_loss/perplexity = 3.85451365/47.2056541 secs/batch = 0.2002s, grad.norm=12.91621590
 32418: 24 [  570/ 1327], train_loss/perplexity = 3.91382766/50.0903130 secs/batch = 0.1993s, grad.norm=13.28118038
 32423: 24 [  575/ 1327], train_loss/perplexity = 3.61534786/37.1642723 secs/batch = 0.2003s, grad.norm=12.71040058
 32428: 24 [  580/ 1327], train_loss/perplexity = 4.11466694/61.2318153 secs/batch = 0.2006s, grad.norm=12.81550121
 32433: 24 [  585/ 1327], train_loss/perplexity = 3.70546961/40.6691399 secs/batch = 0.2004s, grad.norm=12.98656368
 32438: 24 [  590/ 1327], train_loss/perplexity = 4.08699608/59.5607071 secs/batch = 0.2006s, grad.norm=12.39676571
 32443: 24 [  595/ 1327], train_loss/perplexity = 4.05815935/57.8676987 secs/batch = 0.1996s, grad.norm=13.12345505
 32448: 24 [  600/ 1327], train_loss/perplexity = 4.13984919/62.7933502 secs/batch = 0.2014s, grad.norm=12.11063004
 32453: 24 [  605/ 1327], train_loss/perplexity = 4.11062765/60.9849815 secs/batch = 0.1960s, grad.norm=12.14992619
 32458: 24 [  610/ 1327], train_loss/perplexity = 4.29951382/73.6639709 secs/batch = 0.1997s, grad.norm=12.91458702
 32463: 24 [  615/ 1327], train_loss/perplexity = 3.92058301/50.4298363 secs/batch = 0.1995s, grad.norm=12.29807472
 32468: 24 [  620/ 1327], train_loss/perplexity = 4.24602175/69.8270721 secs/batch = 0.2000s, grad.norm=12.87547684
 32473: 24 [  625/ 1327], train_loss/perplexity = 4.17170525/64.8259048 secs/batch = 0.1989s, grad.norm=12.29397392
 32478: 24 [  630/ 1327], train_loss/perplexity = 4.29525089/73.3506165 secs/batch = 0.1983s, grad.norm=12.72059727
 32483: 24 [  635/ 1327], train_loss/perplexity = 3.97461939/53.2298546 secs/batch = 0.1986s, grad.norm=12.35003948
 32488: 24 [  640/ 1327], train_loss/perplexity = 3.99128103/54.1241798 secs/batch = 0.1995s, grad.norm=12.62171555
 32493: 24 [  645/ 1327], train_loss/perplexity = 4.31863022/75.0857086 secs/batch = 0.1988s, grad.norm=12.87477875
 32498: 24 [  650/ 1327], train_loss/perplexity = 3.74576259/42.3412857 secs/batch = 0.1979s, grad.norm=13.04579353
 32503: 24 [  655/ 1327], train_loss/perplexity = 3.89416599/49.1150742 secs/batch = 0.1998s, grad.norm=12.98611259
 32508: 24 [  660/ 1327], train_loss/perplexity = 3.82062626/45.6327782 secs/batch = 0.1991s, grad.norm=12.81676388
 32513: 24 [  665/ 1327], train_loss/perplexity = 3.95382714/52.1345100 secs/batch = 0.2013s, grad.norm=12.70130253
 32518: 24 [  670/ 1327], train_loss/perplexity = 3.94177914/51.5101624 secs/batch = 0.1994s, grad.norm=13.02895927
 32523: 24 [  675/ 1327], train_loss/perplexity = 3.79846454/44.6325989 secs/batch = 0.1990s, grad.norm=13.11881351
 32528: 24 [  680/ 1327], train_loss/perplexity = 3.94296265/51.5711632 secs/batch = 0.1987s, grad.norm=12.84502029
 32533: 24 [  685/ 1327], train_loss/perplexity = 3.71455789/41.0404396 secs/batch = 0.1998s, grad.norm=12.24056625
 32538: 24 [  690/ 1327], train_loss/perplexity = 4.18317890/65.5739746 secs/batch = 0.1999s, grad.norm=12.18103123
 32543: 24 [  695/ 1327], train_loss/perplexity = 3.99017477/54.0643387 secs/batch = 0.1999s, grad.norm=12.50807476
 32548: 24 [  700/ 1327], train_loss/perplexity = 4.19230223/66.1749649 secs/batch = 0.1965s, grad.norm=12.88297749
 32553: 24 [  705/ 1327], train_loss/perplexity = 3.90683341/49.7411919 secs/batch = 0.1994s, grad.norm=12.68033409
 32558: 24 [  710/ 1327], train_loss/perplexity = 3.84070277/46.5581818 secs/batch = 0.1988s, grad.norm=12.67284679
 32563: 24 [  715/ 1327], train_loss/perplexity = 3.77564955/43.6258354 secs/batch = 0.1995s, grad.norm=12.44839096
 32568: 24 [  720/ 1327], train_loss/perplexity = 3.77166557/43.4523773 secs/batch = 0.2002s, grad.norm=12.63420105
 32573: 24 [  725/ 1327], train_loss/perplexity = 3.76274061/43.0662918 secs/batch = 0.1987s, grad.norm=12.41909027
 32578: 24 [  730/ 1327], train_loss/perplexity = 3.98024559/53.5301781 secs/batch = 0.2001s, grad.norm=12.81507301
 32583: 24 [  735/ 1327], train_loss/perplexity = 4.06640291/58.3467064 secs/batch = 0.1993s, grad.norm=13.06350708
 32588: 24 [  740/ 1327], train_loss/perplexity = 3.48089385/32.4887505 secs/batch = 0.1968s, grad.norm=11.69398022
 32593: 24 [  745/ 1327], train_loss/perplexity = 4.09884596/60.2706909 secs/batch = 0.2000s, grad.norm=12.67935753
 32598: 24 [  750/ 1327], train_loss/perplexity = 3.82049012/45.6265640 secs/batch = 0.1998s, grad.norm=12.52989197
 32603: 24 [  755/ 1327], train_loss/perplexity = 3.77859402/43.7544823 secs/batch = 0.1994s, grad.norm=12.48878670
 32608: 24 [  760/ 1327], train_loss/perplexity = 3.56912971/35.4856949 secs/batch = 0.2001s, grad.norm=11.88750458
 32613: 24 [  765/ 1327], train_loss/perplexity = 3.70954132/40.8350716 secs/batch = 0.1989s, grad.norm=11.82958984
 32618: 24 [  770/ 1327], train_loss/perplexity = 3.62871122/37.6642456 secs/batch = 0.1994s, grad.norm=12.74384308
 32623: 24 [  775/ 1327], train_loss/perplexity = 3.72493601/41.4685783 secs/batch = 0.2010s, grad.norm=12.71624088
 32628: 24 [  780/ 1327], train_loss/perplexity = 4.05700588/57.8009872 secs/batch = 0.1999s, grad.norm=13.08305264
 32633: 24 [  785/ 1327], train_loss/perplexity = 4.00508070/54.8762512 secs/batch = 0.1991s, grad.norm=12.96445560
 32638: 24 [  790/ 1327], train_loss/perplexity = 3.73643494/41.9481735 secs/batch = 0.2000s, grad.norm=12.48426819
 32643: 24 [  795/ 1327], train_loss/perplexity = 4.13227463/62.3195152 secs/batch = 0.1989s, grad.norm=13.71582890
 32648: 24 [  800/ 1327], train_loss/perplexity = 3.99657297/54.4113617 secs/batch = 0.1995s, grad.norm=13.05310059
 32653: 24 [  805/ 1327], train_loss/perplexity = 4.38450718/80.1986923 secs/batch = 0.1991s, grad.norm=13.01821899
 32658: 24 [  810/ 1327], train_loss/perplexity = 3.87792730/48.3239517 secs/batch = 0.2001s, grad.norm=11.78129768
 32663: 24 [  815/ 1327], train_loss/perplexity = 3.80185413/44.7841415 secs/batch = 0.1937s, grad.norm=12.24863815
 32668: 24 [  820/ 1327], train_loss/perplexity = 3.70142889/40.5051422 secs/batch = 0.2000s, grad.norm=11.91109562
 32673: 24 [  825/ 1327], train_loss/perplexity = 3.87239122/48.0571632 secs/batch = 0.2001s, grad.norm=12.34014320
 32678: 24 [  830/ 1327], train_loss/perplexity = 3.59738636/36.5027046 secs/batch = 0.1992s, grad.norm=12.37301922
 32683: 24 [  835/ 1327], train_loss/perplexity = 3.93501282/51.1628075 secs/batch = 0.1998s, grad.norm=13.41590023
 32688: 24 [  840/ 1327], train_loss/perplexity = 3.98540735/53.8072052 secs/batch = 0.1954s, grad.norm=12.44952202
 32693: 24 [  845/ 1327], train_loss/perplexity = 3.89648771/49.2292366 secs/batch = 0.1991s, grad.norm=12.79640293
 32698: 24 [  850/ 1327], train_loss/perplexity = 3.85954690/47.4438515 secs/batch = 0.1992s, grad.norm=12.20059872
 32703: 24 [  855/ 1327], train_loss/perplexity = 3.85450172/47.2050896 secs/batch = 0.2012s, grad.norm=12.92871952
 32708: 24 [  860/ 1327], train_loss/perplexity = 3.59046412/36.2508965 secs/batch = 0.1999s, grad.norm=12.12520218
 32713: 24 [  865/ 1327], train_loss/perplexity = 4.04638863/57.1905479 secs/batch = 0.1921s, grad.norm=12.76192474
 32718: 24 [  870/ 1327], train_loss/perplexity = 3.89374733/49.0945168 secs/batch = 0.1996s, grad.norm=12.55550385
 32723: 24 [  875/ 1327], train_loss/perplexity = 3.59506512/36.4180717 secs/batch = 0.1939s, grad.norm=12.35713291
 32728: 24 [  880/ 1327], train_loss/perplexity = 3.83994889/46.5230980 secs/batch = 0.1997s, grad.norm=12.26493549
 32733: 24 [  885/ 1327], train_loss/perplexity = 4.01277399/55.3000603 secs/batch = 0.1990s, grad.norm=12.65752506
 32738: 24 [  890/ 1327], train_loss/perplexity = 4.07138014/58.6378365 secs/batch = 0.2002s, grad.norm=12.48780060
 32743: 24 [  895/ 1327], train_loss/perplexity = 4.03037357/56.2819328 secs/batch = 0.2002s, grad.norm=12.42485905
 32748: 24 [  900/ 1327], train_loss/perplexity = 3.90160966/49.4820328 secs/batch = 0.2001s, grad.norm=12.07511330
 32753: 24 [  905/ 1327], train_loss/perplexity = 3.71272039/40.9650955 secs/batch = 0.2008s, grad.norm=11.60063457
 32758: 24 [  910/ 1327], train_loss/perplexity = 3.90028191/49.4163780 secs/batch = 0.1997s, grad.norm=11.69177246
 32763: 24 [  915/ 1327], train_loss/perplexity = 4.01586676/55.4713554 secs/batch = 0.1998s, grad.norm=12.36994171
 32768: 24 [  920/ 1327], train_loss/perplexity = 4.17176771/64.8299484 secs/batch = 0.1998s, grad.norm=12.25909996
 32773: 24 [  925/ 1327], train_loss/perplexity = 4.04149485/56.9113541 secs/batch = 0.1985s, grad.norm=12.85577488
 32778: 24 [  930/ 1327], train_loss/perplexity = 4.00896978/55.0900879 secs/batch = 0.1995s, grad.norm=12.18435860
 32783: 24 [  935/ 1327], train_loss/perplexity = 4.11568832/61.2943916 secs/batch = 0.1993s, grad.norm=12.43410301
 32788: 24 [  940/ 1327], train_loss/perplexity = 4.02604294/56.0387230 secs/batch = 0.1990s, grad.norm=12.23132801
 32793: 24 [  945/ 1327], train_loss/perplexity = 4.27309322/71.7432098 secs/batch = 0.1995s, grad.norm=12.45935059
 32798: 24 [  950/ 1327], train_loss/perplexity = 4.02312422/55.8754005 secs/batch = 0.1991s, grad.norm=12.81898594
 32803: 24 [  955/ 1327], train_loss/perplexity = 3.91009521/49.9037018 secs/batch = 0.2010s, grad.norm=12.43877602
 32808: 24 [  960/ 1327], train_loss/perplexity = 4.32151031/75.3022690 secs/batch = 0.1984s, grad.norm=12.86377811
 32813: 24 [  965/ 1327], train_loss/perplexity = 4.00031948/54.6155968 secs/batch = 0.2001s, grad.norm=12.74807930
 32818: 24 [  970/ 1327], train_loss/perplexity = 4.27807713/72.1016617 secs/batch = 0.1997s, grad.norm=12.27438736
 32823: 24 [  975/ 1327], train_loss/perplexity = 3.86937428/47.9123955 secs/batch = 0.1995s, grad.norm=13.20206451
 32828: 24 [  980/ 1327], train_loss/perplexity = 3.76154184/43.0146980 secs/batch = 0.1992s, grad.norm=12.27625465
 32833: 24 [  985/ 1327], train_loss/perplexity = 3.91720510/50.2597771 secs/batch = 0.1997s, grad.norm=12.85964298
 32838: 24 [  990/ 1327], train_loss/perplexity = 4.04299927/56.9970360 secs/batch = 0.2010s, grad.norm=12.90360832
 32843: 24 [  995/ 1327], train_loss/perplexity = 4.09907913/60.2847481 secs/batch = 0.2003s, grad.norm=12.63658524
 32848: 24 [ 1000/ 1327], train_loss/perplexity = 3.67415738/39.4154320 secs/batch = 0.2009s, grad.norm=12.06853771
 32853: 24 [ 1005/ 1327], train_loss/perplexity = 4.07209444/58.6797371 secs/batch = 0.1998s, grad.norm=12.63113213
 32858: 24 [ 1010/ 1327], train_loss/perplexity = 3.76034331/42.9631729 secs/batch = 0.2001s, grad.norm=11.79029465
 32863: 24 [ 1015/ 1327], train_loss/perplexity = 4.21990919/68.0273056 secs/batch = 0.1996s, grad.norm=12.52092361
 32868: 24 [ 1020/ 1327], train_loss/perplexity = 4.25929403/70.7600098 secs/batch = 0.2001s, grad.norm=12.58414936
 32873: 24 [ 1025/ 1327], train_loss/perplexity = 4.14008951/62.8084450 secs/batch = 0.1976s, grad.norm=12.28623581
 32878: 24 [ 1030/ 1327], train_loss/perplexity = 3.91479301/50.1386909 secs/batch = 0.1992s, grad.norm=12.19661236
 32883: 24 [ 1035/ 1327], train_loss/perplexity = 3.91705799/50.2523842 secs/batch = 0.1996s, grad.norm=12.57207394
 32888: 24 [ 1040/ 1327], train_loss/perplexity = 4.19278383/66.2068405 secs/batch = 0.2003s, grad.norm=13.10199833
 32893: 24 [ 1045/ 1327], train_loss/perplexity = 3.59926295/36.5712700 secs/batch = 0.1993s, grad.norm=12.06901646
 32898: 24 [ 1050/ 1327], train_loss/perplexity = 3.72291517/41.3848610 secs/batch = 0.1997s, grad.norm=12.05780983
 32903: 24 [ 1055/ 1327], train_loss/perplexity = 3.85581470/47.2671089 secs/batch = 0.2003s, grad.norm=13.14047623
 32908: 24 [ 1060/ 1327], train_loss/perplexity = 3.46512032/31.9803066 secs/batch = 0.1988s, grad.norm=12.90321445
 32913: 24 [ 1065/ 1327], train_loss/perplexity = 3.61004138/36.9675827 secs/batch = 0.2007s, grad.norm=12.70566559
 32918: 24 [ 1070/ 1327], train_loss/perplexity = 3.86994433/47.9397163 secs/batch = 0.2003s, grad.norm=13.03517628
 32923: 24 [ 1075/ 1327], train_loss/perplexity = 3.64621353/38.3292580 secs/batch = 0.2005s, grad.norm=12.81402206
 32928: 24 [ 1080/ 1327], train_loss/perplexity = 3.67134142/39.3045959 secs/batch = 0.1985s, grad.norm=12.61529636
 32933: 24 [ 1085/ 1327], train_loss/perplexity = 3.57233977/35.5997925 secs/batch = 0.1996s, grad.norm=12.58031273
 32938: 24 [ 1090/ 1327], train_loss/perplexity = 3.79870057/44.6431351 secs/batch = 0.1993s, grad.norm=13.12010193
 32943: 24 [ 1095/ 1327], train_loss/perplexity = 3.86796284/47.8448181 secs/batch = 0.1993s, grad.norm=12.87914181
 32948: 24 [ 1100/ 1327], train_loss/perplexity = 3.57448435/35.6762199 secs/batch = 0.1963s, grad.norm=13.48743248
 32953: 24 [ 1105/ 1327], train_loss/perplexity = 3.59252787/36.3257866 secs/batch = 0.1994s, grad.norm=12.26350784
 32958: 24 [ 1110/ 1327], train_loss/perplexity = 3.89345193/49.0800133 secs/batch = 0.1998s, grad.norm=12.75627232
 32963: 24 [ 1115/ 1327], train_loss/perplexity = 3.70423269/40.6188698 secs/batch = 0.1995s, grad.norm=11.98731041
 32968: 24 [ 1120/ 1327], train_loss/perplexity = 3.88377500/48.6073608 secs/batch = 0.1998s, grad.norm=11.95482635
 32973: 24 [ 1125/ 1327], train_loss/perplexity = 4.06029892/57.9916420 secs/batch = 0.2005s, grad.norm=12.94638634
 32978: 24 [ 1130/ 1327], train_loss/perplexity = 3.77297306/43.5092278 secs/batch = 0.1995s, grad.norm=12.40009785
 32983: 24 [ 1135/ 1327], train_loss/perplexity = 3.77699828/43.6847153 secs/batch = 0.1990s, grad.norm=12.46679211
 32988: 24 [ 1140/ 1327], train_loss/perplexity = 4.05006218/57.4010277 secs/batch = 0.1993s, grad.norm=12.97601414
 32993: 24 [ 1145/ 1327], train_loss/perplexity = 3.87205791/48.0411491 secs/batch = 0.1979s, grad.norm=12.22618771
 32998: 24 [ 1150/ 1327], train_loss/perplexity = 3.84182119/46.6102829 secs/batch = 0.1990s, grad.norm=12.54082584
 33003: 24 [ 1155/ 1327], train_loss/perplexity = 3.92169333/50.4858627 secs/batch = 0.1996s, grad.norm=12.84019375
 33008: 24 [ 1160/ 1327], train_loss/perplexity = 3.84280014/46.6559334 secs/batch = 0.1989s, grad.norm=12.34408665
 33013: 24 [ 1165/ 1327], train_loss/perplexity = 3.90078020/49.4410095 secs/batch = 0.1997s, grad.norm=12.52731609
 33018: 24 [ 1170/ 1327], train_loss/perplexity = 3.79363084/44.4173813 secs/batch = 0.1994s, grad.norm=12.56817627
 33023: 24 [ 1175/ 1327], train_loss/perplexity = 3.61424303/37.1232338 secs/batch = 0.1997s, grad.norm=12.52749443
 33028: 24 [ 1180/ 1327], train_loss/perplexity = 3.60851765/36.9112968 secs/batch = 0.1992s, grad.norm=12.93579578
 33033: 24 [ 1185/ 1327], train_loss/perplexity = 3.74108672/42.1437645 secs/batch = 0.1993s, grad.norm=12.62141895
 33038: 24 [ 1190/ 1327], train_loss/perplexity = 3.80960464/45.1325912 secs/batch = 0.1986s, grad.norm=12.42442989
 33043: 24 [ 1195/ 1327], train_loss/perplexity = 3.60148215/36.6525192 secs/batch = 0.1992s, grad.norm=12.30325222
 33048: 24 [ 1200/ 1327], train_loss/perplexity = 3.66022348/38.8700294 secs/batch = 0.1993s, grad.norm=12.66763115
 33053: 24 [ 1205/ 1327], train_loss/perplexity = 3.59799385/36.5248871 secs/batch = 0.1984s, grad.norm=12.44006920
 33058: 24 [ 1210/ 1327], train_loss/perplexity = 3.31024599/27.3918629 secs/batch = 0.2006s, grad.norm=11.97624683
 33063: 24 [ 1215/ 1327], train_loss/perplexity = 3.52898955/34.0895042 secs/batch = 0.1988s, grad.norm=11.63355350
 33068: 24 [ 1220/ 1327], train_loss/perplexity = 3.64843369/38.4144516 secs/batch = 0.2007s, grad.norm=12.70145893
 33073: 24 [ 1225/ 1327], train_loss/perplexity = 3.47418809/32.2716179 secs/batch = 0.1986s, grad.norm=12.83246613
 33078: 24 [ 1230/ 1327], train_loss/perplexity = 3.72268915/41.3755112 secs/batch = 0.2005s, grad.norm=12.03633499
 33083: 24 [ 1235/ 1327], train_loss/perplexity = 3.68478680/39.8366280 secs/batch = 0.1993s, grad.norm=12.04636288
 33088: 24 [ 1240/ 1327], train_loss/perplexity = 3.92606258/50.7069283 secs/batch = 0.2004s, grad.norm=12.67914963
 33093: 24 [ 1245/ 1327], train_loss/perplexity = 3.80992866/45.1472168 secs/batch = 0.2001s, grad.norm=12.40474796
 33098: 24 [ 1250/ 1327], train_loss/perplexity = 3.95010686/51.9409180 secs/batch = 0.2000s, grad.norm=12.12195492
 33103: 24 [ 1255/ 1327], train_loss/perplexity = 3.98367667/53.7141609 secs/batch = 0.2006s, grad.norm=12.37241268
 33108: 24 [ 1260/ 1327], train_loss/perplexity = 3.67505455/39.4508095 secs/batch = 0.2005s, grad.norm=12.71074963
 33113: 24 [ 1265/ 1327], train_loss/perplexity = 3.94304156/51.5752335 secs/batch = 0.1966s, grad.norm=13.08247185
 33118: 24 [ 1270/ 1327], train_loss/perplexity = 3.61351013/37.0960350 secs/batch = 0.1936s, grad.norm=12.48494530
 33123: 24 [ 1275/ 1327], train_loss/perplexity = 3.83324456/46.2122345 secs/batch = 0.1993s, grad.norm=12.90394592
 33128: 24 [ 1280/ 1327], train_loss/perplexity = 3.70722294/40.7405090 secs/batch = 0.1991s, grad.norm=13.18304539
 33133: 24 [ 1285/ 1327], train_loss/perplexity = 3.54104519/34.5029640 secs/batch = 0.1989s, grad.norm=12.56238079
 33138: 24 [ 1290/ 1327], train_loss/perplexity = 3.86597085/47.7496071 secs/batch = 0.1989s, grad.norm=12.16067028
 33143: 24 [ 1295/ 1327], train_loss/perplexity = 3.84534311/46.7747307 secs/batch = 0.1993s, grad.norm=12.47051334
 33148: 24 [ 1300/ 1327], train_loss/perplexity = 3.95181775/52.0298576 secs/batch = 0.1944s, grad.norm=11.74735260
 33153: 24 [ 1305/ 1327], train_loss/perplexity = 4.05319643/57.5812187 secs/batch = 0.2001s, grad.norm=12.73285961
 33158: 24 [ 1310/ 1327], train_loss/perplexity = 4.32966518/75.9188614 secs/batch = 0.1992s, grad.norm=12.82971382
 33163: 24 [ 1315/ 1327], train_loss/perplexity = 4.16850710/64.6189117 secs/batch = 0.2000s, grad.norm=12.81218529
 33168: 24 [ 1320/ 1327], train_loss/perplexity = 4.10667562/60.7444458 secs/batch = 0.1987s, grad.norm=12.40475082
 33173: 24 [ 1325/ 1327], train_loss/perplexity = 4.09332085/59.9386101 secs/batch = 0.1993s, grad.norm=12.70009041
Epoch training time: 264.1052465438843
	> validation loss = 4.71832037, perplexity = 111.98001099
	> validation loss = 4.63047934, perplexity = 102.56321716
	> validation loss = 4.57905388, perplexity = 97.42218018
	> validation loss = 4.60461998, perplexity = 99.94499207
	> validation loss = 4.80644464, perplexity = 122.29603577
	> validation loss = 4.63814020, perplexity = 103.35195160
	> validation loss = 4.68374920, perplexity = 108.17488098
	> validation loss = 4.47109032, perplexity = 87.45201874
	> validation loss = 4.29496098, perplexity = 73.32935333
	> validation loss = 4.37957954, perplexity = 79.80447388
	> validation loss = 4.53906059, perplexity = 93.60282898
	> validation loss = 4.61084414, perplexity = 100.56900787
	> validation loss = 4.52887678, perplexity = 92.65443420
	> validation loss = 4.31635952, perplexity = 74.91540527
	> validation loss = 4.26044273, perplexity = 70.84133911
	> validation loss = 4.25102520, perplexity = 70.17732239
	> validation loss = 4.72237492, perplexity = 112.43495941
	> validation loss = 4.24221039, perplexity = 69.56143951
	> validation loss = 4.75099945, perplexity = 115.69985962
	> validation loss = 4.61030054, perplexity = 100.51435089
	> validation loss = 4.41555882, perplexity = 82.72805786
at the end of epoch: 24
train loss = 3.96122912, perplexity = 52.52184201
validation loss = 4.53236041, perplexity = 92.97776757
Saved model cv/epoch024_4.5324.model
validation perplexity did not improve enough, decay learning rate
learning rate was: 0.000976562
new learning rate is: 0.00048828125
